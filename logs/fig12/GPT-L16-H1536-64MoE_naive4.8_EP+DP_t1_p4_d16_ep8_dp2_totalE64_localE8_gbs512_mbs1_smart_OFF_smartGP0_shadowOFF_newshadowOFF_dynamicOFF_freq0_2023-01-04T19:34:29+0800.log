using world size: 64, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 4 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 2
  expert_ep_size .................................. 8
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  gshard_cap ...................................... 4.8
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 8
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 4
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 102400
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 64
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 32
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 4
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0, 8]
[INFO] 8 in DP group [0, 8]
[INFO] 1 in DP group [1, 9]
[INFO] 9 in DP group [1, 9]
[INFO] 2 in DP group [2, 10]
[INFO] 10 in DP group [2, 10]
[INFO] 3 in DP group [3, 11]
[INFO] 11 in DP group [3, 11]
[INFO] 4 in DP group [4, 12]
[INFO] 12 in DP group [4, 12]
[INFO] 5 in DP group [5, 13]
[INFO] 13 in DP group [5, 13]
[INFO] 6 in DP group [6, 14]
[INFO] 14 in DP group [6, 14]
[INFO] 7 in DP group [7, 15]
[INFO] 15 in DP group [7, 15]
[INFO] 16 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 17 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 18 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 19 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 22 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 20 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 21 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 23 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 24 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 29 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 28 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 30 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 31 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 27 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 25 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 26 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 16 in DP group [16, 24]
[INFO] 24 in DP group [16, 24]
[INFO] 17 in DP group [17, 25]
[INFO] 25 in DP group [17, 25]
[INFO] 18 in DP group [18, 26]
[INFO] 26 in DP group [18, 26]
[INFO] 19 in DP group [19, 27]
[INFO] 27 in DP group [19, 27]
[INFO] 20 in DP group [20, 28]
[INFO] 28 in DP group [20, 28]
[INFO] 21 in DP group [21, 29]
[INFO] 29 in DP group [21, 29]
[INFO] 22 in DP group [22, 30]
[INFO] 30 in DP group [22, 30]
[INFO] 23 in DP group [23, 31]
[INFO] 31 in DP group [23, 31]
[INFO] 32 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 36 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 39 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 37 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 34 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 33 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 35 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 38 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 40 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 42 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 43 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 46 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 47 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 41 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 45 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 44 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 32 in DP group [32, 40]
[INFO] 40 in DP group [32, 40]
[INFO] 33 in DP group [33, 41]
[INFO] 41 in DP group [33, 41]
[INFO] 34 in DP group [34, 42]
[INFO] 42 in DP group [34, 42]
[INFO] 35 in DP group [35, 43]
[INFO] 43 in DP group [35, 43]
[INFO] 36 in DP group [36, 44]
[INFO] 44 in DP group [36, 44]
[INFO] 37 in DP group [37, 45]
[INFO] 45 in DP group [37, 45]
[INFO] 38 in DP group [38, 46]
[INFO] 46 in DP group [38, 46]
[INFO] 39 in DP group [39, 47]
[INFO] 47 in DP group [39, 47]
[INFO] 48 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 50 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 51 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 52 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 53 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 55 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 54 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 49 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 56 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 62 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 59 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 57 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 60 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 63 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 61 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 58 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 48 in DP group [48, 56]
[INFO] 56 in DP group [48, 56]
[INFO] 49 in DP group [49, 57]
[INFO] 57 in DP group [49, 57]
[INFO] 50 in DP group [50, 58]
[INFO] 58 in DP group [50, 58]
[INFO] 51 in DP group [51, 59]
[INFO] 59 in DP group [51, 59]
[INFO] 52 in DP group [52, 60]
[INFO] 60 in DP group [52, 60]
[INFO] 53 in DP group [53, 61]
[INFO] 61 in DP group [53, 61]
[INFO] 54 in DP group [54, 62]
[INFO] 62 in DP group [54, 62]
> setting random seeds to 1234 ...
[INFO] 55 in DP group [55, 63]
[INFO] 63 in DP group [55, 63]
[W ProcessGroupNCCL.cpp:1569] Rank 25 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 26 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 27 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 24 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 36 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 37 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 38 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 39 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 59 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 33 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 32 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 34 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 35 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 60 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[W ProcessGroupNCCL.cpp:1569] Rank 49 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 16 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 18 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 17 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 52 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 53 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 55 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 50 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 51 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 63 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 2 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 54 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 62 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 56 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 28 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 29 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 58 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 57 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 19 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 12 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 31 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> compiling dataset index builder ...
[W ProcessGroupNCCL.cpp:1569] Rank 13 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 15 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 14 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 43 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 3 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 30 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 41 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 40 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 61 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 42 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 48 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 5 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 6 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 4 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 7 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 10 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 9 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 8 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 11 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 22 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 23 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 20 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 21 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 45 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 44 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 46 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 47 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
make: 进入目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
make: 对“default”无需做任何事。
make: 离开目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
>>> done with dataset index builder. Compilation time: 0.233 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
>>> done with compiling and loading fused kernels. Compilation time: 12.268 seconds
time to initialize megatron (seconds): 120.762
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
[after megatron is initialized] datetime: 2023-01-04 19:37:28 
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
building GPT model ...
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 340328704
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 419168512
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 417598720
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 340328704
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-04 19:37:38 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: -512
    test:       -512
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.063115 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 1.450 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-04 19:37:53 
done with setup ...
training ...
[before the start of training step] datetime: 2023-01-04 19:37:53 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 8006.25 | max allocated: 8006.25244140625 | reserved: 9042.0 | max reserved: 9042.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 6510.73828125 | max allocated: 6510.74072265625 | reserved: 7246.0 | max reserved: 7246.0
[Rank 32] (after 1 iterations) memory (MB) | allocated: 6510.9091796875 | max allocated: 6510.91162109375 | reserved: 7034.0 | max reserved: 7034.0
[Rank 48] (after 1 iterations) memory (MB) | allocated: 7987.1396484375 | max allocated: 7987.17041015625 | reserved: 8200.0 | max reserved: 8200.0
 iteration        1/     200 | consumed samples:          512 | elapsed time per iteration (ms): 26266.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.082732E+01 | loss scale: 131072.0 | grad norm: 9.198 | num zeros: 140845.0 | params norm: 233.037 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 16853.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.049265E+01 | loss scale: 131072.0 | grad norm: 4.307 | num zeros: 748861696.0 | params norm: 233.081 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 14904.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.034247E+01 | loss scale: 131072.0 | grad norm: 4.034 | num zeros: 719271232.0 | params norm: 233.125 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 17630.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.151336E+01 | loss scale: 131072.0 | grad norm: 3.952 | num zeros: 804392064.0 | params norm: 233.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 12775.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.146659E+01 | loss scale: 131072.0 | grad norm: 4.020 | num zeros: 851388480.0 | params norm: 233.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 13247.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.102703E+01 | loss scale: 131072.0 | grad norm: 11.028 | num zeros: 842538368.0 | params norm: 233.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 15185.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.024274E+01 | loss scale: 131072.0 | grad norm: 4.027 | num zeros: 899124928.0 | params norm: 233.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 13850.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.015838E+01 | loss scale: 131072.0 | grad norm: 3.945 | num zeros: 983856640.0 | params norm: 233.355 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 14493.1 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 131072.0 | params norm: 233.355 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 11812.4 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 65536.0 | params norm: 233.355 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 12253.2 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 32768.0 | params norm: 233.355 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 12962.6 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 16384.0 | params norm: 233.355 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 12483.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.035505E+01 | loss scale: 16384.0 | grad norm: 71.608 | num zeros: 409450848.0 | params norm: 233.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 13256.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.000751E+01 | loss scale: 16384.0 | grad norm: 3.928 | num zeros: 1041521664.0 | params norm: 233.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 10922.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.919335E+00 | loss scale: 16384.0 | grad norm: 3.925 | num zeros: 1036733952.0 | params norm: 233.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 10183.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.819720E+00 | loss scale: 16384.0 | grad norm: 3.914 | num zeros: 1040295616.0 | params norm: 233.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 12021.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.712069E+00 | loss scale: 16384.0 | grad norm: 3.903 | num zeros: 1038664960.0 | params norm: 233.587 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 11974.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.596520E+00 | loss scale: 16384.0 | grad norm: 3.913 | num zeros: 1049119488.0 | params norm: 233.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 12248.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.481914E+00 | loss scale: 16384.0 | grad norm: 3.901 | num zeros: 978744576.0 | params norm: 233.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 10613.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.362409E+00 | loss scale: 16384.0 | grad norm: 3.897 | num zeros: 863078464.0 | params norm: 233.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 12473.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.241199E+00 | loss scale: 16384.0 | grad norm: 3.883 | num zeros: 944582464.0 | params norm: 233.775 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 11396.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.122400E+00 | loss scale: 16384.0 | grad norm: 3.876 | num zeros: 977075904.0 | params norm: 233.822 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 10014.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.004853E+00 | loss scale: 16384.0 | grad norm: 3.878 | num zeros: 955037952.0 | params norm: 233.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 10665.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.889832E+00 | loss scale: 16384.0 | grad norm: 3.862 | num zeros: 962996224.0 | params norm: 233.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 11586.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.780149E+00 | loss scale: 16384.0 | grad norm: 3.855 | num zeros: 972092288.0 | params norm: 233.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 13172.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.671818E+00 | loss scale: 16384.0 | grad norm: 3.853 | num zeros: 933771648.0 | params norm: 234.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 11586.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.574648E+00 | loss scale: 16384.0 | grad norm: 3.834 | num zeros: 880917632.0 | params norm: 234.055 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 10974.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.480541E+00 | loss scale: 16384.0 | grad norm: 3.777 | num zeros: 843481472.0 | params norm: 234.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 11242.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.382577E+00 | loss scale: 16384.0 | grad norm: 3.769 | num zeros: 927309056.0 | params norm: 234.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 10245.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.293773E+00 | loss scale: 16384.0 | grad norm: 3.726 | num zeros: 975454976.0 | params norm: 234.197 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 12240.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.213738E+00 | loss scale: 16384.0 | grad norm: 3.675 | num zeros: 896488320.0 | params norm: 234.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 12724.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.121319E+00 | loss scale: 16384.0 | grad norm: 3.661 | num zeros: 912731776.0 | params norm: 234.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 14572.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.044321E+00 | loss scale: 16384.0 | grad norm: 3.608 | num zeros: 939280448.0 | params norm: 234.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 16014.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.968538E+00 | loss scale: 16384.0 | grad norm: 3.532 | num zeros: 885497344.0 | params norm: 234.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 9325.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.893062E+00 | loss scale: 16384.0 | grad norm: 3.467 | num zeros: 889462272.0 | params norm: 234.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 12158.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.828265E+00 | loss scale: 16384.0 | grad norm: 3.368 | num zeros: 883089472.0 | params norm: 234.470 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 11931.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.765969E+00 | loss scale: 16384.0 | grad norm: 3.262 | num zeros: 842805888.0 | params norm: 234.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 11433.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.699301E+00 | loss scale: 16384.0 | grad norm: 3.137 | num zeros: 856813312.0 | params norm: 234.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 11021.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.649335E+00 | loss scale: 16384.0 | grad norm: 2.961 | num zeros: 873163136.0 | params norm: 234.604 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 10140.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.592084E+00 | loss scale: 16384.0 | grad norm: 2.807 | num zeros: 837856448.0 | params norm: 234.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 11561.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.542335E+00 | loss scale: 16384.0 | grad norm: 2.620 | num zeros: 725907968.0 | params norm: 234.690 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 11657.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.505279E+00 | loss scale: 16384.0 | grad norm: 2.364 | num zeros: 628577216.0 | params norm: 234.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 9718.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.464663E+00 | loss scale: 16384.0 | grad norm: 2.126 | num zeros: 761862016.0 | params norm: 234.774 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 13122.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.430040E+00 | loss scale: 16384.0 | grad norm: 1.847 | num zeros: 797498560.0 | params norm: 234.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 9127.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.406116E+00 | loss scale: 16384.0 | grad norm: 1.583 | num zeros: 743361920.0 | params norm: 234.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 11913.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.374134E+00 | loss scale: 16384.0 | grad norm: 1.339 | num zeros: 727543424.0 | params norm: 234.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 10592.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.346500E+00 | loss scale: 16384.0 | grad norm: 1.202 | num zeros: 766601088.0 | params norm: 234.942 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 8837.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.338736E+00 | loss scale: 16384.0 | grad norm: 1.220 | num zeros: 828436352.0 | params norm: 234.985 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 10783.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.318427E+00 | loss scale: 16384.0 | grad norm: 0.970 | num zeros: 765604928.0 | params norm: 235.027 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 10850.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.305227E+00 | loss scale: 16384.0 | grad norm: 0.768 | num zeros: 764332608.0 | params norm: 235.072 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 9734.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.297246E+00 | loss scale: 16384.0 | grad norm: 0.705 | num zeros: 772219072.0 | params norm: 235.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 9994.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.283728E+00 | loss scale: 16384.0 | grad norm: 0.642 | num zeros: 750632064.0 | params norm: 235.162 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 10095.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.273592E+00 | loss scale: 16384.0 | grad norm: 0.472 | num zeros: 708951296.0 | params norm: 235.208 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 9699.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.270886E+00 | loss scale: 16384.0 | grad norm: 0.405 | num zeros: 710097152.0 | params norm: 235.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 9155.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.255084E+00 | loss scale: 16384.0 | grad norm: 0.461 | num zeros: 715470592.0 | params norm: 235.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 9518.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.234460E+00 | loss scale: 16384.0 | grad norm: 0.492 | num zeros: 708705792.0 | params norm: 235.343 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 10390.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.226548E+00 | loss scale: 16384.0 | grad norm: 0.612 | num zeros: 734093952.0 | params norm: 235.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 8980.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.211383E+00 | loss scale: 16384.0 | grad norm: 0.529 | num zeros: 706140800.0 | params norm: 235.436 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 9902.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.205726E+00 | loss scale: 16384.0 | grad norm: 0.409 | num zeros: 740881216.0 | params norm: 235.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 9709.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.191288E+00 | loss scale: 16384.0 | grad norm: 0.353 | num zeros: 689938048.0 | params norm: 235.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 10380.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.180897E+00 | loss scale: 16384.0 | grad norm: 0.270 | num zeros: 682354432.0 | params norm: 235.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 11646.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.170149E+00 | loss scale: 16384.0 | grad norm: 0.314 | num zeros: 735666496.0 | params norm: 235.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 8231.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.170450E+00 | loss scale: 16384.0 | grad norm: 0.220 | num zeros: 747350016.0 | params norm: 235.667 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 8949.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.163648E+00 | loss scale: 16384.0 | grad norm: 0.228 | num zeros: 721876288.0 | params norm: 235.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 9237.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.163808E+00 | loss scale: 16384.0 | grad norm: 0.342 | num zeros: 744624832.0 | params norm: 235.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 10492.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.145487E+00 | loss scale: 16384.0 | grad norm: 0.243 | num zeros: 762142464.0 | params norm: 235.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 12832.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.154527E+00 | loss scale: 16384.0 | grad norm: 0.357 | num zeros: 819073152.0 | params norm: 235.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 8164.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.153177E+00 | loss scale: 16384.0 | grad norm: 0.708 | num zeros: 835342208.0 | params norm: 235.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 12342.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.138015E+00 | loss scale: 16384.0 | grad norm: 0.250 | num zeros: 848500224.0 | params norm: 235.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 8810.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.149707E+00 | loss scale: 16384.0 | grad norm: 0.528 | num zeros: 816169408.0 | params norm: 235.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 10321.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.126716E+00 | loss scale: 16384.0 | grad norm: 0.390 | num zeros: 823293056.0 | params norm: 235.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 19349.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.134481E+00 | loss scale: 16384.0 | grad norm: 0.381 | num zeros: 828534272.0 | params norm: 236.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 10286.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.132848E+00 | loss scale: 16384.0 | grad norm: 0.442 | num zeros: 826907648.0 | params norm: 236.069 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 9660.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.126431E+00 | loss scale: 16384.0 | grad norm: 0.361 | num zeros: 848063616.0 | params norm: 236.104 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 9964.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.127966E+00 | loss scale: 16384.0 | grad norm: 0.359 | num zeros: 838785408.0 | params norm: 236.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 9645.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.121487E+00 | loss scale: 16384.0 | grad norm: 0.339 | num zeros: 842814848.0 | params norm: 236.170 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 10883.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.110284E+00 | loss scale: 16384.0 | grad norm: 0.294 | num zeros: 855876480.0 | params norm: 236.201 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 8809.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.112944E+00 | loss scale: 16384.0 | grad norm: 0.401 | num zeros: 837056832.0 | params norm: 236.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 9591.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.111329E+00 | loss scale: 16384.0 | grad norm: 0.300 | num zeros: 838052480.0 | params norm: 236.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 8309.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.105317E+00 | loss scale: 16384.0 | grad norm: 0.279 | num zeros: 830003584.0 | params norm: 236.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 9618.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.109460E+00 | loss scale: 16384.0 | grad norm: 0.365 | num zeros: 858168704.0 | params norm: 236.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 7611.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.103567E+00 | loss scale: 16384.0 | grad norm: 0.270 | num zeros: 828167616.0 | params norm: 236.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 7824.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.090249E+00 | loss scale: 16384.0 | grad norm: 0.290 | num zeros: 823355840.0 | params norm: 236.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 8808.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.073801E+00 | loss scale: 16384.0 | grad norm: 0.341 | num zeros: 833842624.0 | params norm: 236.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 8468.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.075210E+00 | loss scale: 16384.0 | grad norm: 0.354 | num zeros: 854132544.0 | params norm: 236.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 10259.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.074311E+00 | loss scale: 16384.0 | grad norm: 0.445 | num zeros: 826101888.0 | params norm: 236.450 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 10033.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.087694E+00 | loss scale: 16384.0 | grad norm: 1.645 | num zeros: 819939968.0 | params norm: 236.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 8263.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.066536E+00 | loss scale: 16384.0 | grad norm: 0.692 | num zeros: 833554112.0 | params norm: 236.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 9456.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.056175E+00 | loss scale: 16384.0 | grad norm: 0.578 | num zeros: 850723264.0 | params norm: 236.523 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 41152.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.040778E+00 | loss scale: 16384.0 | grad norm: 0.396 | num zeros: 817793280.0 | params norm: 236.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 8501.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.060421E+00 | loss scale: 16384.0 | grad norm: 0.556 | num zeros: 823501504.0 | params norm: 236.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 8158.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.043369E+00 | loss scale: 16384.0 | grad norm: 0.379 | num zeros: 825838912.0 | params norm: 236.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 11778.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.026612E+00 | loss scale: 16384.0 | grad norm: 0.330 | num zeros: 812081408.0 | params norm: 236.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 8635.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.043970E+00 | loss scale: 16384.0 | grad norm: 0.825 | num zeros: 845682304.0 | params norm: 236.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 8914.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.033787E+00 | loss scale: 16384.0 | grad norm: 0.834 | num zeros: 791257216.0 | params norm: 236.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 11839.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.022600E+00 | loss scale: 16384.0 | grad norm: 0.386 | num zeros: 830808320.0 | params norm: 236.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 10082.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.013902E+00 | loss scale: 16384.0 | grad norm: 0.640 | num zeros: 847278720.0 | params norm: 236.696 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 8058.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.004527E+00 | loss scale: 16384.0 | grad norm: 0.408 | num zeros: 820099136.0 | params norm: 236.718 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 10026.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.000572E+00 | loss scale: 16384.0 | grad norm: 0.528 | num zeros: 808351168.0 | params norm: 236.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 11215.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.996933E+00 | loss scale: 16384.0 | grad norm: 0.393 | num zeros: 822847424.0 | params norm: 236.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        51712 | elapsed time per iteration (ms): 8948.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.995988E+00 | loss scale: 16384.0 | grad norm: 0.678 | num zeros: 791556480.0 | params norm: 236.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        52224 | elapsed time per iteration (ms): 7838.1 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.979867E+00 | loss scale: 16384.0 | grad norm: 0.310 | num zeros: 804122624.0 | params norm: 236.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        52736 | elapsed time per iteration (ms): 7965.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.974576E+00 | loss scale: 16384.0 | grad norm: 0.886 | num zeros: 702871424.0 | params norm: 236.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        53248 | elapsed time per iteration (ms): 8841.1 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.001950E+00 | loss scale: 16384.0 | grad norm: 1.346 | num zeros: 693350912.0 | params norm: 236.849 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        53760 | elapsed time per iteration (ms): 7831.1 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.970034E+00 | loss scale: 16384.0 | grad norm: 0.416 | num zeros: 741805440.0 | params norm: 236.871 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        54272 | elapsed time per iteration (ms): 8780.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.971474E+00 | loss scale: 16384.0 | grad norm: 0.608 | num zeros: 758540736.0 | params norm: 236.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        54784 | elapsed time per iteration (ms): 8244.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.975056E+00 | loss scale: 16384.0 | grad norm: 0.570 | num zeros: 779905600.0 | params norm: 236.914 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        55296 | elapsed time per iteration (ms): 8496.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.947455E+00 | loss scale: 16384.0 | grad norm: 0.333 | num zeros: 741167808.0 | params norm: 236.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        55808 | elapsed time per iteration (ms): 7549.3 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.947540E+00 | loss scale: 16384.0 | grad norm: 0.494 | num zeros: 730681024.0 | params norm: 236.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        56320 | elapsed time per iteration (ms): 8697.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.939836E+00 | loss scale: 16384.0 | grad norm: 0.430 | num zeros: 720667648.0 | params norm: 236.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        56832 | elapsed time per iteration (ms): 7250.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.936715E+00 | loss scale: 16384.0 | grad norm: 0.344 | num zeros: 676042944.0 | params norm: 237.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        57344 | elapsed time per iteration (ms): 8136.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.928713E+00 | loss scale: 16384.0 | grad norm: 0.455 | num zeros: 695726912.0 | params norm: 237.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        57856 | elapsed time per iteration (ms): 12482.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.919309E+00 | loss scale: 16384.0 | grad norm: 0.324 | num zeros: 633982272.0 | params norm: 237.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        58368 | elapsed time per iteration (ms): 9146.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.924143E+00 | loss scale: 16384.0 | grad norm: 0.351 | num zeros: 527595904.0 | params norm: 237.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        58880 | elapsed time per iteration (ms): 9744.7 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.920375E+00 | loss scale: 16384.0 | grad norm: 0.485 | num zeros: 561040640.0 | params norm: 237.108 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        59392 | elapsed time per iteration (ms): 7176.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.904153E+00 | loss scale: 16384.0 | grad norm: 0.553 | num zeros: 571321792.0 | params norm: 237.135 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        59904 | elapsed time per iteration (ms): 7527.1 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.896277E+00 | loss scale: 16384.0 | grad norm: 0.366 | num zeros: 584775680.0 | params norm: 237.164 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        60416 | elapsed time per iteration (ms): 7773.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.894712E+00 | loss scale: 16384.0 | grad norm: 0.406 | num zeros: 548891520.0 | params norm: 237.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        60928 | elapsed time per iteration (ms): 8465.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.877252E+00 | loss scale: 16384.0 | grad norm: 0.316 | num zeros: 580604928.0 | params norm: 237.223 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        61440 | elapsed time per iteration (ms): 7128.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.876659E+00 | loss scale: 16384.0 | grad norm: 0.434 | num zeros: 514647872.0 | params norm: 237.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        61952 | elapsed time per iteration (ms): 7293.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.857305E+00 | loss scale: 16384.0 | grad norm: 0.466 | num zeros: 536270656.0 | params norm: 237.284 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        62464 | elapsed time per iteration (ms): 7506.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.866122E+00 | loss scale: 16384.0 | grad norm: 0.334 | num zeros: 509545344.0 | params norm: 237.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        62976 | elapsed time per iteration (ms): 7570.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.846176E+00 | loss scale: 16384.0 | grad norm: 0.265 | num zeros: 485865184.0 | params norm: 237.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        63488 | elapsed time per iteration (ms): 8149.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.851996E+00 | loss scale: 16384.0 | grad norm: 0.380 | num zeros: 478555776.0 | params norm: 237.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        64000 | elapsed time per iteration (ms): 9160.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.841899E+00 | loss scale: 16384.0 | grad norm: 0.618 | num zeros: 467864608.0 | params norm: 237.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        64512 | elapsed time per iteration (ms): 8124.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.839170E+00 | loss scale: 16384.0 | grad norm: 0.831 | num zeros: 452241056.0 | params norm: 237.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        65024 | elapsed time per iteration (ms): 7020.9 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.826569E+00 | loss scale: 16384.0 | grad norm: 0.574 | num zeros: 440802240.0 | params norm: 237.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        65536 | elapsed time per iteration (ms): 8714.9 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.819961E+00 | loss scale: 16384.0 | grad norm: 0.373 | num zeros: 467792320.0 | params norm: 237.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        66048 | elapsed time per iteration (ms): 7846.1 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.812522E+00 | loss scale: 16384.0 | grad norm: 0.427 | num zeros: 461658112.0 | params norm: 237.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        66560 | elapsed time per iteration (ms): 7706.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.809042E+00 | loss scale: 16384.0 | grad norm: 0.327 | num zeros: 469989376.0 | params norm: 237.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        67072 | elapsed time per iteration (ms): 7843.6 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.811422E+00 | loss scale: 16384.0 | grad norm: 0.482 | num zeros: 455411840.0 | params norm: 237.619 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        67584 | elapsed time per iteration (ms): 7435.1 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.790137E+00 | loss scale: 16384.0 | grad norm: 0.450 | num zeros: 425886208.0 | params norm: 237.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        68096 | elapsed time per iteration (ms): 6978.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.784788E+00 | loss scale: 16384.0 | grad norm: 0.316 | num zeros: 506896256.0 | params norm: 237.690 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        68608 | elapsed time per iteration (ms): 7282.6 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.790647E+00 | loss scale: 16384.0 | grad norm: 0.268 | num zeros: 483343968.0 | params norm: 237.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        69120 | elapsed time per iteration (ms): 6302.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.787265E+00 | loss scale: 16384.0 | grad norm: 0.410 | num zeros: 517945440.0 | params norm: 237.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        69632 | elapsed time per iteration (ms): 8484.4 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.785007E+00 | loss scale: 16384.0 | grad norm: 0.424 | num zeros: 455714880.0 | params norm: 237.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        70144 | elapsed time per iteration (ms): 8225.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.773405E+00 | loss scale: 16384.0 | grad norm: 0.424 | num zeros: 497873408.0 | params norm: 237.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        70656 | elapsed time per iteration (ms): 7554.9 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.766943E+00 | loss scale: 16384.0 | grad norm: 0.345 | num zeros: 508155072.0 | params norm: 237.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        71168 | elapsed time per iteration (ms): 6962.9 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.761267E+00 | loss scale: 16384.0 | grad norm: 0.358 | num zeros: 483684608.0 | params norm: 237.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        71680 | elapsed time per iteration (ms): 7065.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.762740E+00 | loss scale: 16384.0 | grad norm: 0.363 | num zeros: 548820672.0 | params norm: 237.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        72192 | elapsed time per iteration (ms): 6551.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.744974E+00 | loss scale: 16384.0 | grad norm: 0.323 | num zeros: 536451840.0 | params norm: 237.986 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        72704 | elapsed time per iteration (ms): 7620.7 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.750857E+00 | loss scale: 16384.0 | grad norm: 0.371 | num zeros: 529343392.0 | params norm: 238.023 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        73216 | elapsed time per iteration (ms): 6616.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.741783E+00 | loss scale: 16384.0 | grad norm: 0.470 | num zeros: 523874272.0 | params norm: 238.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        73728 | elapsed time per iteration (ms): 6907.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.743638E+00 | loss scale: 16384.0 | grad norm: 0.502 | num zeros: 497674528.0 | params norm: 238.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        74240 | elapsed time per iteration (ms): 8478.8 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.738036E+00 | loss scale: 16384.0 | grad norm: 0.440 | num zeros: 502426496.0 | params norm: 238.137 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        74752 | elapsed time per iteration (ms): 6768.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.734679E+00 | loss scale: 16384.0 | grad norm: 0.253 | num zeros: 547938624.0 | params norm: 238.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        75264 | elapsed time per iteration (ms): 7413.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.745206E+00 | loss scale: 16384.0 | grad norm: 0.317 | num zeros: 526774144.0 | params norm: 238.214 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        75776 | elapsed time per iteration (ms): 8563.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.723811E+00 | loss scale: 16384.0 | grad norm: 0.432 | num zeros: 553549312.0 | params norm: 238.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        76288 | elapsed time per iteration (ms): 7763.1 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.734756E+00 | loss scale: 16384.0 | grad norm: 0.363 | num zeros: 554622976.0 | params norm: 238.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        76800 | elapsed time per iteration (ms): 10931.1 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.721522E+00 | loss scale: 16384.0 | grad norm: 0.310 | num zeros: 550796800.0 | params norm: 238.328 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        77312 | elapsed time per iteration (ms): 7686.2 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.715478E+00 | loss scale: 16384.0 | grad norm: 0.370 | num zeros: 540789888.0 | params norm: 238.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        77824 | elapsed time per iteration (ms): 6013.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.716479E+00 | loss scale: 16384.0 | grad norm: 0.397 | num zeros: 553121472.0 | params norm: 238.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        78336 | elapsed time per iteration (ms): 6848.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.719469E+00 | loss scale: 16384.0 | grad norm: 0.357 | num zeros: 532312928.0 | params norm: 238.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        78848 | elapsed time per iteration (ms): 6298.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.708122E+00 | loss scale: 16384.0 | grad norm: 0.452 | num zeros: 575675136.0 | params norm: 238.481 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        79360 | elapsed time per iteration (ms): 11627.6 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.708884E+00 | loss scale: 16384.0 | grad norm: 0.612 | num zeros: 539684480.0 | params norm: 238.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        79872 | elapsed time per iteration (ms): 6884.1 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.708828E+00 | loss scale: 16384.0 | grad norm: 0.607 | num zeros: 552320576.0 | params norm: 238.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        80384 | elapsed time per iteration (ms): 6828.6 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.704984E+00 | loss scale: 16384.0 | grad norm: 0.729 | num zeros: 565082368.0 | params norm: 238.597 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        80896 | elapsed time per iteration (ms): 6795.9 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.700119E+00 | loss scale: 16384.0 | grad norm: 0.647 | num zeros: 503033664.0 | params norm: 238.635 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        81408 | elapsed time per iteration (ms): 6865.6 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.701703E+00 | loss scale: 16384.0 | grad norm: 0.364 | num zeros: 519357600.0 | params norm: 238.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        81920 | elapsed time per iteration (ms): 7114.1 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.705089E+00 | loss scale: 16384.0 | grad norm: 0.817 | num zeros: 479147616.0 | params norm: 238.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        82432 | elapsed time per iteration (ms): 6214.8 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.702751E+00 | loss scale: 16384.0 | grad norm: 0.738 | num zeros: 479909504.0 | params norm: 238.748 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        82944 | elapsed time per iteration (ms): 7288.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.708148E+00 | loss scale: 16384.0 | grad norm: 0.971 | num zeros: 503510944.0 | params norm: 238.786 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        83456 | elapsed time per iteration (ms): 5580.8 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.687121E+00 | loss scale: 16384.0 | grad norm: 0.691 | num zeros: 472584928.0 | params norm: 238.822 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        83968 | elapsed time per iteration (ms): 6440.5 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.688049E+00 | loss scale: 16384.0 | grad norm: 0.644 | num zeros: 497185408.0 | params norm: 238.860 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        84480 | elapsed time per iteration (ms): 8144.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.696807E+00 | loss scale: 16384.0 | grad norm: 1.366 | num zeros: 478571008.0 | params norm: 238.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        84992 | elapsed time per iteration (ms): 8616.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.679322E+00 | loss scale: 16384.0 | grad norm: 0.472 | num zeros: 487201824.0 | params norm: 238.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        85504 | elapsed time per iteration (ms): 6931.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.683156E+00 | loss scale: 16384.0 | grad norm: 0.840 | num zeros: 510323648.0 | params norm: 238.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        86016 | elapsed time per iteration (ms): 6876.5 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.673160E+00 | loss scale: 16384.0 | grad norm: 0.635 | num zeros: 501022016.0 | params norm: 239.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        86528 | elapsed time per iteration (ms): 9266.7 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.666725E+00 | loss scale: 16384.0 | grad norm: 0.645 | num zeros: 493342080.0 | params norm: 239.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        87040 | elapsed time per iteration (ms): 7323.5 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.657025E+00 | loss scale: 16384.0 | grad norm: 0.537 | num zeros: 458417376.0 | params norm: 239.081 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        87552 | elapsed time per iteration (ms): 7985.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.657998E+00 | loss scale: 16384.0 | grad norm: 0.648 | num zeros: 502769408.0 | params norm: 239.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        88064 | elapsed time per iteration (ms): 5863.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.662477E+00 | loss scale: 16384.0 | grad norm: 0.449 | num zeros: 504308896.0 | params norm: 239.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        88576 | elapsed time per iteration (ms): 6556.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.656483E+00 | loss scale: 16384.0 | grad norm: 0.589 | num zeros: 519886688.0 | params norm: 239.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        89088 | elapsed time per iteration (ms): 6121.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.641420E+00 | loss scale: 16384.0 | grad norm: 0.676 | num zeros: 481811776.0 | params norm: 239.230 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        89600 | elapsed time per iteration (ms): 6289.9 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.667496E+00 | loss scale: 16384.0 | grad norm: 1.239 | num zeros: 502051136.0 | params norm: 239.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        90112 | elapsed time per iteration (ms): 7812.3 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.652894E+00 | loss scale: 16384.0 | grad norm: 0.551 | num zeros: 526831360.0 | params norm: 239.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        90624 | elapsed time per iteration (ms): 7011.6 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.658983E+00 | loss scale: 16384.0 | grad norm: 0.827 | num zeros: 504735424.0 | params norm: 239.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        91136 | elapsed time per iteration (ms): 7074.5 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.667357E+00 | loss scale: 16384.0 | grad norm: 1.278 | num zeros: 512540512.0 | params norm: 239.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        91648 | elapsed time per iteration (ms): 8110.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.672960E+00 | loss scale: 16384.0 | grad norm: 1.018 | num zeros: 452114464.0 | params norm: 239.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        92160 | elapsed time per iteration (ms): 6051.0 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.682808E+00 | loss scale: 16384.0 | grad norm: 2.058 | num zeros: 478062720.0 | params norm: 239.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        92672 | elapsed time per iteration (ms): 5832.4 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.669476E+00 | loss scale: 16384.0 | grad norm: 1.060 | num zeros: 492443648.0 | params norm: 239.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        93184 | elapsed time per iteration (ms): 5457.3 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.680342E+00 | loss scale: 16384.0 | grad norm: 1.054 | num zeros: 459989120.0 | params norm: 239.518 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        93696 | elapsed time per iteration (ms): 10621.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.683682E+00 | loss scale: 16384.0 | grad norm: 1.061 | num zeros: 478050112.0 | params norm: 239.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        94208 | elapsed time per iteration (ms): 6088.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.664369E+00 | loss scale: 16384.0 | grad norm: 0.686 | num zeros: 488018240.0 | params norm: 239.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        94720 | elapsed time per iteration (ms): 6709.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.659992E+00 | loss scale: 16384.0 | grad norm: 0.712 | num zeros: 486693664.0 | params norm: 239.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        95232 | elapsed time per iteration (ms): 7542.4 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.651486E+00 | loss scale: 16384.0 | grad norm: 0.487 | num zeros: 518730816.0 | params norm: 239.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        95744 | elapsed time per iteration (ms): 6536.9 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.654773E+00 | loss scale: 16384.0 | grad norm: 0.676 | num zeros: 514363040.0 | params norm: 239.694 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        96256 | elapsed time per iteration (ms): 11855.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.655023E+00 | loss scale: 16384.0 | grad norm: 0.933 | num zeros: 496545248.0 | params norm: 239.729 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        96768 | elapsed time per iteration (ms): 5760.4 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.659724E+00 | loss scale: 16384.0 | grad norm: 0.960 | num zeros: 492933824.0 | params norm: 239.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        97280 | elapsed time per iteration (ms): 5875.9 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.646503E+00 | loss scale: 16384.0 | grad norm: 0.548 | num zeros: 477022496.0 | params norm: 239.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        97792 | elapsed time per iteration (ms): 5702.0 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.636600E+00 | loss scale: 16384.0 | grad norm: 0.441 | num zeros: 494451200.0 | params norm: 239.832 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        98304 | elapsed time per iteration (ms): 5692.2 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.631630E+00 | loss scale: 16384.0 | grad norm: 0.560 | num zeros: 500881216.0 | params norm: 239.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        98816 | elapsed time per iteration (ms): 8060.2 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.630622E+00 | loss scale: 16384.0 | grad norm: 0.785 | num zeros: 500258112.0 | params norm: 239.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        99328 | elapsed time per iteration (ms): 6107.9 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.617859E+00 | loss scale: 16384.0 | grad norm: 0.670 | num zeros: 484274208.0 | params norm: 239.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        99840 | elapsed time per iteration (ms): 6507.4 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.617649E+00 | loss scale: 16384.0 | grad norm: 0.398 | num zeros: 470193152.0 | params norm: 239.961 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:       100352 | elapsed time per iteration (ms): 5529.0 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.610418E+00 | loss scale: 16384.0 | grad norm: 0.646 | num zeros: 499425856.0 | params norm: 239.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:       100864 | elapsed time per iteration (ms): 5416.6 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.634248E+00 | loss scale: 16384.0 | grad norm: 1.025 | num zeros: 462663296.0 | params norm: 240.024 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:       101376 | elapsed time per iteration (ms): 6551.1 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.603907E+00 | loss scale: 16384.0 | grad norm: 0.617 | num zeros: 459035616.0 | params norm: 240.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:       101888 | elapsed time per iteration (ms): 7025.4 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.604036E+00 | loss scale: 16384.0 | grad norm: 0.460 | num zeros: 468341024.0 | params norm: 240.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:       102400 | elapsed time per iteration (ms): 7628.3 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.603098E+00 | loss scale: 16384.0 | grad norm: 0.471 | num zeros: 473934144.0 | params norm: 240.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-01-04 20:09:26 
