using world size: 64, data-parallel-size: 64, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 64
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 64
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  gshard_cap ...................................... 4.8
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 1
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 102400
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 64
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 31 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 25 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 36 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 38 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 39 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 41 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 57 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 42 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 43 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 26 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 33 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 34 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 32 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 59 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 24 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 27 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 18 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 58 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 44 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 63 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 46 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 16 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 56 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 47 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 29 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 30 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 20 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 21 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 22 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 37 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 23 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 60 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 45 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 61 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 62 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 28 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 40 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 19 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 17 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 49 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 51 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 50 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 48 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 35 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 52 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 53 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 55 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 54 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 0 in DP group [0]
[INFO] 1 in DP group [1]
[INFO] 2 in DP group [2]
[INFO] 3 in DP group [3]
[INFO] 4 in DP group [4]
[INFO] 5 in DP group [5]
[INFO] 6 in DP group [6]
[INFO] 7 in DP group [7]
[INFO] 8 in DP group [8]
[INFO] 9 in DP group [9]
[INFO] 10 in DP group [10]
[INFO] 11 in DP group [11]
[INFO] 12 in DP group [12]
[INFO] 13 in DP group [13]
[INFO] 14 in DP group [14]
[INFO] 15 in DP group [15]
[INFO] 16 in DP group [16]
[INFO] 17 in DP group [17]
[INFO] 18 in DP group [18]
[INFO] 19 in DP group [19]
[INFO] 20 in DP group [20]
[INFO] 21 in DP group [21]
[INFO] 22 in DP group [22]
[INFO] 23 in DP group [23]
[INFO] 24 in DP group [24]
[INFO] 25 in DP group [25]
[INFO] 26 in DP group [26]
[INFO] 27 in DP group [27]
[INFO] 28 in DP group [28]
[INFO] 29 in DP group [29]
[INFO] 30 in DP group [30]
[INFO] 31 in DP group [31]
[INFO] 32 in DP group [32]
[INFO] 33 in DP group [33]
[INFO] 34 in DP group [34]
[INFO] 35 in DP group [35]
[INFO] 36 in DP group [36]
[INFO] 37 in DP group [37]
[INFO] 38 in DP group [38]
[INFO] 39 in DP group [39]
[INFO] 40 in DP group [40]
[INFO] 41 in DP group [41]
[INFO] 42 in DP group [42]
[INFO] 43 in DP group [43]
[INFO] 44 in DP group [44]
[INFO] 45 in DP group [45]
[INFO] 46 in DP group [46]
[INFO] 47 in DP group [47]
[INFO] 48 in DP group [48]
[INFO] 49 in DP group [49]
[INFO] 50 in DP group [50]
[INFO] 51 in DP group [51]
[INFO] 52 in DP group [52]
[INFO] 53 in DP group [53]
[INFO] 54 in DP group [54]
[INFO] 55 in DP group [55]
[INFO] 56 in DP group [56]
[INFO] 57 in DP group [57]
[INFO] 58 in DP group [58]
[INFO] 59 in DP group [59]
[INFO] 60 in DP group [60]
[INFO] 61 in DP group [61]
[INFO] 62 in DP group [62]
[W ProcessGroupNCCL.cpp:1569] Rank 21 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 22 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> setting random seeds to 1234 ...
[INFO] 63 in DP group [63]
[W ProcessGroupNCCL.cpp:1569] Rank 23 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 40 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 42 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 50 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 51 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 48 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[W ProcessGroupNCCL.cpp:1569] Rank 24 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 27 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 13 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 15 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 14 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 41 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 2 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> compiling dataset index builder ...
[W ProcessGroupNCCL.cpp:1569] Rank 4 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 60 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 61 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 36 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 63 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 37 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 39 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 44 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 45 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 46 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 38 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 43 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 12 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 57 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 59 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 5 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 47 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 52 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 58 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 29 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 53 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 55 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 30 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 28 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 31 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 35 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 33 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 56 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 20 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 25 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 54 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 9 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 11 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 16 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 19 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 18 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 10 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 62 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 26 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 6 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 49 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 3 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 17 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 7 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 34 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 32 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 8 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
make: 进入目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
make: 对“default”无需做任何事。
make: 离开目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
>>> done with dataset index builder. Compilation time: 0.201 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
>>> done with compiling and loading fused kernels. Compilation time: 13.718 seconds
time to initialize megatron (seconds): 38.759
[after megatron is initialized] datetime: 2023-01-04 20:12:22 
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
building GPT model ...
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 382676992
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-04 20:12:27 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: -512
    test:       -512
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.069651 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.234 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-04 20:12:41 
done with setup ...
training ...
[before the start of training step] datetime: 2023-01-04 20:12:41 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 7330.7392578125 | max allocated: 7330.74658203125 | reserved: 7816.0 | max reserved: 7816.0
 iteration        1/     200 | consumed samples:          512 | elapsed time per iteration (ms): 22192.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.082744E+01 | loss scale: 131072.0 | grad norm: 12.910 | num zeros: 9847.0 | params norm: 228.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 14978.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.050972E+01 | loss scale: 131072.0 | grad norm: 4.220 | num zeros: 76163688.0 | params norm: 228.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 16245.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.035001E+01 | loss scale: 131072.0 | grad norm: 4.139 | num zeros: 95668616.0 | params norm: 228.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 15562.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.016287E+01 | loss scale: 131072.0 | grad norm: 3.994 | num zeros: 114718376.0 | params norm: 228.309 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 15055.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.000761E+01 | loss scale: 131072.0 | grad norm: 3.998 | num zeros: 96126704.0 | params norm: 228.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 16553.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.842469E+00 | loss scale: 131072.0 | grad norm: 3.947 | num zeros: 96089184.0 | params norm: 228.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 16894.6 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 131072.0 | params norm: 228.342 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 16713.5 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 65536.0 | params norm: 228.342 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 16869.8 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 32768.0 | params norm: 228.342 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 16702.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.003597E+01 | loss scale: 32768.0 | grad norm: 33.244 | num zeros: 86442416.0 | params norm: 228.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 15762.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.558427E+00 | loss scale: 32768.0 | grad norm: 3.837 | num zeros: 105552800.0 | params norm: 228.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 15722.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.411260E+00 | loss scale: 32768.0 | grad norm: 3.856 | num zeros: 78482552.0 | params norm: 228.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 16198.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.276829E+00 | loss scale: 32768.0 | grad norm: 3.839 | num zeros: 120991048.0 | params norm: 228.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 16782.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.143976E+00 | loss scale: 32768.0 | grad norm: 3.836 | num zeros: 122583456.0 | params norm: 228.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 16232.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.012456E+00 | loss scale: 32768.0 | grad norm: 3.828 | num zeros: 120169816.0 | params norm: 228.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 16155.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.887297E+00 | loss scale: 32768.0 | grad norm: 3.813 | num zeros: 118734696.0 | params norm: 228.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 16337.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.766918E+00 | loss scale: 32768.0 | grad norm: 3.789 | num zeros: 117539440.0 | params norm: 228.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 16180.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.650270E+00 | loss scale: 32768.0 | grad norm: 3.785 | num zeros: 117887848.0 | params norm: 228.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 16012.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.545485E+00 | loss scale: 32768.0 | grad norm: 3.761 | num zeros: 59204932.0 | params norm: 228.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 16420.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.440925E+00 | loss scale: 32768.0 | grad norm: 3.738 | num zeros: 119711552.0 | params norm: 228.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 16644.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.341430E+00 | loss scale: 32768.0 | grad norm: 3.702 | num zeros: 119607680.0 | params norm: 228.719 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 16267.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.247610E+00 | loss scale: 32768.0 | grad norm: 3.666 | num zeros: 117699592.0 | params norm: 228.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 16539.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.155062E+00 | loss scale: 32768.0 | grad norm: 3.639 | num zeros: 126520224.0 | params norm: 228.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 16730.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.065413E+00 | loss scale: 32768.0 | grad norm: 3.586 | num zeros: 115795264.0 | params norm: 228.840 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 16282.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.982829E+00 | loss scale: 32768.0 | grad norm: 3.525 | num zeros: 114955304.0 | params norm: 228.881 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 15938.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.900873E+00 | loss scale: 32768.0 | grad norm: 3.462 | num zeros: 114891792.0 | params norm: 228.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 15949.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.832490E+00 | loss scale: 32768.0 | grad norm: 3.391 | num zeros: 114795616.0 | params norm: 228.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 15438.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.768034E+00 | loss scale: 32768.0 | grad norm: 3.230 | num zeros: 114754984.0 | params norm: 229.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 14564.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.695519E+00 | loss scale: 32768.0 | grad norm: 3.121 | num zeros: 105861272.0 | params norm: 229.045 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 14316.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.631180E+00 | loss scale: 32768.0 | grad norm: 2.974 | num zeros: 105239800.0 | params norm: 229.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 14921.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.581501E+00 | loss scale: 32768.0 | grad norm: 2.796 | num zeros: 114642216.0 | params norm: 229.129 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 15607.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.515174E+00 | loss scale: 32768.0 | grad norm: 2.627 | num zeros: 116489432.0 | params norm: 229.171 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 15022.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.469384E+00 | loss scale: 32768.0 | grad norm: 2.430 | num zeros: 114521592.0 | params norm: 229.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 15008.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.425519E+00 | loss scale: 32768.0 | grad norm: 2.179 | num zeros: 114593528.0 | params norm: 229.255 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 18803.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.385006E+00 | loss scale: 32768.0 | grad norm: 1.931 | num zeros: 114566360.0 | params norm: 229.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 15124.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.356444E+00 | loss scale: 32768.0 | grad norm: 1.669 | num zeros: 96198736.0 | params norm: 229.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 14569.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.332596E+00 | loss scale: 32768.0 | grad norm: 1.448 | num zeros: 114512680.0 | params norm: 229.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 15200.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.319035E+00 | loss scale: 32768.0 | grad norm: 1.328 | num zeros: 105059480.0 | params norm: 229.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 13904.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.300832E+00 | loss scale: 32768.0 | grad norm: 0.947 | num zeros: 105102752.0 | params norm: 229.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 13241.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.281727E+00 | loss scale: 32768.0 | grad norm: 0.819 | num zeros: 114462448.0 | params norm: 229.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 13765.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.270078E+00 | loss scale: 32768.0 | grad norm: 0.692 | num zeros: 114536624.0 | params norm: 229.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 12891.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.265879E+00 | loss scale: 32768.0 | grad norm: 0.557 | num zeros: 95796632.0 | params norm: 229.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 12709.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.249518E+00 | loss scale: 32768.0 | grad norm: 0.458 | num zeros: 105004840.0 | params norm: 229.665 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 12454.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.234496E+00 | loss scale: 32768.0 | grad norm: 0.603 | num zeros: 86647016.0 | params norm: 229.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 13366.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.231803E+00 | loss scale: 32768.0 | grad norm: 0.733 | num zeros: 90443360.0 | params norm: 229.765 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 12693.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.213612E+00 | loss scale: 32768.0 | grad norm: 0.720 | num zeros: 91224176.0 | params norm: 229.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 12274.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.190295E+00 | loss scale: 32768.0 | grad norm: 0.393 | num zeros: 95557880.0 | params norm: 229.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 28695.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.188943E+00 | loss scale: 32768.0 | grad norm: 0.631 | num zeros: 95559992.0 | params norm: 229.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 12216.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.176696E+00 | loss scale: 32768.0 | grad norm: 0.534 | num zeros: 95577936.0 | params norm: 229.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 12451.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.166373E+00 | loss scale: 32768.0 | grad norm: 0.415 | num zeros: 105004920.0 | params norm: 230.019 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 12653.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.158343E+00 | loss scale: 32768.0 | grad norm: 0.416 | num zeros: 105036736.0 | params norm: 230.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 13618.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.149519E+00 | loss scale: 32768.0 | grad norm: 0.482 | num zeros: 105167288.0 | params norm: 230.119 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 13664.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.141404E+00 | loss scale: 32768.0 | grad norm: 0.443 | num zeros: 105115392.0 | params norm: 230.170 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 14557.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.153314E+00 | loss scale: 32768.0 | grad norm: 0.899 | num zeros: 114508600.0 | params norm: 230.216 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 13096.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.125460E+00 | loss scale: 32768.0 | grad norm: 0.574 | num zeros: 105073016.0 | params norm: 230.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 12901.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.115602E+00 | loss scale: 32768.0 | grad norm: 0.362 | num zeros: 105125016.0 | params norm: 230.309 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 13022.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.114581E+00 | loss scale: 32768.0 | grad norm: 0.504 | num zeros: 105204792.0 | params norm: 230.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 12561.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.102728E+00 | loss scale: 32768.0 | grad norm: 0.408 | num zeros: 99570720.0 | params norm: 230.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 12433.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.105300E+00 | loss scale: 32768.0 | grad norm: 0.318 | num zeros: 105104624.0 | params norm: 230.434 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 12152.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.096951E+00 | loss scale: 32768.0 | grad norm: 0.443 | num zeros: 114430144.0 | params norm: 230.476 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 12560.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.087740E+00 | loss scale: 32768.0 | grad norm: 0.242 | num zeros: 105094976.0 | params norm: 230.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 13008.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.084686E+00 | loss scale: 32768.0 | grad norm: 0.474 | num zeros: 106510824.0 | params norm: 230.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 12308.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.085394E+00 | loss scale: 32768.0 | grad norm: 0.535 | num zeros: 106183040.0 | params norm: 230.589 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 12377.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.083298E+00 | loss scale: 32768.0 | grad norm: 0.820 | num zeros: 114398872.0 | params norm: 230.626 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 12089.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.081046E+00 | loss scale: 32768.0 | grad norm: 0.809 | num zeros: 114488808.0 | params norm: 230.664 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 12642.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.060037E+00 | loss scale: 32768.0 | grad norm: 0.732 | num zeros: 96382496.0 | params norm: 230.699 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 12463.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.065155E+00 | loss scale: 32768.0 | grad norm: 0.589 | num zeros: 95655144.0 | params norm: 230.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 12042.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.047830E+00 | loss scale: 32768.0 | grad norm: 0.832 | num zeros: 86372384.0 | params norm: 230.765 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 12252.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.051106E+00 | loss scale: 32768.0 | grad norm: 1.068 | num zeros: 96227952.0 | params norm: 230.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 12351.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.046906E+00 | loss scale: 32768.0 | grad norm: 0.438 | num zeros: 95677352.0 | params norm: 230.832 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 12093.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.025243E+00 | loss scale: 32768.0 | grad norm: 0.758 | num zeros: 95708072.0 | params norm: 230.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 12144.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.030775E+00 | loss scale: 32768.0 | grad norm: 0.634 | num zeros: 96348432.0 | params norm: 230.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 11903.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.023002E+00 | loss scale: 32768.0 | grad norm: 0.460 | num zeros: 106666592.0 | params norm: 230.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 11724.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.015948E+00 | loss scale: 32768.0 | grad norm: 0.638 | num zeros: 106858512.0 | params norm: 230.963 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 11998.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.013396E+00 | loss scale: 32768.0 | grad norm: 0.435 | num zeros: 98458720.0 | params norm: 230.995 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 11963.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.002919E+00 | loss scale: 32768.0 | grad norm: 0.484 | num zeros: 101246336.0 | params norm: 231.026 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 11677.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 6.992119E+00 | loss scale: 32768.0 | grad norm: 0.421 | num zeros: 93139184.0 | params norm: 231.058 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 11776.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 6.992329E+00 | loss scale: 32768.0 | grad norm: 0.381 | num zeros: 94714032.0 | params norm: 231.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 11854.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 6.983737E+00 | loss scale: 32768.0 | grad norm: 0.463 | num zeros: 102820424.0 | params norm: 231.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 11763.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 6.973157E+00 | loss scale: 32768.0 | grad norm: 0.289 | num zeros: 124504976.0 | params norm: 231.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 11669.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 6.979421E+00 | loss scale: 32768.0 | grad norm: 0.634 | num zeros: 107956456.0 | params norm: 231.182 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 11741.1 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.969245E+00 | loss scale: 32768.0 | grad norm: 0.323 | num zeros: 96204464.0 | params norm: 231.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 11978.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.953716E+00 | loss scale: 32768.0 | grad norm: 0.520 | num zeros: 95678720.0 | params norm: 231.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 11771.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.932613E+00 | loss scale: 32768.0 | grad norm: 0.441 | num zeros: 67231328.0 | params norm: 231.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 12090.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.954452E+00 | loss scale: 32768.0 | grad norm: 1.129 | num zeros: 77028560.0 | params norm: 231.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 12062.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.931889E+00 | loss scale: 32768.0 | grad norm: 0.854 | num zeros: 66873856.0 | params norm: 231.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 12071.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.905488E+00 | loss scale: 32768.0 | grad norm: 0.417 | num zeros: 66963608.0 | params norm: 231.363 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 11821.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.901437E+00 | loss scale: 32768.0 | grad norm: 0.349 | num zeros: 85725528.0 | params norm: 231.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 11900.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.887702E+00 | loss scale: 32768.0 | grad norm: 0.410 | num zeros: 67021184.0 | params norm: 231.422 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 11822.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.869720E+00 | loss scale: 32768.0 | grad norm: 0.447 | num zeros: 67065244.0 | params norm: 231.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 11635.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.878256E+00 | loss scale: 32768.0 | grad norm: 0.561 | num zeros: 49014168.0 | params norm: 231.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 12091.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.858760E+00 | loss scale: 32768.0 | grad norm: 0.384 | num zeros: 67078116.0 | params norm: 231.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 11925.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.847003E+00 | loss scale: 32768.0 | grad norm: 0.314 | num zeros: 67664192.0 | params norm: 231.547 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 11930.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.852823E+00 | loss scale: 32768.0 | grad norm: 0.305 | num zeros: 70844752.0 | params norm: 231.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 11931.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.838860E+00 | loss scale: 32768.0 | grad norm: 0.367 | num zeros: 59526848.0 | params norm: 231.610 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 12051.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.836604E+00 | loss scale: 32768.0 | grad norm: 0.312 | num zeros: 58079232.0 | params norm: 231.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 12089.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.819916E+00 | loss scale: 32768.0 | grad norm: 0.304 | num zeros: 59357564.0 | params norm: 231.675 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 11937.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.815775E+00 | loss scale: 32768.0 | grad norm: 0.250 | num zeros: 62848656.0 | params norm: 231.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 11991.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.815037E+00 | loss scale: 32768.0 | grad norm: 0.356 | num zeros: 58480208.0 | params norm: 231.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 11730.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.811304E+00 | loss scale: 32768.0 | grad norm: 0.384 | num zeros: 73190136.0 | params norm: 231.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        51712 | elapsed time per iteration (ms): 11457.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.806698E+00 | loss scale: 32768.0 | grad norm: 0.305 | num zeros: 64497436.0 | params norm: 231.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        52224 | elapsed time per iteration (ms): 11823.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.798392E+00 | loss scale: 32768.0 | grad norm: 0.262 | num zeros: 95720744.0 | params norm: 231.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        52736 | elapsed time per iteration (ms): 11558.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.785030E+00 | loss scale: 32768.0 | grad norm: 0.327 | num zeros: 85776064.0 | params norm: 231.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        53248 | elapsed time per iteration (ms): 11318.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.787834E+00 | loss scale: 32768.0 | grad norm: 0.315 | num zeros: 67731904.0 | params norm: 231.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        53760 | elapsed time per iteration (ms): 11281.7 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.783779E+00 | loss scale: 32768.0 | grad norm: 0.296 | num zeros: 77062952.0 | params norm: 231.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        54272 | elapsed time per iteration (ms): 11234.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.773576E+00 | loss scale: 32768.0 | grad norm: 0.269 | num zeros: 76850928.0 | params norm: 231.965 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        54784 | elapsed time per iteration (ms): 11521.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.776046E+00 | loss scale: 32768.0 | grad norm: 0.283 | num zeros: 85757344.0 | params norm: 232.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        55296 | elapsed time per iteration (ms): 11242.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.755594E+00 | loss scale: 32768.0 | grad norm: 0.241 | num zeros: 76320144.0 | params norm: 232.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        55808 | elapsed time per iteration (ms): 11280.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.752548E+00 | loss scale: 32768.0 | grad norm: 0.254 | num zeros: 76340528.0 | params norm: 232.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        56320 | elapsed time per iteration (ms): 11157.3 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.745039E+00 | loss scale: 32768.0 | grad norm: 0.273 | num zeros: 67518208.0 | params norm: 232.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        56832 | elapsed time per iteration (ms): 11297.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.746148E+00 | loss scale: 32768.0 | grad norm: 0.299 | num zeros: 85707456.0 | params norm: 232.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        57344 | elapsed time per iteration (ms): 11167.2 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.735571E+00 | loss scale: 32768.0 | grad norm: 0.387 | num zeros: 85732056.0 | params norm: 232.171 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        57856 | elapsed time per iteration (ms): 11459.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.732325E+00 | loss scale: 32768.0 | grad norm: 0.494 | num zeros: 77062280.0 | params norm: 232.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        58368 | elapsed time per iteration (ms): 11349.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.739837E+00 | loss scale: 32768.0 | grad norm: 0.603 | num zeros: 68178656.0 | params norm: 232.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        58880 | elapsed time per iteration (ms): 11202.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.735332E+00 | loss scale: 32768.0 | grad norm: 0.377 | num zeros: 77998560.0 | params norm: 232.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        59392 | elapsed time per iteration (ms): 11218.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.714940E+00 | loss scale: 32768.0 | grad norm: 0.322 | num zeros: 77762520.0 | params norm: 232.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        59904 | elapsed time per iteration (ms): 11845.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.712313E+00 | loss scale: 32768.0 | grad norm: 0.266 | num zeros: 68288256.0 | params norm: 232.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        60416 | elapsed time per iteration (ms): 11228.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.712378E+00 | loss scale: 32768.0 | grad norm: 0.343 | num zeros: 68100280.0 | params norm: 232.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        60928 | elapsed time per iteration (ms): 11122.3 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.701715E+00 | loss scale: 32768.0 | grad norm: 0.279 | num zeros: 85959464.0 | params norm: 232.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        61440 | elapsed time per iteration (ms): 11241.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.700164E+00 | loss scale: 32768.0 | grad norm: 0.341 | num zeros: 77006320.0 | params norm: 232.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        61952 | elapsed time per iteration (ms): 10941.3 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.687877E+00 | loss scale: 32768.0 | grad norm: 0.592 | num zeros: 77046864.0 | params norm: 232.480 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        62464 | elapsed time per iteration (ms): 10931.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.699686E+00 | loss scale: 32768.0 | grad norm: 0.498 | num zeros: 76818136.0 | params norm: 232.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        62976 | elapsed time per iteration (ms): 11033.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.681689E+00 | loss scale: 32768.0 | grad norm: 0.302 | num zeros: 76652368.0 | params norm: 232.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        63488 | elapsed time per iteration (ms): 11930.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.688232E+00 | loss scale: 32768.0 | grad norm: 0.301 | num zeros: 67550976.0 | params norm: 232.583 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        64000 | elapsed time per iteration (ms): 10984.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.676945E+00 | loss scale: 32768.0 | grad norm: 0.314 | num zeros: 67695632.0 | params norm: 232.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        64512 | elapsed time per iteration (ms): 10849.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.670116E+00 | loss scale: 32768.0 | grad norm: 0.336 | num zeros: 49174796.0 | params norm: 232.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        65024 | elapsed time per iteration (ms): 11059.6 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.667002E+00 | loss scale: 32768.0 | grad norm: 0.329 | num zeros: 49125864.0 | params norm: 232.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        65536 | elapsed time per iteration (ms): 13931.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.663442E+00 | loss scale: 32768.0 | grad norm: 0.248 | num zeros: 48714864.0 | params norm: 232.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        66048 | elapsed time per iteration (ms): 19559.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.657334E+00 | loss scale: 32768.0 | grad norm: 0.316 | num zeros: 48927692.0 | params norm: 232.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        66560 | elapsed time per iteration (ms): 23423.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.656982E+00 | loss scale: 32768.0 | grad norm: 0.388 | num zeros: 58548424.0 | params norm: 232.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        67072 | elapsed time per iteration (ms): 10731.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.659616E+00 | loss scale: 32768.0 | grad norm: 0.364 | num zeros: 39419080.0 | params norm: 232.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        67584 | elapsed time per iteration (ms): 10792.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.636586E+00 | loss scale: 32768.0 | grad norm: 0.243 | num zeros: 49427896.0 | params norm: 232.871 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        68096 | elapsed time per iteration (ms): 10932.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.632458E+00 | loss scale: 32768.0 | grad norm: 0.244 | num zeros: 67187664.0 | params norm: 232.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        68608 | elapsed time per iteration (ms): 10851.4 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.642468E+00 | loss scale: 32768.0 | grad norm: 0.343 | num zeros: 49277636.0 | params norm: 232.946 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        69120 | elapsed time per iteration (ms): 10810.7 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.637593E+00 | loss scale: 32768.0 | grad norm: 0.404 | num zeros: 49920176.0 | params norm: 232.984 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        69632 | elapsed time per iteration (ms): 10833.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.637140E+00 | loss scale: 32768.0 | grad norm: 0.436 | num zeros: 40145932.0 | params norm: 233.022 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        70144 | elapsed time per iteration (ms): 11180.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.621321E+00 | loss scale: 32768.0 | grad norm: 0.231 | num zeros: 58008712.0 | params norm: 233.060 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        70656 | elapsed time per iteration (ms): 11000.9 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.617351E+00 | loss scale: 32768.0 | grad norm: 0.223 | num zeros: 62682348.0 | params norm: 233.098 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        71168 | elapsed time per iteration (ms): 11082.1 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.613463E+00 | loss scale: 32768.0 | grad norm: 0.303 | num zeros: 59565864.0 | params norm: 233.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        71680 | elapsed time per iteration (ms): 11168.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.615774E+00 | loss scale: 32768.0 | grad norm: 0.438 | num zeros: 55959072.0 | params norm: 233.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        72192 | elapsed time per iteration (ms): 11153.1 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.597534E+00 | loss scale: 32768.0 | grad norm: 0.362 | num zeros: 76205856.0 | params norm: 233.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        72704 | elapsed time per iteration (ms): 11097.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.600147E+00 | loss scale: 32768.0 | grad norm: 0.194 | num zeros: 41308064.0 | params norm: 233.252 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        73216 | elapsed time per iteration (ms): 11282.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.589186E+00 | loss scale: 32768.0 | grad norm: 0.350 | num zeros: 55329516.0 | params norm: 233.291 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        73728 | elapsed time per iteration (ms): 11191.1 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.588096E+00 | loss scale: 32768.0 | grad norm: 0.365 | num zeros: 49467060.0 | params norm: 233.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        74240 | elapsed time per iteration (ms): 11167.6 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.589253E+00 | loss scale: 32768.0 | grad norm: 0.400 | num zeros: 40343292.0 | params norm: 233.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        74752 | elapsed time per iteration (ms): 11002.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.581207E+00 | loss scale: 32768.0 | grad norm: 0.271 | num zeros: 48489072.0 | params norm: 233.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        75264 | elapsed time per iteration (ms): 11143.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.593460E+00 | loss scale: 32768.0 | grad norm: 0.295 | num zeros: 47988424.0 | params norm: 233.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        75776 | elapsed time per iteration (ms): 11303.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.567782E+00 | loss scale: 32768.0 | grad norm: 0.349 | num zeros: 39899640.0 | params norm: 233.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        76288 | elapsed time per iteration (ms): 12425.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.576941E+00 | loss scale: 32768.0 | grad norm: 0.235 | num zeros: 44293932.0 | params norm: 233.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        76800 | elapsed time per iteration (ms): 10906.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.569987E+00 | loss scale: 32768.0 | grad norm: 0.538 | num zeros: 39116152.0 | params norm: 233.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        77312 | elapsed time per iteration (ms): 10833.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.567440E+00 | loss scale: 32768.0 | grad norm: 0.604 | num zeros: 36035012.0 | params norm: 233.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        77824 | elapsed time per iteration (ms): 10763.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.563685E+00 | loss scale: 32768.0 | grad norm: 0.334 | num zeros: 40201844.0 | params norm: 233.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        78336 | elapsed time per iteration (ms): 10826.6 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.564626E+00 | loss scale: 32768.0 | grad norm: 0.349 | num zeros: 48594928.0 | params norm: 233.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        78848 | elapsed time per iteration (ms): 10941.0 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.555980E+00 | loss scale: 32768.0 | grad norm: 0.361 | num zeros: 48053908.0 | params norm: 233.724 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        79360 | elapsed time per iteration (ms): 10720.8 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.546991E+00 | loss scale: 32768.0 | grad norm: 0.311 | num zeros: 48558624.0 | params norm: 233.765 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        79872 | elapsed time per iteration (ms): 10826.3 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.552470E+00 | loss scale: 32768.0 | grad norm: 0.389 | num zeros: 39460304.0 | params norm: 233.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        80384 | elapsed time per iteration (ms): 10605.3 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.541675E+00 | loss scale: 32768.0 | grad norm: 0.255 | num zeros: 48460632.0 | params norm: 233.845 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        80896 | elapsed time per iteration (ms): 10614.6 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.538037E+00 | loss scale: 32768.0 | grad norm: 0.285 | num zeros: 39509396.0 | params norm: 233.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        81408 | elapsed time per iteration (ms): 10847.6 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.544606E+00 | loss scale: 32768.0 | grad norm: 0.238 | num zeros: 30220346.0 | params norm: 233.927 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        81920 | elapsed time per iteration (ms): 10446.9 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.541153E+00 | loss scale: 32768.0 | grad norm: 0.248 | num zeros: 38487996.0 | params norm: 233.969 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        82432 | elapsed time per iteration (ms): 10457.8 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.533891E+00 | loss scale: 32768.0 | grad norm: 0.259 | num zeros: 21238618.0 | params norm: 234.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        82944 | elapsed time per iteration (ms): 10439.0 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.534808E+00 | loss scale: 32768.0 | grad norm: 0.245 | num zeros: 30230916.0 | params norm: 234.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        83456 | elapsed time per iteration (ms): 10363.8 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.521039E+00 | loss scale: 32768.0 | grad norm: 0.324 | num zeros: 20849768.0 | params norm: 234.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        83968 | elapsed time per iteration (ms): 10322.9 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.525559E+00 | loss scale: 32768.0 | grad norm: 0.327 | num zeros: 32145528.0 | params norm: 234.140 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        84480 | elapsed time per iteration (ms): 10323.7 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.517321E+00 | loss scale: 32768.0 | grad norm: 0.359 | num zeros: 30301750.0 | params norm: 234.184 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        84992 | elapsed time per iteration (ms): 10477.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.516108E+00 | loss scale: 32768.0 | grad norm: 0.248 | num zeros: 30681308.0 | params norm: 234.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        85504 | elapsed time per iteration (ms): 10406.6 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.511563E+00 | loss scale: 32768.0 | grad norm: 0.222 | num zeros: 30201086.0 | params norm: 234.272 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        86016 | elapsed time per iteration (ms): 10205.0 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.504517E+00 | loss scale: 32768.0 | grad norm: 0.196 | num zeros: 53852824.0 | params norm: 234.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        86528 | elapsed time per iteration (ms): 11810.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.498963E+00 | loss scale: 32768.0 | grad norm: 0.347 | num zeros: 53164812.0 | params norm: 234.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        87040 | elapsed time per iteration (ms): 10522.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.489954E+00 | loss scale: 32768.0 | grad norm: 0.304 | num zeros: 58455548.0 | params norm: 234.409 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        87552 | elapsed time per iteration (ms): 10324.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.491346E+00 | loss scale: 32768.0 | grad norm: 0.234 | num zeros: 43895296.0 | params norm: 234.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        88064 | elapsed time per iteration (ms): 10176.9 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.500185E+00 | loss scale: 32768.0 | grad norm: 0.261 | num zeros: 51135344.0 | params norm: 234.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        88576 | elapsed time per iteration (ms): 11134.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.487395E+00 | loss scale: 32768.0 | grad norm: 0.196 | num zeros: 46732024.0 | params norm: 234.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        89088 | elapsed time per iteration (ms): 10208.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.473218E+00 | loss scale: 32768.0 | grad norm: 0.276 | num zeros: 30638956.0 | params norm: 234.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        89600 | elapsed time per iteration (ms): 10181.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.480900E+00 | loss scale: 32768.0 | grad norm: 0.296 | num zeros: 41081512.0 | params norm: 234.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        90112 | elapsed time per iteration (ms): 10184.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.480584E+00 | loss scale: 32768.0 | grad norm: 0.314 | num zeros: 32325842.0 | params norm: 234.691 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        90624 | elapsed time per iteration (ms): 10553.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.473424E+00 | loss scale: 32768.0 | grad norm: 0.221 | num zeros: 21933246.0 | params norm: 234.739 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        91136 | elapsed time per iteration (ms): 10318.7 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.464767E+00 | loss scale: 32768.0 | grad norm: 0.219 | num zeros: 34789856.0 | params norm: 234.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        91648 | elapsed time per iteration (ms): 10235.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.471947E+00 | loss scale: 32768.0 | grad norm: 0.284 | num zeros: 31684796.0 | params norm: 234.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        92160 | elapsed time per iteration (ms): 10263.4 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.452404E+00 | loss scale: 32768.0 | grad norm: 0.261 | num zeros: 42047776.0 | params norm: 234.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        92672 | elapsed time per iteration (ms): 10310.3 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.455855E+00 | loss scale: 32768.0 | grad norm: 0.238 | num zeros: 30057956.0 | params norm: 234.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        93184 | elapsed time per iteration (ms): 10474.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.457679E+00 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 30080118.0 | params norm: 234.986 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        93696 | elapsed time per iteration (ms): 10162.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.459158E+00 | loss scale: 32768.0 | grad norm: 0.187 | num zeros: 41650596.0 | params norm: 235.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        94208 | elapsed time per iteration (ms): 10252.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.443558E+00 | loss scale: 32768.0 | grad norm: 0.264 | num zeros: 36457048.0 | params norm: 235.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        94720 | elapsed time per iteration (ms): 10154.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.446920E+00 | loss scale: 32768.0 | grad norm: 0.257 | num zeros: 33534796.0 | params norm: 235.137 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        95232 | elapsed time per iteration (ms): 12322.6 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.441551E+00 | loss scale: 32768.0 | grad norm: 0.244 | num zeros: 11824871.0 | params norm: 235.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        95744 | elapsed time per iteration (ms): 9980.2 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.444224E+00 | loss scale: 32768.0 | grad norm: 0.209 | num zeros: 21268108.0 | params norm: 235.238 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        96256 | elapsed time per iteration (ms): 10136.9 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.436083E+00 | loss scale: 32768.0 | grad norm: 0.188 | num zeros: 26460916.0 | params norm: 235.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        96768 | elapsed time per iteration (ms): 10174.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.440343E+00 | loss scale: 32768.0 | grad norm: 0.244 | num zeros: 25429280.0 | params norm: 235.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        97280 | elapsed time per iteration (ms): 10022.6 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.436603E+00 | loss scale: 32768.0 | grad norm: 0.267 | num zeros: 21250712.0 | params norm: 235.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        97792 | elapsed time per iteration (ms): 9909.1 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.432324E+00 | loss scale: 32768.0 | grad norm: 0.270 | num zeros: 12638792.0 | params norm: 235.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        98304 | elapsed time per iteration (ms): 10350.4 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.422978E+00 | loss scale: 32768.0 | grad norm: 0.210 | num zeros: 20046516.0 | params norm: 235.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        98816 | elapsed time per iteration (ms): 10206.2 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.413522E+00 | loss scale: 32768.0 | grad norm: 0.237 | num zeros: 20549444.0 | params norm: 235.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        99328 | elapsed time per iteration (ms): 10029.0 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.407207E+00 | loss scale: 32768.0 | grad norm: 0.201 | num zeros: 13052864.0 | params norm: 235.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        99840 | elapsed time per iteration (ms): 9869.7 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.410740E+00 | loss scale: 32768.0 | grad norm: 0.196 | num zeros: 25216476.0 | params norm: 235.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:       100352 | elapsed time per iteration (ms): 9984.9 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.401643E+00 | loss scale: 32768.0 | grad norm: 0.224 | num zeros: 26379576.0 | params norm: 235.699 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:       100864 | elapsed time per iteration (ms): 9918.3 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.422006E+00 | loss scale: 32768.0 | grad norm: 0.278 | num zeros: 16040695.0 | params norm: 235.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:       101376 | elapsed time per iteration (ms): 9813.9 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.401343E+00 | loss scale: 32768.0 | grad norm: 0.236 | num zeros: 26728196.0 | params norm: 235.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:       101888 | elapsed time per iteration (ms): 9850.8 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.402413E+00 | loss scale: 32768.0 | grad norm: 0.248 | num zeros: 14274055.0 | params norm: 235.859 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:       102400 | elapsed time per iteration (ms): 11408.9 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.398493E+00 | loss scale: 32768.0 | grad norm: 0.215 | num zeros: 28089612.0 | params norm: 235.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-01-04 20:54:21 
