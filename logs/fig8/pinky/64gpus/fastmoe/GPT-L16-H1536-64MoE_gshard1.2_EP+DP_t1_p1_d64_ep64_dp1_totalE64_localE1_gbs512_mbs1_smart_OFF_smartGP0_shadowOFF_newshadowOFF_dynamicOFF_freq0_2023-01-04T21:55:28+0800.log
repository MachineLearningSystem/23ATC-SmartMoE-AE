using world size: 64, data-parallel-size: 64, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 64
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 64
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 1
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 102400
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 64
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 22 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 28 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 30 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 17 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 31 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 45 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 18 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 29 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 47 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 44 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 46 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 56 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 51 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 57 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 58 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 50 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 49 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 54 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 48 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 37 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 42 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 38 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 39 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 36 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 27 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 61 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 24 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 25 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 20 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 21 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 16 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 19 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 53 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 23 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 60 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 41 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 33 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 35 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 52 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 40 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 32 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 43 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 34 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 55 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 62 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 63 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 26 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 59 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 0 in DP group [0]
[INFO] 1 in DP group [1]
[INFO] 2 in DP group [2]
[INFO] 3 in DP group [3]
[INFO] 4 in DP group [4]
[INFO] 5 in DP group [5]
[INFO] 6 in DP group [6]
[INFO] 7 in DP group [7]
[INFO] 8 in DP group [8]
[INFO] 9 in DP group [9]
[INFO] 10 in DP group [10]
[INFO] 11 in DP group [11]
[INFO] 12 in DP group [12]
[INFO] 13 in DP group [13]
[INFO] 14 in DP group [14]
[INFO] 15 in DP group [15]
[INFO] 16 in DP group [16]
[INFO] 17 in DP group [17]
[INFO] 18 in DP group [18]
[INFO] 19 in DP group [19]
[INFO] 20 in DP group [20]
[INFO] 21 in DP group [21]
[INFO] 22 in DP group [22]
[INFO] 23 in DP group [23]
[INFO] 24 in DP group [24]
[INFO] 25 in DP group [25]
[INFO] 26 in DP group [26]
[INFO] 27 in DP group [27]
[INFO] 28 in DP group [28]
[INFO] 29 in DP group [29]
[INFO] 30 in DP group [30]
[INFO] 31 in DP group [31]
[INFO] 32 in DP group [32]
[INFO] 33 in DP group [33]
[INFO] 34 in DP group [34]
[INFO] 35 in DP group [35]
[INFO] 36 in DP group [36]
[INFO] 37 in DP group [37]
[INFO] 38 in DP group [38]
[INFO] 39 in DP group [39]
[INFO] 40 in DP group [40]
[INFO] 41 in DP group [41]
[INFO] 42 in DP group [42]
[INFO] 43 in DP group [43]
[INFO] 44 in DP group [44]
[INFO] 45 in DP group [45]
[INFO] 46 in DP group [46]
[INFO] 47 in DP group [47]
[INFO] 48 in DP group [48]
[INFO] 49 in DP group [49]
[INFO] 50 in DP group [50]
[INFO] 51 in DP group [51]
[INFO] 52 in DP group [52]
[INFO] 53 in DP group [53]
[INFO] 54 in DP group [54]
[INFO] 55 in DP group [55]
[INFO] 56 in DP group [56]
[INFO] 57 in DP group [57]
[INFO] 58 in DP group [58]
[INFO] 59 in DP group [59]
[INFO] 60 in DP group [60]
[INFO] 61 in DP group [61]
[INFO] 62 in DP group [62]
[INFO] 63 in DP group [63]
> setting random seeds to 1234 ...
[W ProcessGroupNCCL.cpp:1569] Rank 54 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 55 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 5 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 53 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 7 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 8 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[W ProcessGroupNCCL.cpp:1569] Rank 10 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 33 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 11 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 47 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 6 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 45 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 4 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 35 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 56 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 52 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 16 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 9 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 57 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 18 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 3 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 61 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 20 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 37 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 38 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 39 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 24 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 63 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 2 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 27 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 26 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 46 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 25 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 21 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 19 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 28 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 44 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 22 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> compiling dataset index builder ...
[W ProcessGroupNCCL.cpp:1569] Rank 23 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 13 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 15 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 14 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 58 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 17 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 40 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 32 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 12 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 42 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 43 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 31 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 36 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 60 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 30 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 29 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 49 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 62 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 50 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 48 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 59 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 34 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 41 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 51 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
make: 进入目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
make: 对“default”无需做任何事。
make: 离开目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
>>> done with dataset index builder. Compilation time: 0.207 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
>>> done with compiling and loading fused kernels. Compilation time: 12.344 seconds
time to initialize megatron (seconds): 129.407
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
[after megatron is initialized] datetime: 2023-01-04 21:58:25 
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
building GPT model ...
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 382676992
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-04 21:58:29 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: -512
    test:       -512
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.053542 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.093 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-04 21:58:42 
done with setup ...
training ...
[before the start of training step] datetime: 2023-01-04 21:58:42 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 7334.2080078125 | max allocated: 7334.21533203125 | reserved: 7854.0 | max reserved: 7854.0
 iteration        1/     200 | consumed samples:          512 | elapsed time per iteration (ms): 18409.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.082745E+01 | gshard_loss: 2.470383E-04 | loss scale: 131072.0 | grad norm: 12.098 | num zeros: 9670.0 | params norm: 228.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 11198.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.048935E+01 | gshard_loss: 7.475991E-04 | loss scale: 131072.0 | grad norm: 4.490 | num zeros: 75987784.0 | params norm: 228.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 10402.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.045305E+01 | gshard_loss: 7.438747E-04 | loss scale: 131072.0 | grad norm: 8.955 | num zeros: 85433736.0 | params norm: 228.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 9596.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.021074E+01 | gshard_loss: 7.399923E-04 | loss scale: 131072.0 | grad norm: 3.951 | num zeros: 122860352.0 | params norm: 228.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 9466.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.004612E+01 | gshard_loss: 6.688588E-04 | loss scale: 131072.0 | grad norm: 3.926 | num zeros: 113921464.0 | params norm: 228.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 8282.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.885412E+00 | gshard_loss: 5.980670E-04 | loss scale: 131072.0 | grad norm: 3.928 | num zeros: 113643352.0 | params norm: 228.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 7540.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.733900E+00 | gshard_loss: 5.392030E-04 | loss scale: 131072.0 | grad norm: 3.903 | num zeros: 94706576.0 | params norm: 228.363 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 8652.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.584638E+00 | gshard_loss: 5.112813E-04 | loss scale: 131072.0 | grad norm: 3.909 | num zeros: 85150456.0 | params norm: 228.388 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 12400.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.435969E+00 | gshard_loss: 4.959311E-04 | loss scale: 131072.0 | grad norm: 3.912 | num zeros: 85150048.0 | params norm: 228.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 10251.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.288923E+00 | gshard_loss: 4.684214E-04 | loss scale: 131072.0 | grad norm: 3.910 | num zeros: 85197248.0 | params norm: 228.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 7284.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.152116E+00 | gshard_loss: 4.328835E-04 | loss scale: 131072.0 | grad norm: 3.875 | num zeros: 94739784.0 | params norm: 228.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 7101.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.012572E+00 | gshard_loss: 4.106028E-04 | loss scale: 131072.0 | grad norm: 3.888 | num zeros: 95482432.0 | params norm: 228.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 9175.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.888610E+00 | gshard_loss: 3.917576E-04 | loss scale: 131072.0 | grad norm: 3.861 | num zeros: 85495192.0 | params norm: 228.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 8736.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.768709E+00 | gshard_loss: 3.834026E-04 | loss scale: 131072.0 | grad norm: 3.849 | num zeros: 86269384.0 | params norm: 228.574 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 7998.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.652167E+00 | gshard_loss: 3.748212E-04 | loss scale: 131072.0 | grad norm: 3.838 | num zeros: 85793096.0 | params norm: 228.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 9800.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.542557E+00 | gshard_loss: 3.642913E-04 | loss scale: 131072.0 | grad norm: 3.810 | num zeros: 66923120.0 | params norm: 228.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 10050.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.437804E+00 | gshard_loss: 3.556151E-04 | loss scale: 131072.0 | grad norm: 3.781 | num zeros: 56951984.0 | params norm: 228.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 8279.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.336075E+00 | gshard_loss: 3.512155E-04 | loss scale: 131072.0 | grad norm: 3.765 | num zeros: 56935944.0 | params norm: 228.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 8109.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.246207E+00 | gshard_loss: 3.469227E-04 | loss scale: 131072.0 | grad norm: 3.722 | num zeros: 57493740.0 | params norm: 228.766 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 7947.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.158073E+00 | gshard_loss: 3.419137E-04 | loss scale: 131072.0 | grad norm: 3.682 | num zeros: 66396888.0 | params norm: 228.806 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 7951.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.073972E+00 | gshard_loss: 3.372289E-04 | loss scale: 131072.0 | grad norm: 3.624 | num zeros: 67142776.0 | params norm: 228.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 8982.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.992364E+00 | gshard_loss: 3.337095E-04 | loss scale: 131072.0 | grad norm: 3.563 | num zeros: 76082352.0 | params norm: 228.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 8222.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.913787E+00 | gshard_loss: 3.309543E-04 | loss scale: 131072.0 | grad norm: 3.502 | num zeros: 66745200.0 | params norm: 228.930 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 8469.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.836613E+00 | gshard_loss: 3.273450E-04 | loss scale: 131072.0 | grad norm: 3.409 | num zeros: 57785816.0 | params norm: 228.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 7277.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.768377E+00 | gshard_loss: 3.256548E-04 | loss scale: 131072.0 | grad norm: 3.310 | num zeros: 66720756.0 | params norm: 229.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 7945.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.698785E+00 | gshard_loss: 3.228151E-04 | loss scale: 131072.0 | grad norm: 3.199 | num zeros: 66757684.0 | params norm: 229.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 9224.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.644109E+00 | gshard_loss: 3.199528E-04 | loss scale: 131072.0 | grad norm: 3.050 | num zeros: 57487504.0 | params norm: 229.098 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 7952.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.597171E+00 | gshard_loss: 3.154230E-04 | loss scale: 131072.0 | grad norm: 2.839 | num zeros: 56660032.0 | params norm: 229.141 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 8313.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.538227E+00 | gshard_loss: 3.127581E-04 | loss scale: 131072.0 | grad norm: 2.649 | num zeros: 57691776.0 | params norm: 229.184 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 10348.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.491853E+00 | gshard_loss: 3.080285E-04 | loss scale: 131072.0 | grad norm: 2.399 | num zeros: 57612164.0 | params norm: 229.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 7408.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.466600E+00 | gshard_loss: 3.037202E-04 | loss scale: 131072.0 | grad norm: 2.114 | num zeros: 56656120.0 | params norm: 229.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 6304.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.415516E+00 | gshard_loss: 2.986156E-04 | loss scale: 131072.0 | grad norm: 1.847 | num zeros: 57630540.0 | params norm: 229.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 9551.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.392628E+00 | gshard_loss: 2.987141E-04 | loss scale: 131072.0 | grad norm: 1.521 | num zeros: 48618652.0 | params norm: 229.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 11681.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.371017E+00 | gshard_loss: 2.981660E-04 | loss scale: 131072.0 | grad norm: 1.157 | num zeros: 48021400.0 | params norm: 229.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 10672.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.352977E+00 | gshard_loss: 2.969950E-04 | loss scale: 131072.0 | grad norm: 0.801 | num zeros: 38501252.0 | params norm: 229.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 8071.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.348096E+00 | gshard_loss: 2.952058E-04 | loss scale: 131072.0 | grad norm: 0.454 | num zeros: 38485080.0 | params norm: 229.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 8444.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.343801E+00 | gshard_loss: 2.936235E-04 | loss scale: 131072.0 | grad norm: 0.299 | num zeros: 38427692.0 | params norm: 229.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 7511.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.337965E+00 | gshard_loss: 2.936503E-04 | loss scale: 131072.0 | grad norm: 0.442 | num zeros: 38278680.0 | params norm: 229.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 6627.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.348192E+00 | gshard_loss: 2.936613E-04 | loss scale: 131072.0 | grad norm: 0.630 | num zeros: 38199792.0 | params norm: 229.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 7730.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.340955E+00 | gshard_loss: 2.942450E-04 | loss scale: 131072.0 | grad norm: 0.678 | num zeros: 38202136.0 | params norm: 229.670 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 7246.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.336313E+00 | gshard_loss: 2.999113E-04 | loss scale: 131072.0 | grad norm: 0.696 | num zeros: 38080896.0 | params norm: 229.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 10331.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.339904E+00 | gshard_loss: 3.025435E-04 | loss scale: 131072.0 | grad norm: 0.681 | num zeros: 38913880.0 | params norm: 229.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 8144.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.329188E+00 | gshard_loss: 3.030037E-04 | loss scale: 131072.0 | grad norm: 0.547 | num zeros: 56747616.0 | params norm: 229.809 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 7810.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.319457E+00 | gshard_loss: 3.066856E-04 | loss scale: 131072.0 | grad norm: 0.460 | num zeros: 56718516.0 | params norm: 229.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 8381.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.317263E+00 | gshard_loss: 3.126745E-04 | loss scale: 131072.0 | grad norm: 0.364 | num zeros: 56704936.0 | params norm: 229.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 6813.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.302009E+00 | gshard_loss: 3.166804E-04 | loss scale: 131072.0 | grad norm: 0.310 | num zeros: 47229548.0 | params norm: 229.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 7565.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.289033E+00 | gshard_loss: 3.144402E-04 | loss scale: 131072.0 | grad norm: 0.236 | num zeros: 47229644.0 | params norm: 229.988 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 7216.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.289911E+00 | gshard_loss: 3.124185E-04 | loss scale: 131072.0 | grad norm: 0.227 | num zeros: 37799708.0 | params norm: 230.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 8389.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.286560E+00 | gshard_loss: 3.120402E-04 | loss scale: 131072.0 | grad norm: 0.261 | num zeros: 38441056.0 | params norm: 230.074 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 9905.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.278966E+00 | gshard_loss: 3.141576E-04 | loss scale: 131072.0 | grad norm: 0.254 | num zeros: 38242428.0 | params norm: 230.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 10650.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.274249E+00 | gshard_loss: 3.169745E-04 | loss scale: 131072.0 | grad norm: 0.272 | num zeros: 47221752.0 | params norm: 230.157 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 7496.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.263650E+00 | gshard_loss: 3.183215E-04 | loss scale: 131072.0 | grad norm: 0.292 | num zeros: 37781556.0 | params norm: 230.199 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 9390.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.254701E+00 | gshard_loss: 3.248279E-04 | loss scale: 131072.0 | grad norm: 0.268 | num zeros: 37778012.0 | params norm: 230.240 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 7932.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.249123E+00 | gshard_loss: 3.277439E-04 | loss scale: 131072.0 | grad norm: 0.256 | num zeros: 37779144.0 | params norm: 230.281 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 7391.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.233762E+00 | gshard_loss: 3.292832E-04 | loss scale: 131072.0 | grad norm: 0.257 | num zeros: 28337392.0 | params norm: 230.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 9063.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.219992E+00 | gshard_loss: 3.363718E-04 | loss scale: 131072.0 | grad norm: 0.225 | num zeros: 19379948.0 | params norm: 230.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 9058.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.217044E+00 | gshard_loss: 3.544821E-04 | loss scale: 131072.0 | grad norm: 0.185 | num zeros: 28338164.0 | params norm: 230.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 8475.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.201049E+00 | gshard_loss: 3.692395E-04 | loss scale: 131072.0 | grad norm: 0.198 | num zeros: 18925774.0 | params norm: 230.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 10561.1 | learning rate: 9.999E-05 | global batch size:   512 | loss scale: 131072.0 | params norm: 230.437 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 9628.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.201352E+00 | gshard_loss: 3.603787E-04 | loss scale: 131072.0 | grad norm: 0.351 | num zeros: 18899576.0 | params norm: 230.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 7027.8 | learning rate: 9.999E-05 | global batch size:   512 | loss scale: 65536.0 | params norm: 230.475 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 7711.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.187345E+00 | gshard_loss: 3.822660E-04 | loss scale: 65536.0 | grad norm: 0.237 | num zeros: 18903976.0 | params norm: 230.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 8973.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.191562E+00 | gshard_loss: 4.029675E-04 | loss scale: 65536.0 | grad norm: 0.457 | num zeros: 18889304.0 | params norm: 230.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 7138.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.181860E+00 | gshard_loss: 4.006563E-04 | loss scale: 65536.0 | grad norm: 0.264 | num zeros: 18963956.0 | params norm: 230.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 12814.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.182817E+00 | gshard_loss: 4.058507E-04 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 18900872.0 | params norm: 230.615 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 10724.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.174394E+00 | gshard_loss: 4.283977E-04 | loss scale: 65536.0 | grad norm: 0.142 | num zeros: 18978532.0 | params norm: 230.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 9931.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.179687E+00 | gshard_loss: 4.496814E-04 | loss scale: 65536.0 | grad norm: 0.144 | num zeros: 28402028.0 | params norm: 230.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 7110.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.161575E+00 | gshard_loss: 4.640876E-04 | loss scale: 65536.0 | grad norm: 0.230 | num zeros: 28340908.0 | params norm: 230.714 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 6742.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.158612E+00 | gshard_loss: 4.782724E-04 | loss scale: 65536.0 | grad norm: 0.200 | num zeros: 28403880.0 | params norm: 230.745 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 8809.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.167917E+00 | gshard_loss: 4.818632E-04 | loss scale: 65536.0 | grad norm: 0.167 | num zeros: 29402816.0 | params norm: 230.776 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 7406.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.141718E+00 | gshard_loss: 4.876354E-04 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 37782480.0 | params norm: 230.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 7027.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.146783E+00 | gshard_loss: 5.110619E-04 | loss scale: 65536.0 | grad norm: 0.146 | num zeros: 37780268.0 | params norm: 230.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 6960.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.143966E+00 | gshard_loss: 5.324491E-04 | loss scale: 65536.0 | grad norm: 0.138 | num zeros: 37780532.0 | params norm: 230.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 7726.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.137947E+00 | gshard_loss: 5.529401E-04 | loss scale: 65536.0 | grad norm: 0.134 | num zeros: 37780104.0 | params norm: 230.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 9242.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.134601E+00 | gshard_loss: 5.523883E-04 | loss scale: 65536.0 | grad norm: 0.127 | num zeros: 37782284.0 | params norm: 230.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 7547.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.126880E+00 | gshard_loss: 5.652852E-04 | loss scale: 65536.0 | grad norm: 0.118 | num zeros: 37796144.0 | params norm: 230.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 7667.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.117106E+00 | gshard_loss: 5.697232E-04 | loss scale: 65536.0 | grad norm: 0.109 | num zeros: 38192160.0 | params norm: 230.988 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 6931.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.118545E+00 | gshard_loss: 5.606628E-04 | loss scale: 65536.0 | grad norm: 0.126 | num zeros: 37828324.0 | params norm: 231.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 6062.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.114382E+00 | gshard_loss: 5.654435E-04 | loss scale: 65536.0 | grad norm: 0.154 | num zeros: 37779556.0 | params norm: 231.045 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 9843.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.108723E+00 | gshard_loss: 5.527043E-04 | loss scale: 65536.0 | grad norm: 0.194 | num zeros: 37781752.0 | params norm: 231.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 9256.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.111617E+00 | gshard_loss: 5.606034E-04 | loss scale: 65536.0 | grad norm: 0.146 | num zeros: 37784992.0 | params norm: 231.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 7189.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.108981E+00 | gshard_loss: 5.376368E-04 | loss scale: 65536.0 | grad norm: 0.138 | num zeros: 37787208.0 | params norm: 231.129 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 7600.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.095078E+00 | gshard_loss: 5.250313E-04 | loss scale: 65536.0 | grad norm: 0.120 | num zeros: 37825336.0 | params norm: 231.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 6678.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.081290E+00 | gshard_loss: 5.225893E-04 | loss scale: 65536.0 | grad norm: 0.130 | num zeros: 37779412.0 | params norm: 231.184 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 8484.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.085999E+00 | gshard_loss: 5.277768E-04 | loss scale: 65536.0 | grad norm: 0.163 | num zeros: 37778088.0 | params norm: 231.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 9428.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.085756E+00 | gshard_loss: 5.294624E-04 | loss scale: 65536.0 | grad norm: 0.165 | num zeros: 38257044.0 | params norm: 231.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 6429.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.072217E+00 | gshard_loss: 5.305689E-04 | loss scale: 65536.0 | grad norm: 0.166 | num zeros: 37789184.0 | params norm: 231.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 6398.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.078256E+00 | gshard_loss: 5.284851E-04 | loss scale: 65536.0 | grad norm: 0.253 | num zeros: 47213588.0 | params norm: 231.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 6710.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.077562E+00 | gshard_loss: 6.405400E-04 | loss scale: 65536.0 | grad norm: 0.449 | num zeros: 47213716.0 | params norm: 231.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 7300.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.057842E+00 | gshard_loss: 5.692198E-04 | loss scale: 65536.0 | grad norm: 0.250 | num zeros: 28346808.0 | params norm: 231.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 9356.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.073148E+00 | gshard_loss: 5.968969E-04 | loss scale: 65536.0 | grad norm: 0.397 | num zeros: 37772976.0 | params norm: 231.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 6313.1 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.059905E+00 | gshard_loss: 6.862535E-04 | loss scale: 65536.0 | grad norm: 0.250 | num zeros: 47240932.0 | params norm: 231.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 10492.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.047452E+00 | gshard_loss: 6.058889E-04 | loss scale: 65536.0 | grad norm: 0.248 | num zeros: 37813600.0 | params norm: 231.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 6369.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.054630E+00 | gshard_loss: 6.068622E-04 | loss scale: 65536.0 | grad norm: 0.279 | num zeros: 28780404.0 | params norm: 231.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 7396.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.042283E+00 | gshard_loss: 7.036451E-04 | loss scale: 65536.0 | grad norm: 0.322 | num zeros: 28392232.0 | params norm: 231.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 5913.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.041368E+00 | gshard_loss: 6.758726E-04 | loss scale: 65536.0 | grad norm: 0.359 | num zeros: 37772988.0 | params norm: 231.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 6506.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.022202E+00 | gshard_loss: 7.886653E-04 | loss scale: 65536.0 | grad norm: 0.224 | num zeros: 19850516.0 | params norm: 231.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 8039.1 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.021026E+00 | gshard_loss: 7.615155E-04 | loss scale: 65536.0 | grad norm: 0.270 | num zeros: 28791160.0 | params norm: 231.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 8303.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.011922E+00 | gshard_loss: 8.490325E-04 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 38317376.0 | params norm: 231.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 7328.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.010287E+00 | gshard_loss: 8.809493E-04 | loss scale: 65536.0 | grad norm: 0.274 | num zeros: 37813860.0 | params norm: 231.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        51712 | elapsed time per iteration (ms): 7475.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.003513E+00 | gshard_loss: 9.063432E-04 | loss scale: 65536.0 | grad norm: 0.266 | num zeros: 38231992.0 | params norm: 231.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        52224 | elapsed time per iteration (ms): 6730.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.993941E+00 | gshard_loss: 9.898118E-04 | loss scale: 65536.0 | grad norm: 0.275 | num zeros: 38221624.0 | params norm: 231.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        52736 | elapsed time per iteration (ms): 8271.1 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.978213E+00 | gshard_loss: 9.489494E-04 | loss scale: 65536.0 | grad norm: 0.264 | num zeros: 38185708.0 | params norm: 231.676 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        53248 | elapsed time per iteration (ms): 8784.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.981453E+00 | gshard_loss: 1.023133E-03 | loss scale: 65536.0 | grad norm: 0.438 | num zeros: 38206368.0 | params norm: 231.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        53760 | elapsed time per iteration (ms): 5798.1 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.985071E+00 | gshard_loss: 8.738431E-04 | loss scale: 65536.0 | grad norm: 0.618 | num zeros: 19871888.0 | params norm: 231.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        54272 | elapsed time per iteration (ms): 6530.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.963523E+00 | gshard_loss: 1.036424E-03 | loss scale: 65536.0 | grad norm: 0.353 | num zeros: 38276892.0 | params norm: 231.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        54784 | elapsed time per iteration (ms): 6137.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.960102E+00 | gshard_loss: 9.733323E-04 | loss scale: 65536.0 | grad norm: 0.304 | num zeros: 28385154.0 | params norm: 231.785 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        55296 | elapsed time per iteration (ms): 5968.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.943979E+00 | gshard_loss: 8.370539E-04 | loss scale: 65536.0 | grad norm: 0.515 | num zeros: 19391106.0 | params norm: 231.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        55808 | elapsed time per iteration (ms): 6398.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.935818E+00 | gshard_loss: 8.546215E-04 | loss scale: 65536.0 | grad norm: 0.393 | num zeros: 18926300.0 | params norm: 231.843 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        56320 | elapsed time per iteration (ms): 6930.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.926607E+00 | gshard_loss: 9.644924E-04 | loss scale: 65536.0 | grad norm: 0.359 | num zeros: 28347380.0 | params norm: 231.871 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        56832 | elapsed time per iteration (ms): 6100.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.921668E+00 | gshard_loss: 9.727668E-04 | loss scale: 65536.0 | grad norm: 0.281 | num zeros: 28763584.0 | params norm: 231.901 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        57344 | elapsed time per iteration (ms): 5998.1 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.906151E+00 | gshard_loss: 9.669085E-04 | loss scale: 65536.0 | grad norm: 0.355 | num zeros: 37774632.0 | params norm: 231.932 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        57856 | elapsed time per iteration (ms): 7600.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.903288E+00 | gshard_loss: 1.023301E-03 | loss scale: 65536.0 | grad norm: 0.286 | num zeros: 28374624.0 | params norm: 231.964 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        58368 | elapsed time per iteration (ms): 6836.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.896833E+00 | gshard_loss: 1.006592E-03 | loss scale: 65536.0 | grad norm: 0.193 | num zeros: 37775588.0 | params norm: 231.996 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        58880 | elapsed time per iteration (ms): 5891.7 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.897365E+00 | gshard_loss: 9.841386E-04 | loss scale: 65536.0 | grad norm: 0.320 | num zeros: 28790380.0 | params norm: 232.030 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        59392 | elapsed time per iteration (ms): 8201.2 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.876771E+00 | gshard_loss: 1.061930E-03 | loss scale: 65536.0 | grad norm: 0.374 | num zeros: 28334012.0 | params norm: 232.064 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        59904 | elapsed time per iteration (ms): 8183.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.869707E+00 | gshard_loss: 9.752583E-04 | loss scale: 65536.0 | grad norm: 0.292 | num zeros: 28337008.0 | params norm: 232.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        60416 | elapsed time per iteration (ms): 5540.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.869413E+00 | gshard_loss: 9.602377E-04 | loss scale: 65536.0 | grad norm: 0.449 | num zeros: 28331748.0 | params norm: 232.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        60928 | elapsed time per iteration (ms): 8735.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.852015E+00 | gshard_loss: 1.013468E-03 | loss scale: 65536.0 | grad norm: 0.374 | num zeros: 28332880.0 | params norm: 232.172 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        61440 | elapsed time per iteration (ms): 7638.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.845190E+00 | gshard_loss: 9.594378E-04 | loss scale: 65536.0 | grad norm: 0.222 | num zeros: 28333134.0 | params norm: 232.208 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        61952 | elapsed time per iteration (ms): 8451.1 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.831847E+00 | gshard_loss: 8.730108E-04 | loss scale: 65536.0 | grad norm: 0.488 | num zeros: 19333532.0 | params norm: 232.246 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        62464 | elapsed time per iteration (ms): 7612.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.834778E+00 | gshard_loss: 9.700955E-04 | loss scale: 65536.0 | grad norm: 0.387 | num zeros: 28361198.0 | params norm: 232.284 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        62976 | elapsed time per iteration (ms): 6642.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.812267E+00 | gshard_loss: 9.436881E-04 | loss scale: 65536.0 | grad norm: 0.250 | num zeros: 28768520.0 | params norm: 232.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        63488 | elapsed time per iteration (ms): 5747.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.821852E+00 | gshard_loss: 8.983354E-04 | loss scale: 65536.0 | grad norm: 0.539 | num zeros: 18894972.0 | params norm: 232.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        64000 | elapsed time per iteration (ms): 9305.4 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.799365E+00 | gshard_loss: 9.547632E-04 | loss scale: 65536.0 | grad norm: 0.264 | num zeros: 28335034.0 | params norm: 232.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        64512 | elapsed time per iteration (ms): 5666.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.793394E+00 | gshard_loss: 9.912921E-04 | loss scale: 65536.0 | grad norm: 0.304 | num zeros: 19867218.0 | params norm: 232.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        65024 | elapsed time per iteration (ms): 5549.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.787860E+00 | gshard_loss: 9.297115E-04 | loss scale: 65536.0 | grad norm: 0.349 | num zeros: 19320028.0 | params norm: 232.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        65536 | elapsed time per iteration (ms): 8060.9 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.786061E+00 | gshard_loss: 9.196888E-04 | loss scale: 65536.0 | grad norm: 0.386 | num zeros: 18893188.0 | params norm: 232.523 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        66048 | elapsed time per iteration (ms): 5776.7 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.770357E+00 | gshard_loss: 9.291991E-04 | loss scale: 65536.0 | grad norm: 0.274 | num zeros: 28359128.0 | params norm: 232.564 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        66560 | elapsed time per iteration (ms): 6067.1 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.767280E+00 | gshard_loss: 9.471079E-04 | loss scale: 65536.0 | grad norm: 0.297 | num zeros: 19344044.0 | params norm: 232.607 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        67072 | elapsed time per iteration (ms): 6471.4 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.771399E+00 | gshard_loss: 9.509817E-04 | loss scale: 65536.0 | grad norm: 0.446 | num zeros: 28337420.0 | params norm: 232.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        67584 | elapsed time per iteration (ms): 6634.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.742911E+00 | gshard_loss: 9.316154E-04 | loss scale: 65536.0 | grad norm: 0.269 | num zeros: 19352428.0 | params norm: 232.694 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        68096 | elapsed time per iteration (ms): 6606.0 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.736618E+00 | gshard_loss: 9.470129E-04 | loss scale: 65536.0 | grad norm: 0.265 | num zeros: 28789660.0 | params norm: 232.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        68608 | elapsed time per iteration (ms): 6472.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.739572E+00 | gshard_loss: 9.207440E-04 | loss scale: 65536.0 | grad norm: 0.264 | num zeros: 28371034.0 | params norm: 232.783 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        69120 | elapsed time per iteration (ms): 8046.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.729809E+00 | gshard_loss: 9.162404E-04 | loss scale: 65536.0 | grad norm: 0.279 | num zeros: 10965301.0 | params norm: 232.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        69632 | elapsed time per iteration (ms): 7338.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.728560E+00 | gshard_loss: 9.469235E-04 | loss scale: 65536.0 | grad norm: 0.279 | num zeros: 28796404.0 | params norm: 232.875 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        70144 | elapsed time per iteration (ms): 9789.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.710218E+00 | gshard_loss: 9.170428E-04 | loss scale: 65536.0 | grad norm: 0.284 | num zeros: 19378716.0 | params norm: 232.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        70656 | elapsed time per iteration (ms): 8256.7 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.705079E+00 | gshard_loss: 9.125427E-04 | loss scale: 65536.0 | grad norm: 0.371 | num zeros: 18907886.0 | params norm: 232.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        71168 | elapsed time per iteration (ms): 5688.4 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.694079E+00 | gshard_loss: 9.007182E-04 | loss scale: 65536.0 | grad norm: 0.300 | num zeros: 10014871.0 | params norm: 233.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        71680 | elapsed time per iteration (ms): 8892.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.692028E+00 | gshard_loss: 9.057545E-04 | loss scale: 65536.0 | grad norm: 0.313 | num zeros: 18943188.0 | params norm: 233.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        72192 | elapsed time per iteration (ms): 6066.2 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.670089E+00 | gshard_loss: 8.656600E-04 | loss scale: 65536.0 | grad norm: 0.243 | num zeros: 9944247.0 | params norm: 233.107 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        72704 | elapsed time per iteration (ms): 6432.4 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.673295E+00 | gshard_loss: 8.506896E-04 | loss scale: 65536.0 | grad norm: 0.287 | num zeros: 18893492.0 | params norm: 233.153 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        73216 | elapsed time per iteration (ms): 8539.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.658031E+00 | gshard_loss: 8.555353E-04 | loss scale: 65536.0 | grad norm: 0.268 | num zeros: 512671.0 | params norm: 233.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        73728 | elapsed time per iteration (ms): 5640.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.651248E+00 | gshard_loss: 8.754801E-04 | loss scale: 65536.0 | grad norm: 0.239 | num zeros: 10032988.0 | params norm: 233.246 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        74240 | elapsed time per iteration (ms): 6429.6 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.650080E+00 | gshard_loss: 8.494582E-04 | loss scale: 65536.0 | grad norm: 0.315 | num zeros: 9510760.0 | params norm: 233.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        74752 | elapsed time per iteration (ms): 5603.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.645535E+00 | gshard_loss: 8.636640E-04 | loss scale: 65536.0 | grad norm: 0.354 | num zeros: 9501184.0 | params norm: 233.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        75264 | elapsed time per iteration (ms): 5939.1 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.654925E+00 | gshard_loss: 8.232212E-04 | loss scale: 65536.0 | grad norm: 0.484 | num zeros: 9456973.0 | params norm: 233.386 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        75776 | elapsed time per iteration (ms): 6146.4 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.624824E+00 | gshard_loss: 8.581499E-04 | loss scale: 65536.0 | grad norm: 0.388 | num zeros: 9918224.0 | params norm: 233.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        76288 | elapsed time per iteration (ms): 7614.7 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.628682E+00 | gshard_loss: 8.353622E-04 | loss scale: 65536.0 | grad norm: 0.224 | num zeros: 18894688.0 | params norm: 233.479 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        76800 | elapsed time per iteration (ms): 5089.8 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.618733E+00 | gshard_loss: 8.077341E-04 | loss scale: 65536.0 | grad norm: 0.418 | num zeros: 9460580.0 | params norm: 233.527 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        77312 | elapsed time per iteration (ms): 8664.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.609313E+00 | gshard_loss: 8.433204E-04 | loss scale: 65536.0 | grad norm: 0.383 | num zeros: 18892860.0 | params norm: 233.574 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        77824 | elapsed time per iteration (ms): 5639.7 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.605217E+00 | gshard_loss: 8.259312E-04 | loss scale: 65536.0 | grad norm: 0.346 | num zeros: 18892812.0 | params norm: 233.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        78336 | elapsed time per iteration (ms): 5982.6 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.605379E+00 | gshard_loss: 8.148538E-04 | loss scale: 65536.0 | grad norm: 0.362 | num zeros: 9974053.0 | params norm: 233.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        78848 | elapsed time per iteration (ms): 9178.3 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.590887E+00 | gshard_loss: 8.363138E-04 | loss scale: 65536.0 | grad norm: 0.363 | num zeros: 9940621.0 | params norm: 233.717 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        79360 | elapsed time per iteration (ms): 7054.0 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.580970E+00 | gshard_loss: 8.304942E-04 | loss scale: 65536.0 | grad norm: 0.296 | num zeros: 9951717.0 | params norm: 233.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        79872 | elapsed time per iteration (ms): 5521.6 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.580131E+00 | gshard_loss: 8.062119E-04 | loss scale: 65536.0 | grad norm: 0.329 | num zeros: 9454071.0 | params norm: 233.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        80384 | elapsed time per iteration (ms): 7495.0 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.569473E+00 | gshard_loss: 8.352272E-04 | loss scale: 65536.0 | grad norm: 0.301 | num zeros: 9450954.0 | params norm: 233.861 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        80896 | elapsed time per iteration (ms): 5949.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.559634E+00 | gshard_loss: 8.323177E-04 | loss scale: 65536.0 | grad norm: 0.213 | num zeros: 9460938.0 | params norm: 233.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        81408 | elapsed time per iteration (ms): 5379.7 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.567787E+00 | gshard_loss: 8.131160E-04 | loss scale: 65536.0 | grad norm: 0.394 | num zeros: 680125.0 | params norm: 233.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        81920 | elapsed time per iteration (ms): 4841.5 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.559486E+00 | gshard_loss: 8.181609E-04 | loss scale: 65536.0 | grad norm: 0.213 | num zeros: 18897348.0 | params norm: 234.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        82432 | elapsed time per iteration (ms): 5951.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.550216E+00 | gshard_loss: 8.195107E-04 | loss scale: 65536.0 | grad norm: 0.247 | num zeros: 9906411.0 | params norm: 234.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        82944 | elapsed time per iteration (ms): 4906.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.548910E+00 | gshard_loss: 8.363957E-04 | loss scale: 65536.0 | grad norm: 0.276 | num zeros: 9515453.0 | params norm: 234.106 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        83456 | elapsed time per iteration (ms): 7269.7 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.531001E+00 | gshard_loss: 8.273752E-04 | loss scale: 65536.0 | grad norm: 0.249 | num zeros: 9514208.0 | params norm: 234.155 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        83968 | elapsed time per iteration (ms): 7462.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.534606E+00 | gshard_loss: 8.143926E-04 | loss scale: 65536.0 | grad norm: 0.254 | num zeros: 18897068.0 | params norm: 234.205 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        84480 | elapsed time per iteration (ms): 6741.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.521285E+00 | gshard_loss: 8.168725E-04 | loss scale: 65536.0 | grad norm: 0.196 | num zeros: 18893412.0 | params norm: 234.255 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        84992 | elapsed time per iteration (ms): 5742.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.523190E+00 | gshard_loss: 8.248556E-04 | loss scale: 65536.0 | grad norm: 0.265 | num zeros: 9968964.0 | params norm: 234.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        85504 | elapsed time per iteration (ms): 11873.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.515380E+00 | gshard_loss: 8.222675E-04 | loss scale: 65536.0 | grad norm: 0.247 | num zeros: 18939156.0 | params norm: 234.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        86016 | elapsed time per iteration (ms): 5479.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.505576E+00 | gshard_loss: 8.323999E-04 | loss scale: 65536.0 | grad norm: 0.267 | num zeros: 18952778.0 | params norm: 234.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        86528 | elapsed time per iteration (ms): 5223.3 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.496033E+00 | gshard_loss: 8.192043E-04 | loss scale: 65536.0 | grad norm: 0.320 | num zeros: 18946970.0 | params norm: 234.458 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        87040 | elapsed time per iteration (ms): 8202.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.483830E+00 | gshard_loss: 8.402342E-04 | loss scale: 65536.0 | grad norm: 0.211 | num zeros: 18894676.0 | params norm: 234.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        87552 | elapsed time per iteration (ms): 6513.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.484811E+00 | gshard_loss: 8.312561E-04 | loss scale: 65536.0 | grad norm: 0.244 | num zeros: 18892662.0 | params norm: 234.561 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        88064 | elapsed time per iteration (ms): 6456.9 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.490743E+00 | gshard_loss: 8.322431E-04 | loss scale: 65536.0 | grad norm: 0.269 | num zeros: 18898730.0 | params norm: 234.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        88576 | elapsed time per iteration (ms): 5614.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.477448E+00 | gshard_loss: 8.276250E-04 | loss scale: 65536.0 | grad norm: 0.218 | num zeros: 9997793.0 | params norm: 234.665 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        89088 | elapsed time per iteration (ms): 5087.5 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.459223E+00 | gshard_loss: 8.203964E-04 | loss scale: 65536.0 | grad norm: 0.190 | num zeros: 10041787.0 | params norm: 234.718 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        89600 | elapsed time per iteration (ms): 6448.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.465910E+00 | gshard_loss: 8.267063E-04 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 18895128.0 | params norm: 234.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        90112 | elapsed time per iteration (ms): 5923.9 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.463214E+00 | gshard_loss: 8.248310E-04 | loss scale: 65536.0 | grad norm: 0.256 | num zeros: 19438076.0 | params norm: 234.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        90624 | elapsed time per iteration (ms): 5294.1 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.457497E+00 | gshard_loss: 8.311811E-04 | loss scale: 65536.0 | grad norm: 0.255 | num zeros: 19459500.0 | params norm: 234.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        91136 | elapsed time per iteration (ms): 7138.2 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.451884E+00 | gshard_loss: 8.328722E-04 | loss scale: 65536.0 | grad norm: 0.445 | num zeros: 19347796.0 | params norm: 234.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        91648 | elapsed time per iteration (ms): 6003.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.465859E+00 | gshard_loss: 8.477325E-04 | loss scale: 65536.0 | grad norm: 0.695 | num zeros: 9953190.0 | params norm: 234.984 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        92160 | elapsed time per iteration (ms): 4838.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.436224E+00 | gshard_loss: 7.983466E-04 | loss scale: 65536.0 | grad norm: 0.448 | num zeros: 18944492.0 | params norm: 235.037 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        92672 | elapsed time per iteration (ms): 5677.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.436924E+00 | gshard_loss: 8.134435E-04 | loss scale: 65536.0 | grad norm: 0.407 | num zeros: 19364480.0 | params norm: 235.091 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        93184 | elapsed time per iteration (ms): 4612.6 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.441073E+00 | gshard_loss: 8.469402E-04 | loss scale: 65536.0 | grad norm: 0.470 | num zeros: 19350370.0 | params norm: 235.144 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        93696 | elapsed time per iteration (ms): 5309.3 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.436385E+00 | gshard_loss: 8.359883E-04 | loss scale: 65536.0 | grad norm: 0.257 | num zeros: 9945814.0 | params norm: 235.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        94208 | elapsed time per iteration (ms): 7513.0 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.424006E+00 | gshard_loss: 8.116924E-04 | loss scale: 65536.0 | grad norm: 0.477 | num zeros: 9487851.0 | params norm: 235.252 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        94720 | elapsed time per iteration (ms): 5445.4 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.419522E+00 | gshard_loss: 8.163328E-04 | loss scale: 65536.0 | grad norm: 0.222 | num zeros: 9981475.0 | params norm: 235.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        95232 | elapsed time per iteration (ms): 5957.4 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.415087E+00 | gshard_loss: 8.438928E-04 | loss scale: 65536.0 | grad norm: 0.343 | num zeros: 9580172.0 | params norm: 235.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        95744 | elapsed time per iteration (ms): 7374.7 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.415966E+00 | gshard_loss: 8.239088E-04 | loss scale: 65536.0 | grad norm: 0.285 | num zeros: 18897150.0 | params norm: 235.414 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        96256 | elapsed time per iteration (ms): 8979.0 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.408207E+00 | gshard_loss: 8.275575E-04 | loss scale: 65536.0 | grad norm: 0.307 | num zeros: 18894528.0 | params norm: 235.469 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        96768 | elapsed time per iteration (ms): 6976.6 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.409189E+00 | gshard_loss: 8.414641E-04 | loss scale: 65536.0 | grad norm: 0.254 | num zeros: 9971791.0 | params norm: 235.523 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        97280 | elapsed time per iteration (ms): 6694.8 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.404404E+00 | gshard_loss: 8.315729E-04 | loss scale: 65536.0 | grad norm: 0.220 | num zeros: 9533855.0 | params norm: 235.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        97792 | elapsed time per iteration (ms): 5135.3 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.398204E+00 | gshard_loss: 8.409452E-04 | loss scale: 65536.0 | grad norm: 0.262 | num zeros: 9886379.0 | params norm: 235.632 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        98304 | elapsed time per iteration (ms): 4885.7 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.387927E+00 | gshard_loss: 8.210899E-04 | loss scale: 65536.0 | grad norm: 0.242 | num zeros: 9999700.0 | params norm: 235.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        98816 | elapsed time per iteration (ms): 5309.1 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.377254E+00 | gshard_loss: 8.094582E-04 | loss scale: 65536.0 | grad norm: 0.205 | num zeros: 471266.0 | params norm: 235.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        99328 | elapsed time per iteration (ms): 6049.7 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.368445E+00 | gshard_loss: 8.129538E-04 | loss scale: 65536.0 | grad norm: 0.238 | num zeros: 1014741.0 | params norm: 235.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        99840 | elapsed time per iteration (ms): 8422.1 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.372803E+00 | gshard_loss: 8.163966E-04 | loss scale: 65536.0 | grad norm: 0.219 | num zeros: 10053230.0 | params norm: 235.850 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:       100352 | elapsed time per iteration (ms): 5210.1 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.360019E+00 | gshard_loss: 8.204856E-04 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 9977624.0 | params norm: 235.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:       100864 | elapsed time per iteration (ms): 5367.2 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.378194E+00 | gshard_loss: 8.188902E-04 | loss scale: 65536.0 | grad norm: 0.185 | num zeros: 10547588.0 | params norm: 235.960 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:       101376 | elapsed time per iteration (ms): 5454.6 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.354719E+00 | gshard_loss: 8.150648E-04 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 10484201.0 | params norm: 236.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:       101888 | elapsed time per iteration (ms): 5910.7 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.355121E+00 | gshard_loss: 8.138943E-04 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 9527384.0 | params norm: 236.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:       102400 | elapsed time per iteration (ms): 7081.3 | learning rate: 9.990E-05 | global batch size:   512 | lm loss: 6.351338E+00 | gshard_loss: 8.112881E-04 | loss scale: 65536.0 | grad norm: 0.157 | num zeros: 73115.0 | params norm: 236.125 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-01-04 22:23:52 
