using world size: 64, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 4 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... True
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 2
  expert_ep_size .................................. 8
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 8
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 4
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 102400
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 64
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 32
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 4
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0, 8]
[INFO] 8 in DP group [0, 8]
[INFO] 1 in DP group [1, 9]
[INFO] 9 in DP group [1, 9]
[INFO] 2 in DP group [2, 10]
[INFO] 10 in DP group [2, 10]
[INFO] 3 in DP group [3, 11]
[INFO] 11 in DP group [3, 11]
[INFO] 4 in DP group [4, 12]
[INFO] 12 in DP group [4, 12]
[INFO] 5 in DP group [5, 13]
[INFO] 13 in DP group [5, 13]
[INFO] 6 in DP group [6, 14]
[INFO] 14 in DP group [6, 14]
[INFO] 7 in DP group [7, 15]
[INFO] 15 in DP group [7, 15]
[INFO] 16 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 17 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 18 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 19 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 20 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 22 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 23 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 21 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 24 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 28 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 31 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 29 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 30 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 25 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 27 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 26 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 16 in DP group [16, 24]
[INFO] 24 in DP group [16, 24]
[INFO] 17 in DP group [17, 25]
[INFO] 25 in DP group [17, 25]
[INFO] 18 in DP group [18, 26]
[INFO] 26 in DP group [18, 26]
[INFO] 19 in DP group [19, 27]
[INFO] 27 in DP group [19, 27]
[INFO] 20 in DP group [20, 28]
[INFO] 28 in DP group [20, 28]
[INFO] 21 in DP group [21, 29]
[INFO] 29 in DP group [21, 29]
[INFO] 22 in DP group [22, 30]
[INFO] 30 in DP group [22, 30]
[INFO] 23 in DP group [23, 31]
[INFO] 31 in DP group [23, 31]
[INFO] 32 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 37 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 39 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 38 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 34 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 35 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 36 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 33 in EP group [32, 33, 34, 35, 36, 37, 38, 39]
[INFO] 40 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 41 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 42 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 44 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 45 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 46 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 47 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 43 in EP group [40, 41, 42, 43, 44, 45, 46, 47]
[INFO] 32 in DP group [32, 40]
[INFO] 40 in DP group [32, 40]
[INFO] 33 in DP group [33, 41]
[INFO] 41 in DP group [33, 41]
[INFO] 34 in DP group [34, 42]
[INFO] 42 in DP group [34, 42]
[INFO] 35 in DP group [35, 43]
[INFO] 43 in DP group [35, 43]
[INFO] 36 in DP group [36, 44]
[INFO] 44 in DP group [36, 44]
[INFO] 37 in DP group [37, 45]
[INFO] 45 in DP group [37, 45]
[INFO] 38 in DP group [38, 46]
[INFO] 46 in DP group [38, 46]
[INFO] 39 in DP group [39, 47]
[INFO] 47 in DP group [39, 47]
[INFO] 48 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 51 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 54 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 49 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 50 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 53 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 55 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 52 in EP group [48, 49, 50, 51, 52, 53, 54, 55]
[INFO] 56 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 60 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 61 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 62 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 63 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 59 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 58 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 57 in EP group [56, 57, 58, 59, 60, 61, 62, 63]
[INFO] 48 in DP group [48, 56]
[INFO] 56 in DP group [48, 56]
[INFO] 49 in DP group [49, 57]
[INFO] 57 in DP group [49, 57]
[INFO] 50 in DP group [50, 58]
[INFO] 58 in DP group [50, 58]
[INFO] 51 in DP group [51, 59]
[INFO] 59 in DP group [51, 59]
[INFO] 52 in DP group [52, 60]
[INFO] 60 in DP group [52, 60]
[INFO] 53 in DP group [53, 61]
[INFO] 61 in DP group [53, 61]
[INFO] 54 in DP group [54, 62]
[INFO] 62 in DP group [54, 62]
[W ProcessGroupNCCL.cpp:1569] Rank 14 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 62 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 49 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 8 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 56 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 9 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 11 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 57 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 59 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 39 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 38 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 26 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 27 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 41 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 20 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 24 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 25 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 22 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 30 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 13 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 3 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 48 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 44 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 42 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 23 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 51 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 47 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 45 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 12 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 43 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 40 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 4 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 2 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 5 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 6 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 7 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 21 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[INFO] 55 in DP group [55, 63]
[W ProcessGroupNCCL.cpp:1569] Rank 35 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 46 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 16 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 34 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 31 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 29 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> setting random seeds to 1234 ...
[W ProcessGroupNCCL.cpp:1569] Rank 36 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 60 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 58 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[INFO] 63 in DP group [55, 63]
[W ProcessGroupNCCL.cpp:1569] Rank 61 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 28 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 55 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 15 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 53 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 37 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 54 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 17 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 52 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 18 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 10 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 50 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[W ProcessGroupNCCL.cpp:1569] Rank 33 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 63 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> compiling dataset index builder ...
[W ProcessGroupNCCL.cpp:1569] Rank 19 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 32 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
make: 进入目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
make: 对“default”无需做任何事。
make: 离开目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
>>> done with dataset index builder. Compilation time: 0.203 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
>>> done with compiling and loading fused kernels. Compilation time: 14.428 seconds
time to initialize megatron (seconds): 25.199
[after megatron is initialized] datetime: 2023-01-04 22:56:25 
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
building GPT model ...
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
hhs=3072
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 340328704
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 340328704
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 417598720
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 419168512
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-04 22:56:33 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: -512
    test:       -512
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.067172 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.208 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-04 22:56:47 
done with setup ...
training ...
[before the start of training step] datetime: 2023-01-04 22:56:47 
 iteration        1/     200 | consumed samples:          512 | elapsed time per iteration (ms): 20247.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.082732E+01 | gshard_loss: 3.962678E-03 | loss scale: 131072.0 | grad norm: 11.838 | num zeros: 140518.0 | params norm: 233.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 8005.62109375 | max allocated: 8005.62353515625 | reserved: 8984.0 | max reserved: 8984.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 6510.751953125 | max allocated: 6510.75439453125 | reserved: 7214.0 | max reserved: 7214.0
[Rank 48] (after 1 iterations) memory (MB) | allocated: 8175.919921875 | max allocated: 8175.95068359375 | reserved: 8330.0 | max reserved: 8330.0
[Rank 32] (after 1 iterations) memory (MB) | allocated: 6509.712890625 | max allocated: 6509.71533203125 | reserved: 7054.0 | max reserved: 7054.0
 iteration        2/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 10150.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.050322E+01 | gshard_loss: 1.285371E-02 | loss scale: 131072.0 | grad norm: 7.404 | num zeros: 626255360.0 | params norm: 233.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 9475.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.035546E+01 | gshard_loss: 1.277557E-02 | loss scale: 131072.0 | grad norm: 5.252 | num zeros: 569157952.0 | params norm: 233.121 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 11102.2 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 131072.0 | params norm: 233.121 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 9878.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.147622E+01 | gshard_loss: 1.055488E-02 | loss scale: 131072.0 | grad norm: 3.515 | num zeros: 222335216.0 | params norm: 233.158 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 11412.8 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 65536.0 | params norm: 233.158 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 8988.3 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 32768.0 | params norm: 233.158 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 8886.6 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 16384.0 | params norm: 233.158 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 9105.7 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 8192.0 | params norm: 233.158 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 16384.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.005605E+01 | gshard_loss: 8.393673E-03 | loss scale: 8192.0 | grad norm: 14.704 | num zeros: 451032384.0 | params norm: 233.203 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 8503.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.104408E+01 | gshard_loss: 7.632088E-03 | loss scale: 8192.0 | grad norm: 112.093 | num zeros: 191416048.0 | params norm: 233.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 8082.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.901169E+00 | gshard_loss: 1.633639E-02 | loss scale: 8192.0 | grad norm: 13.593 | num zeros: 439328000.0 | params norm: 233.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 8042.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.792048E+00 | gshard_loss: 1.101025E-02 | loss scale: 8192.0 | grad norm: 14.889 | num zeros: 872411008.0 | params norm: 233.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 10845.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.674272E+00 | gshard_loss: 9.879014E-03 | loss scale: 8192.0 | grad norm: 13.379 | num zeros: 678679744.0 | params norm: 233.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 8565.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.708018E+00 | gshard_loss: 1.714979E-02 | loss scale: 8192.0 | grad norm: 34.830 | num zeros: 299579360.0 | params norm: 233.459 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 8071.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.470037E+00 | gshard_loss: 1.032184E-02 | loss scale: 8192.0 | grad norm: 13.166 | num zeros: 747659584.0 | params norm: 233.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 8848.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.355470E+00 | gshard_loss: 9.203829E-03 | loss scale: 8192.0 | grad norm: 13.114 | num zeros: 902666240.0 | params norm: 233.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 8511.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.231116E+00 | gshard_loss: 8.445960E-03 | loss scale: 8192.0 | grad norm: 13.146 | num zeros: 915690880.0 | params norm: 233.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 7540.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.111556E+00 | gshard_loss: 7.870347E-03 | loss scale: 8192.0 | grad norm: 13.056 | num zeros: 921105344.0 | params norm: 233.677 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 9795.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.992866E+00 | gshard_loss: 7.493284E-03 | loss scale: 8192.0 | grad norm: 13.035 | num zeros: 794234112.0 | params norm: 233.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 7665.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.889741E+00 | gshard_loss: 7.493856E-03 | loss scale: 8192.0 | grad norm: 13.028 | num zeros: 806907520.0 | params norm: 233.786 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 6790.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.770535E+00 | gshard_loss: 7.739298E-03 | loss scale: 8192.0 | grad norm: 12.955 | num zeros: 817673280.0 | params norm: 233.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 9071.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.655119E+00 | gshard_loss: 7.283972E-03 | loss scale: 8192.0 | grad norm: 12.891 | num zeros: 798756288.0 | params norm: 233.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 7730.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.546946E+00 | gshard_loss: 7.208672E-03 | loss scale: 8192.0 | grad norm: 12.812 | num zeros: 819708544.0 | params norm: 233.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 8470.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.447415E+00 | gshard_loss: 6.796256E-03 | loss scale: 8192.0 | grad norm: 12.777 | num zeros: 827178752.0 | params norm: 234.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 7999.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.348467E+00 | gshard_loss: 6.402952E-03 | loss scale: 8192.0 | grad norm: 12.745 | num zeros: 757210304.0 | params norm: 234.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 7438.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.260739E+00 | gshard_loss: 6.097488E-03 | loss scale: 8192.0 | grad norm: 12.627 | num zeros: 743293056.0 | params norm: 234.119 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 7704.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.175395E+00 | gshard_loss: 6.190101E-03 | loss scale: 8192.0 | grad norm: 12.394 | num zeros: 596866368.0 | params norm: 234.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 7852.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.087387E+00 | gshard_loss: 6.390785E-03 | loss scale: 8192.0 | grad norm: 12.287 | num zeros: 708108480.0 | params norm: 234.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 7906.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.008039E+00 | gshard_loss: 6.456117E-03 | loss scale: 8192.0 | grad norm: 12.042 | num zeros: 820069952.0 | params norm: 234.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 6908.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.942807E+00 | gshard_loss: 6.726620E-03 | loss scale: 8192.0 | grad norm: 11.845 | num zeros: 534155776.0 | params norm: 234.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 7437.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.856166E+00 | gshard_loss: 6.637713E-03 | loss scale: 8192.0 | grad norm: 11.585 | num zeros: 752425600.0 | params norm: 234.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 7704.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.787758E+00 | gshard_loss: 6.457299E-03 | loss scale: 8192.0 | grad norm: 11.250 | num zeros: 801083904.0 | params norm: 234.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 7410.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.721712E+00 | gshard_loss: 6.167673E-03 | loss scale: 8192.0 | grad norm: 10.780 | num zeros: 792063104.0 | params norm: 234.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 7275.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.657884E+00 | gshard_loss: 6.270058E-03 | loss scale: 8192.0 | grad norm: 10.321 | num zeros: 809682560.0 | params norm: 234.550 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 8222.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.605172E+00 | gshard_loss: 6.347049E-03 | loss scale: 8192.0 | grad norm: 9.718 | num zeros: 819255104.0 | params norm: 234.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 8608.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.556144E+00 | gshard_loss: 6.358183E-03 | loss scale: 8192.0 | grad norm: 9.064 | num zeros: 821252224.0 | params norm: 234.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 7403.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.504379E+00 | gshard_loss: 6.147908E-03 | loss scale: 8192.0 | grad norm: 8.317 | num zeros: 817344384.0 | params norm: 234.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 6923.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.472532E+00 | gshard_loss: 6.036908E-03 | loss scale: 8192.0 | grad norm: 7.374 | num zeros: 809391488.0 | params norm: 234.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 7560.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.434809E+00 | gshard_loss: 5.940760E-03 | loss scale: 8192.0 | grad norm: 6.440 | num zeros: 762265344.0 | params norm: 234.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 8510.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.406219E+00 | gshard_loss: 5.951453E-03 | loss scale: 8192.0 | grad norm: 5.457 | num zeros: 770742400.0 | params norm: 234.856 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 9056.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.390367E+00 | gshard_loss: 5.871230E-03 | loss scale: 8192.0 | grad norm: 4.138 | num zeros: 754108864.0 | params norm: 234.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 6533.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.373082E+00 | gshard_loss: 5.590974E-03 | loss scale: 8192.0 | grad norm: 2.898 | num zeros: 745726912.0 | params norm: 234.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 6637.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.360199E+00 | gshard_loss: 5.473558E-03 | loss scale: 8192.0 | grad norm: 1.683 | num zeros: 710656384.0 | params norm: 235.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 6579.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.357365E+00 | gshard_loss: 5.322147E-03 | loss scale: 8192.0 | grad norm: 1.017 | num zeros: 713307904.0 | params norm: 235.055 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 8374.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.347080E+00 | gshard_loss: 5.229156E-03 | loss scale: 8192.0 | grad norm: 1.165 | num zeros: 683117696.0 | params norm: 235.107 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 8504.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.339609E+00 | gshard_loss: 5.269598E-03 | loss scale: 8192.0 | grad norm: 1.566 | num zeros: 640826624.0 | params norm: 235.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 6607.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.344314E+00 | gshard_loss: 5.357919E-03 | loss scale: 8192.0 | grad norm: 1.976 | num zeros: 586786752.0 | params norm: 235.216 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 6433.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.341588E+00 | gshard_loss: 5.615636E-03 | loss scale: 8192.0 | grad norm: 1.997 | num zeros: 528308224.0 | params norm: 235.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 7765.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.337311E+00 | gshard_loss: 5.759561E-03 | loss scale: 8192.0 | grad norm: 1.950 | num zeros: 523418432.0 | params norm: 235.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 6350.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.345782E+00 | gshard_loss: 6.022481E-03 | loss scale: 8192.0 | grad norm: 11.645 | num zeros: 336859328.0 | params norm: 235.384 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 7458.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.333554E+00 | gshard_loss: 6.048118E-03 | loss scale: 8192.0 | grad norm: 2.250 | num zeros: 333092384.0 | params norm: 235.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 6698.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.331686E+00 | gshard_loss: 5.983692E-03 | loss scale: 8192.0 | grad norm: 1.380 | num zeros: 268063520.0 | params norm: 235.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 8071.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.334710E+00 | gshard_loss: 5.934355E-03 | loss scale: 8192.0 | grad norm: 1.144 | num zeros: 240914912.0 | params norm: 235.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 6291.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.314114E+00 | gshard_loss: 5.949038E-03 | loss scale: 8192.0 | grad norm: 1.019 | num zeros: 242866848.0 | params norm: 235.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 6957.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.296749E+00 | gshard_loss: 6.017966E-03 | loss scale: 8192.0 | grad norm: 1.228 | num zeros: 240666688.0 | params norm: 235.666 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 9300.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.290279E+00 | gshard_loss: 6.129283E-03 | loss scale: 8192.0 | grad norm: 8.568 | num zeros: 166775808.0 | params norm: 235.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 7545.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.272876E+00 | gshard_loss: 6.720491E-03 | loss scale: 8192.0 | grad norm: 2.836 | num zeros: 164870272.0 | params norm: 235.786 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 11086.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.271188E+00 | gshard_loss: 6.649816E-03 | loss scale: 8192.0 | grad norm: 1.592 | num zeros: 192921888.0 | params norm: 235.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 8850.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.259302E+00 | gshard_loss: 6.166406E-03 | loss scale: 8192.0 | grad norm: 2.671 | num zeros: 262174144.0 | params norm: 235.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 9671.0 | learning rate: 9.999E-05 | global batch size:   512 | loss scale: 4096.0 | params norm: 235.909 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 9871.9 | learning rate: 9.999E-05 | global batch size:   512 | loss scale: 2048.0 | params norm: 235.909 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 8188.9 | learning rate: 9.999E-05 | global batch size:   512 | loss scale: 1024.0 | params norm: 235.909 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 6547.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 9.378657E+00 | gshard_loss: 9.467917E-03 | loss scale: 1024.0 | grad norm: 139.785 | num zeros: 52354616.0 | params norm: 235.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 6322.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.357936E+00 | gshard_loss: 7.630612E-03 | loss scale: 1024.0 | grad norm: 4.834 | num zeros: 353248128.0 | params norm: 236.024 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 6484.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.336519E+00 | gshard_loss: 7.802796E-03 | loss scale: 1024.0 | grad norm: 4.176 | num zeros: 282423072.0 | params norm: 236.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 7042.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.330615E+00 | gshard_loss: 7.953219E-03 | loss scale: 1024.0 | grad norm: 2.239 | num zeros: 348873888.0 | params norm: 236.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 7157.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.307044E+00 | gshard_loss: 7.899583E-03 | loss scale: 1024.0 | grad norm: 1.124 | num zeros: 319453696.0 | params norm: 236.177 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 6846.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.314476E+00 | gshard_loss: 8.029177E-03 | loss scale: 1024.0 | grad norm: 3.029 | num zeros: 322062528.0 | params norm: 236.227 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 7824.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.314334E+00 | gshard_loss: 8.234009E-03 | loss scale: 1024.0 | grad norm: 3.900 | num zeros: 259966176.0 | params norm: 236.278 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 8769.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.318398E+00 | gshard_loss: 9.986683E-03 | loss scale: 1024.0 | grad norm: 5.469 | num zeros: 252781904.0 | params norm: 236.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 6284.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.311713E+00 | gshard_loss: 8.603005E-03 | loss scale: 1024.0 | grad norm: 3.304 | num zeros: 316172896.0 | params norm: 236.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 6407.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.279510E+00 | gshard_loss: 8.164745E-03 | loss scale: 1024.0 | grad norm: 2.009 | num zeros: 348322688.0 | params norm: 236.418 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 6239.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.253201E+00 | gshard_loss: 7.411843E-03 | loss scale: 1024.0 | grad norm: 1.275 | num zeros: 351929600.0 | params norm: 236.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 6461.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.256867E+00 | gshard_loss: 6.893592E-03 | loss scale: 1024.0 | grad norm: 3.072 | num zeros: 342218560.0 | params norm: 236.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 6997.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.240909E+00 | gshard_loss: 7.036692E-03 | loss scale: 1024.0 | grad norm: 2.523 | num zeros: 396168288.0 | params norm: 236.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 6419.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.222435E+00 | gshard_loss: 7.456467E-03 | loss scale: 1024.0 | grad norm: 2.244 | num zeros: 487580192.0 | params norm: 236.600 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 7143.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.217731E+00 | gshard_loss: 7.587865E-03 | loss scale: 1024.0 | grad norm: 2.234 | num zeros: 521184704.0 | params norm: 236.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 6163.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.208400E+00 | gshard_loss: 7.378690E-03 | loss scale: 1024.0 | grad norm: 1.833 | num zeros: 544967424.0 | params norm: 236.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 8306.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.203205E+00 | gshard_loss: 7.260593E-03 | loss scale: 1024.0 | grad norm: 2.020 | num zeros: 581068096.0 | params norm: 236.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 6363.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.212228E+00 | gshard_loss: 8.203250E-03 | loss scale: 1024.0 | grad norm: 2.173 | num zeros: 517039328.0 | params norm: 236.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 6997.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.199526E+00 | gshard_loss: 7.675793E-03 | loss scale: 1024.0 | grad norm: 1.365 | num zeros: 534266240.0 | params norm: 236.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 6555.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.189438E+00 | gshard_loss: 7.421619E-03 | loss scale: 1024.0 | grad norm: 2.287 | num zeros: 548602304.0 | params norm: 236.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 6648.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.169965E+00 | gshard_loss: 8.530352E-03 | loss scale: 1024.0 | grad norm: 1.146 | num zeros: 552979072.0 | params norm: 236.890 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 6938.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.166923E+00 | gshard_loss: 8.957799E-03 | loss scale: 1024.0 | grad norm: 1.030 | num zeros: 532454848.0 | params norm: 236.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 8197.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.162038E+00 | gshard_loss: 8.020810E-03 | loss scale: 1024.0 | grad norm: 1.273 | num zeros: 520851328.0 | params norm: 236.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 7962.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.146522E+00 | gshard_loss: 7.570656E-03 | loss scale: 1024.0 | grad norm: 1.236 | num zeros: 539844160.0 | params norm: 237.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 6454.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.147437E+00 | gshard_loss: 7.971801E-03 | loss scale: 1024.0 | grad norm: 1.394 | num zeros: 536023744.0 | params norm: 237.059 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 8176.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.138650E+00 | gshard_loss: 7.583963E-03 | loss scale: 1024.0 | grad norm: 1.202 | num zeros: 525467584.0 | params norm: 237.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 7387.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.130360E+00 | gshard_loss: 6.950846E-03 | loss scale: 1024.0 | grad norm: 2.890 | num zeros: 539054720.0 | params norm: 237.144 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 6947.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.149009E+00 | gshard_loss: 7.311261E-03 | loss scale: 1024.0 | grad norm: 2.295 | num zeros: 528998560.0 | params norm: 237.184 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 7042.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.149143E+00 | gshard_loss: 6.819841E-03 | loss scale: 1024.0 | grad norm: 5.444 | num zeros: 519885312.0 | params norm: 237.223 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 6331.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.127397E+00 | gshard_loss: 7.387046E-03 | loss scale: 1024.0 | grad norm: 1.000 | num zeros: 484012032.0 | params norm: 237.263 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 7005.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.141304E+00 | gshard_loss: 7.563234E-03 | loss scale: 1024.0 | grad norm: 2.705 | num zeros: 501367232.0 | params norm: 237.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 8820.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.120937E+00 | gshard_loss: 6.998147E-03 | loss scale: 1024.0 | grad norm: 1.101 | num zeros: 509438496.0 | params norm: 237.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 6443.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.127049E+00 | gshard_loss: 6.634218E-03 | loss scale: 1024.0 | grad norm: 2.340 | num zeros: 489314720.0 | params norm: 237.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 8093.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.107531E+00 | gshard_loss: 7.144889E-03 | loss scale: 1024.0 | grad norm: 1.213 | num zeros: 459620800.0 | params norm: 237.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 6276.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.106448E+00 | gshard_loss: 7.829452E-03 | loss scale: 1024.0 | grad norm: 2.321 | num zeros: 417109632.0 | params norm: 237.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 6261.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.098117E+00 | gshard_loss: 7.735809E-03 | loss scale: 1024.0 | grad norm: 1.375 | num zeros: 402685312.0 | params norm: 237.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 7728.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.100841E+00 | gshard_loss: 7.234816E-03 | loss scale: 1024.0 | grad norm: 2.450 | num zeros: 390161600.0 | params norm: 237.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        51712 | elapsed time per iteration (ms): 6112.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.102920E+00 | gshard_loss: 7.429673E-03 | loss scale: 1024.0 | grad norm: 1.381 | num zeros: 366253056.0 | params norm: 237.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        52224 | elapsed time per iteration (ms): 6261.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.096698E+00 | gshard_loss: 7.483328E-03 | loss scale: 1024.0 | grad norm: 3.177 | num zeros: 366212352.0 | params norm: 237.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        52736 | elapsed time per iteration (ms): 6293.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.083617E+00 | gshard_loss: 7.176261E-03 | loss scale: 1024.0 | grad norm: 2.646 | num zeros: 327027232.0 | params norm: 237.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        53248 | elapsed time per iteration (ms): 6108.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.084853E+00 | gshard_loss: 7.221022E-03 | loss scale: 1024.0 | grad norm: 3.156 | num zeros: 369095808.0 | params norm: 237.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        53760 | elapsed time per iteration (ms): 6350.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.085386E+00 | gshard_loss: 7.577553E-03 | loss scale: 1024.0 | grad norm: 2.651 | num zeros: 330385984.0 | params norm: 237.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        54272 | elapsed time per iteration (ms): 6603.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.076923E+00 | gshard_loss: 7.659330E-03 | loss scale: 1024.0 | grad norm: 1.568 | num zeros: 260526496.0 | params norm: 237.739 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        54784 | elapsed time per iteration (ms): 6211.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.083251E+00 | gshard_loss: 7.551518E-03 | loss scale: 1024.0 | grad norm: 29.022 | num zeros: 298882112.0 | params norm: 237.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        55296 | elapsed time per iteration (ms): 6115.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.074628E+00 | gshard_loss: 7.621541E-03 | loss scale: 1024.0 | grad norm: 3.072 | num zeros: 312419552.0 | params norm: 237.804 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        55808 | elapsed time per iteration (ms): 6541.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.072781E+00 | gshard_loss: 8.457585E-03 | loss scale: 1024.0 | grad norm: 23.386 | num zeros: 315227360.0 | params norm: 237.836 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        56320 | elapsed time per iteration (ms): 7438.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.093204E+00 | gshard_loss: 9.142942E-03 | loss scale: 1024.0 | grad norm: 5.968 | num zeros: 369581824.0 | params norm: 237.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        56832 | elapsed time per iteration (ms): 7419.3 | learning rate: 9.997E-05 | global batch size:   512 | loss scale: 512.0 | params norm: 237.865 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        57344 | elapsed time per iteration (ms): 7837.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 9.598657E+00 | gshard_loss: 8.680594E-03 | loss scale: 512.0 | grad norm: 290.988 | num zeros: 270549472.0 | params norm: 237.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        57856 | elapsed time per iteration (ms): 6193.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.154761E+00 | gshard_loss: 9.333081E-03 | loss scale: 512.0 | grad norm: 8.260 | num zeros: 397690304.0 | params norm: 237.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        58368 | elapsed time per iteration (ms): 6171.3 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.133676E+00 | gshard_loss: 9.209903E-03 | loss scale: 512.0 | grad norm: 12.894 | num zeros: 435174528.0 | params norm: 237.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        58880 | elapsed time per iteration (ms): 6201.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.111372E+00 | gshard_loss: 8.960004E-03 | loss scale: 512.0 | grad norm: 3.263 | num zeros: 510612800.0 | params norm: 237.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        59392 | elapsed time per iteration (ms): 6309.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.075140E+00 | gshard_loss: 8.968032E-03 | loss scale: 512.0 | grad norm: 1.389 | num zeros: 535598784.0 | params norm: 237.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        59904 | elapsed time per iteration (ms): 6915.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.101564E+00 | gshard_loss: 9.397589E-03 | loss scale: 512.0 | grad norm: 3.864 | num zeros: 560249344.0 | params norm: 238.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        60416 | elapsed time per iteration (ms): 6214.3 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.102080E+00 | gshard_loss: 9.654206E-03 | loss scale: 512.0 | grad norm: 3.697 | num zeros: 632408128.0 | params norm: 238.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        60928 | elapsed time per iteration (ms): 7301.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.071893E+00 | gshard_loss: 9.625956E-03 | loss scale: 512.0 | grad norm: 1.259 | num zeros: 661109056.0 | params norm: 238.053 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        61440 | elapsed time per iteration (ms): 6907.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.075357E+00 | gshard_loss: 9.378077E-03 | loss scale: 512.0 | grad norm: 1.487 | num zeros: 655049472.0 | params norm: 238.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        61952 | elapsed time per iteration (ms): 6348.7 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.188849E+00 | gshard_loss: 7.918972E-03 | loss scale: 512.0 | grad norm: 6.567 | num zeros: 325816096.0 | params norm: 238.093 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        62464 | elapsed time per iteration (ms): 6108.7 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.139864E+00 | gshard_loss: 8.909932E-03 | loss scale: 512.0 | grad norm: 3.045 | num zeros: 309243520.0 | params norm: 238.113 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        62976 | elapsed time per iteration (ms): 6278.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.097060E+00 | gshard_loss: 1.004453E-02 | loss scale: 512.0 | grad norm: 2.093 | num zeros: 356874880.0 | params norm: 238.134 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        63488 | elapsed time per iteration (ms): 6139.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.110869E+00 | gshard_loss: 1.022231E-02 | loss scale: 512.0 | grad norm: 2.262 | num zeros: 380385504.0 | params norm: 238.154 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        64000 | elapsed time per iteration (ms): 6210.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.112185E+00 | gshard_loss: 9.542016E-03 | loss scale: 512.0 | grad norm: 2.344 | num zeros: 399762624.0 | params norm: 238.173 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        64512 | elapsed time per iteration (ms): 6363.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.105297E+00 | gshard_loss: 8.626868E-03 | loss scale: 512.0 | grad norm: 2.109 | num zeros: 422815360.0 | params norm: 238.191 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        65024 | elapsed time per iteration (ms): 6112.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.092392E+00 | gshard_loss: 8.300420E-03 | loss scale: 512.0 | grad norm: 2.019 | num zeros: 437464128.0 | params norm: 238.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        65536 | elapsed time per iteration (ms): 6109.7 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.081887E+00 | gshard_loss: 8.317515E-03 | loss scale: 512.0 | grad norm: 1.530 | num zeros: 480251136.0 | params norm: 238.229 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        66048 | elapsed time per iteration (ms): 6199.0 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.071792E+00 | gshard_loss: 8.360788E-03 | loss scale: 512.0 | grad norm: 2.171 | num zeros: 484735424.0 | params norm: 238.247 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        66560 | elapsed time per iteration (ms): 7630.0 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.071933E+00 | gshard_loss: 8.418461E-03 | loss scale: 512.0 | grad norm: 1.199 | num zeros: 613968320.0 | params norm: 238.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        67072 | elapsed time per iteration (ms): 6467.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.102314E+00 | gshard_loss: 8.044065E-03 | loss scale: 512.0 | grad norm: 2.733 | num zeros: 515093792.0 | params norm: 238.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        67584 | elapsed time per iteration (ms): 6120.6 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.075447E+00 | gshard_loss: 8.231148E-03 | loss scale: 512.0 | grad norm: 2.916 | num zeros: 504452224.0 | params norm: 238.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        68096 | elapsed time per iteration (ms): 7809.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.063284E+00 | gshard_loss: 8.528772E-03 | loss scale: 512.0 | grad norm: 1.444 | num zeros: 493897440.0 | params norm: 238.320 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        68608 | elapsed time per iteration (ms): 6438.7 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.088500E+00 | gshard_loss: 8.511654E-03 | loss scale: 512.0 | grad norm: 3.021 | num zeros: 476565856.0 | params norm: 238.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        69120 | elapsed time per iteration (ms): 7279.6 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.072813E+00 | gshard_loss: 8.102039E-03 | loss scale: 512.0 | grad norm: 1.887 | num zeros: 502254080.0 | params norm: 238.358 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        69632 | elapsed time per iteration (ms): 6566.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.064281E+00 | gshard_loss: 7.608815E-03 | loss scale: 512.0 | grad norm: 1.224 | num zeros: 571415808.0 | params norm: 238.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        70144 | elapsed time per iteration (ms): 6494.7 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.056849E+00 | gshard_loss: 7.524035E-03 | loss scale: 512.0 | grad norm: 1.549 | num zeros: 589214144.0 | params norm: 238.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        70656 | elapsed time per iteration (ms): 6152.7 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.052548E+00 | gshard_loss: 7.764118E-03 | loss scale: 512.0 | grad norm: 1.423 | num zeros: 574026752.0 | params norm: 238.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        71168 | elapsed time per iteration (ms): 6192.4 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.047866E+00 | gshard_loss: 8.171408E-03 | loss scale: 512.0 | grad norm: 1.307 | num zeros: 599717952.0 | params norm: 238.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        71680 | elapsed time per iteration (ms): 8017.1 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.044941E+00 | gshard_loss: 8.638844E-03 | loss scale: 512.0 | grad norm: 1.636 | num zeros: 497543296.0 | params norm: 238.456 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        72192 | elapsed time per iteration (ms): 7329.9 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.052732E+00 | gshard_loss: 8.124188E-03 | loss scale: 512.0 | grad norm: 1.467 | num zeros: 404819328.0 | params norm: 238.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        72704 | elapsed time per iteration (ms): 6168.7 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.046713E+00 | gshard_loss: 8.548791E-03 | loss scale: 512.0 | grad norm: 1.034 | num zeros: 407011200.0 | params norm: 238.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        73216 | elapsed time per iteration (ms): 6192.7 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.047065E+00 | gshard_loss: 8.924166E-03 | loss scale: 512.0 | grad norm: 1.555 | num zeros: 438506208.0 | params norm: 238.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        73728 | elapsed time per iteration (ms): 6109.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.047256E+00 | gshard_loss: 9.191532E-03 | loss scale: 512.0 | grad norm: 1.937 | num zeros: 444515712.0 | params norm: 238.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        74240 | elapsed time per iteration (ms): 6117.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 7.032800E+00 | gshard_loss: 8.953803E-03 | loss scale: 512.0 | grad norm: 1.105 | num zeros: 417938304.0 | params norm: 238.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        74752 | elapsed time per iteration (ms): 9192.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 7.023810E+00 | gshard_loss: 8.246147E-03 | loss scale: 512.0 | grad norm: 1.614 | num zeros: 438377760.0 | params norm: 238.596 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        75264 | elapsed time per iteration (ms): 7862.6 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 7.031928E+00 | gshard_loss: 8.035158E-03 | loss scale: 512.0 | grad norm: 2.282 | num zeros: 446979456.0 | params norm: 238.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        75776 | elapsed time per iteration (ms): 6372.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 7.002775E+00 | gshard_loss: 8.117696E-03 | loss scale: 512.0 | grad norm: 2.176 | num zeros: 407924224.0 | params norm: 238.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        76288 | elapsed time per iteration (ms): 6141.2 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 7.004045E+00 | gshard_loss: 8.054091E-03 | loss scale: 512.0 | grad norm: 1.689 | num zeros: 425776704.0 | params norm: 238.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        76800 | elapsed time per iteration (ms): 7637.6 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.993707E+00 | gshard_loss: 7.882554E-03 | loss scale: 512.0 | grad norm: 1.733 | num zeros: 446868864.0 | params norm: 238.717 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        77312 | elapsed time per iteration (ms): 6639.7 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 7.055528E+00 | gshard_loss: 8.412498E-03 | loss scale: 512.0 | grad norm: 15.390 | num zeros: 454663264.0 | params norm: 238.748 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        77824 | elapsed time per iteration (ms): 6853.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 7.022515E+00 | gshard_loss: 8.121016E-03 | loss scale: 512.0 | grad norm: 10.190 | num zeros: 451385152.0 | params norm: 238.778 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        78336 | elapsed time per iteration (ms): 6113.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 7.012270E+00 | gshard_loss: 7.885347E-03 | loss scale: 512.0 | grad norm: 5.631 | num zeros: 420253344.0 | params norm: 238.806 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        78848 | elapsed time per iteration (ms): 7366.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.996884E+00 | gshard_loss: 8.100924E-03 | loss scale: 512.0 | grad norm: 2.337 | num zeros: 469393504.0 | params norm: 238.832 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        79360 | elapsed time per iteration (ms): 6133.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.984899E+00 | gshard_loss: 8.508705E-03 | loss scale: 512.0 | grad norm: 2.160 | num zeros: 441741984.0 | params norm: 238.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        79872 | elapsed time per iteration (ms): 6141.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.983062E+00 | gshard_loss: 8.261316E-03 | loss scale: 512.0 | grad norm: 2.550 | num zeros: 477289856.0 | params norm: 238.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        80384 | elapsed time per iteration (ms): 6167.8 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.969949E+00 | gshard_loss: 7.748282E-03 | loss scale: 512.0 | grad norm: 2.093 | num zeros: 468598848.0 | params norm: 238.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        80896 | elapsed time per iteration (ms): 6160.8 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.967692E+00 | gshard_loss: 7.528064E-03 | loss scale: 512.0 | grad norm: 31.150 | num zeros: 489437696.0 | params norm: 238.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        81408 | elapsed time per iteration (ms): 6086.2 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.979848E+00 | gshard_loss: 7.383067E-03 | loss scale: 512.0 | grad norm: 3.598 | num zeros: 460811264.0 | params norm: 238.969 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        81920 | elapsed time per iteration (ms): 8050.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.960752E+00 | gshard_loss: 7.777800E-03 | loss scale: 512.0 | grad norm: 4.657 | num zeros: 477785728.0 | params norm: 238.999 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        82432 | elapsed time per iteration (ms): 6235.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.989993E+00 | gshard_loss: 8.394348E-03 | loss scale: 512.0 | grad norm: 2.732 | num zeros: 454908992.0 | params norm: 239.028 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        82944 | elapsed time per iteration (ms): 6100.5 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.963014E+00 | gshard_loss: 8.225898E-03 | loss scale: 512.0 | grad norm: 1.761 | num zeros: 502154880.0 | params norm: 239.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        83456 | elapsed time per iteration (ms): 6199.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.960937E+00 | gshard_loss: 8.058168E-03 | loss scale: 512.0 | grad norm: 3.127 | num zeros: 480868544.0 | params norm: 239.089 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        83968 | elapsed time per iteration (ms): 6762.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.941165E+00 | gshard_loss: 8.319109E-03 | loss scale: 512.0 | grad norm: 6.124 | num zeros: 483571776.0 | params norm: 239.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        84480 | elapsed time per iteration (ms): 6224.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.938772E+00 | gshard_loss: 8.730283E-03 | loss scale: 512.0 | grad norm: 1.876 | num zeros: 475519136.0 | params norm: 239.153 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        84992 | elapsed time per iteration (ms): 6804.3 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.937798E+00 | gshard_loss: 8.640486E-03 | loss scale: 512.0 | grad norm: 1.890 | num zeros: 495285152.0 | params norm: 239.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        85504 | elapsed time per iteration (ms): 6162.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.919775E+00 | gshard_loss: 8.308524E-03 | loss scale: 512.0 | grad norm: 1.819 | num zeros: 499616608.0 | params norm: 239.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        86016 | elapsed time per iteration (ms): 6024.7 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.919455E+00 | gshard_loss: 8.134476E-03 | loss scale: 512.0 | grad norm: 1.904 | num zeros: 485015488.0 | params norm: 239.267 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        86528 | elapsed time per iteration (ms): 6021.3 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.909081E+00 | gshard_loss: 8.266661E-03 | loss scale: 512.0 | grad norm: 1.623 | num zeros: 521483072.0 | params norm: 239.313 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        87040 | elapsed time per iteration (ms): 7442.8 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.890230E+00 | gshard_loss: 8.710375E-03 | loss scale: 512.0 | grad norm: 1.506 | num zeros: 513025856.0 | params norm: 239.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        87552 | elapsed time per iteration (ms): 6086.5 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.987946E+00 | gshard_loss: 1.157755E-02 | loss scale: 512.0 | grad norm: 4.563 | num zeros: 479532384.0 | params norm: 239.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        88064 | elapsed time per iteration (ms): 6101.8 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.912655E+00 | gshard_loss: 1.042880E-02 | loss scale: 512.0 | grad norm: 7.468 | num zeros: 507081664.0 | params norm: 239.451 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        88576 | elapsed time per iteration (ms): 6086.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.967001E+00 | gshard_loss: 9.015988E-03 | loss scale: 512.0 | grad norm: 17.584 | num zeros: 475351872.0 | params norm: 239.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        89088 | elapsed time per iteration (ms): 6945.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 7.010700E+00 | gshard_loss: 8.487846E-03 | loss scale: 512.0 | grad norm: 7.768 | num zeros: 476380832.0 | params norm: 239.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        89600 | elapsed time per iteration (ms): 6894.0 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.919625E+00 | gshard_loss: 1.004165E-02 | loss scale: 512.0 | grad norm: 2.737 | num zeros: 532055136.0 | params norm: 239.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        90112 | elapsed time per iteration (ms): 6445.7 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.923809E+00 | gshard_loss: 1.235539E-02 | loss scale: 512.0 | grad norm: 3.268 | num zeros: 592422144.0 | params norm: 239.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        90624 | elapsed time per iteration (ms): 6120.0 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.930206E+00 | gshard_loss: 1.220075E-02 | loss scale: 512.0 | grad norm: 3.740 | num zeros: 596071296.0 | params norm: 239.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        91136 | elapsed time per iteration (ms): 6349.0 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.905523E+00 | gshard_loss: 1.108789E-02 | loss scale: 512.0 | grad norm: 2.052 | num zeros: 623214208.0 | params norm: 239.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        91648 | elapsed time per iteration (ms): 6066.0 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.913308E+00 | gshard_loss: 9.734550E-03 | loss scale: 512.0 | grad norm: 2.946 | num zeros: 596302208.0 | params norm: 239.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        92160 | elapsed time per iteration (ms): 7519.7 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.885221E+00 | gshard_loss: 8.972407E-03 | loss scale: 512.0 | grad norm: 2.643 | num zeros: 590691584.0 | params norm: 239.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        92672 | elapsed time per iteration (ms): 6322.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.892196E+00 | gshard_loss: 7.438389E-03 | loss scale: 512.0 | grad norm: 1.997 | num zeros: 489509664.0 | params norm: 239.822 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        93184 | elapsed time per iteration (ms): 7004.3 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.890803E+00 | gshard_loss: 7.555488E-03 | loss scale: 512.0 | grad norm: 1.820 | num zeros: 502484288.0 | params norm: 239.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        93696 | elapsed time per iteration (ms): 6233.3 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.881119E+00 | gshard_loss: 7.730800E-03 | loss scale: 512.0 | grad norm: 2.010 | num zeros: 537307392.0 | params norm: 239.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        94208 | elapsed time per iteration (ms): 6210.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.854959E+00 | gshard_loss: 7.764272E-03 | loss scale: 512.0 | grad norm: 1.121 | num zeros: 582134656.0 | params norm: 239.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        94720 | elapsed time per iteration (ms): 6719.2 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.856596E+00 | gshard_loss: 7.658935E-03 | loss scale: 512.0 | grad norm: 1.607 | num zeros: 557708992.0 | params norm: 239.989 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        95232 | elapsed time per iteration (ms): 6066.3 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.852011E+00 | gshard_loss: 7.515198E-03 | loss scale: 512.0 | grad norm: 1.676 | num zeros: 532977376.0 | params norm: 240.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        95744 | elapsed time per iteration (ms): 6093.2 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.848052E+00 | gshard_loss: 7.447863E-03 | loss scale: 512.0 | grad norm: 1.149 | num zeros: 565693824.0 | params norm: 240.084 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        96256 | elapsed time per iteration (ms): 6067.5 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.838995E+00 | gshard_loss: 7.452488E-03 | loss scale: 512.0 | grad norm: 0.951 | num zeros: 565450880.0 | params norm: 240.135 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        96768 | elapsed time per iteration (ms): 6919.2 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.837798E+00 | gshard_loss: 7.384500E-03 | loss scale: 512.0 | grad norm: 0.880 | num zeros: 556026816.0 | params norm: 240.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        97280 | elapsed time per iteration (ms): 7550.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.836851E+00 | gshard_loss: 7.335861E-03 | loss scale: 512.0 | grad norm: 1.204 | num zeros: 433529088.0 | params norm: 240.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        97792 | elapsed time per iteration (ms): 9371.4 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.944954E+00 | gshard_loss: 7.058756E-03 | loss scale: 512.0 | grad norm: 3.335 | num zeros: 249536608.0 | params norm: 240.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        98304 | elapsed time per iteration (ms): 6143.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.888066E+00 | gshard_loss: 7.842310E-03 | loss scale: 512.0 | grad norm: 1.965 | num zeros: 266147824.0 | params norm: 240.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        98816 | elapsed time per iteration (ms): 6099.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.852417E+00 | gshard_loss: 8.235832E-03 | loss scale: 512.0 | grad norm: 1.612 | num zeros: 301782496.0 | params norm: 240.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        99328 | elapsed time per iteration (ms): 6253.6 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.843174E+00 | gshard_loss: 8.027961E-03 | loss scale: 512.0 | grad norm: 1.408 | num zeros: 312376000.0 | params norm: 240.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        99840 | elapsed time per iteration (ms): 7187.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.853562E+00 | gshard_loss: 7.870988E-03 | loss scale: 512.0 | grad norm: 1.864 | num zeros: 319801248.0 | params norm: 240.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:       100352 | elapsed time per iteration (ms): 6293.8 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.832613E+00 | gshard_loss: 8.345796E-03 | loss scale: 512.0 | grad norm: 1.366 | num zeros: 349702368.0 | params norm: 240.531 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:       100864 | elapsed time per iteration (ms): 6665.8 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.844557E+00 | gshard_loss: 9.053500E-03 | loss scale: 512.0 | grad norm: 1.350 | num zeros: 380391744.0 | params norm: 240.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:       101376 | elapsed time per iteration (ms): 6488.1 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.818212E+00 | gshard_loss: 9.316628E-03 | loss scale: 512.0 | grad norm: 1.145 | num zeros: 349234048.0 | params norm: 240.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:       101888 | elapsed time per iteration (ms): 6234.9 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.819582E+00 | gshard_loss: 9.024615E-03 | loss scale: 512.0 | grad norm: 1.421 | num zeros: 404201440.0 | params norm: 240.673 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:       102400 | elapsed time per iteration (ms): 7503.2 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.820872E+00 | gshard_loss: 8.664005E-03 | loss scale: 512.0 | grad norm: 1.932 | num zeros: 392290720.0 | params norm: 240.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-01-04 23:21:05 
