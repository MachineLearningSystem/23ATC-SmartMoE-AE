using world size: 32, data-parallel-size: 32, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 32
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 32
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 1
  num_layers ...................................... 8
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 51200
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 32
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 21 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 23 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 18 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 16 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 17 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 24 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 20 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 27 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 25 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 22 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 26 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 19 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 31 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 28 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 29 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 30 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 0 in DP group [0]
[INFO] 1 in DP group [1]
[INFO] 2 in DP group [2]
[INFO] 3 in DP group [3]
[INFO] 4 in DP group [4]
[INFO] 5 in DP group [5]
[INFO] 6 in DP group [6]
[INFO] 7 in DP group [7]
[INFO] 8 in DP group [8]
[INFO] 9 in DP group [9]
[INFO] 10 in DP group [10]
[INFO] 11 in DP group [11]
[INFO] 12 in DP group [12]
[INFO] 13 in DP group [13]
[INFO] 14 in DP group [14]
[INFO] 15 in DP group [15]
[INFO] 16 in DP group [16]
[INFO] 17 in DP group [17]
[INFO] 18 in DP group [18]
[INFO] 19 in DP group [19]
[INFO] 20 in DP group [20]
[INFO] 21 in DP group [21]
[INFO] 22 in DP group [22]
[INFO] 23 in DP group [23]
[INFO] 24 in DP group [24]
[INFO] 25 in DP group [25]
[INFO] 26 in DP group [26]
[INFO] 27 in DP group [27]
[INFO] 28 in DP group [28]
[INFO] 29 in DP group [29]
[INFO] 30 in DP group [30]
[W ProcessGroupNCCL.cpp:1569] Rank 20 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 26 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 5 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 4 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 6 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 7 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[W ProcessGroupNCCL.cpp:1569] Rank 28 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 3 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> compiling dataset index builder ...
[W ProcessGroupNCCL.cpp:1569] Rank 24 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 21 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 8 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 22 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 23 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 9 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 11 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 2 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[INFO] 31 in DP group [31]
[W ProcessGroupNCCL.cpp:1569] Rank 25 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 27 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 31 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 18 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 16 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 19 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 17 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 14 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 13 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 15 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 30 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 10 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 12 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 29 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
make: 进入目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
make: 对“default”无需做任何事。
make: 离开目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
>>> done with dataset index builder. Compilation time: 0.177 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
>>> done with compiling and loading fused kernels. Compilation time: 9.487 seconds
time to initialize megatron (seconds): 44.476
[after megatron is initialized] datetime: 2023-01-05 05:31:24 
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
building GPT model ...
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 374264064
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-05 05:31:27 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.023279 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.024 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-05 05:31:40 
done with setup ...
training ...
[before the start of training step] datetime: 2023-01-05 05:31:40 
 iteration        1/     200 | consumed samples:          256 | elapsed time per iteration (ms): 5718.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082603E+01 | gshard_loss: 9.904956E-04 | loss scale: 131072.0 | grad norm: 10.908 | num zeros: 7966.0 | params norm: 189.791 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 7140.841796875 | max allocated: 7140.84912109375 | reserved: 7338.0 | max reserved: 7338.0
 iteration        2/     200 | consumed samples:          512 | elapsed time per iteration (ms): 1628.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.043509E+01 | gshard_loss: 2.161621E-03 | loss scale: 131072.0 | grad norm: 5.907 | num zeros: 83334.0 | params norm: 189.801 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:          768 | elapsed time per iteration (ms): 1572.3 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 189.801 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 1587.7 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 189.801 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         1280 | elapsed time per iteration (ms): 1587.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.129406E+01 | gshard_loss: 2.208009E-03 | loss scale: 65536.0 | grad norm: 37.301 | num zeros: 16787050.0 | params norm: 189.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 1471.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.019324E+01 | gshard_loss: 3.156770E-03 | loss scale: 65536.0 | grad norm: 4.551 | num zeros: 83952928.0 | params norm: 189.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         1792 | elapsed time per iteration (ms): 1585.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.953526E+00 | gshard_loss: 2.205884E-03 | loss scale: 65536.0 | grad norm: 4.469 | num zeros: 33585108.0 | params norm: 189.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 1607.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.765159E+00 | gshard_loss: 1.811582E-03 | loss scale: 65536.0 | grad norm: 4.854 | num zeros: 16787576.0 | params norm: 189.851 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         2304 | elapsed time per iteration (ms): 1588.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.581402E+00 | gshard_loss: 1.757503E-03 | loss scale: 65536.0 | grad norm: 4.408 | num zeros: 34454368.0 | params norm: 189.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 1601.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.408061E+00 | gshard_loss: 1.665880E-03 | loss scale: 65536.0 | grad norm: 4.424 | num zeros: 33585644.0 | params norm: 189.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         2816 | elapsed time per iteration (ms): 1564.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.233542E+00 | gshard_loss: 1.686683E-03 | loss scale: 65536.0 | grad norm: 4.411 | num zeros: 33590584.0 | params norm: 189.941 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 1644.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.060460E+00 | gshard_loss: 1.687718E-03 | loss scale: 65536.0 | grad norm: 4.404 | num zeros: 33604268.0 | params norm: 189.977 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         3328 | elapsed time per iteration (ms): 1610.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.897767E+00 | gshard_loss: 1.563367E-03 | loss scale: 65536.0 | grad norm: 4.344 | num zeros: 33624232.0 | params norm: 190.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 1660.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.744362E+00 | gshard_loss: 1.493305E-03 | loss scale: 65536.0 | grad norm: 4.339 | num zeros: 33650424.0 | params norm: 190.055 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         3840 | elapsed time per iteration (ms): 1678.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.595238E+00 | gshard_loss: 1.450406E-03 | loss scale: 65536.0 | grad norm: 4.325 | num zeros: 33643564.0 | params norm: 190.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 1713.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.467516E+00 | gshard_loss: 1.439206E-03 | loss scale: 65536.0 | grad norm: 4.260 | num zeros: 33639768.0 | params norm: 190.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         4352 | elapsed time per iteration (ms): 1681.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.333464E+00 | gshard_loss: 1.426646E-03 | loss scale: 65536.0 | grad norm: 4.229 | num zeros: 33654900.0 | params norm: 190.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 1712.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.214808E+00 | gshard_loss: 1.408352E-03 | loss scale: 65536.0 | grad norm: 4.192 | num zeros: 33658652.0 | params norm: 190.236 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         4864 | elapsed time per iteration (ms): 1742.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.099813E+00 | gshard_loss: 1.384219E-03 | loss scale: 65536.0 | grad norm: 4.124 | num zeros: 1740823.0 | params norm: 190.287 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 1765.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.996202E+00 | gshard_loss: 1.346765E-03 | loss scale: 65536.0 | grad norm: 4.019 | num zeros: 34417068.0 | params norm: 190.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:         5376 | elapsed time per iteration (ms): 1822.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.906997E+00 | gshard_loss: 1.315375E-03 | loss scale: 65536.0 | grad norm: 3.906 | num zeros: 2521789.0 | params norm: 190.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 1814.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.817460E+00 | gshard_loss: 1.302487E-03 | loss scale: 65536.0 | grad norm: 3.720 | num zeros: 34421096.0 | params norm: 190.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:         5888 | elapsed time per iteration (ms): 1784.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.720376E+00 | gshard_loss: 1.284936E-03 | loss scale: 65536.0 | grad norm: 3.595 | num zeros: 50356896.0 | params norm: 190.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 1796.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.645399E+00 | gshard_loss: 1.276874E-03 | loss scale: 65536.0 | grad norm: 3.359 | num zeros: 2023176.0 | params norm: 190.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:         6400 | elapsed time per iteration (ms): 1803.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.575834E+00 | gshard_loss: 1.255909E-03 | loss scale: 65536.0 | grad norm: 3.084 | num zeros: 16836316.0 | params norm: 190.597 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 1833.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.522005E+00 | gshard_loss: 1.233253E-03 | loss scale: 65536.0 | grad norm: 2.753 | num zeros: 33586388.0 | params norm: 190.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:         6912 | elapsed time per iteration (ms): 1812.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.468606E+00 | gshard_loss: 1.222748E-03 | loss scale: 65536.0 | grad norm: 2.351 | num zeros: 17753578.0 | params norm: 190.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 1839.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.429411E+00 | gshard_loss: 1.205376E-03 | loss scale: 65536.0 | grad norm: 1.891 | num zeros: 1921705.0 | params norm: 190.751 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:         7424 | elapsed time per iteration (ms): 1883.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.394823E+00 | gshard_loss: 1.186554E-03 | loss scale: 65536.0 | grad norm: 1.376 | num zeros: 16930096.0 | params norm: 190.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 1895.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.365233E+00 | gshard_loss: 1.174514E-03 | loss scale: 65536.0 | grad norm: 0.840 | num zeros: 16902808.0 | params norm: 190.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:         7936 | elapsed time per iteration (ms): 1868.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.359269E+00 | gshard_loss: 1.188335E-03 | loss scale: 65536.0 | grad norm: 0.429 | num zeros: 166974.0 | params norm: 190.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 1774.7 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 32768.0 | params norm: 190.911 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:         8448 | elapsed time per iteration (ms): 1893.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.353724E+00 | gshard_loss: 1.195846E-03 | loss scale: 32768.0 | grad norm: 0.602 | num zeros: 9042.0 | params norm: 190.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 1797.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.342260E+00 | gshard_loss: 1.216249E-03 | loss scale: 32768.0 | grad norm: 0.886 | num zeros: 25772.0 | params norm: 191.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:         8960 | elapsed time per iteration (ms): 1782.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.337549E+00 | gshard_loss: 1.243253E-03 | loss scale: 32768.0 | grad norm: 1.013 | num zeros: 33722.0 | params norm: 191.065 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 1772.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.325361E+00 | gshard_loss: 1.245865E-03 | loss scale: 32768.0 | grad norm: 0.874 | num zeros: 64713.0 | params norm: 191.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:         9472 | elapsed time per iteration (ms): 1761.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.334154E+00 | gshard_loss: 1.254830E-03 | loss scale: 32768.0 | grad norm: 0.711 | num zeros: 16813104.0 | params norm: 191.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 1780.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.325408E+00 | gshard_loss: 1.281886E-03 | loss scale: 32768.0 | grad norm: 0.641 | num zeros: 109990.0 | params norm: 191.223 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:         9984 | elapsed time per iteration (ms): 1742.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.325699E+00 | gshard_loss: 1.368358E-03 | loss scale: 32768.0 | grad norm: 0.568 | num zeros: 923006.0 | params norm: 191.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 1739.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.292544E+00 | gshard_loss: 1.385938E-03 | loss scale: 32768.0 | grad norm: 0.420 | num zeros: 16802978.0 | params norm: 191.328 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        10496 | elapsed time per iteration (ms): 1740.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.286857E+00 | gshard_loss: 1.350725E-03 | loss scale: 32768.0 | grad norm: 0.319 | num zeros: 973843.0 | params norm: 191.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 1683.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.302450E+00 | gshard_loss: 1.330644E-03 | loss scale: 32768.0 | grad norm: 0.274 | num zeros: 16799792.0 | params norm: 191.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        11008 | elapsed time per iteration (ms): 2984.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.268618E+00 | gshard_loss: 1.345058E-03 | loss scale: 32768.0 | grad norm: 0.296 | num zeros: 16797632.0 | params norm: 191.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 1772.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.257775E+00 | gshard_loss: 1.329164E-03 | loss scale: 32768.0 | grad norm: 0.346 | num zeros: 955401.0 | params norm: 191.533 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        11520 | elapsed time per iteration (ms): 2010.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.247492E+00 | gshard_loss: 1.331176E-03 | loss scale: 32768.0 | grad norm: 0.347 | num zeros: 16068.0 | params norm: 191.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 1704.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.228810E+00 | gshard_loss: 1.354510E-03 | loss scale: 32768.0 | grad norm: 0.379 | num zeros: 13211.0 | params norm: 191.632 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        12032 | elapsed time per iteration (ms): 1715.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.216115E+00 | gshard_loss: 1.341315E-03 | loss scale: 32768.0 | grad norm: 0.411 | num zeros: 11517.0 | params norm: 191.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 1718.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.202702E+00 | gshard_loss: 1.362501E-03 | loss scale: 32768.0 | grad norm: 0.322 | num zeros: 9760.0 | params norm: 191.730 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        12544 | elapsed time per iteration (ms): 1839.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.194984E+00 | gshard_loss: 1.384153E-03 | loss scale: 32768.0 | grad norm: 0.262 | num zeros: 9445.0 | params norm: 191.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 1848.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.170137E+00 | gshard_loss: 1.352235E-03 | loss scale: 32768.0 | grad norm: 0.368 | num zeros: 7533.0 | params norm: 191.828 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        13056 | elapsed time per iteration (ms): 3352.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.171627E+00 | gshard_loss: 1.393273E-03 | loss scale: 32768.0 | grad norm: 0.267 | num zeros: 9091.0 | params norm: 191.875 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 2618.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.149475E+00 | gshard_loss: 1.410943E-03 | loss scale: 32768.0 | grad norm: 0.332 | num zeros: 22651.0 | params norm: 191.922 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        13568 | elapsed time per iteration (ms): 1730.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.167179E+00 | gshard_loss: 1.421280E-03 | loss scale: 32768.0 | grad norm: 0.305 | num zeros: 139326.0 | params norm: 191.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 1810.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.144035E+00 | gshard_loss: 1.497027E-03 | loss scale: 32768.0 | grad norm: 0.208 | num zeros: 16791400.0 | params norm: 192.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        14080 | elapsed time per iteration (ms): 1908.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.135791E+00 | gshard_loss: 1.528073E-03 | loss scale: 32768.0 | grad norm: 0.290 | num zeros: 16792106.0 | params norm: 192.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 1878.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.157165E+00 | gshard_loss: 1.591297E-03 | loss scale: 32768.0 | grad norm: 0.258 | num zeros: 16793790.0 | params norm: 192.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        14592 | elapsed time per iteration (ms): 1710.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.128968E+00 | gshard_loss: 1.662024E-03 | loss scale: 32768.0 | grad norm: 0.325 | num zeros: 16790238.0 | params norm: 192.145 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 2698.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.119025E+00 | gshard_loss: 1.922632E-03 | loss scale: 32768.0 | grad norm: 0.246 | num zeros: 16796280.0 | params norm: 192.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        15104 | elapsed time per iteration (ms): 1696.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.109435E+00 | gshard_loss: 1.748427E-03 | loss scale: 32768.0 | grad norm: 0.266 | num zeros: 16791720.0 | params norm: 192.230 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 1663.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.106104E+00 | gshard_loss: 1.769298E-03 | loss scale: 32768.0 | grad norm: 0.457 | num zeros: 16788494.0 | params norm: 192.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        15616 | elapsed time per iteration (ms): 1658.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.105517E+00 | gshard_loss: 1.961746E-03 | loss scale: 32768.0 | grad norm: 0.282 | num zeros: 16791344.0 | params norm: 192.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 1678.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.088646E+00 | gshard_loss: 1.900226E-03 | loss scale: 32768.0 | grad norm: 0.240 | num zeros: 16793300.0 | params norm: 192.351 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        16128 | elapsed time per iteration (ms): 1682.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.067502E+00 | gshard_loss: 1.885665E-03 | loss scale: 32768.0 | grad norm: 0.391 | num zeros: 16791456.0 | params norm: 192.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 1704.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.059039E+00 | gshard_loss: 2.010005E-03 | loss scale: 32768.0 | grad norm: 0.239 | num zeros: 16791356.0 | params norm: 192.428 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        16640 | elapsed time per iteration (ms): 1641.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.060868E+00 | gshard_loss: 2.035917E-03 | loss scale: 32768.0 | grad norm: 0.225 | num zeros: 16793044.0 | params norm: 192.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 1619.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.044056E+00 | gshard_loss: 1.976991E-03 | loss scale: 32768.0 | grad norm: 0.351 | num zeros: 16791032.0 | params norm: 192.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        17152 | elapsed time per iteration (ms): 1760.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.036427E+00 | gshard_loss: 2.111515E-03 | loss scale: 32768.0 | grad norm: 0.252 | num zeros: 16803702.0 | params norm: 192.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 1568.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.024798E+00 | gshard_loss: 2.294634E-03 | loss scale: 32768.0 | grad norm: 0.326 | num zeros: 33572644.0 | params norm: 192.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        17664 | elapsed time per iteration (ms): 1588.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.009510E+00 | gshard_loss: 2.234668E-03 | loss scale: 32768.0 | grad norm: 0.246 | num zeros: 16801496.0 | params norm: 192.614 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 1604.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.999828E+00 | gshard_loss: 2.216558E-03 | loss scale: 32768.0 | grad norm: 0.271 | num zeros: 16798828.0 | params norm: 192.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        18176 | elapsed time per iteration (ms): 1584.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.986263E+00 | gshard_loss: 2.360974E-03 | loss scale: 32768.0 | grad norm: 0.258 | num zeros: 16792948.0 | params norm: 192.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 1608.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.997876E+00 | gshard_loss: 2.550545E-03 | loss scale: 32768.0 | grad norm: 0.296 | num zeros: 16794652.0 | params norm: 192.722 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        18688 | elapsed time per iteration (ms): 1592.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.987028E+00 | gshard_loss: 2.423387E-03 | loss scale: 32768.0 | grad norm: 0.277 | num zeros: 16791640.0 | params norm: 192.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 1611.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.967687E+00 | gshard_loss: 2.648658E-03 | loss scale: 32768.0 | grad norm: 0.360 | num zeros: 16788438.0 | params norm: 192.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        19200 | elapsed time per iteration (ms): 1613.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.971392E+00 | gshard_loss: 2.445810E-03 | loss scale: 32768.0 | grad norm: 0.446 | num zeros: 16786848.0 | params norm: 192.826 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 2220.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.953591E+00 | gshard_loss: 2.721235E-03 | loss scale: 32768.0 | grad norm: 0.506 | num zeros: 16786494.0 | params norm: 192.859 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        19712 | elapsed time per iteration (ms): 1766.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.964876E+00 | gshard_loss: 2.383627E-03 | loss scale: 32768.0 | grad norm: 0.291 | num zeros: 16789930.0 | params norm: 192.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 1699.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.951437E+00 | gshard_loss: 2.273056E-03 | loss scale: 32768.0 | grad norm: 0.559 | num zeros: 16786384.0 | params norm: 192.925 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        20224 | elapsed time per iteration (ms): 3112.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.927465E+00 | gshard_loss: 2.490456E-03 | loss scale: 32768.0 | grad norm: 0.432 | num zeros: 16787356.0 | params norm: 192.957 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 2548.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.934763E+00 | gshard_loss: 2.489530E-03 | loss scale: 32768.0 | grad norm: 0.382 | num zeros: 16787520.0 | params norm: 192.989 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        20736 | elapsed time per iteration (ms): 1692.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.928064E+00 | gshard_loss: 2.247553E-03 | loss scale: 32768.0 | grad norm: 0.451 | num zeros: 16788080.0 | params norm: 193.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 1881.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.907231E+00 | gshard_loss: 2.259991E-03 | loss scale: 32768.0 | grad norm: 0.438 | num zeros: 16787548.0 | params norm: 193.055 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        21248 | elapsed time per iteration (ms): 2257.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.931171E+00 | gshard_loss: 2.442894E-03 | loss scale: 32768.0 | grad norm: 0.461 | num zeros: 16787200.0 | params norm: 193.089 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 1699.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.883078E+00 | gshard_loss: 2.325168E-03 | loss scale: 32768.0 | grad norm: 0.315 | num zeros: 16789296.0 | params norm: 193.122 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        21760 | elapsed time per iteration (ms): 1611.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.880097E+00 | gshard_loss: 2.280687E-03 | loss scale: 32768.0 | grad norm: 0.596 | num zeros: 16788518.0 | params norm: 193.155 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 1567.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.886415E+00 | gshard_loss: 2.421030E-03 | loss scale: 32768.0 | grad norm: 0.432 | num zeros: 16820996.0 | params norm: 193.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        22272 | elapsed time per iteration (ms): 1559.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.862127E+00 | gshard_loss: 2.434552E-03 | loss scale: 32768.0 | grad norm: 0.308 | num zeros: 17807440.0 | params norm: 193.220 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 1584.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.860136E+00 | gshard_loss: 2.415490E-03 | loss scale: 32768.0 | grad norm: 0.421 | num zeros: 17765752.0 | params norm: 193.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        22784 | elapsed time per iteration (ms): 1559.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.857629E+00 | gshard_loss: 2.476209E-03 | loss scale: 32768.0 | grad norm: 0.503 | num zeros: 33592280.0 | params norm: 193.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 1593.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.843387E+00 | gshard_loss: 2.385497E-03 | loss scale: 32768.0 | grad norm: 0.321 | num zeros: 35609112.0 | params norm: 193.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        23296 | elapsed time per iteration (ms): 1593.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.836828E+00 | gshard_loss: 2.366361E-03 | loss scale: 32768.0 | grad norm: 0.686 | num zeros: 50353532.0 | params norm: 193.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 1569.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.820047E+00 | gshard_loss: 2.525012E-03 | loss scale: 32768.0 | grad norm: 0.520 | num zeros: 33715196.0 | params norm: 193.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        23808 | elapsed time per iteration (ms): 1562.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.807963E+00 | gshard_loss: 2.523299E-03 | loss scale: 32768.0 | grad norm: 0.414 | num zeros: 50363276.0 | params norm: 193.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 1590.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.802679E+00 | gshard_loss: 2.327826E-03 | loss scale: 32768.0 | grad norm: 0.608 | num zeros: 50354944.0 | params norm: 193.469 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        24320 | elapsed time per iteration (ms): 1588.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.800337E+00 | gshard_loss: 2.318006E-03 | loss scale: 32768.0 | grad norm: 0.437 | num zeros: 33668444.0 | params norm: 193.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 1606.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.791176E+00 | gshard_loss: 2.374501E-03 | loss scale: 32768.0 | grad norm: 0.441 | num zeros: 33776184.0 | params norm: 193.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        24832 | elapsed time per iteration (ms): 1605.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.793577E+00 | gshard_loss: 2.422567E-03 | loss scale: 32768.0 | grad norm: 0.395 | num zeros: 50355248.0 | params norm: 193.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 1616.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.774669E+00 | gshard_loss: 2.285128E-03 | loss scale: 32768.0 | grad norm: 0.489 | num zeros: 33574472.0 | params norm: 193.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        25344 | elapsed time per iteration (ms): 1694.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.770094E+00 | gshard_loss: 2.367440E-03 | loss scale: 32768.0 | grad norm: 0.502 | num zeros: 2416407.0 | params norm: 193.660 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 1590.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.766790E+00 | gshard_loss: 2.490079E-03 | loss scale: 32768.0 | grad norm: 0.455 | num zeros: 35378632.0 | params norm: 193.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        25856 | elapsed time per iteration (ms): 1605.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.761328E+00 | gshard_loss: 2.442823E-03 | loss scale: 32768.0 | grad norm: 0.425 | num zeros: 50356136.0 | params norm: 193.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 1613.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.747033E+00 | gshard_loss: 2.398411E-03 | loss scale: 32768.0 | grad norm: 0.442 | num zeros: 33668632.0 | params norm: 193.776 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        26368 | elapsed time per iteration (ms): 1615.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.734897E+00 | gshard_loss: 2.413370E-03 | loss scale: 32768.0 | grad norm: 0.430 | num zeros: 34450296.0 | params norm: 193.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 1596.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.740251E+00 | gshard_loss: 2.409345E-03 | loss scale: 32768.0 | grad norm: 0.373 | num zeros: 50354780.0 | params norm: 193.854 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        26880 | elapsed time per iteration (ms): 1613.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.728885E+00 | gshard_loss: 2.364927E-03 | loss scale: 32768.0 | grad norm: 0.563 | num zeros: 33599964.0 | params norm: 193.895 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 1592.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.720920E+00 | gshard_loss: 2.440988E-03 | loss scale: 32768.0 | grad norm: 0.365 | num zeros: 50354112.0 | params norm: 193.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        27392 | elapsed time per iteration (ms): 1923.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.726227E+00 | gshard_loss: 2.361117E-03 | loss scale: 32768.0 | grad norm: 0.468 | num zeros: 34611192.0 | params norm: 193.976 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 1647.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.708516E+00 | gshard_loss: 2.307574E-03 | loss scale: 32768.0 | grad norm: 0.347 | num zeros: 18309172.0 | params norm: 194.019 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        27904 | elapsed time per iteration (ms): 1587.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.698992E+00 | gshard_loss: 2.407134E-03 | loss scale: 32768.0 | grad norm: 0.392 | num zeros: 50353100.0 | params norm: 194.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 1596.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.692809E+00 | gshard_loss: 2.358807E-03 | loss scale: 32768.0 | grad norm: 0.303 | num zeros: 33575768.0 | params norm: 194.103 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        28416 | elapsed time per iteration (ms): 1611.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.681909E+00 | gshard_loss: 2.336032E-03 | loss scale: 32768.0 | grad norm: 0.365 | num zeros: 16909666.0 | params norm: 194.146 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 1609.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.673222E+00 | gshard_loss: 2.342470E-03 | loss scale: 32768.0 | grad norm: 0.384 | num zeros: 33574160.0 | params norm: 194.189 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        28928 | elapsed time per iteration (ms): 2186.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.659606E+00 | gshard_loss: 2.348886E-03 | loss scale: 32768.0 | grad norm: 0.382 | num zeros: 33573856.0 | params norm: 194.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 1591.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.660517E+00 | gshard_loss: 2.424806E-03 | loss scale: 32768.0 | grad norm: 0.327 | num zeros: 18069156.0 | params norm: 194.275 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        29440 | elapsed time per iteration (ms): 1614.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.649081E+00 | gshard_loss: 2.428777E-03 | loss scale: 32768.0 | grad norm: 0.361 | num zeros: 17901584.0 | params norm: 194.320 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 1597.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.637996E+00 | gshard_loss: 2.433456E-03 | loss scale: 32768.0 | grad norm: 0.294 | num zeros: 33575944.0 | params norm: 194.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        29952 | elapsed time per iteration (ms): 1729.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.637338E+00 | gshard_loss: 2.374588E-03 | loss scale: 32768.0 | grad norm: 0.279 | num zeros: 16846568.0 | params norm: 194.409 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 1601.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.631707E+00 | gshard_loss: 2.416489E-03 | loss scale: 32768.0 | grad norm: 0.284 | num zeros: 34561448.0 | params norm: 194.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        30464 | elapsed time per iteration (ms): 3092.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.613940E+00 | gshard_loss: 2.482128E-03 | loss scale: 32768.0 | grad norm: 0.259 | num zeros: 50355432.0 | params norm: 194.502 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 1597.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.615636E+00 | gshard_loss: 2.491248E-03 | loss scale: 32768.0 | grad norm: 0.342 | num zeros: 17037460.0 | params norm: 194.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        30976 | elapsed time per iteration (ms): 1600.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.603907E+00 | gshard_loss: 2.502257E-03 | loss scale: 32768.0 | grad norm: 0.457 | num zeros: 18038444.0 | params norm: 194.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 1635.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.607474E+00 | gshard_loss: 2.313497E-03 | loss scale: 32768.0 | grad norm: 0.674 | num zeros: 16806384.0 | params norm: 194.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        31488 | elapsed time per iteration (ms): 1596.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.595466E+00 | gshard_loss: 2.505345E-03 | loss scale: 32768.0 | grad norm: 0.538 | num zeros: 17690744.0 | params norm: 194.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 1631.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.576210E+00 | gshard_loss: 2.469425E-03 | loss scale: 32768.0 | grad norm: 0.310 | num zeros: 16796570.0 | params norm: 194.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        32000 | elapsed time per iteration (ms): 1646.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.583784E+00 | gshard_loss: 2.347396E-03 | loss scale: 32768.0 | grad norm: 0.543 | num zeros: 16792296.0 | params norm: 194.781 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 1696.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.583856E+00 | gshard_loss: 2.384609E-03 | loss scale: 32768.0 | grad norm: 0.356 | num zeros: 16794960.0 | params norm: 194.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        32512 | elapsed time per iteration (ms): 1647.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.578412E+00 | gshard_loss: 2.300747E-03 | loss scale: 32768.0 | grad norm: 0.363 | num zeros: 16796810.0 | params norm: 194.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 1619.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.561929E+00 | gshard_loss: 2.240055E-03 | loss scale: 32768.0 | grad norm: 0.446 | num zeros: 16794864.0 | params norm: 194.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        33024 | elapsed time per iteration (ms): 1627.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.564890E+00 | gshard_loss: 2.273231E-03 | loss scale: 32768.0 | grad norm: 0.371 | num zeros: 16796180.0 | params norm: 194.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 1609.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.563956E+00 | gshard_loss: 2.294235E-03 | loss scale: 32768.0 | grad norm: 0.313 | num zeros: 16792164.0 | params norm: 195.022 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        33536 | elapsed time per iteration (ms): 2388.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.538925E+00 | gshard_loss: 2.204915E-03 | loss scale: 32768.0 | grad norm: 0.375 | num zeros: 16797248.0 | params norm: 195.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 1657.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.534705E+00 | gshard_loss: 2.152483E-03 | loss scale: 32768.0 | grad norm: 0.294 | num zeros: 16792944.0 | params norm: 195.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        34048 | elapsed time per iteration (ms): 1662.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.541482E+00 | gshard_loss: 2.159589E-03 | loss scale: 32768.0 | grad norm: 0.298 | num zeros: 16794352.0 | params norm: 195.170 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 1714.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.528303E+00 | gshard_loss: 2.165199E-03 | loss scale: 32768.0 | grad norm: 0.288 | num zeros: 16793082.0 | params norm: 195.220 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        34560 | elapsed time per iteration (ms): 2036.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.513337E+00 | gshard_loss: 2.161861E-03 | loss scale: 32768.0 | grad norm: 0.311 | num zeros: 16794860.0 | params norm: 195.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 1760.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.516550E+00 | gshard_loss: 2.133562E-03 | loss scale: 32768.0 | grad norm: 0.330 | num zeros: 16791612.0 | params norm: 195.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        35072 | elapsed time per iteration (ms): 1650.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.504713E+00 | gshard_loss: 2.087247E-03 | loss scale: 32768.0 | grad norm: 0.273 | num zeros: 16791688.0 | params norm: 195.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 1658.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.496245E+00 | gshard_loss: 2.103219E-03 | loss scale: 32768.0 | grad norm: 0.251 | num zeros: 16792020.0 | params norm: 195.422 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        35584 | elapsed time per iteration (ms): 2122.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.512245E+00 | gshard_loss: 2.070472E-03 | loss scale: 32768.0 | grad norm: 0.292 | num zeros: 16790764.0 | params norm: 195.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 1657.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.502530E+00 | gshard_loss: 2.080764E-03 | loss scale: 32768.0 | grad norm: 0.272 | num zeros: 16791692.0 | params norm: 195.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        36096 | elapsed time per iteration (ms): 1667.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.474643E+00 | gshard_loss: 2.044938E-03 | loss scale: 32768.0 | grad norm: 0.287 | num zeros: 16790058.0 | params norm: 195.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 2095.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.471297E+00 | gshard_loss: 2.055892E-03 | loss scale: 32768.0 | grad norm: 0.293 | num zeros: 16790996.0 | params norm: 195.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        36608 | elapsed time per iteration (ms): 1664.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.481380E+00 | gshard_loss: 2.053893E-03 | loss scale: 32768.0 | grad norm: 0.263 | num zeros: 16790208.0 | params norm: 195.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 1693.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.465630E+00 | gshard_loss: 2.035665E-03 | loss scale: 32768.0 | grad norm: 0.271 | num zeros: 16793564.0 | params norm: 195.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        37120 | elapsed time per iteration (ms): 1682.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.460177E+00 | gshard_loss: 2.049080E-03 | loss scale: 32768.0 | grad norm: 0.252 | num zeros: 16790572.0 | params norm: 195.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 1873.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.468468E+00 | gshard_loss: 2.007235E-03 | loss scale: 32768.0 | grad norm: 0.271 | num zeros: 16792040.0 | params norm: 195.840 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        37632 | elapsed time per iteration (ms): 1746.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.449239E+00 | gshard_loss: 2.009033E-03 | loss scale: 32768.0 | grad norm: 0.282 | num zeros: 16790470.0 | params norm: 195.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 1675.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.450966E+00 | gshard_loss: 1.970360E-03 | loss scale: 32768.0 | grad norm: 0.308 | num zeros: 16791160.0 | params norm: 195.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        38144 | elapsed time per iteration (ms): 2110.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.439072E+00 | gshard_loss: 2.007198E-03 | loss scale: 32768.0 | grad norm: 0.297 | num zeros: 16790776.0 | params norm: 196.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 1677.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.448657E+00 | gshard_loss: 1.960044E-03 | loss scale: 32768.0 | grad norm: 0.252 | num zeros: 16792124.0 | params norm: 196.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        38656 | elapsed time per iteration (ms): 1659.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.425446E+00 | gshard_loss: 1.973771E-03 | loss scale: 32768.0 | grad norm: 0.304 | num zeros: 16791500.0 | params norm: 196.110 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 1666.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.436874E+00 | gshard_loss: 1.989843E-03 | loss scale: 32768.0 | grad norm: 0.321 | num zeros: 16790606.0 | params norm: 196.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        39168 | elapsed time per iteration (ms): 1664.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.409939E+00 | gshard_loss: 1.959515E-03 | loss scale: 32768.0 | grad norm: 0.434 | num zeros: 16788938.0 | params norm: 196.220 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 1725.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.423580E+00 | gshard_loss: 1.975333E-03 | loss scale: 32768.0 | grad norm: 0.535 | num zeros: 16788700.0 | params norm: 196.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        39680 | elapsed time per iteration (ms): 1765.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.418549E+00 | gshard_loss: 1.958760E-03 | loss scale: 32768.0 | grad norm: 0.337 | num zeros: 16790704.0 | params norm: 196.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 1740.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.406815E+00 | gshard_loss: 1.959995E-03 | loss scale: 32768.0 | grad norm: 0.421 | num zeros: 16789760.0 | params norm: 196.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        40192 | elapsed time per iteration (ms): 1680.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.408245E+00 | gshard_loss: 2.007747E-03 | loss scale: 32768.0 | grad norm: 0.460 | num zeros: 16789082.0 | params norm: 196.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 1667.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.405139E+00 | gshard_loss: 1.946280E-03 | loss scale: 32768.0 | grad norm: 0.301 | num zeros: 16791292.0 | params norm: 196.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        40704 | elapsed time per iteration (ms): 1733.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.399905E+00 | gshard_loss: 1.929659E-03 | loss scale: 32768.0 | grad norm: 0.500 | num zeros: 16789492.0 | params norm: 196.551 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 1660.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.391225E+00 | gshard_loss: 1.937227E-03 | loss scale: 32768.0 | grad norm: 0.249 | num zeros: 16791572.0 | params norm: 196.608 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        41216 | elapsed time per iteration (ms): 1663.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.403580E+00 | gshard_loss: 1.933629E-03 | loss scale: 32768.0 | grad norm: 0.392 | num zeros: 16790344.0 | params norm: 196.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 1653.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.390762E+00 | gshard_loss: 1.951740E-03 | loss scale: 32768.0 | grad norm: 0.256 | num zeros: 16791704.0 | params norm: 196.718 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        41728 | elapsed time per iteration (ms): 1679.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.399831E+00 | gshard_loss: 1.953426E-03 | loss scale: 32768.0 | grad norm: 0.407 | num zeros: 16789528.0 | params norm: 196.775 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 1709.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.382081E+00 | gshard_loss: 1.918392E-03 | loss scale: 32768.0 | grad norm: 0.264 | num zeros: 16793652.0 | params norm: 196.832 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        42240 | elapsed time per iteration (ms): 1686.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.388161E+00 | gshard_loss: 1.976472E-03 | loss scale: 32768.0 | grad norm: 0.326 | num zeros: 16791980.0 | params norm: 196.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 1678.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.357508E+00 | gshard_loss: 1.947477E-03 | loss scale: 32768.0 | grad norm: 0.290 | num zeros: 16791054.0 | params norm: 196.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        42752 | elapsed time per iteration (ms): 1668.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.361435E+00 | gshard_loss: 1.960988E-03 | loss scale: 32768.0 | grad norm: 0.293 | num zeros: 16790832.0 | params norm: 197.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 1664.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.343216E+00 | gshard_loss: 1.922909E-03 | loss scale: 32768.0 | grad norm: 0.242 | num zeros: 16794032.0 | params norm: 197.058 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        43264 | elapsed time per iteration (ms): 2296.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.351260E+00 | gshard_loss: 1.952364E-03 | loss scale: 32768.0 | grad norm: 0.302 | num zeros: 16791660.0 | params norm: 197.114 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 1683.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.360314E+00 | gshard_loss: 1.998574E-03 | loss scale: 32768.0 | grad norm: 0.235 | num zeros: 16793064.0 | params norm: 197.171 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        43776 | elapsed time per iteration (ms): 1674.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.352278E+00 | gshard_loss: 1.960285E-03 | loss scale: 32768.0 | grad norm: 0.305 | num zeros: 16790374.0 | params norm: 197.229 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 1654.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.347002E+00 | gshard_loss: 1.917973E-03 | loss scale: 32768.0 | grad norm: 0.253 | num zeros: 16792284.0 | params norm: 197.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        44288 | elapsed time per iteration (ms): 1672.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.327176E+00 | gshard_loss: 1.907324E-03 | loss scale: 32768.0 | grad norm: 0.260 | num zeros: 16791440.0 | params norm: 197.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 1667.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.330805E+00 | gshard_loss: 1.923263E-03 | loss scale: 32768.0 | grad norm: 0.237 | num zeros: 959153.0 | params norm: 197.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        44800 | elapsed time per iteration (ms): 1656.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.334905E+00 | gshard_loss: 1.932819E-03 | loss scale: 32768.0 | grad norm: 0.267 | num zeros: 16792828.0 | params norm: 197.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 1728.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.321109E+00 | gshard_loss: 1.968846E-03 | loss scale: 32768.0 | grad norm: 0.227 | num zeros: 977398.0 | params norm: 197.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        45312 | elapsed time per iteration (ms): 1682.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.323704E+00 | gshard_loss: 1.957832E-03 | loss scale: 32768.0 | grad norm: 0.264 | num zeros: 14161.0 | params norm: 197.572 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 1693.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.317688E+00 | gshard_loss: 1.952840E-03 | loss scale: 32768.0 | grad norm: 0.258 | num zeros: 14737.0 | params norm: 197.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        45824 | elapsed time per iteration (ms): 1661.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.309452E+00 | gshard_loss: 1.942081E-03 | loss scale: 32768.0 | grad norm: 0.301 | num zeros: 13689.0 | params norm: 197.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 1678.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.296586E+00 | gshard_loss: 1.923323E-03 | loss scale: 32768.0 | grad norm: 0.269 | num zeros: 16182.0 | params norm: 197.746 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        46336 | elapsed time per iteration (ms): 1695.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.303559E+00 | gshard_loss: 2.070260E-03 | loss scale: 32768.0 | grad norm: 0.261 | num zeros: 12770.0 | params norm: 197.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 1697.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.317588E+00 | gshard_loss: 1.898137E-03 | loss scale: 32768.0 | grad norm: 0.249 | num zeros: 12226.0 | params norm: 197.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        46848 | elapsed time per iteration (ms): 1711.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.299921E+00 | gshard_loss: 1.933655E-03 | loss scale: 32768.0 | grad norm: 0.260 | num zeros: 12374.0 | params norm: 197.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 1664.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.291540E+00 | gshard_loss: 1.948153E-03 | loss scale: 32768.0 | grad norm: 0.234 | num zeros: 10985.0 | params norm: 197.981 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        47360 | elapsed time per iteration (ms): 1685.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.284271E+00 | gshard_loss: 1.915063E-03 | loss scale: 32768.0 | grad norm: 0.281 | num zeros: 11840.0 | params norm: 198.040 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 1810.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.290926E+00 | gshard_loss: 1.894907E-03 | loss scale: 32768.0 | grad norm: 0.405 | num zeros: 7281.0 | params norm: 198.098 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        47872 | elapsed time per iteration (ms): 1697.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.297706E+00 | gshard_loss: 1.899881E-03 | loss scale: 32768.0 | grad norm: 0.553 | num zeros: 7499.0 | params norm: 198.157 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 1670.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.292031E+00 | gshard_loss: 1.877261E-03 | loss scale: 32768.0 | grad norm: 0.486 | num zeros: 7396.0 | params norm: 198.216 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        48384 | elapsed time per iteration (ms): 1667.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.283453E+00 | gshard_loss: 1.880769E-03 | loss scale: 32768.0 | grad norm: 0.302 | num zeros: 9659.0 | params norm: 198.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 1673.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.271575E+00 | gshard_loss: 1.906081E-03 | loss scale: 32768.0 | grad norm: 0.353 | num zeros: 8414.0 | params norm: 198.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        48896 | elapsed time per iteration (ms): 1662.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.278976E+00 | gshard_loss: 1.901985E-03 | loss scale: 32768.0 | grad norm: 0.368 | num zeros: 10343.0 | params norm: 198.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 1670.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.272706E+00 | gshard_loss: 1.904122E-03 | loss scale: 32768.0 | grad norm: 0.219 | num zeros: 12501.0 | params norm: 198.449 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        49408 | elapsed time per iteration (ms): 1671.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.255535E+00 | gshard_loss: 1.862674E-03 | loss scale: 32768.0 | grad norm: 0.370 | num zeros: 8177.0 | params norm: 198.508 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 1669.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.258334E+00 | gshard_loss: 1.879586E-03 | loss scale: 32768.0 | grad norm: 0.241 | num zeros: 10717.0 | params norm: 198.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        49920 | elapsed time per iteration (ms): 1738.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.256996E+00 | gshard_loss: 1.901247E-03 | loss scale: 32768.0 | grad norm: 0.256 | num zeros: 13069.0 | params norm: 198.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 1648.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.251887E+00 | gshard_loss: 1.882408E-03 | loss scale: 32768.0 | grad norm: 0.326 | num zeros: 9894.0 | params norm: 198.684 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:        50432 | elapsed time per iteration (ms): 1674.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.252315E+00 | gshard_loss: 1.865900E-03 | loss scale: 32768.0 | grad norm: 0.241 | num zeros: 11883.0 | params norm: 198.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 1668.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.247312E+00 | gshard_loss: 1.860532E-03 | loss scale: 32768.0 | grad norm: 0.262 | num zeros: 10500.0 | params norm: 198.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:        50944 | elapsed time per iteration (ms): 1665.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.251925E+00 | gshard_loss: 1.831444E-03 | loss scale: 32768.0 | grad norm: 0.300 | num zeros: 10714.0 | params norm: 198.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 1669.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.248740E+00 | gshard_loss: 1.812526E-03 | loss scale: 32768.0 | grad norm: 0.241 | num zeros: 11715.0 | params norm: 198.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-01-05 05:37:34 
