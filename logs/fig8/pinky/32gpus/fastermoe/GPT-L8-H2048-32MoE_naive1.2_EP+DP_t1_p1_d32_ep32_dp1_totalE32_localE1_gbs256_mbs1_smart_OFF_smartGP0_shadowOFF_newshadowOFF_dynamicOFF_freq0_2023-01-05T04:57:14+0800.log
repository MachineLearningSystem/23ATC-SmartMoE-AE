using world size: 32, data-parallel-size: 32, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 32
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 32
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 1
  num_layers ...................................... 8
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 51200
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 32
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 31 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 29 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 28 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 22 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 30 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 21 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 24 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 26 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 23 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 20 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 27 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 25 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 18 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 16 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 17 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 19 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 0 in DP group [0]
[INFO] 1 in DP group [1]
[INFO] 2 in DP group [2]
[INFO] 3 in DP group [3]
[INFO] 4 in DP group [4]
[INFO] 5 in DP group [5]
[INFO] 6 in DP group [6]
[INFO] 7 in DP group [7]
[INFO] 8 in DP group [8]
[INFO] 9 in DP group [9]
[INFO] 10 in DP group [10]
[INFO] 11 in DP group [11]
[INFO] 12 in DP group [12]
[INFO] 13 in DP group [13]
[INFO] 14 in DP group [14]
[INFO] 15 in DP group [15]
[INFO] 16 in DP group [16]
[INFO] 17 in DP group [17]
[INFO] 18 in DP group [18]
[INFO] 19 in DP group [19]
[INFO] 20 in DP group [20]
[INFO] 21 in DP group [21]
[INFO] 22 in DP group [22]
[INFO] 23 in DP group [23]
[INFO] 24 in DP group [24]
[INFO] 25 in DP group [25]
[INFO] 26 in DP group [26]
[INFO] 27 in DP group [27]
[INFO] 28 in DP group [28]
[INFO] 29 in DP group [29]
[INFO] 30 in DP group [30]
> setting random seeds to 1234 ...
[INFO] 31 in DP group [31]
[W ProcessGroupNCCL.cpp:1569] Rank 27 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[W ProcessGroupNCCL.cpp:1569] Rank 26 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 25 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 11 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 9 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 2 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 10 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 3 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> compiling dataset index builder ...
[W ProcessGroupNCCL.cpp:1569] Rank 29 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 13 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 4 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 20 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 30 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 14 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 23 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 31 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 15 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 6 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 21 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 28 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 22 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 17 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 8 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 18 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 19 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 16 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 24 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 7 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 12 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 5 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
make: 进入目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
make: 对“default”无需做任何事。
make: 离开目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
>>> done with dataset index builder. Compilation time: 0.208 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
>>> done with compiling and loading fused kernels. Compilation time: 10.227 seconds
time to initialize megatron (seconds): -45.772
[after megatron is initialized] datetime: 2023-01-05 04:57:54 
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
building GPT model ...
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 374264064
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-05 04:57:58 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.054952 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.103 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-05 04:58:11 
done with setup ...
training ...
[before the start of training step] datetime: 2023-01-05 04:58:11 
 iteration        1/     200 | consumed samples:          256 | elapsed time per iteration (ms): 8639.4 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 189.784 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 7140.552734375 | max allocated: 7140.56005859375 | reserved: 7312.0 | max reserved: 7312.0
 iteration        2/     200 | consumed samples:          512 | elapsed time per iteration (ms): 5498.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.045207E+01 | loss scale: 131072.0 | grad norm: 7.916 | num zeros: 141967.0 | params norm: 189.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:          768 | elapsed time per iteration (ms): 4930.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.021500E+01 | loss scale: 131072.0 | grad norm: 4.582 | num zeros: 33814056.0 | params norm: 189.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 5881.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.986746E+00 | loss scale: 131072.0 | grad norm: 4.528 | num zeros: 101091368.0 | params norm: 189.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         1280 | elapsed time per iteration (ms): 4792.2 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 189.812 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 4737.9 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 32768.0 | params norm: 189.812 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         1792 | elapsed time per iteration (ms): 4733.0 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 16384.0 | params norm: 189.812 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 4700.9 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 8192.0 | params norm: 189.812 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         2304 | elapsed time per iteration (ms): 4798.2 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 4096.0 | params norm: 189.812 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 4715.6 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 2048.0 | params norm: 189.812 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         2816 | elapsed time per iteration (ms): 4943.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.933135E+00 | loss scale: 2048.0 | grad norm: 282.776 | num zeros: 83641.0 | params norm: 189.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 6608.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.628492E+00 | loss scale: 2048.0 | grad norm: 8.892 | num zeros: 103282648.0 | params norm: 189.842 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         3328 | elapsed time per iteration (ms): 6410.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.459046E+00 | loss scale: 2048.0 | grad norm: 8.818 | num zeros: 125922064.0 | params norm: 189.876 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 6601.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.274816E+00 | loss scale: 2048.0 | grad norm: 8.826 | num zeros: 101430248.0 | params norm: 189.910 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         3840 | elapsed time per iteration (ms): 6534.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.102202E+00 | loss scale: 2048.0 | grad norm: 8.818 | num zeros: 90602144.0 | params norm: 189.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 6650.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.941884E+00 | loss scale: 2048.0 | grad norm: 8.728 | num zeros: 97248400.0 | params norm: 189.989 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         4352 | elapsed time per iteration (ms): 6462.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.779529E+00 | loss scale: 2048.0 | grad norm: 8.718 | num zeros: 122934504.0 | params norm: 190.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 6472.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.631248E+00 | loss scale: 2048.0 | grad norm: 8.701 | num zeros: 121454880.0 | params norm: 190.075 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         4864 | elapsed time per iteration (ms): 6385.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.486801E+00 | loss scale: 2048.0 | grad norm: 8.640 | num zeros: 69069832.0 | params norm: 190.121 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 6488.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.355252E+00 | loss scale: 2048.0 | grad norm: 8.507 | num zeros: 86651552.0 | params norm: 190.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:         5376 | elapsed time per iteration (ms): 6636.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.239481E+00 | loss scale: 2048.0 | grad norm: 8.398 | num zeros: 85486240.0 | params norm: 190.218 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 6513.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.125619E+00 | loss scale: 2048.0 | grad norm: 8.168 | num zeros: 86085552.0 | params norm: 190.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:         5888 | elapsed time per iteration (ms): 6646.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.008413E+00 | loss scale: 2048.0 | grad norm: 8.094 | num zeros: 87177152.0 | params norm: 190.325 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 6747.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.910231E+00 | loss scale: 2048.0 | grad norm: 7.842 | num zeros: 86345528.0 | params norm: 190.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:         6400 | elapsed time per iteration (ms): 6822.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.818521E+00 | loss scale: 2048.0 | grad norm: 7.565 | num zeros: 88857536.0 | params norm: 190.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 6752.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.738254E+00 | loss scale: 2048.0 | grad norm: 7.162 | num zeros: 69483584.0 | params norm: 190.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:         6912 | elapsed time per iteration (ms): 6679.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.659299E+00 | loss scale: 2048.0 | grad norm: 6.746 | num zeros: 69615168.0 | params norm: 190.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 6688.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.590716E+00 | loss scale: 2048.0 | grad norm: 6.184 | num zeros: 85810584.0 | params norm: 190.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:         7424 | elapsed time per iteration (ms): 6647.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.526207E+00 | loss scale: 2048.0 | grad norm: 5.508 | num zeros: 86124464.0 | params norm: 190.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 6728.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.464950E+00 | loss scale: 2048.0 | grad norm: 4.717 | num zeros: 86283800.0 | params norm: 190.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:         7936 | elapsed time per iteration (ms): 6610.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.430267E+00 | loss scale: 2048.0 | grad norm: 3.762 | num zeros: 104226016.0 | params norm: 190.753 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 6723.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.385462E+00 | loss scale: 2048.0 | grad norm: 2.791 | num zeros: 86916520.0 | params norm: 190.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:         8448 | elapsed time per iteration (ms): 6610.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.375371E+00 | loss scale: 2048.0 | grad norm: 1.665 | num zeros: 104756288.0 | params norm: 190.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 6769.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.355579E+00 | loss scale: 2048.0 | grad norm: 0.758 | num zeros: 94380832.0 | params norm: 190.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:         8960 | elapsed time per iteration (ms): 6600.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.346843E+00 | loss scale: 2048.0 | grad norm: 0.656 | num zeros: 94908944.0 | params norm: 190.982 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 6555.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.338771E+00 | loss scale: 2048.0 | grad norm: 1.016 | num zeros: 90176560.0 | params norm: 191.041 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:         9472 | elapsed time per iteration (ms): 6367.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.351229E+00 | loss scale: 2048.0 | grad norm: 1.391 | num zeros: 67934920.0 | params norm: 191.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 6315.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.349118E+00 | loss scale: 2048.0 | grad norm: 1.452 | num zeros: 85025776.0 | params norm: 191.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:         9984 | elapsed time per iteration (ms): 6411.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.357955E+00 | loss scale: 2048.0 | grad norm: 1.114 | num zeros: 101920296.0 | params norm: 191.222 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 6207.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.327874E+00 | loss scale: 2048.0 | grad norm: 0.692 | num zeros: 101692456.0 | params norm: 191.284 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        10496 | elapsed time per iteration (ms): 6167.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.322374E+00 | loss scale: 2048.0 | grad norm: 0.865 | num zeros: 85469888.0 | params norm: 191.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 6143.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.343225E+00 | loss scale: 2048.0 | grad norm: 0.553 | num zeros: 84706384.0 | params norm: 191.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        11008 | elapsed time per iteration (ms): 6072.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.310894E+00 | loss scale: 2048.0 | grad norm: 0.745 | num zeros: 61523008.0 | params norm: 191.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 5798.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.297271E+00 | loss scale: 2048.0 | grad norm: 1.860 | num zeros: 68570184.0 | params norm: 191.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        11520 | elapsed time per iteration (ms): 5854.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.278257E+00 | loss scale: 2048.0 | grad norm: 1.019 | num zeros: 67985584.0 | params norm: 191.572 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 5934.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.272367E+00 | loss scale: 2048.0 | grad norm: 1.632 | num zeros: 50623844.0 | params norm: 191.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        12032 | elapsed time per iteration (ms): 5962.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.252419E+00 | loss scale: 2048.0 | grad norm: 1.215 | num zeros: 54436696.0 | params norm: 191.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 5577.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.242472E+00 | loss scale: 2048.0 | grad norm: 1.246 | num zeros: 34060800.0 | params norm: 191.746 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        12544 | elapsed time per iteration (ms): 5706.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.233925E+00 | loss scale: 2048.0 | grad norm: 1.135 | num zeros: 37866692.0 | params norm: 191.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 5374.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.206286E+00 | loss scale: 2048.0 | grad norm: 0.995 | num zeros: 47592216.0 | params norm: 191.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        13056 | elapsed time per iteration (ms): 4911.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.198758E+00 | loss scale: 2048.0 | grad norm: 0.651 | num zeros: 59499088.0 | params norm: 191.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 4736.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.177266E+00 | loss scale: 2048.0 | grad norm: 0.944 | num zeros: 64118192.0 | params norm: 191.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        13568 | elapsed time per iteration (ms): 4614.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.182429E+00 | loss scale: 2048.0 | grad norm: 1.079 | num zeros: 43255052.0 | params norm: 192.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 4613.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.160410E+00 | loss scale: 2048.0 | grad norm: 0.946 | num zeros: 61468508.0 | params norm: 192.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        14080 | elapsed time per iteration (ms): 4733.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.151456E+00 | loss scale: 2048.0 | grad norm: 0.586 | num zeros: 66042480.0 | params norm: 192.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 4447.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.164554E+00 | loss scale: 2048.0 | grad norm: 0.784 | num zeros: 44170744.0 | params norm: 192.220 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        14592 | elapsed time per iteration (ms): 4575.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.132344E+00 | loss scale: 2048.0 | grad norm: 0.920 | num zeros: 71390576.0 | params norm: 192.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 4735.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.122596E+00 | loss scale: 2048.0 | grad norm: 0.737 | num zeros: 72641232.0 | params norm: 192.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        15104 | elapsed time per iteration (ms): 4747.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.116435E+00 | loss scale: 2048.0 | grad norm: 0.691 | num zeros: 68497976.0 | params norm: 192.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 4790.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.108436E+00 | loss scale: 2048.0 | grad norm: 0.930 | num zeros: 73651256.0 | params norm: 192.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        15616 | elapsed time per iteration (ms): 4738.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.115087E+00 | loss scale: 2048.0 | grad norm: 0.891 | num zeros: 51320092.0 | params norm: 192.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 4522.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.100614E+00 | loss scale: 2048.0 | grad norm: 1.051 | num zeros: 35009604.0 | params norm: 192.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        16128 | elapsed time per iteration (ms): 4459.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.078465E+00 | loss scale: 2048.0 | grad norm: 0.748 | num zeros: 51409024.0 | params norm: 192.627 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 4437.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.073763E+00 | loss scale: 2048.0 | grad norm: 1.939 | num zeros: 51230552.0 | params norm: 192.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        16640 | elapsed time per iteration (ms): 4532.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.079363E+00 | loss scale: 2048.0 | grad norm: 0.798 | num zeros: 51908732.0 | params norm: 192.733 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 4544.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.055959E+00 | loss scale: 2048.0 | grad norm: 0.674 | num zeros: 52852136.0 | params norm: 192.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        17152 | elapsed time per iteration (ms): 4567.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.056339E+00 | loss scale: 2048.0 | grad norm: 0.622 | num zeros: 52809036.0 | params norm: 192.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 4523.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.051130E+00 | loss scale: 2048.0 | grad norm: 0.726 | num zeros: 68929360.0 | params norm: 192.885 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        17664 | elapsed time per iteration (ms): 4563.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.040939E+00 | loss scale: 2048.0 | grad norm: 0.935 | num zeros: 84181232.0 | params norm: 192.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 4446.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.032000E+00 | loss scale: 2048.0 | grad norm: 0.867 | num zeros: 84174160.0 | params norm: 192.983 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        18176 | elapsed time per iteration (ms): 4492.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.015859E+00 | loss scale: 2048.0 | grad norm: 0.633 | num zeros: 84146768.0 | params norm: 193.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 4531.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.029727E+00 | loss scale: 2048.0 | grad norm: 1.038 | num zeros: 84086616.0 | params norm: 193.079 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        18688 | elapsed time per iteration (ms): 4478.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.019816E+00 | loss scale: 2048.0 | grad norm: 0.809 | num zeros: 61274856.0 | params norm: 193.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 4325.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.998440E+00 | loss scale: 2048.0 | grad norm: 0.941 | num zeros: 59554336.0 | params norm: 193.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        19200 | elapsed time per iteration (ms): 4584.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.993926E+00 | loss scale: 2048.0 | grad norm: 0.938 | num zeros: 47736148.0 | params norm: 193.222 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 4315.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.977778E+00 | loss scale: 2048.0 | grad norm: 1.217 | num zeros: 44720452.0 | params norm: 193.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        19712 | elapsed time per iteration (ms): 4431.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.990546E+00 | loss scale: 2048.0 | grad norm: 1.093 | num zeros: 36417928.0 | params norm: 193.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 4541.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.981887E+00 | loss scale: 2048.0 | grad norm: 1.955 | num zeros: 51933176.0 | params norm: 193.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        20224 | elapsed time per iteration (ms): 4417.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.958629E+00 | loss scale: 2048.0 | grad norm: 1.354 | num zeros: 54497048.0 | params norm: 193.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 4264.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.970144E+00 | loss scale: 2048.0 | grad norm: 1.385 | num zeros: 48132512.0 | params norm: 193.449 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        20736 | elapsed time per iteration (ms): 4299.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.955164E+00 | loss scale: 2048.0 | grad norm: 1.068 | num zeros: 43784492.0 | params norm: 193.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 4530.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.930021E+00 | loss scale: 2048.0 | grad norm: 1.018 | num zeros: 38250740.0 | params norm: 193.535 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        21248 | elapsed time per iteration (ms): 4429.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.960864E+00 | loss scale: 2048.0 | grad norm: 1.045 | num zeros: 34084764.0 | params norm: 193.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 4354.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.918848E+00 | loss scale: 2048.0 | grad norm: 0.960 | num zeros: 35493680.0 | params norm: 193.623 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        21760 | elapsed time per iteration (ms): 4387.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.912599E+00 | loss scale: 2048.0 | grad norm: 0.930 | num zeros: 33967512.0 | params norm: 193.666 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 4332.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.924310E+00 | loss scale: 2048.0 | grad norm: 0.845 | num zeros: 34563212.0 | params norm: 193.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        22272 | elapsed time per iteration (ms): 4270.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.905882E+00 | loss scale: 2048.0 | grad norm: 0.914 | num zeros: 19075720.0 | params norm: 193.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 4459.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.897111E+00 | loss scale: 2048.0 | grad norm: 0.719 | num zeros: 17738904.0 | params norm: 193.804 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        22784 | elapsed time per iteration (ms): 4455.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.890856E+00 | loss scale: 2048.0 | grad norm: 0.834 | num zeros: 20846748.0 | params norm: 193.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 4480.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.888842E+00 | loss scale: 2048.0 | grad norm: 0.600 | num zeros: 21911660.0 | params norm: 193.901 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        23296 | elapsed time per iteration (ms): 4460.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.869028E+00 | loss scale: 2048.0 | grad norm: 0.755 | num zeros: 25192012.0 | params norm: 193.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 4552.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.859682E+00 | loss scale: 2048.0 | grad norm: 0.614 | num zeros: 6472013.0 | params norm: 194.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        23808 | elapsed time per iteration (ms): 4581.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.846383E+00 | loss scale: 2048.0 | grad norm: 0.890 | num zeros: 25012360.0 | params norm: 194.050 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 4593.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.842373E+00 | loss scale: 2048.0 | grad norm: 1.955 | num zeros: 18222062.0 | params norm: 194.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        24320 | elapsed time per iteration (ms): 4431.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.842259E+00 | loss scale: 2048.0 | grad norm: 1.087 | num zeros: 9815107.0 | params norm: 194.149 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 4472.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.832767E+00 | loss scale: 2048.0 | grad norm: 1.013 | num zeros: 17423480.0 | params norm: 194.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        24832 | elapsed time per iteration (ms): 4502.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.841549E+00 | loss scale: 2048.0 | grad norm: 1.508 | num zeros: 25772954.0 | params norm: 194.247 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 4505.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.813458E+00 | loss scale: 2048.0 | grad norm: 1.114 | num zeros: 18544016.0 | params norm: 194.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        25344 | elapsed time per iteration (ms): 4550.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.810556E+00 | loss scale: 2048.0 | grad norm: 1.058 | num zeros: 27233418.0 | params norm: 194.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 4562.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.816671E+00 | loss scale: 2048.0 | grad norm: 0.821 | num zeros: 26019858.0 | params norm: 194.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        25856 | elapsed time per iteration (ms): 4608.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.817848E+00 | loss scale: 2048.0 | grad norm: 1.174 | num zeros: 27022824.0 | params norm: 194.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 4731.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.798197E+00 | loss scale: 2048.0 | grad norm: 0.818 | num zeros: 27880754.0 | params norm: 194.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        26368 | elapsed time per iteration (ms): 4537.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.789236E+00 | loss scale: 2048.0 | grad norm: 0.697 | num zeros: 16414414.0 | params norm: 194.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 4461.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.800129E+00 | loss scale: 2048.0 | grad norm: 0.825 | num zeros: 20928756.0 | params norm: 194.599 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        26880 | elapsed time per iteration (ms): 4502.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.785872E+00 | loss scale: 2048.0 | grad norm: 0.610 | num zeros: 23643852.0 | params norm: 194.652 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 4536.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.784747E+00 | loss scale: 2048.0 | grad norm: 0.847 | num zeros: 34693232.0 | params norm: 194.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        27392 | elapsed time per iteration (ms): 4441.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.783577E+00 | loss scale: 2048.0 | grad norm: 0.760 | num zeros: 31975004.0 | params norm: 194.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 4524.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.768474E+00 | loss scale: 2048.0 | grad norm: 0.841 | num zeros: 16812928.0 | params norm: 194.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        27904 | elapsed time per iteration (ms): 4385.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.759559E+00 | loss scale: 2048.0 | grad norm: 0.640 | num zeros: 11169035.0 | params norm: 194.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 4402.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.758232E+00 | loss scale: 2048.0 | grad norm: 0.857 | num zeros: 20051366.0 | params norm: 194.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        28416 | elapsed time per iteration (ms): 4307.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.750622E+00 | loss scale: 2048.0 | grad norm: 0.940 | num zeros: 21517740.0 | params norm: 194.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 4272.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.748606E+00 | loss scale: 2048.0 | grad norm: 1.168 | num zeros: 24168196.0 | params norm: 195.025 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        28928 | elapsed time per iteration (ms): 4179.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.740991E+00 | loss scale: 2048.0 | grad norm: 1.376 | num zeros: 22290088.0 | params norm: 195.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 4167.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.741374E+00 | loss scale: 2048.0 | grad norm: 0.715 | num zeros: 25586260.0 | params norm: 195.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        29440 | elapsed time per iteration (ms): 4277.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.732408E+00 | loss scale: 2048.0 | grad norm: 0.740 | num zeros: 21832732.0 | params norm: 195.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 4190.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.728694E+00 | loss scale: 2048.0 | grad norm: 1.029 | num zeros: 25032732.0 | params norm: 195.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        29952 | elapsed time per iteration (ms): 4160.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.725546E+00 | loss scale: 2048.0 | grad norm: 0.756 | num zeros: 25115720.0 | params norm: 195.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 4221.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.728987E+00 | loss scale: 2048.0 | grad norm: 0.882 | num zeros: 24935356.0 | params norm: 195.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        30464 | elapsed time per iteration (ms): 4178.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.716248E+00 | loss scale: 2048.0 | grad norm: 1.013 | num zeros: 24278038.0 | params norm: 195.402 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 4208.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.716966E+00 | loss scale: 2048.0 | grad norm: 0.923 | num zeros: 25676396.0 | params norm: 195.459 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        30976 | elapsed time per iteration (ms): 4284.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.704192E+00 | loss scale: 2048.0 | grad norm: 0.909 | num zeros: 25283796.0 | params norm: 195.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 4206.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.700098E+00 | loss scale: 2048.0 | grad norm: 0.537 | num zeros: 26494222.0 | params norm: 195.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        31488 | elapsed time per iteration (ms): 4222.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.698447E+00 | loss scale: 2048.0 | grad norm: 0.823 | num zeros: 26245552.0 | params norm: 195.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 4211.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.686828E+00 | loss scale: 2048.0 | grad norm: 0.773 | num zeros: 26524074.0 | params norm: 195.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        32000 | elapsed time per iteration (ms): 4211.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.688722E+00 | loss scale: 2048.0 | grad norm: 0.763 | num zeros: 26101292.0 | params norm: 195.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 4156.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.696205E+00 | loss scale: 2048.0 | grad norm: 0.761 | num zeros: 30088116.0 | params norm: 195.801 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        32512 | elapsed time per iteration (ms): 4185.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.690031E+00 | loss scale: 2048.0 | grad norm: 0.559 | num zeros: 26495716.0 | params norm: 195.860 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 4196.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.677251E+00 | loss scale: 2048.0 | grad norm: 0.786 | num zeros: 26128700.0 | params norm: 195.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        33024 | elapsed time per iteration (ms): 4202.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.686779E+00 | loss scale: 2048.0 | grad norm: 1.128 | num zeros: 26536342.0 | params norm: 195.978 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 4208.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.690206E+00 | loss scale: 2048.0 | grad norm: 1.538 | num zeros: 27504884.0 | params norm: 196.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        33536 | elapsed time per iteration (ms): 4165.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.664708E+00 | loss scale: 2048.0 | grad norm: 0.908 | num zeros: 27099604.0 | params norm: 196.094 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 4155.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.661506E+00 | loss scale: 2048.0 | grad norm: 0.637 | num zeros: 26836988.0 | params norm: 196.153 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        34048 | elapsed time per iteration (ms): 4248.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.676113E+00 | loss scale: 2048.0 | grad norm: 1.256 | num zeros: 26820250.0 | params norm: 196.211 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 4210.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.664048E+00 | loss scale: 2048.0 | grad norm: 0.822 | num zeros: 27129926.0 | params norm: 196.271 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        34560 | elapsed time per iteration (ms): 4178.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.653213E+00 | loss scale: 2048.0 | grad norm: 0.692 | num zeros: 19731968.0 | params norm: 196.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 4159.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.658143E+00 | loss scale: 2048.0 | grad norm: 0.661 | num zeros: 28532602.0 | params norm: 196.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        35072 | elapsed time per iteration (ms): 4312.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.650945E+00 | loss scale: 2048.0 | grad norm: 0.721 | num zeros: 34162188.0 | params norm: 196.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 4165.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.645592E+00 | loss scale: 2048.0 | grad norm: 0.597 | num zeros: 34500440.0 | params norm: 196.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        35584 | elapsed time per iteration (ms): 4138.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.656408E+00 | loss scale: 2048.0 | grad norm: 0.938 | num zeros: 38159128.0 | params norm: 196.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 4153.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.655284E+00 | loss scale: 2048.0 | grad norm: 0.704 | num zeros: 31186866.0 | params norm: 196.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        36096 | elapsed time per iteration (ms): 4137.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.630895E+00 | loss scale: 2048.0 | grad norm: 0.682 | num zeros: 43137944.0 | params norm: 196.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 4130.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.633980E+00 | loss scale: 2048.0 | grad norm: 0.664 | num zeros: 42816264.0 | params norm: 196.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        36608 | elapsed time per iteration (ms): 4130.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.643560E+00 | loss scale: 2048.0 | grad norm: 0.582 | num zeros: 29870288.0 | params norm: 196.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 4171.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.633218E+00 | loss scale: 2048.0 | grad norm: 0.885 | num zeros: 28796232.0 | params norm: 196.876 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        37120 | elapsed time per iteration (ms): 4044.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.631801E+00 | loss scale: 2048.0 | grad norm: 0.945 | num zeros: 27859728.0 | params norm: 196.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 4007.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.636236E+00 | loss scale: 2048.0 | grad norm: 0.698 | num zeros: 25860688.0 | params norm: 196.994 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        37632 | elapsed time per iteration (ms): 4015.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.617880E+00 | loss scale: 2048.0 | grad norm: 0.583 | num zeros: 27271412.0 | params norm: 197.053 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 4034.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.623713E+00 | loss scale: 2048.0 | grad norm: 0.702 | num zeros: 26454312.0 | params norm: 197.112 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        38144 | elapsed time per iteration (ms): 3974.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.612743E+00 | loss scale: 2048.0 | grad norm: 0.697 | num zeros: 26058348.0 | params norm: 197.172 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 4036.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.622307E+00 | loss scale: 2048.0 | grad norm: 0.583 | num zeros: 20380716.0 | params norm: 197.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        38656 | elapsed time per iteration (ms): 3979.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.608978E+00 | loss scale: 2048.0 | grad norm: 0.804 | num zeros: 27234260.0 | params norm: 197.291 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 3964.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.613643E+00 | loss scale: 2048.0 | grad norm: 0.769 | num zeros: 27487196.0 | params norm: 197.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        39168 | elapsed time per iteration (ms): 3952.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.593070E+00 | loss scale: 2048.0 | grad norm: 0.918 | num zeros: 24008900.0 | params norm: 197.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 3951.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.605146E+00 | loss scale: 2048.0 | grad norm: 0.813 | num zeros: 20209246.0 | params norm: 197.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        39680 | elapsed time per iteration (ms): 3922.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.603447E+00 | loss scale: 2048.0 | grad norm: 0.628 | num zeros: 33904968.0 | params norm: 197.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 3926.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.592251E+00 | loss scale: 2048.0 | grad norm: 0.906 | num zeros: 34117784.0 | params norm: 197.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        40192 | elapsed time per iteration (ms): 3940.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.588769E+00 | loss scale: 2048.0 | grad norm: 1.012 | num zeros: 25553988.0 | params norm: 197.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 3953.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.594386E+00 | loss scale: 2048.0 | grad norm: 0.843 | num zeros: 34063288.0 | params norm: 197.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        40704 | elapsed time per iteration (ms): 4102.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.582281E+00 | loss scale: 2048.0 | grad norm: 0.624 | num zeros: 20620064.0 | params norm: 197.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 3914.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.582527E+00 | loss scale: 2048.0 | grad norm: 1.075 | num zeros: 24455100.0 | params norm: 197.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        41216 | elapsed time per iteration (ms): 3892.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.585180E+00 | loss scale: 2048.0 | grad norm: 0.638 | num zeros: 19230800.0 | params norm: 197.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 3935.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.581832E+00 | loss scale: 2048.0 | grad norm: 0.823 | num zeros: 28638496.0 | params norm: 197.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        41728 | elapsed time per iteration (ms): 3990.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.582503E+00 | loss scale: 2048.0 | grad norm: 0.750 | num zeros: 36744384.0 | params norm: 198.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 3966.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.572710E+00 | loss scale: 2048.0 | grad norm: 1.257 | num zeros: 37179640.0 | params norm: 198.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        42240 | elapsed time per iteration (ms): 3932.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.571110E+00 | loss scale: 2048.0 | grad norm: 0.984 | num zeros: 24681956.0 | params norm: 198.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 3921.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.546267E+00 | loss scale: 2048.0 | grad norm: 0.786 | num zeros: 21943196.0 | params norm: 198.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        42752 | elapsed time per iteration (ms): 4082.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.549144E+00 | loss scale: 2048.0 | grad norm: 0.735 | num zeros: 30668052.0 | params norm: 198.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 3940.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.532821E+00 | loss scale: 2048.0 | grad norm: 0.882 | num zeros: 22531740.0 | params norm: 198.290 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        43264 | elapsed time per iteration (ms): 3940.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.540052E+00 | loss scale: 2048.0 | grad norm: 0.821 | num zeros: 25692520.0 | params norm: 198.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 3961.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.548772E+00 | loss scale: 2048.0 | grad norm: 0.634 | num zeros: 27763652.0 | params norm: 198.404 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        43776 | elapsed time per iteration (ms): 3956.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.540048E+00 | loss scale: 2048.0 | grad norm: 0.849 | num zeros: 28486904.0 | params norm: 198.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 3917.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.537559E+00 | loss scale: 2048.0 | grad norm: 0.916 | num zeros: 27891292.0 | params norm: 198.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        44288 | elapsed time per iteration (ms): 3937.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.517159E+00 | loss scale: 2048.0 | grad norm: 0.940 | num zeros: 31784116.0 | params norm: 198.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 4004.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.521875E+00 | loss scale: 2048.0 | grad norm: 0.940 | num zeros: 21269692.0 | params norm: 198.632 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        44800 | elapsed time per iteration (ms): 3905.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.524129E+00 | loss scale: 2048.0 | grad norm: 0.672 | num zeros: 34622592.0 | params norm: 198.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 3924.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.514596E+00 | loss scale: 2048.0 | grad norm: 0.803 | num zeros: 26405996.0 | params norm: 198.746 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        45312 | elapsed time per iteration (ms): 3892.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.513644E+00 | loss scale: 2048.0 | grad norm: 0.745 | num zeros: 20873272.0 | params norm: 198.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 3880.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.511644E+00 | loss scale: 2048.0 | grad norm: 0.827 | num zeros: 18032550.0 | params norm: 198.863 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        45824 | elapsed time per iteration (ms): 3881.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.501369E+00 | loss scale: 2048.0 | grad norm: 0.617 | num zeros: 29018048.0 | params norm: 198.922 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 3870.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.489701E+00 | loss scale: 2048.0 | grad norm: 0.606 | num zeros: 24331916.0 | params norm: 198.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        46336 | elapsed time per iteration (ms): 3903.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.501799E+00 | loss scale: 2048.0 | grad norm: 0.869 | num zeros: 26356796.0 | params norm: 199.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 3843.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.510993E+00 | loss scale: 2048.0 | grad norm: 1.182 | num zeros: 25419156.0 | params norm: 199.091 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        46848 | elapsed time per iteration (ms): 3847.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.495180E+00 | loss scale: 2048.0 | grad norm: 0.722 | num zeros: 38038136.0 | params norm: 199.147 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 3872.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.488785E+00 | loss scale: 2048.0 | grad norm: 0.663 | num zeros: 41508984.0 | params norm: 199.202 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        47360 | elapsed time per iteration (ms): 3850.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.481214E+00 | loss scale: 2048.0 | grad norm: 0.755 | num zeros: 30261178.0 | params norm: 199.257 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 3863.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.484293E+00 | loss scale: 2048.0 | grad norm: 0.797 | num zeros: 30977154.0 | params norm: 199.312 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        47872 | elapsed time per iteration (ms): 3851.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.489083E+00 | loss scale: 2048.0 | grad norm: 0.716 | num zeros: 43172856.0 | params norm: 199.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 3846.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.481907E+00 | loss scale: 2048.0 | grad norm: 0.688 | num zeros: 36025960.0 | params norm: 199.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        48384 | elapsed time per iteration (ms): 3862.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.477410E+00 | loss scale: 2048.0 | grad norm: 0.705 | num zeros: 43437528.0 | params norm: 199.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 4029.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.467569E+00 | loss scale: 2048.0 | grad norm: 0.704 | num zeros: 44698848.0 | params norm: 199.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        48896 | elapsed time per iteration (ms): 3849.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.473692E+00 | loss scale: 2048.0 | grad norm: 0.907 | num zeros: 26188950.0 | params norm: 199.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 3840.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.473681E+00 | loss scale: 2048.0 | grad norm: 0.843 | num zeros: 44984304.0 | params norm: 199.641 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        49408 | elapsed time per iteration (ms): 3865.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.456495E+00 | loss scale: 2048.0 | grad norm: 1.306 | num zeros: 28606828.0 | params norm: 199.694 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 4007.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.459678E+00 | loss scale: 2048.0 | grad norm: 0.784 | num zeros: 23731884.0 | params norm: 199.749 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        49920 | elapsed time per iteration (ms): 3911.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.456844E+00 | loss scale: 2048.0 | grad norm: 0.906 | num zeros: 24249672.0 | params norm: 199.804 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 3879.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.453657E+00 | loss scale: 2048.0 | grad norm: 1.384 | num zeros: 38686272.0 | params norm: 199.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:        50432 | elapsed time per iteration (ms): 3996.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.455329E+00 | loss scale: 2048.0 | grad norm: 1.053 | num zeros: 18733838.0 | params norm: 199.912 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 3837.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.445936E+00 | loss scale: 2048.0 | grad norm: 0.568 | num zeros: 17310448.0 | params norm: 199.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:        50944 | elapsed time per iteration (ms): 3855.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.452032E+00 | loss scale: 2048.0 | grad norm: 0.749 | num zeros: 19647080.0 | params norm: 200.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 3858.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.454577E+00 | loss scale: 2048.0 | grad norm: 1.078 | num zeros: 23981708.0 | params norm: 200.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-01-05 05:13:52 
