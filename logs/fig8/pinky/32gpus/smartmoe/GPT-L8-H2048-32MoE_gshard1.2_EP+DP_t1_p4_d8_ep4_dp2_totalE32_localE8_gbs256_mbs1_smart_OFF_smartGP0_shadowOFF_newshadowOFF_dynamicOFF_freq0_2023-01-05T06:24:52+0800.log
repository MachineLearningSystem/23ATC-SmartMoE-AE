using world size: 32, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 4 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 8
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 2
  expert_ep_size .................................. 4
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 8
  num_layers ...................................... 8
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 4
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 51200
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 32
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 32
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 4
[INFO] 0 in EP group [0, 1, 2, 3]
[INFO] 3 in EP group [0, 1, 2, 3]
[INFO] 2 in EP group [0, 1, 2, 3]
[INFO] 1 in EP group [0, 1, 2, 3]
[INFO] 4 in EP group [4, 5, 6, 7]
[INFO] 7 in EP group [4, 5, 6, 7]
[INFO] 5 in EP group [4, 5, 6, 7]
[INFO] 6 in EP group [4, 5, 6, 7]
[INFO] 0 in DP group [0, 4]
[INFO] 4 in DP group [0, 4]
[INFO] 1 in DP group [1, 5]
[INFO] 5 in DP group [1, 5]
[INFO] 2 in DP group [2, 6]
[INFO] 6 in DP group [2, 6]
[INFO] 3 in DP group [3, 7]
[INFO] 7 in DP group [3, 7]
[INFO] 8 in EP group [8, 9, 10, 11]
[INFO] 11 in EP group [8, 9, 10, 11]
[INFO] 9 in EP group [8, 9, 10, 11]
[INFO] 10 in EP group [8, 9, 10, 11]
[INFO] 12 in EP group [12, 13, 14, 15]
[INFO] 15 in EP group [12, 13, 14, 15]
[INFO] 14 in EP group [12, 13, 14, 15]
[INFO] 13 in EP group [12, 13, 14, 15]
[INFO] 8 in DP group [8, 12]
[INFO] 12 in DP group [8, 12]
[INFO] 9 in DP group [9, 13]
[INFO] 13 in DP group [9, 13]
[INFO] 10 in DP group [10, 14]
[INFO] 14 in DP group [10, 14]
[INFO] 11 in DP group [11, 15]
[INFO] 15 in DP group [11, 15]
[INFO] 16 in EP group [16, 17, 18, 19]
[INFO] 19 in EP group [16, 17, 18, 19]
[INFO] 17 in EP group [16, 17, 18, 19]
[INFO] 18 in EP group [16, 17, 18, 19]
[INFO] 20 in EP group [20, 21, 22, 23]
[INFO] 21 in EP group [20, 21, 22, 23]
[INFO] 22 in EP group [20, 21, 22, 23]
[INFO] 23 in EP group [20, 21, 22, 23]
[INFO] 16 in DP group [16, 20]
[INFO] 20 in DP group [16, 20]
[INFO] 17 in DP group [17, 21]
[INFO] 21 in DP group [17, 21]
[INFO] 18 in DP group [18, 22]
[INFO] 22 in DP group [18, 22]
[INFO] 19 in DP group [19, 23]
[INFO] 23 in DP group [19, 23]
[INFO] 24 in EP group [24, 25, 26, 27]
[INFO] 26 in EP group [24, 25, 26, 27]
[INFO] 27 in EP group [24, 25, 26, 27]
[INFO] 25 in EP group [24, 25, 26, 27]
[INFO] 28 in EP group [28, 29, 30, 31]
[INFO] 30 in EP group [28, 29, 30, 31]
[INFO] 31 in EP group [28, 29, 30, 31]
[INFO] 29 in EP group [28, 29, 30, 31]
[INFO] 24 in DP group [24, 28]
[INFO] 28 in DP group [24, 28]
[INFO] 25 in DP group [25, 29]
[INFO] 29 in DP group [25, 29]
[INFO] 26 in DP group [26, 30]
[INFO] 30 in DP group [26, 30]
> setting random seeds to 1234 ...
[INFO] 27 in DP group [27, 31]
[INFO] 31 in DP group [27, 31]
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[W ProcessGroupNCCL.cpp:1569] Rank 2 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 3 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 12 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 20 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 21 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 13 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 22 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 15 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 23 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 14 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 27 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 28 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 29 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 26 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 4 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 30 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 31 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 25 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 8 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 5 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 6 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 7 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 10 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 11 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 16 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 17 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 18 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 19 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 9 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> compiling dataset index builder ...
[W ProcessGroupNCCL.cpp:1569] Rank 24 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
make: 进入目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
make: 对“default”无需做任何事。
make: 离开目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
>>> done with dataset index builder. Compilation time: 0.227 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
>>> done with compiling and loading fused kernels. Compilation time: 11.677 seconds
time to initialize megatron (seconds): -42.056
hhs=4096
hhs=4096
hhs=4096
hhs=4096
[after megatron is initialized] datetime: 2023-01-05 06:25:25 
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
building GPT model ...
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 302252096
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 407371840
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 405278784
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 302252096
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-05 06:25:33 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.020524 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.028 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-05 06:25:46 
done with setup ...
training ...
[before the start of training step] datetime: 2023-01-05 06:25:46 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 7772.359375 | max allocated: 7772.36181640625 | reserved: 8432.0 | max reserved: 8432.0
[Rank 24] (after 1 iterations) memory (MB) | allocated: 7990.728515625 | max allocated: 7990.75927734375 | reserved: 8070.0 | max reserved: 8070.0
[Rank 8] (after 1 iterations) memory (MB) | allocated: 5773.01171875 | max allocated: 5773.01416015625 | reserved: 6172.0 | max reserved: 6172.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 5773.01171875 | max allocated: 5773.01416015625 | reserved: 6044.0 | max reserved: 6044.0
 iteration        1/     200 | consumed samples:          256 | elapsed time per iteration (ms): 10743.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082704E+01 | gshard_loss: 1.597109E-02 | loss scale: 131072.0 | grad norm: 8.020 | num zeros: 129480.0 | params norm: 195.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/     200 | consumed samples:          512 | elapsed time per iteration (ms): 2544.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.040557E+01 | gshard_loss: 3.220351E-02 | loss scale: 131072.0 | grad norm: 6.455 | num zeros: 454934528.0 | params norm: 195.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:          768 | elapsed time per iteration (ms): 2509.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.027526E+01 | gshard_loss: 5.687105E-02 | loss scale: 131072.0 | grad norm: 8.362 | num zeros: 470949632.0 | params norm: 195.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 2444.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.162183E+01 | gshard_loss: 3.374613E-01 | loss scale: 131072.0 | grad norm: 5.798 | num zeros: 571620288.0 | params norm: 195.147 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         1280 | elapsed time per iteration (ms): 2523.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.007832E+01 | gshard_loss: 5.806652E-02 | loss scale: 131072.0 | grad norm: 5.969 | num zeros: 489361952.0 | params norm: 195.201 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 2499.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.944658E+00 | gshard_loss: 5.895133E-02 | loss scale: 131072.0 | grad norm: 5.484 | num zeros: 489627648.0 | params norm: 195.257 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         1792 | elapsed time per iteration (ms): 2446.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.001029E+01 | gshard_loss: 2.205806E-01 | loss scale: 131072.0 | grad norm: 17.834 | num zeros: 689221632.0 | params norm: 195.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 2723.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.697503E+00 | gshard_loss: 4.446073E-02 | loss scale: 131072.0 | grad norm: 4.685 | num zeros: 723023808.0 | params norm: 195.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         2304 | elapsed time per iteration (ms): 2424.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.553818E+00 | gshard_loss: 4.249392E-02 | loss scale: 131072.0 | grad norm: 4.611 | num zeros: 707520896.0 | params norm: 195.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 2733.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.394352E+00 | gshard_loss: 4.002134E-02 | loss scale: 131072.0 | grad norm: 4.605 | num zeros: 675214720.0 | params norm: 195.502 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         2816 | elapsed time per iteration (ms): 2495.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.229445E+00 | gshard_loss: 3.399214E-02 | loss scale: 131072.0 | grad norm: 4.507 | num zeros: 687371648.0 | params norm: 195.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 2487.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.066820E+00 | gshard_loss: 3.381557E-02 | loss scale: 131072.0 | grad norm: 4.507 | num zeros: 720868288.0 | params norm: 195.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         3328 | elapsed time per iteration (ms): 2575.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.914248E+00 | gshard_loss: 3.020812E-02 | loss scale: 131072.0 | grad norm: 4.413 | num zeros: 772251904.0 | params norm: 195.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 2568.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.769134E+00 | gshard_loss: 2.918481E-02 | loss scale: 131072.0 | grad norm: 4.409 | num zeros: 758924032.0 | params norm: 195.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         3840 | elapsed time per iteration (ms): 2593.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.624743E+00 | gshard_loss: 2.628454E-02 | loss scale: 131072.0 | grad norm: 4.393 | num zeros: 743339520.0 | params norm: 195.836 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 2563.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.500196E+00 | gshard_loss: 2.737033E-02 | loss scale: 131072.0 | grad norm: 4.341 | num zeros: 761942528.0 | params norm: 195.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         4352 | elapsed time per iteration (ms): 2589.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.366624E+00 | gshard_loss: 2.667676E-02 | loss scale: 131072.0 | grad norm: 4.312 | num zeros: 710489216.0 | params norm: 195.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 2564.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.248231E+00 | gshard_loss: 2.515661E-02 | loss scale: 131072.0 | grad norm: 4.275 | num zeros: 726364736.0 | params norm: 196.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         4864 | elapsed time per iteration (ms): 2610.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.133882E+00 | gshard_loss: 2.469796E-02 | loss scale: 131072.0 | grad norm: 4.209 | num zeros: 712832320.0 | params norm: 196.114 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 2534.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.030012E+00 | gshard_loss: 2.446167E-02 | loss scale: 131072.0 | grad norm: 4.115 | num zeros: 727202944.0 | params norm: 196.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:         5376 | elapsed time per iteration (ms): 2578.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.939568E+00 | gshard_loss: 2.451884E-02 | loss scale: 131072.0 | grad norm: 4.020 | num zeros: 697872000.0 | params norm: 196.257 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 2647.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.848463E+00 | gshard_loss: 2.281340E-02 | loss scale: 131072.0 | grad norm: 3.835 | num zeros: 715129600.0 | params norm: 196.328 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:         5888 | elapsed time per iteration (ms): 2660.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.750425E+00 | gshard_loss: 2.164336E-02 | loss scale: 131072.0 | grad norm: 3.713 | num zeros: 681343680.0 | params norm: 196.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 2619.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.673255E+00 | gshard_loss: 2.164667E-02 | loss scale: 131072.0 | grad norm: 3.493 | num zeros: 750072576.0 | params norm: 196.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:         6400 | elapsed time per iteration (ms): 2627.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.601056E+00 | gshard_loss: 2.144137E-02 | loss scale: 131072.0 | grad norm: 3.234 | num zeros: 632612928.0 | params norm: 196.537 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 2605.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.543468E+00 | gshard_loss: 2.124049E-02 | loss scale: 131072.0 | grad norm: 2.894 | num zeros: 679563392.0 | params norm: 196.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:         6912 | elapsed time per iteration (ms): 2648.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.486675E+00 | gshard_loss: 2.059875E-02 | loss scale: 131072.0 | grad norm: 2.528 | num zeros: 685456832.0 | params norm: 196.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 2665.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.444171E+00 | gshard_loss: 2.058983E-02 | loss scale: 131072.0 | grad norm: 2.080 | num zeros: 697916608.0 | params norm: 196.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:         7424 | elapsed time per iteration (ms): 2695.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.406709E+00 | gshard_loss: 2.062305E-02 | loss scale: 131072.0 | grad norm: 1.590 | num zeros: 698600384.0 | params norm: 196.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 2712.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.374570E+00 | gshard_loss: 1.968603E-02 | loss scale: 131072.0 | grad norm: 1.045 | num zeros: 729159360.0 | params norm: 196.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:         7936 | elapsed time per iteration (ms): 2643.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.368480E+00 | gshard_loss: 1.969280E-02 | loss scale: 131072.0 | grad norm: 0.586 | num zeros: 731668928.0 | params norm: 196.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 2763.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.350472E+00 | gshard_loss: 1.964426E-02 | loss scale: 131072.0 | grad norm: 0.625 | num zeros: 660010240.0 | params norm: 196.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:         8448 | elapsed time per iteration (ms): 2684.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.365255E+00 | gshard_loss: 1.941205E-02 | loss scale: 131072.0 | grad norm: 1.040 | num zeros: 695284864.0 | params norm: 197.063 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 2736.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.361454E+00 | gshard_loss: 1.913937E-02 | loss scale: 131072.0 | grad norm: 1.321 | num zeros: 582126976.0 | params norm: 197.129 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:         8960 | elapsed time per iteration (ms): 2801.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.361006E+00 | gshard_loss: 1.911259E-02 | loss scale: 131072.0 | grad norm: 1.476 | num zeros: 728994240.0 | params norm: 197.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 2737.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.353699E+00 | gshard_loss: 1.977911E-02 | loss scale: 131072.0 | grad norm: 1.443 | num zeros: 711287488.0 | params norm: 197.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:         9472 | elapsed time per iteration (ms): 2721.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.364702E+00 | gshard_loss: 1.880708E-02 | loss scale: 131072.0 | grad norm: 1.168 | num zeros: 689603072.0 | params norm: 197.320 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 2715.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.359034E+00 | gshard_loss: 1.987286E-02 | loss scale: 131072.0 | grad norm: 0.951 | num zeros: 396827360.0 | params norm: 197.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:         9984 | elapsed time per iteration (ms): 2667.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.365765E+00 | gshard_loss: 1.873226E-02 | loss scale: 131072.0 | grad norm: 0.642 | num zeros: 594070784.0 | params norm: 197.427 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 2719.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.334757E+00 | gshard_loss: 1.870849E-02 | loss scale: 131072.0 | grad norm: 0.494 | num zeros: 606666304.0 | params norm: 197.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        10496 | elapsed time per iteration (ms): 2698.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.328151E+00 | gshard_loss: 1.951068E-02 | loss scale: 131072.0 | grad norm: 0.506 | num zeros: 627222400.0 | params norm: 197.523 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 2674.8 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 197.523 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        11008 | elapsed time per iteration (ms): 2730.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.330733E+00 | gshard_loss: 1.913770E-02 | loss scale: 131072.0 | grad norm: 0.444 | num zeros: 623275776.0 | params norm: 197.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 2660.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.326050E+00 | gshard_loss: 1.880408E-02 | loss scale: 131072.0 | grad norm: 0.383 | num zeros: 208180704.0 | params norm: 197.616 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        11520 | elapsed time per iteration (ms): 2727.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.326245E+00 | gshard_loss: 1.884989E-02 | loss scale: 131072.0 | grad norm: 0.550 | num zeros: 159446560.0 | params norm: 197.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 2697.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.313912E+00 | gshard_loss: 1.989910E-02 | loss scale: 131072.0 | grad norm: 0.469 | num zeros: 592268160.0 | params norm: 197.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        12032 | elapsed time per iteration (ms): 2719.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.312783E+00 | gshard_loss: 1.951127E-02 | loss scale: 131072.0 | grad norm: 0.433 | num zeros: 641431424.0 | params norm: 197.745 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 2688.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.316581E+00 | gshard_loss: 1.901621E-02 | loss scale: 131072.0 | grad norm: 0.461 | num zeros: 672455936.0 | params norm: 197.789 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        12544 | elapsed time per iteration (ms): 2738.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.318387E+00 | gshard_loss: 1.910766E-02 | loss scale: 131072.0 | grad norm: 0.491 | num zeros: 624353792.0 | params norm: 197.834 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 2693.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.306561E+00 | gshard_loss: 1.939817E-02 | loss scale: 131072.0 | grad norm: 0.515 | num zeros: 525038720.0 | params norm: 197.880 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        13056 | elapsed time per iteration (ms): 2687.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.306930E+00 | gshard_loss: 1.936628E-02 | loss scale: 131072.0 | grad norm: 0.478 | num zeros: 521064000.0 | params norm: 197.925 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 2799.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.289737E+00 | gshard_loss: 1.926598E-02 | loss scale: 131072.0 | grad norm: 0.421 | num zeros: 453598944.0 | params norm: 197.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        13568 | elapsed time per iteration (ms): 2687.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.303123E+00 | gshard_loss: 1.925136E-02 | loss scale: 131072.0 | grad norm: 0.334 | num zeros: 356242272.0 | params norm: 198.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 2770.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.286159E+00 | gshard_loss: 1.983671E-02 | loss scale: 131072.0 | grad norm: 0.312 | num zeros: 357203232.0 | params norm: 198.059 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        14080 | elapsed time per iteration (ms): 2675.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.281394E+00 | gshard_loss: 1.980777E-02 | loss scale: 131072.0 | grad norm: 0.368 | num zeros: 369575168.0 | params norm: 198.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 2671.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.297478E+00 | gshard_loss: 2.319626E-02 | loss scale: 131072.0 | grad norm: 0.541 | num zeros: 203682464.0 | params norm: 198.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        14592 | elapsed time per iteration (ms): 2647.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.271998E+00 | gshard_loss: 1.933757E-02 | loss scale: 131072.0 | grad norm: 0.508 | num zeros: 371740928.0 | params norm: 198.183 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 2714.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.262351E+00 | gshard_loss: 2.004912E-02 | loss scale: 131072.0 | grad norm: 0.484 | num zeros: 378557856.0 | params norm: 198.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        15104 | elapsed time per iteration (ms): 2678.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.250740E+00 | gshard_loss: 1.927476E-02 | loss scale: 131072.0 | grad norm: 0.373 | num zeros: 381066816.0 | params norm: 198.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 2703.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.240598E+00 | gshard_loss: 1.931769E-02 | loss scale: 131072.0 | grad norm: 0.304 | num zeros: 419721536.0 | params norm: 198.306 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        15616 | elapsed time per iteration (ms): 2712.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.244861E+00 | gshard_loss: 1.867603E-02 | loss scale: 131072.0 | grad norm: 0.293 | num zeros: 349012960.0 | params norm: 198.347 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 2677.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.224047E+00 | gshard_loss: 1.897118E-02 | loss scale: 131072.0 | grad norm: 0.391 | num zeros: 405371392.0 | params norm: 198.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        16128 | elapsed time per iteration (ms): 2771.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.197895E+00 | gshard_loss: 1.877522E-02 | loss scale: 131072.0 | grad norm: 0.414 | num zeros: 391805856.0 | params norm: 198.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 2757.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.192004E+00 | gshard_loss: 1.830895E-02 | loss scale: 131072.0 | grad norm: 0.433 | num zeros: 425550976.0 | params norm: 198.473 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        16640 | elapsed time per iteration (ms): 2682.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.190573E+00 | gshard_loss: 1.864173E-02 | loss scale: 131072.0 | grad norm: 0.331 | num zeros: 390018208.0 | params norm: 198.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 2635.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.166559E+00 | gshard_loss: 1.962721E-02 | loss scale: 131072.0 | grad norm: 0.382 | num zeros: 385035424.0 | params norm: 198.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        17152 | elapsed time per iteration (ms): 2646.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.162302E+00 | gshard_loss: 1.884282E-02 | loss scale: 131072.0 | grad norm: 0.393 | num zeros: 337712896.0 | params norm: 198.597 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 2645.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.152993E+00 | gshard_loss: 2.016382E-02 | loss scale: 131072.0 | grad norm: 0.414 | num zeros: 363232448.0 | params norm: 198.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        17664 | elapsed time per iteration (ms): 2705.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.140334E+00 | gshard_loss: 1.842561E-02 | loss scale: 131072.0 | grad norm: 1.177 | num zeros: 337110048.0 | params norm: 198.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 2683.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.129205E+00 | gshard_loss: 1.907090E-02 | loss scale: 131072.0 | grad norm: 0.464 | num zeros: 422925376.0 | params norm: 198.717 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        18176 | elapsed time per iteration (ms): 2709.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.114727E+00 | gshard_loss: 1.968224E-02 | loss scale: 131072.0 | grad norm: 0.550 | num zeros: 356406144.0 | params norm: 198.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 2740.4 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 198.756 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        18688 | elapsed time per iteration (ms): 2594.1 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 32768.0 | params norm: 198.756 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 2628.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.113327E+00 | gshard_loss: 1.970144E-02 | loss scale: 32768.0 | grad norm: 0.438 | num zeros: 343292608.0 | params norm: 198.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        19200 | elapsed time per iteration (ms): 2642.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.116770E+00 | gshard_loss: 2.050980E-02 | loss scale: 32768.0 | grad norm: 0.369 | num zeros: 444100512.0 | params norm: 198.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 2725.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.096681E+00 | gshard_loss: 1.774066E-02 | loss scale: 32768.0 | grad norm: 0.525 | num zeros: 391942176.0 | params norm: 198.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        19712 | elapsed time per iteration (ms): 2832.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.112792E+00 | gshard_loss: 1.771770E-02 | loss scale: 32768.0 | grad norm: 0.436 | num zeros: 429824256.0 | params norm: 198.900 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 2703.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.092577E+00 | gshard_loss: 1.796062E-02 | loss scale: 32768.0 | grad norm: 0.486 | num zeros: 408980768.0 | params norm: 198.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        20224 | elapsed time per iteration (ms): 2729.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.080092E+00 | gshard_loss: 1.790080E-02 | loss scale: 32768.0 | grad norm: 0.555 | num zeros: 342014464.0 | params norm: 198.969 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 2673.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.086250E+00 | gshard_loss: 1.856016E-02 | loss scale: 32768.0 | grad norm: 0.686 | num zeros: 433446784.0 | params norm: 199.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        20736 | elapsed time per iteration (ms): 2654.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.070137E+00 | gshard_loss: 1.706710E-02 | loss scale: 32768.0 | grad norm: 0.421 | num zeros: 461295040.0 | params norm: 199.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 2733.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.047470E+00 | gshard_loss: 1.729973E-02 | loss scale: 32768.0 | grad norm: 0.500 | num zeros: 426724576.0 | params norm: 199.068 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        21248 | elapsed time per iteration (ms): 2659.8 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 16384.0 | params norm: 199.068 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 2765.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.042109E+00 | gshard_loss: 1.835126E-02 | loss scale: 16384.0 | grad norm: 0.407 | num zeros: 463594112.0 | params norm: 199.100 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        21760 | elapsed time per iteration (ms): 2742.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.032380E+00 | gshard_loss: 1.809408E-02 | loss scale: 16384.0 | grad norm: 0.294 | num zeros: 423408256.0 | params norm: 199.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 2735.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.050747E+00 | gshard_loss: 1.711425E-02 | loss scale: 16384.0 | grad norm: 0.542 | num zeros: 431171296.0 | params norm: 199.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        22272 | elapsed time per iteration (ms): 2839.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.031018E+00 | gshard_loss: 1.847762E-02 | loss scale: 16384.0 | grad norm: 0.383 | num zeros: 395684384.0 | params norm: 199.199 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 2722.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.022954E+00 | gshard_loss: 1.787784E-02 | loss scale: 16384.0 | grad norm: 0.354 | num zeros: 424162304.0 | params norm: 199.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        22784 | elapsed time per iteration (ms): 2703.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.018672E+00 | gshard_loss: 1.695759E-02 | loss scale: 16384.0 | grad norm: 0.665 | num zeros: 371865792.0 | params norm: 199.264 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 2688.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.009844E+00 | gshard_loss: 1.808587E-02 | loss scale: 16384.0 | grad norm: 0.692 | num zeros: 376757504.0 | params norm: 199.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        23296 | elapsed time per iteration (ms): 2726.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.989324E+00 | gshard_loss: 1.805299E-02 | loss scale: 16384.0 | grad norm: 0.319 | num zeros: 378235584.0 | params norm: 199.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 2749.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.991943E+00 | gshard_loss: 1.692412E-02 | loss scale: 16384.0 | grad norm: 0.661 | num zeros: 375150080.0 | params norm: 199.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        23808 | elapsed time per iteration (ms): 2728.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.980673E+00 | gshard_loss: 1.795844E-02 | loss scale: 16384.0 | grad norm: 0.499 | num zeros: 359264096.0 | params norm: 199.397 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 2794.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.968053E+00 | gshard_loss: 1.745120E-02 | loss scale: 16384.0 | grad norm: 0.367 | num zeros: 343477120.0 | params norm: 199.430 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        24320 | elapsed time per iteration (ms): 2744.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.974687E+00 | gshard_loss: 1.662974E-02 | loss scale: 16384.0 | grad norm: 0.668 | num zeros: 340368192.0 | params norm: 199.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 2746.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.954075E+00 | gshard_loss: 1.710856E-02 | loss scale: 16384.0 | grad norm: 0.280 | num zeros: 325539168.0 | params norm: 199.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        24832 | elapsed time per iteration (ms): 2758.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.961151E+00 | gshard_loss: 1.736923E-02 | loss scale: 16384.0 | grad norm: 0.424 | num zeros: 385500288.0 | params norm: 199.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 2863.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.934459E+00 | gshard_loss: 1.691628E-02 | loss scale: 16384.0 | grad norm: 0.280 | num zeros: 279678784.0 | params norm: 199.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        25344 | elapsed time per iteration (ms): 2703.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.934901E+00 | gshard_loss: 1.677395E-02 | loss scale: 16384.0 | grad norm: 0.536 | num zeros: 257045920.0 | params norm: 199.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 2723.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.934262E+00 | gshard_loss: 1.709787E-02 | loss scale: 16384.0 | grad norm: 0.259 | num zeros: 292542560.0 | params norm: 199.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        25856 | elapsed time per iteration (ms): 2751.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.933619E+00 | gshard_loss: 1.766644E-02 | loss scale: 16384.0 | grad norm: 0.419 | num zeros: 326424544.0 | params norm: 199.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 2778.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.915587E+00 | gshard_loss: 1.685583E-02 | loss scale: 16384.0 | grad norm: 0.250 | num zeros: 293520544.0 | params norm: 199.711 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        26368 | elapsed time per iteration (ms): 2716.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.908533E+00 | gshard_loss: 1.622002E-02 | loss scale: 16384.0 | grad norm: 0.501 | num zeros: 275850720.0 | params norm: 199.749 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 2750.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.912705E+00 | gshard_loss: 1.637834E-02 | loss scale: 16384.0 | grad norm: 0.291 | num zeros: 273688128.0 | params norm: 199.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        26880 | elapsed time per iteration (ms): 2777.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.901263E+00 | gshard_loss: 1.657451E-02 | loss scale: 16384.0 | grad norm: 0.370 | num zeros: 273414912.0 | params norm: 199.826 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 2764.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.893368E+00 | gshard_loss: 1.604415E-02 | loss scale: 16384.0 | grad norm: 0.269 | num zeros: 247326208.0 | params norm: 199.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        27392 | elapsed time per iteration (ms): 2782.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.896916E+00 | gshard_loss: 1.607440E-02 | loss scale: 16384.0 | grad norm: 0.376 | num zeros: 222173696.0 | params norm: 199.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 2737.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.875303E+00 | gshard_loss: 1.642505E-02 | loss scale: 16384.0 | grad norm: 0.272 | num zeros: 223213776.0 | params norm: 199.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        27904 | elapsed time per iteration (ms): 2740.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.866904E+00 | gshard_loss: 1.659497E-02 | loss scale: 16384.0 | grad norm: 0.303 | num zeros: 190293392.0 | params norm: 199.994 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 2761.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.863729E+00 | gshard_loss: 1.622105E-02 | loss scale: 16384.0 | grad norm: 0.439 | num zeros: 140147264.0 | params norm: 200.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        28416 | elapsed time per iteration (ms): 2709.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.851457E+00 | gshard_loss: 1.676404E-02 | loss scale: 16384.0 | grad norm: 0.291 | num zeros: 158449600.0 | params norm: 200.083 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 2716.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.843043E+00 | gshard_loss: 1.633752E-02 | loss scale: 16384.0 | grad norm: 0.260 | num zeros: 188018032.0 | params norm: 200.130 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        28928 | elapsed time per iteration (ms): 2711.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.828279E+00 | gshard_loss: 1.623344E-02 | loss scale: 16384.0 | grad norm: 0.344 | num zeros: 165832736.0 | params norm: 200.179 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 2708.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.828920E+00 | gshard_loss: 1.703825E-02 | loss scale: 16384.0 | grad norm: 0.382 | num zeros: 137458288.0 | params norm: 200.227 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        29440 | elapsed time per iteration (ms): 2740.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.814443E+00 | gshard_loss: 1.662016E-02 | loss scale: 16384.0 | grad norm: 0.254 | num zeros: 134541440.0 | params norm: 200.278 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 2745.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.806029E+00 | gshard_loss: 1.637397E-02 | loss scale: 16384.0 | grad norm: 0.308 | num zeros: 185435488.0 | params norm: 200.330 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        29952 | elapsed time per iteration (ms): 2788.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.805417E+00 | gshard_loss: 1.690796E-02 | loss scale: 16384.0 | grad norm: 0.300 | num zeros: 177730080.0 | params norm: 200.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 2759.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.800014E+00 | gshard_loss: 1.639386E-02 | loss scale: 16384.0 | grad norm: 0.327 | num zeros: 145379904.0 | params norm: 200.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        30464 | elapsed time per iteration (ms): 2774.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.784386E+00 | gshard_loss: 1.641698E-02 | loss scale: 16384.0 | grad norm: 0.294 | num zeros: 171768320.0 | params norm: 200.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 2765.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.784540E+00 | gshard_loss: 1.686848E-02 | loss scale: 16384.0 | grad norm: 0.325 | num zeros: 155164800.0 | params norm: 200.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        30976 | elapsed time per iteration (ms): 2796.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.767509E+00 | gshard_loss: 1.613355E-02 | loss scale: 16384.0 | grad norm: 0.310 | num zeros: 156291904.0 | params norm: 200.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 2700.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.761979E+00 | gshard_loss: 1.631061E-02 | loss scale: 16384.0 | grad norm: 0.271 | num zeros: 126463704.0 | params norm: 200.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        31488 | elapsed time per iteration (ms): 2677.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.754556E+00 | gshard_loss: 1.676470E-02 | loss scale: 16384.0 | grad norm: 0.254 | num zeros: 153576736.0 | params norm: 200.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 2653.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.745630E+00 | gshard_loss: 1.671128E-02 | loss scale: 16384.0 | grad norm: 0.370 | num zeros: 142386016.0 | params norm: 200.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        32000 | elapsed time per iteration (ms): 2790.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.745161E+00 | gshard_loss: 1.732241E-02 | loss scale: 16384.0 | grad norm: 0.379 | num zeros: 187443328.0 | params norm: 200.816 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 2792.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.748810E+00 | gshard_loss: 1.637525E-02 | loss scale: 16384.0 | grad norm: 0.374 | num zeros: 158429792.0 | params norm: 200.873 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        32512 | elapsed time per iteration (ms): 2731.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.739612E+00 | gshard_loss: 1.611848E-02 | loss scale: 16384.0 | grad norm: 0.244 | num zeros: 188220720.0 | params norm: 200.930 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 2724.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.724317E+00 | gshard_loss: 1.627585E-02 | loss scale: 16384.0 | grad norm: 0.297 | num zeros: 188118032.0 | params norm: 200.986 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        33024 | elapsed time per iteration (ms): 2676.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.728112E+00 | gshard_loss: 1.549682E-02 | loss scale: 16384.0 | grad norm: 0.360 | num zeros: 154713904.0 | params norm: 201.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 2718.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.729311E+00 | gshard_loss: 1.575638E-02 | loss scale: 16384.0 | grad norm: 0.502 | num zeros: 155839968.0 | params norm: 201.098 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        33536 | elapsed time per iteration (ms): 2729.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.721862E+00 | gshard_loss: 1.517612E-02 | loss scale: 16384.0 | grad norm: 0.789 | num zeros: 156213472.0 | params norm: 201.153 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 2667.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.714933E+00 | gshard_loss: 1.647303E-02 | loss scale: 16384.0 | grad norm: 0.691 | num zeros: 187169104.0 | params norm: 201.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        34048 | elapsed time per iteration (ms): 2699.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.711759E+00 | gshard_loss: 1.555005E-02 | loss scale: 16384.0 | grad norm: 0.288 | num zeros: 158037824.0 | params norm: 201.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 2720.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.707363E+00 | gshard_loss: 1.529580E-02 | loss scale: 16384.0 | grad norm: 0.594 | num zeros: 186421408.0 | params norm: 201.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        34560 | elapsed time per iteration (ms): 2687.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.686655E+00 | gshard_loss: 1.579161E-02 | loss scale: 16384.0 | grad norm: 0.365 | num zeros: 172079856.0 | params norm: 201.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 2728.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.691679E+00 | gshard_loss: 1.613637E-02 | loss scale: 16384.0 | grad norm: 0.451 | num zeros: 187028624.0 | params norm: 201.421 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        35072 | elapsed time per iteration (ms): 2738.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.680785E+00 | gshard_loss: 1.534382E-02 | loss scale: 16384.0 | grad norm: 0.463 | num zeros: 156511072.0 | params norm: 201.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 2712.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.668955E+00 | gshard_loss: 1.549780E-02 | loss scale: 16384.0 | grad norm: 0.328 | num zeros: 176540224.0 | params norm: 201.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        35584 | elapsed time per iteration (ms): 2757.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.682229E+00 | gshard_loss: 1.665235E-02 | loss scale: 16384.0 | grad norm: 0.457 | num zeros: 162475968.0 | params norm: 201.583 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 2709.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.680120E+00 | gshard_loss: 1.778979E-02 | loss scale: 16384.0 | grad norm: 0.455 | num zeros: 108630768.0 | params norm: 201.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        36096 | elapsed time per iteration (ms): 2747.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.654317E+00 | gshard_loss: 1.555691E-02 | loss scale: 16384.0 | grad norm: 0.579 | num zeros: 113225056.0 | params norm: 201.690 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 2928.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.645900E+00 | gshard_loss: 1.610576E-02 | loss scale: 16384.0 | grad norm: 0.298 | num zeros: 119036776.0 | params norm: 201.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        36608 | elapsed time per iteration (ms): 2741.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.662813E+00 | gshard_loss: 1.665248E-02 | loss scale: 16384.0 | grad norm: 0.471 | num zeros: 154263680.0 | params norm: 201.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 2759.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.645428E+00 | gshard_loss: 1.635351E-02 | loss scale: 16384.0 | grad norm: 0.426 | num zeros: 142008928.0 | params norm: 201.850 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        37120 | elapsed time per iteration (ms): 2711.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.638908E+00 | gshard_loss: 1.664937E-02 | loss scale: 16384.0 | grad norm: 0.364 | num zeros: 170776416.0 | params norm: 201.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 2724.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.647246E+00 | gshard_loss: 1.689396E-02 | loss scale: 16384.0 | grad norm: 0.388 | num zeros: 146599168.0 | params norm: 201.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        37632 | elapsed time per iteration (ms): 2747.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.626125E+00 | gshard_loss: 1.656169E-02 | loss scale: 16384.0 | grad norm: 0.310 | num zeros: 156337216.0 | params norm: 202.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 2732.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.630594E+00 | gshard_loss: 1.640734E-02 | loss scale: 16384.0 | grad norm: 0.321 | num zeros: 126765264.0 | params norm: 202.065 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        38144 | elapsed time per iteration (ms): 2731.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.616299E+00 | gshard_loss: 1.679113E-02 | loss scale: 16384.0 | grad norm: 0.335 | num zeros: 125961936.0 | params norm: 202.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 2700.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.624265E+00 | gshard_loss: 1.613348E-02 | loss scale: 16384.0 | grad norm: 0.265 | num zeros: 157263328.0 | params norm: 202.171 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        38656 | elapsed time per iteration (ms): 2730.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.608422E+00 | gshard_loss: 1.581373E-02 | loss scale: 16384.0 | grad norm: 0.373 | num zeros: 107933632.0 | params norm: 202.223 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 2779.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.616189E+00 | gshard_loss: 1.641779E-02 | loss scale: 16384.0 | grad norm: 0.295 | num zeros: 138427712.0 | params norm: 202.275 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        39168 | elapsed time per iteration (ms): 2682.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.593095E+00 | gshard_loss: 1.621824E-02 | loss scale: 16384.0 | grad norm: 0.265 | num zeros: 96542432.0 | params norm: 202.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 2661.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.599960E+00 | gshard_loss: 1.602256E-02 | loss scale: 16384.0 | grad norm: 0.303 | num zeros: 124269072.0 | params norm: 202.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        39680 | elapsed time per iteration (ms): 2668.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.599929E+00 | gshard_loss: 1.641971E-02 | loss scale: 16384.0 | grad norm: 0.272 | num zeros: 136889984.0 | params norm: 202.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 2733.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.585155E+00 | gshard_loss: 1.640657E-02 | loss scale: 16384.0 | grad norm: 0.309 | num zeros: 93975016.0 | params norm: 202.486 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        40192 | elapsed time per iteration (ms): 2750.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.579831E+00 | gshard_loss: 1.672461E-02 | loss scale: 16384.0 | grad norm: 0.275 | num zeros: 143239440.0 | params norm: 202.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 2726.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.582201E+00 | gshard_loss: 1.699164E-02 | loss scale: 16384.0 | grad norm: 0.490 | num zeros: 154121344.0 | params norm: 202.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        40704 | elapsed time per iteration (ms): 2678.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.577459E+00 | gshard_loss: 1.590820E-02 | loss scale: 16384.0 | grad norm: 0.444 | num zeros: 154389968.0 | params norm: 202.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 2695.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.572419E+00 | gshard_loss: 1.624344E-02 | loss scale: 16384.0 | grad norm: 0.443 | num zeros: 153703840.0 | params norm: 202.697 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        41216 | elapsed time per iteration (ms): 2672.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.576624E+00 | gshard_loss: 1.598270E-02 | loss scale: 16384.0 | grad norm: 0.374 | num zeros: 142265984.0 | params norm: 202.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 2707.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.570033E+00 | gshard_loss: 1.664673E-02 | loss scale: 16384.0 | grad norm: 0.309 | num zeros: 170742160.0 | params norm: 202.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        41728 | elapsed time per iteration (ms): 2676.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.570882E+00 | gshard_loss: 1.646980E-02 | loss scale: 16384.0 | grad norm: 0.255 | num zeros: 140193536.0 | params norm: 202.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 2761.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.555348E+00 | gshard_loss: 1.636604E-02 | loss scale: 16384.0 | grad norm: 0.371 | num zeros: 156083152.0 | params norm: 202.910 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        42240 | elapsed time per iteration (ms): 2824.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.556257E+00 | gshard_loss: 1.660796E-02 | loss scale: 16384.0 | grad norm: 0.420 | num zeros: 138403520.0 | params norm: 202.963 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 2724.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.529241E+00 | gshard_loss: 1.603922E-02 | loss scale: 16384.0 | grad norm: 0.428 | num zeros: 125956472.0 | params norm: 203.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        42752 | elapsed time per iteration (ms): 2707.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.531492E+00 | gshard_loss: 1.623259E-02 | loss scale: 16384.0 | grad norm: 0.288 | num zeros: 172234704.0 | params norm: 203.071 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 2741.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.506191E+00 | gshard_loss: 1.579999E-02 | loss scale: 16384.0 | grad norm: 0.253 | num zeros: 152983696.0 | params norm: 203.127 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        43264 | elapsed time per iteration (ms): 2759.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.514717E+00 | gshard_loss: 1.590155E-02 | loss scale: 16384.0 | grad norm: 0.267 | num zeros: 169428784.0 | params norm: 203.183 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 2742.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.524785E+00 | gshard_loss: 1.624038E-02 | loss scale: 16384.0 | grad norm: 0.293 | num zeros: 171114048.0 | params norm: 203.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        43776 | elapsed time per iteration (ms): 2718.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.512037E+00 | gshard_loss: 1.615601E-02 | loss scale: 16384.0 | grad norm: 0.292 | num zeros: 139423792.0 | params norm: 203.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 2707.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.507287E+00 | gshard_loss: 1.627486E-02 | loss scale: 16384.0 | grad norm: 0.343 | num zeros: 156956288.0 | params norm: 203.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        44288 | elapsed time per iteration (ms): 2677.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.488074E+00 | gshard_loss: 1.625922E-02 | loss scale: 16384.0 | grad norm: 0.334 | num zeros: 92068752.0 | params norm: 203.409 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 2744.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.490677E+00 | gshard_loss: 1.665373E-02 | loss scale: 16384.0 | grad norm: 0.324 | num zeros: 142032448.0 | params norm: 203.466 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        44800 | elapsed time per iteration (ms): 2768.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.491628E+00 | gshard_loss: 1.592596E-02 | loss scale: 16384.0 | grad norm: 0.427 | num zeros: 140469488.0 | params norm: 203.523 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 2715.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.481719E+00 | gshard_loss: 1.676090E-02 | loss scale: 16384.0 | grad norm: 0.573 | num zeros: 126823328.0 | params norm: 203.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        45312 | elapsed time per iteration (ms): 2737.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.476352E+00 | gshard_loss: 1.577376E-02 | loss scale: 16384.0 | grad norm: 0.534 | num zeros: 76474320.0 | params norm: 203.635 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 2665.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.465215E+00 | gshard_loss: 1.603007E-02 | loss scale: 16384.0 | grad norm: 0.301 | num zeros: 123161472.0 | params norm: 203.691 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        45824 | elapsed time per iteration (ms): 2715.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.456298E+00 | gshard_loss: 1.627484E-02 | loss scale: 16384.0 | grad norm: 0.365 | num zeros: 153439856.0 | params norm: 203.746 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 2644.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.444745E+00 | gshard_loss: 1.612550E-02 | loss scale: 16384.0 | grad norm: 0.444 | num zeros: 76238264.0 | params norm: 203.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        46336 | elapsed time per iteration (ms): 2664.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.445311E+00 | gshard_loss: 1.710955E-02 | loss scale: 16384.0 | grad norm: 0.249 | num zeros: 136167168.0 | params norm: 203.859 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 2693.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.461669E+00 | gshard_loss: 1.688124E-02 | loss scale: 16384.0 | grad norm: 0.363 | num zeros: 104129128.0 | params norm: 203.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        46848 | elapsed time per iteration (ms): 2725.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.439752E+00 | gshard_loss: 1.627797E-02 | loss scale: 16384.0 | grad norm: 0.347 | num zeros: 120020680.0 | params norm: 203.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 2665.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.431191E+00 | gshard_loss: 1.616498E-02 | loss scale: 16384.0 | grad norm: 0.292 | num zeros: 102852904.0 | params norm: 204.028 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        47360 | elapsed time per iteration (ms): 2713.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.419163E+00 | gshard_loss: 1.602984E-02 | loss scale: 16384.0 | grad norm: 0.269 | num zeros: 136358352.0 | params norm: 204.084 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 2780.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.421947E+00 | gshard_loss: 1.584485E-02 | loss scale: 16384.0 | grad norm: 0.310 | num zeros: 137243440.0 | params norm: 204.140 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        47872 | elapsed time per iteration (ms): 2660.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.423560E+00 | gshard_loss: 1.667767E-02 | loss scale: 16384.0 | grad norm: 0.280 | num zeros: 121578144.0 | params norm: 204.196 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 2717.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.415874E+00 | gshard_loss: 1.635579E-02 | loss scale: 16384.0 | grad norm: 0.263 | num zeros: 104288304.0 | params norm: 204.252 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        48384 | elapsed time per iteration (ms): 2646.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.410381E+00 | gshard_loss: 1.617809E-02 | loss scale: 16384.0 | grad norm: 0.326 | num zeros: 104870720.0 | params norm: 204.308 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 2690.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.399829E+00 | gshard_loss: 1.634905E-02 | loss scale: 16384.0 | grad norm: 0.302 | num zeros: 154766416.0 | params norm: 204.363 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        48896 | elapsed time per iteration (ms): 2675.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.401526E+00 | gshard_loss: 1.608440E-02 | loss scale: 16384.0 | grad norm: 0.339 | num zeros: 136976544.0 | params norm: 204.418 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 2771.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.398150E+00 | gshard_loss: 1.597865E-02 | loss scale: 16384.0 | grad norm: 0.264 | num zeros: 152359568.0 | params norm: 204.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        49408 | elapsed time per iteration (ms): 2725.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.373826E+00 | gshard_loss: 1.605455E-02 | loss scale: 16384.0 | grad norm: 0.261 | num zeros: 154104832.0 | params norm: 204.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 2688.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.378929E+00 | gshard_loss: 1.596824E-02 | loss scale: 16384.0 | grad norm: 0.300 | num zeros: 110791992.0 | params norm: 204.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        49920 | elapsed time per iteration (ms): 2725.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.372061E+00 | gshard_loss: 1.600684E-02 | loss scale: 16384.0 | grad norm: 0.199 | num zeros: 139930400.0 | params norm: 204.635 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 2792.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.371572E+00 | gshard_loss: 1.594413E-02 | loss scale: 16384.0 | grad norm: 0.255 | num zeros: 120111200.0 | params norm: 204.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:        50432 | elapsed time per iteration (ms): 2703.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.368511E+00 | gshard_loss: 1.591200E-02 | loss scale: 16384.0 | grad norm: 0.230 | num zeros: 135746832.0 | params norm: 204.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 2726.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.362564E+00 | gshard_loss: 1.599166E-02 | loss scale: 16384.0 | grad norm: 0.236 | num zeros: 108823976.0 | params norm: 204.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:        50944 | elapsed time per iteration (ms): 2725.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.361580E+00 | gshard_loss: 1.576607E-02 | loss scale: 16384.0 | grad norm: 0.256 | num zeros: 121833824.0 | params norm: 204.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 2675.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.361100E+00 | gshard_loss: 1.583563E-02 | loss scale: 16384.0 | grad norm: 0.248 | num zeros: 137988992.0 | params norm: 204.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-01-05 06:34:54 
