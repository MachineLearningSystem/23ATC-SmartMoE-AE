using world size: 32, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 4 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 8
  data_path ....................................... ['/GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 2
  expert_ep_size .................................. 4
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 8
  num_layers ...................................... 8
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 4
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 51200
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 32
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 32
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 4
[INFO] 0 in EP group [0, 1, 2, 3]
[INFO] 2 in EP group [0, 1, 2, 3]
[INFO] 1 in EP group [0, 1, 2, 3]
[INFO] 3 in EP group [0, 1, 2, 3]
[INFO] 4 in EP group [4, 5, 6, 7]
[INFO] 5 in EP group [4, 5, 6, 7]
[INFO] 7 in EP group [4, 5, 6, 7]
[INFO] 6 in EP group [4, 5, 6, 7]
[INFO] 0 in DP group [0, 4]
[INFO] 4 in DP group [0, 4]
[INFO] 1 in DP group [1, 5]
[INFO] 5 in DP group [1, 5]
[INFO] 2 in DP group [2, 6]
[INFO] 6 in DP group [2, 6]
[INFO] 3 in DP group [3, 7]
[INFO] 7 in DP group [3, 7]
[INFO] 8 in EP group [8, 9, 10, 11]
[INFO] 11 in EP group [8, 9, 10, 11]
[INFO] 9 in EP group [8, 9, 10, 11]
[INFO] 10 in EP group [8, 9, 10, 11]
[INFO] 12 in EP group [12, 13, 14, 15]
[INFO] 14 in EP group [12, 13, 14, 15]
[INFO] 13 in EP group [12, 13, 14, 15]
[INFO] 15 in EP group [12, 13, 14, 15]
[INFO] 8 in DP group [8, 12]
[INFO] 12 in DP group [8, 12]
[INFO] 9 in DP group [9, 13]
[INFO] 13 in DP group [9, 13]
[INFO] 10 in DP group [10, 14]
[INFO] 14 in DP group [10, 14]
[INFO] 11 in DP group [11, 15]
[INFO] 15 in DP group [11, 15]
[INFO] 16 in EP group [16, 17, 18, 19]
[INFO] 17 in EP group [16, 17, 18, 19]
[INFO] 19 in EP group [16, 17, 18, 19]
[INFO] 18 in EP group [16, 17, 18, 19]
[INFO] 20 in EP group [20, 21, 22, 23]
[INFO] 23 in EP group [20, 21, 22, 23]
[INFO] 21 in EP group [20, 21, 22, 23]
[INFO] 22 in EP group [20, 21, 22, 23]
[INFO] 16 in DP group [16, 20]
[INFO] 20 in DP group [16, 20]
[INFO] 17 in DP group [17, 21]
[INFO] 21 in DP group [17, 21]
[INFO] 18 in DP group [18, 22]
[INFO] 22 in DP group [18, 22]
[INFO] 19 in DP group [19, 23]
[INFO] 23 in DP group [19, 23]
[INFO] 24 in EP group [24, 25, 26, 27]
[INFO] 27 in EP group [24, 25, 26, 27]
[INFO] 26 in EP group [24, 25, 26, 27]
[INFO] 25 in EP group [24, 25, 26, 27]
[INFO] 28 in EP group [28, 29, 30, 31]
[INFO] 29 in EP group [28, 29, 30, 31]
[INFO] 30 in EP group [28, 29, 30, 31]
[INFO] 31 in EP group [28, 29, 30, 31]
[INFO] 24 in DP group [24, 28]
[INFO] 28 in DP group [24, 28]
[INFO] 25 in DP group [25, 29]
[INFO] 29 in DP group [25, 29]
[INFO] 26 in DP group [26, 30]
[INFO] 30 in DP group [26, 30]
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 2 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 30 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 3 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 5 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 6 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 7 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 16 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 17 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[INFO] 27 in DP group [27, 31]
[W ProcessGroupNCCL.cpp:1569] Rank 9 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 10 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 21 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 22 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 23 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 4 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 24 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 26 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 27 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> setting random seeds to 1234 ...
[W ProcessGroupNCCL.cpp:1569] Rank 20 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 19 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 13 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[INFO] 31 in DP group [27, 31]
[W ProcessGroupNCCL.cpp:1569] Rank 12 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 14 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[W ProcessGroupNCCL.cpp:1569] Rank 15 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 28 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 25 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 29 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 18 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
> compiling dataset index builder ...
[W ProcessGroupNCCL.cpp:1569] Rank 11 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 8 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 31 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
make: 进入目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
make: 对“default”无需做任何事。
make: 离开目录“/GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/data”
>>> done with dataset index builder. Compilation time: 0.249 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /GPUFS/thu_wgchen_2/zms/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
>>> done with compiling and loading fused kernels. Compilation time: 8.714 seconds
time to initialize megatron (seconds): 12.856
[after megatron is initialized] datetime: 2023-01-05 05:01:00 
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
building GPT model ...
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
hhs=4096
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 302252096
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 302252096
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 407371840
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 405278784
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-01-05 05:01:08 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.021021 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /GPUFS/thu_wgchen_2/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.011 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-01-05 05:01:21 
done with setup ...
training ...
[before the start of training step] datetime: 2023-01-05 05:01:21 
 iteration        1/     200 | consumed samples:          256 | elapsed time per iteration (ms): 10515.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082706E+01 | loss scale: 131072.0 | grad norm: 8.028 | num zeros: 129398.0 | params norm: 195.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 8] (after 1 iterations) memory (MB) | allocated: 5773.01171875 | max allocated: 5773.01416015625 | reserved: 6226.0 | max reserved: 6226.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 5773.01171875 | max allocated: 5773.01416015625 | reserved: 6030.0 | max reserved: 6030.0
[Rank 24] (after 1 iterations) memory (MB) | allocated: 7742.333984375 | max allocated: 7742.36474609375 | reserved: 7858.0 | max reserved: 7858.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 7771.21484375 | max allocated: 7771.21728515625 | reserved: 8444.0 | max reserved: 8444.0
 iteration        2/     200 | consumed samples:          512 | elapsed time per iteration (ms): 3069.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.041283E+01 | loss scale: 131072.0 | grad norm: 5.004 | num zeros: 491305376.0 | params norm: 195.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:          768 | elapsed time per iteration (ms): 2805.0 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 195.052 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 2903.8 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 195.052 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         1280 | elapsed time per iteration (ms): 2916.2 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 32768.0 | params norm: 195.052 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 2812.1 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 16384.0 | params norm: 195.052 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         1792 | elapsed time per iteration (ms): 2778.8 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 8192.0 | params norm: 195.052 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 2841.1 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 4096.0 | params norm: 195.052 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         2304 | elapsed time per iteration (ms): 2849.1 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 2048.0 | params norm: 195.052 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 2845.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.045651E+01 | loss scale: 2048.0 | grad norm: 391.974 | num zeros: 17099484.0 | params norm: 195.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         2816 | elapsed time per iteration (ms): 2797.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.012301E+01 | loss scale: 2048.0 | grad norm: 4.732 | num zeros: 717351488.0 | params norm: 195.148 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 2744.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.867376E+00 | loss scale: 2048.0 | grad norm: 4.535 | num zeros: 633926208.0 | params norm: 195.208 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         3328 | elapsed time per iteration (ms): 2730.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.691161E+00 | loss scale: 2048.0 | grad norm: 4.488 | num zeros: 698822272.0 | params norm: 195.272 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 2734.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.502667E+00 | loss scale: 2048.0 | grad norm: 4.499 | num zeros: 815377088.0 | params norm: 195.343 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         3840 | elapsed time per iteration (ms): 2750.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.317632E+00 | loss scale: 2048.0 | grad norm: 4.502 | num zeros: 774135168.0 | params norm: 195.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 2764.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.145238E+00 | loss scale: 2048.0 | grad norm: 4.460 | num zeros: 854215552.0 | params norm: 195.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         4352 | elapsed time per iteration (ms): 2693.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.972300E+00 | loss scale: 2048.0 | grad norm: 4.458 | num zeros: 889271168.0 | params norm: 195.562 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 2709.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.812619E+00 | loss scale: 2048.0 | grad norm: 4.455 | num zeros: 877638400.0 | params norm: 195.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         4864 | elapsed time per iteration (ms): 2752.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.659619E+00 | loss scale: 2048.0 | grad norm: 4.432 | num zeros: 891788288.0 | params norm: 195.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 2691.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.518049E+00 | loss scale: 2048.0 | grad norm: 4.379 | num zeros: 887223296.0 | params norm: 195.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:         5376 | elapsed time per iteration (ms): 2723.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.391681E+00 | loss scale: 2048.0 | grad norm: 4.333 | num zeros: 915859200.0 | params norm: 195.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 2707.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.268946E+00 | loss scale: 2048.0 | grad norm: 4.238 | num zeros: 910896512.0 | params norm: 195.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:         5888 | elapsed time per iteration (ms): 2687.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.144370E+00 | loss scale: 2048.0 | grad norm: 4.223 | num zeros: 913174720.0 | params norm: 196.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 2718.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.038374E+00 | loss scale: 2048.0 | grad norm: 4.127 | num zeros: 914834944.0 | params norm: 196.084 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:         6400 | elapsed time per iteration (ms): 2720.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.938809E+00 | loss scale: 2048.0 | grad norm: 4.022 | num zeros: 909334400.0 | params norm: 196.157 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 2727.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.850584E+00 | loss scale: 2048.0 | grad norm: 3.862 | num zeros: 941495040.0 | params norm: 196.232 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:         6912 | elapsed time per iteration (ms): 2723.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.763284E+00 | loss scale: 2048.0 | grad norm: 3.704 | num zeros: 943731776.0 | params norm: 196.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 2713.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.685511E+00 | loss scale: 2048.0 | grad norm: 3.486 | num zeros: 923139712.0 | params norm: 196.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:         7424 | elapsed time per iteration (ms): 2673.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.610932E+00 | loss scale: 2048.0 | grad norm: 3.221 | num zeros: 932983936.0 | params norm: 196.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 2685.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.538220E+00 | loss scale: 2048.0 | grad norm: 2.909 | num zeros: 941441728.0 | params norm: 196.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:         7936 | elapsed time per iteration (ms): 2692.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.490085E+00 | loss scale: 2048.0 | grad norm: 2.511 | num zeros: 906947264.0 | params norm: 196.585 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 2694.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.431264E+00 | loss scale: 2048.0 | grad norm: 2.088 | num zeros: 729949376.0 | params norm: 196.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:         8448 | elapsed time per iteration (ms): 2728.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.407162E+00 | loss scale: 2048.0 | grad norm: 1.562 | num zeros: 846944640.0 | params norm: 196.721 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 2729.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.375483E+00 | loss scale: 2048.0 | grad norm: 1.054 | num zeros: 848100160.0 | params norm: 196.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:         8960 | elapsed time per iteration (ms): 2702.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.355402E+00 | loss scale: 2048.0 | grad norm: 0.500 | num zeros: 857291648.0 | params norm: 196.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 2826.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.340016E+00 | loss scale: 2048.0 | grad norm: 0.239 | num zeros: 766650240.0 | params norm: 196.934 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:         9472 | elapsed time per iteration (ms): 2714.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.349657E+00 | loss scale: 2048.0 | grad norm: 0.398 | num zeros: 667974400.0 | params norm: 197.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 2709.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.349181E+00 | loss scale: 2048.0 | grad norm: 0.430 | num zeros: 840506816.0 | params norm: 197.070 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:         9984 | elapsed time per iteration (ms): 2657.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.361187E+00 | loss scale: 2048.0 | grad norm: 0.701 | num zeros: 702348160.0 | params norm: 197.133 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 2685.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.335433E+00 | loss scale: 2048.0 | grad norm: 0.439 | num zeros: 765888896.0 | params norm: 197.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        10496 | elapsed time per iteration (ms): 2681.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.330507E+00 | loss scale: 2048.0 | grad norm: 0.451 | num zeros: 940835200.0 | params norm: 197.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 2726.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.351985E+00 | loss scale: 2048.0 | grad norm: 0.714 | num zeros: 812025216.0 | params norm: 197.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        11008 | elapsed time per iteration (ms): 2834.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.333973E+00 | loss scale: 2048.0 | grad norm: 0.848 | num zeros: 766177856.0 | params norm: 197.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 2793.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.312290E+00 | loss scale: 2048.0 | grad norm: 0.373 | num zeros: 751760128.0 | params norm: 197.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        11520 | elapsed time per iteration (ms): 2818.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.299500E+00 | loss scale: 2048.0 | grad norm: 0.841 | num zeros: 758492928.0 | params norm: 197.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 2807.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.278966E+00 | loss scale: 2048.0 | grad norm: 0.575 | num zeros: 766137792.0 | params norm: 197.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        12032 | elapsed time per iteration (ms): 2774.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.268894E+00 | loss scale: 2048.0 | grad norm: 0.702 | num zeros: 698152384.0 | params norm: 197.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 2808.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.261284E+00 | loss scale: 2048.0 | grad norm: 0.442 | num zeros: 649534080.0 | params norm: 197.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        12544 | elapsed time per iteration (ms): 2873.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.253139E+00 | loss scale: 2048.0 | grad norm: 0.364 | num zeros: 600622016.0 | params norm: 197.683 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 2971.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.297558E+00 | loss scale: 2048.0 | grad norm: 2.624 | num zeros: 171419472.0 | params norm: 197.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        13056 | elapsed time per iteration (ms): 2865.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.240007E+00 | loss scale: 2048.0 | grad norm: 0.481 | num zeros: 592469696.0 | params norm: 197.781 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 2868.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.223063E+00 | loss scale: 2048.0 | grad norm: 0.525 | num zeros: 644579712.0 | params norm: 197.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        13568 | elapsed time per iteration (ms): 2901.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.228947E+00 | loss scale: 2048.0 | grad norm: 0.374 | num zeros: 677007360.0 | params norm: 197.876 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 2855.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.201834E+00 | loss scale: 2048.0 | grad norm: 0.388 | num zeros: 674971904.0 | params norm: 197.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        14080 | elapsed time per iteration (ms): 2860.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.209991E+00 | loss scale: 2048.0 | grad norm: 3.722 | num zeros: 648383232.0 | params norm: 197.969 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 2878.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.211817E+00 | loss scale: 2048.0 | grad norm: 0.719 | num zeros: 443480128.0 | params norm: 198.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        14592 | elapsed time per iteration (ms): 2878.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.178798E+00 | loss scale: 2048.0 | grad norm: 0.434 | num zeros: 765090624.0 | params norm: 198.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 2845.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.171490E+00 | loss scale: 2048.0 | grad norm: 0.447 | num zeros: 776887232.0 | params norm: 198.105 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        15104 | elapsed time per iteration (ms): 2838.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.162533E+00 | loss scale: 2048.0 | grad norm: 0.287 | num zeros: 684878080.0 | params norm: 198.145 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 2799.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.165430E+00 | loss scale: 2048.0 | grad norm: 0.665 | num zeros: 696528896.0 | params norm: 198.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        15616 | elapsed time per iteration (ms): 2799.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.168608E+00 | loss scale: 2048.0 | grad norm: 0.404 | num zeros: 640286912.0 | params norm: 198.227 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 2839.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.158676E+00 | loss scale: 2048.0 | grad norm: 0.504 | num zeros: 688340928.0 | params norm: 198.263 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        16128 | elapsed time per iteration (ms): 2792.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.137926E+00 | loss scale: 2048.0 | grad norm: 0.274 | num zeros: 690613888.0 | params norm: 198.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 2826.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.142838E+00 | loss scale: 2048.0 | grad norm: 0.605 | num zeros: 689979456.0 | params norm: 198.333 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        16640 | elapsed time per iteration (ms): 2794.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.143062E+00 | loss scale: 2048.0 | grad norm: 0.417 | num zeros: 655446144.0 | params norm: 198.365 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 2819.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.125575E+00 | loss scale: 2048.0 | grad norm: 0.645 | num zeros: 623312896.0 | params norm: 198.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        17152 | elapsed time per iteration (ms): 2821.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.243382E+00 | loss scale: 2048.0 | grad norm: 22.268 | num zeros: 357317760.0 | params norm: 198.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 3421.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.126084E+00 | loss scale: 2048.0 | grad norm: 0.719 | num zeros: 696890560.0 | params norm: 198.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        17664 | elapsed time per iteration (ms): 3441.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.122874E+00 | loss scale: 2048.0 | grad norm: 0.807 | num zeros: 716326144.0 | params norm: 198.467 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 3691.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.112522E+00 | loss scale: 2048.0 | grad norm: 0.543 | num zeros: 770592576.0 | params norm: 198.491 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        18176 | elapsed time per iteration (ms): 3413.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.098745E+00 | loss scale: 2048.0 | grad norm: 0.467 | num zeros: 801358208.0 | params norm: 198.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 3939.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.120042E+00 | loss scale: 2048.0 | grad norm: 1.203 | num zeros: 751106432.0 | params norm: 198.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        18688 | elapsed time per iteration (ms): 3564.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.119106E+00 | loss scale: 2048.0 | grad norm: 0.965 | num zeros: 772387520.0 | params norm: 198.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 4378.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.097798E+00 | loss scale: 2048.0 | grad norm: 0.675 | num zeros: 728373312.0 | params norm: 198.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        19200 | elapsed time per iteration (ms): 3874.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.103210E+00 | loss scale: 2048.0 | grad norm: 0.669 | num zeros: 711105856.0 | params norm: 198.598 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 4500.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.082537E+00 | loss scale: 2048.0 | grad norm: 0.433 | num zeros: 741000448.0 | params norm: 198.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        19712 | elapsed time per iteration (ms): 3756.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.104644E+00 | loss scale: 2048.0 | grad norm: 0.645 | num zeros: 741562240.0 | params norm: 198.634 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 3703.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.084274E+00 | loss scale: 2048.0 | grad norm: 0.456 | num zeros: 729810880.0 | params norm: 198.650 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        20224 | elapsed time per iteration (ms): 3661.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.078397E+00 | loss scale: 2048.0 | grad norm: 0.665 | num zeros: 733683008.0 | params norm: 198.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 3790.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.086035E+00 | loss scale: 2048.0 | grad norm: 0.363 | num zeros: 736443392.0 | params norm: 198.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        20736 | elapsed time per iteration (ms): 3727.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.082474E+00 | loss scale: 2048.0 | grad norm: 0.582 | num zeros: 742523520.0 | params norm: 198.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 4510.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.061635E+00 | loss scale: 2048.0 | grad norm: 0.484 | num zeros: 747642368.0 | params norm: 198.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        21248 | elapsed time per iteration (ms): 3801.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.090865E+00 | loss scale: 2048.0 | grad norm: 0.527 | num zeros: 721712704.0 | params norm: 198.747 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 3939.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.057070E+00 | loss scale: 2048.0 | grad norm: 0.370 | num zeros: 737873216.0 | params norm: 198.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        21760 | elapsed time per iteration (ms): 4512.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.053363E+00 | loss scale: 2048.0 | grad norm: 0.394 | num zeros: 727888064.0 | params norm: 198.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 3701.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.069778E+00 | loss scale: 2048.0 | grad norm: 0.474 | num zeros: 690760448.0 | params norm: 198.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        22272 | elapsed time per iteration (ms): 3204.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.055865E+00 | loss scale: 2048.0 | grad norm: 0.586 | num zeros: 655592384.0 | params norm: 198.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 3658.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.047627E+00 | loss scale: 2048.0 | grad norm: 0.400 | num zeros: 662445440.0 | params norm: 198.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        22784 | elapsed time per iteration (ms): 3133.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.040293E+00 | loss scale: 2048.0 | grad norm: 0.342 | num zeros: 617156096.0 | params norm: 198.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 3767.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.044499E+00 | loss scale: 2048.0 | grad norm: 0.504 | num zeros: 633808448.0 | params norm: 198.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        23296 | elapsed time per iteration (ms): 3783.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.025566E+00 | loss scale: 2048.0 | grad norm: 0.422 | num zeros: 606669952.0 | params norm: 198.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 4321.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.019657E+00 | loss scale: 2048.0 | grad norm: 0.554 | num zeros: 601055360.0 | params norm: 198.943 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        23808 | elapsed time per iteration (ms): 3791.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.011107E+00 | loss scale: 2048.0 | grad norm: 0.433 | num zeros: 542170112.0 | params norm: 198.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 4070.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.004296E+00 | loss scale: 2048.0 | grad norm: 0.496 | num zeros: 585731328.0 | params norm: 198.990 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        24320 | elapsed time per iteration (ms): 3779.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.000203E+00 | loss scale: 2048.0 | grad norm: 0.547 | num zeros: 594331648.0 | params norm: 199.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 3253.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.996801E+00 | loss scale: 2048.0 | grad norm: 0.740 | num zeros: 546616704.0 | params norm: 199.038 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        24832 | elapsed time per iteration (ms): 3811.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.995269E+00 | loss scale: 2048.0 | grad norm: 0.618 | num zeros: 525820160.0 | params norm: 199.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 3395.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.981135E+00 | loss scale: 2048.0 | grad norm: 0.862 | num zeros: 567402944.0 | params norm: 199.086 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        25344 | elapsed time per iteration (ms): 3450.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.967646E+00 | loss scale: 2048.0 | grad norm: 0.534 | num zeros: 538932736.0 | params norm: 199.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 3538.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.965639E+00 | loss scale: 2048.0 | grad norm: 0.426 | num zeros: 542298112.0 | params norm: 199.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        25856 | elapsed time per iteration (ms): 3542.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.955945E+00 | loss scale: 2048.0 | grad norm: 0.515 | num zeros: 590613952.0 | params norm: 199.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 3769.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.940804E+00 | loss scale: 2048.0 | grad norm: 0.731 | num zeros: 521356672.0 | params norm: 199.180 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        26368 | elapsed time per iteration (ms): 3526.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.932374E+00 | loss scale: 2048.0 | grad norm: 1.006 | num zeros: 489618752.0 | params norm: 199.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 3217.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.944881E+00 | loss scale: 2048.0 | grad norm: 0.827 | num zeros: 584366016.0 | params norm: 199.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        26880 | elapsed time per iteration (ms): 3578.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.914814E+00 | loss scale: 2048.0 | grad norm: 0.428 | num zeros: 576557952.0 | params norm: 199.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 3701.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.930848E+00 | loss scale: 2048.0 | grad norm: 1.112 | num zeros: 536337472.0 | params norm: 199.281 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        27392 | elapsed time per iteration (ms): 3908.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.902934E+00 | loss scale: 2048.0 | grad norm: 0.466 | num zeros: 580113024.0 | params norm: 199.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 3569.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.890363E+00 | loss scale: 2048.0 | grad norm: 0.837 | num zeros: 578646784.0 | params norm: 199.328 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        27904 | elapsed time per iteration (ms): 3919.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.866957E+00 | loss scale: 2048.0 | grad norm: 0.354 | num zeros: 604651904.0 | params norm: 199.353 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 3687.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.874038E+00 | loss scale: 2048.0 | grad norm: 0.825 | num zeros: 547693888.0 | params norm: 199.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        28416 | elapsed time per iteration (ms): 3831.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.848804E+00 | loss scale: 2048.0 | grad norm: 0.452 | num zeros: 587988480.0 | params norm: 199.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 3374.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.849821E+00 | loss scale: 2048.0 | grad norm: 0.813 | num zeros: 560466304.0 | params norm: 199.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        28928 | elapsed time per iteration (ms): 3537.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.831650E+00 | loss scale: 2048.0 | grad norm: 0.712 | num zeros: 558830784.0 | params norm: 199.466 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 3299.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.833510E+00 | loss scale: 2048.0 | grad norm: 0.679 | num zeros: 545642432.0 | params norm: 199.493 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        29440 | elapsed time per iteration (ms): 4280.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.818797E+00 | loss scale: 2048.0 | grad norm: 0.634 | num zeros: 554114048.0 | params norm: 199.521 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 3581.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.808490E+00 | loss scale: 2048.0 | grad norm: 0.621 | num zeros: 574415616.0 | params norm: 199.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        29952 | elapsed time per iteration (ms): 3774.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.807617E+00 | loss scale: 2048.0 | grad norm: 0.471 | num zeros: 551712640.0 | params norm: 199.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 3753.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.803351E+00 | loss scale: 2048.0 | grad norm: 0.513 | num zeros: 562961664.0 | params norm: 199.604 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        30464 | elapsed time per iteration (ms): 3914.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.791443E+00 | loss scale: 2048.0 | grad norm: 0.446 | num zeros: 548787840.0 | params norm: 199.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 3559.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.789346E+00 | loss scale: 2048.0 | grad norm: 0.412 | num zeros: 556872640.0 | params norm: 199.659 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        30976 | elapsed time per iteration (ms): 3526.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.776575E+00 | loss scale: 2048.0 | grad norm: 0.381 | num zeros: 560438208.0 | params norm: 199.686 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 3397.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.773923E+00 | loss scale: 2048.0 | grad norm: 0.491 | num zeros: 552432832.0 | params norm: 199.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        31488 | elapsed time per iteration (ms): 3112.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.766543E+00 | loss scale: 2048.0 | grad norm: 0.373 | num zeros: 554320256.0 | params norm: 199.739 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 3030.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.756044E+00 | loss scale: 2048.0 | grad norm: 0.420 | num zeros: 534047648.0 | params norm: 199.767 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        32000 | elapsed time per iteration (ms): 3266.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.752860E+00 | loss scale: 2048.0 | grad norm: 0.336 | num zeros: 529698336.0 | params norm: 199.795 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 3162.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.761840E+00 | loss scale: 2048.0 | grad norm: 0.410 | num zeros: 509070144.0 | params norm: 199.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        32512 | elapsed time per iteration (ms): 3496.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.757011E+00 | loss scale: 2048.0 | grad norm: 0.379 | num zeros: 488816864.0 | params norm: 199.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 3507.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.739939E+00 | loss scale: 2048.0 | grad norm: 0.421 | num zeros: 487825856.0 | params norm: 199.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        33024 | elapsed time per iteration (ms): 2920.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.745783E+00 | loss scale: 2048.0 | grad norm: 0.374 | num zeros: 506154784.0 | params norm: 199.901 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 2908.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.742779E+00 | loss scale: 2048.0 | grad norm: 0.532 | num zeros: 504174080.0 | params norm: 199.927 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        33536 | elapsed time per iteration (ms): 3425.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.727899E+00 | loss scale: 2048.0 | grad norm: 0.660 | num zeros: 504615616.0 | params norm: 199.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 3086.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.719461E+00 | loss scale: 2048.0 | grad norm: 0.371 | num zeros: 504474688.0 | params norm: 199.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        34048 | elapsed time per iteration (ms): 3508.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.732990E+00 | loss scale: 2048.0 | grad norm: 0.680 | num zeros: 523830656.0 | params norm: 200.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 3466.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.725813E+00 | loss scale: 2048.0 | grad norm: 0.871 | num zeros: 517131136.0 | params norm: 200.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        34560 | elapsed time per iteration (ms): 3038.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.727047E+00 | loss scale: 2048.0 | grad norm: 1.174 | num zeros: 474565792.0 | params norm: 200.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 3244.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.717247E+00 | loss scale: 2048.0 | grad norm: 0.655 | num zeros: 504798560.0 | params norm: 200.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        35072 | elapsed time per iteration (ms): 3195.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.711204E+00 | loss scale: 2048.0 | grad norm: 0.783 | num zeros: 493158112.0 | params norm: 200.108 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 3263.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.700622E+00 | loss scale: 2048.0 | grad norm: 0.554 | num zeros: 477508192.0 | params norm: 200.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        35584 | elapsed time per iteration (ms): 3304.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.710303E+00 | loss scale: 2048.0 | grad norm: 0.683 | num zeros: 471334752.0 | params norm: 200.156 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 4050.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.709350E+00 | loss scale: 2048.0 | grad norm: 0.796 | num zeros: 468866976.0 | params norm: 200.180 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        36096 | elapsed time per iteration (ms): 3948.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.678936E+00 | loss scale: 2048.0 | grad norm: 0.541 | num zeros: 491708544.0 | params norm: 200.204 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 3675.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.681674E+00 | loss scale: 2048.0 | grad norm: 0.593 | num zeros: 484044128.0 | params norm: 200.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        36608 | elapsed time per iteration (ms): 3394.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.690030E+00 | loss scale: 2048.0 | grad norm: 0.480 | num zeros: 477408480.0 | params norm: 200.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 3195.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.677392E+00 | loss scale: 2048.0 | grad norm: 0.510 | num zeros: 441720928.0 | params norm: 200.273 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        37120 | elapsed time per iteration (ms): 2877.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.673687E+00 | loss scale: 2048.0 | grad norm: 0.569 | num zeros: 434400384.0 | params norm: 200.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 2786.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.677615E+00 | loss scale: 2048.0 | grad norm: 0.438 | num zeros: 473257856.0 | params norm: 200.322 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        37632 | elapsed time per iteration (ms): 2993.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.661234E+00 | loss scale: 2048.0 | grad norm: 0.616 | num zeros: 467845248.0 | params norm: 200.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 3870.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.663893E+00 | loss scale: 2048.0 | grad norm: 0.461 | num zeros: 459764352.0 | params norm: 200.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        38144 | elapsed time per iteration (ms): 3341.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.652296E+00 | loss scale: 2048.0 | grad norm: 0.498 | num zeros: 439509856.0 | params norm: 200.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 3071.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.659652E+00 | loss scale: 2048.0 | grad norm: 0.356 | num zeros: 452570208.0 | params norm: 200.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        38656 | elapsed time per iteration (ms): 3433.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.643942E+00 | loss scale: 2048.0 | grad norm: 0.469 | num zeros: 477684416.0 | params norm: 200.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 3079.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.650344E+00 | loss scale: 2048.0 | grad norm: 0.713 | num zeros: 467757472.0 | params norm: 200.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        39168 | elapsed time per iteration (ms): 2877.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.629814E+00 | loss scale: 2048.0 | grad norm: 0.857 | num zeros: 506090240.0 | params norm: 200.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 2861.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.638460E+00 | loss scale: 2048.0 | grad norm: 0.665 | num zeros: 509284416.0 | params norm: 200.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        39680 | elapsed time per iteration (ms): 2946.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.638922E+00 | loss scale: 2048.0 | grad norm: 0.674 | num zeros: 503038848.0 | params norm: 200.533 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 2818.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.629839E+00 | loss scale: 2048.0 | grad norm: 1.136 | num zeros: 506752448.0 | params norm: 200.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        40192 | elapsed time per iteration (ms): 2917.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.621510E+00 | loss scale: 2048.0 | grad norm: 1.049 | num zeros: 516022976.0 | params norm: 200.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 2917.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.627990E+00 | loss scale: 2048.0 | grad norm: 0.759 | num zeros: 502500544.0 | params norm: 200.599 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        40704 | elapsed time per iteration (ms): 3010.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.619208E+00 | loss scale: 2048.0 | grad norm: 0.569 | num zeros: 498387584.0 | params norm: 200.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 2805.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.614755E+00 | loss scale: 2048.0 | grad norm: 0.489 | num zeros: 485650496.0 | params norm: 200.642 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        41216 | elapsed time per iteration (ms): 3883.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.623393E+00 | loss scale: 2048.0 | grad norm: 0.708 | num zeros: 521919872.0 | params norm: 200.664 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 3670.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.615685E+00 | loss scale: 2048.0 | grad norm: 0.357 | num zeros: 516687360.0 | params norm: 200.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        41728 | elapsed time per iteration (ms): 3028.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.620639E+00 | loss scale: 2048.0 | grad norm: 0.505 | num zeros: 510300448.0 | params norm: 200.711 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 3065.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.608965E+00 | loss scale: 2048.0 | grad norm: 0.560 | num zeros: 512531520.0 | params norm: 200.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        42240 | elapsed time per iteration (ms): 2811.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.605924E+00 | loss scale: 2048.0 | grad norm: 0.505 | num zeros: 510321088.0 | params norm: 200.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 2936.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.582884E+00 | loss scale: 2048.0 | grad norm: 0.734 | num zeros: 528647200.0 | params norm: 200.782 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        42752 | elapsed time per iteration (ms): 2863.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.586186E+00 | loss scale: 2048.0 | grad norm: 0.685 | num zeros: 523823680.0 | params norm: 200.806 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 3603.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.567011E+00 | loss scale: 2048.0 | grad norm: 0.541 | num zeros: 514411232.0 | params norm: 200.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        43264 | elapsed time per iteration (ms): 3364.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.571846E+00 | loss scale: 2048.0 | grad norm: 0.519 | num zeros: 509641216.0 | params norm: 200.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 3820.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.580359E+00 | loss scale: 2048.0 | grad norm: 0.486 | num zeros: 537433152.0 | params norm: 200.880 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        43776 | elapsed time per iteration (ms): 3016.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.572694E+00 | loss scale: 2048.0 | grad norm: 0.544 | num zeros: 502932288.0 | params norm: 200.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 2876.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.573410E+00 | loss scale: 2048.0 | grad norm: 0.892 | num zeros: 500061248.0 | params norm: 200.932 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        44288 | elapsed time per iteration (ms): 3094.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.550085E+00 | loss scale: 2048.0 | grad norm: 0.617 | num zeros: 503747328.0 | params norm: 200.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 2909.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.552684E+00 | loss scale: 2048.0 | grad norm: 0.411 | num zeros: 499212992.0 | params norm: 200.985 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        44800 | elapsed time per iteration (ms): 3214.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.554061E+00 | loss scale: 2048.0 | grad norm: 0.574 | num zeros: 497165984.0 | params norm: 201.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 3224.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.541033E+00 | loss scale: 2048.0 | grad norm: 0.475 | num zeros: 509746816.0 | params norm: 201.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        45312 | elapsed time per iteration (ms): 2829.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.543738E+00 | loss scale: 2048.0 | grad norm: 0.497 | num zeros: 514629504.0 | params norm: 201.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 3275.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.537940E+00 | loss scale: 2048.0 | grad norm: 0.471 | num zeros: 520350848.0 | params norm: 201.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        45824 | elapsed time per iteration (ms): 2960.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.528867E+00 | loss scale: 2048.0 | grad norm: 0.425 | num zeros: 527854400.0 | params norm: 201.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 2845.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.520167E+00 | loss scale: 2048.0 | grad norm: 0.616 | num zeros: 482933600.0 | params norm: 201.148 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        46336 | elapsed time per iteration (ms): 2867.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.530326E+00 | loss scale: 2048.0 | grad norm: 0.665 | num zeros: 492095552.0 | params norm: 201.177 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 2844.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.537835E+00 | loss scale: 2048.0 | grad norm: 0.414 | num zeros: 495230400.0 | params norm: 201.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        46848 | elapsed time per iteration (ms): 3049.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.522100E+00 | loss scale: 2048.0 | grad norm: 0.446 | num zeros: 492237920.0 | params norm: 201.234 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 2803.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.516117E+00 | loss scale: 2048.0 | grad norm: 0.582 | num zeros: 491505088.0 | params norm: 201.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        47360 | elapsed time per iteration (ms): 3359.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.510089E+00 | loss scale: 2048.0 | grad norm: 0.626 | num zeros: 482882880.0 | params norm: 201.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 2840.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.509717E+00 | loss scale: 2048.0 | grad norm: 0.546 | num zeros: 482997408.0 | params norm: 201.317 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        47872 | elapsed time per iteration (ms): 2872.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.518638E+00 | loss scale: 2048.0 | grad norm: 0.650 | num zeros: 466689440.0 | params norm: 201.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 3280.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.514926E+00 | loss scale: 2048.0 | grad norm: 0.669 | num zeros: 459740224.0 | params norm: 201.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        48384 | elapsed time per iteration (ms): 3045.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.504797E+00 | loss scale: 2048.0 | grad norm: 0.342 | num zeros: 486020736.0 | params norm: 201.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 3199.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.497371E+00 | loss scale: 2048.0 | grad norm: 0.566 | num zeros: 472699744.0 | params norm: 201.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        48896 | elapsed time per iteration (ms): 3042.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.502173E+00 | loss scale: 2048.0 | grad norm: 0.547 | num zeros: 470516512.0 | params norm: 201.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 2857.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.497453E+00 | loss scale: 2048.0 | grad norm: 0.415 | num zeros: 478450688.0 | params norm: 201.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        49408 | elapsed time per iteration (ms): 2939.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.478667E+00 | loss scale: 2048.0 | grad norm: 0.654 | num zeros: 472883264.0 | params norm: 201.525 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 3034.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.482118E+00 | loss scale: 2048.0 | grad norm: 0.575 | num zeros: 466933632.0 | params norm: 201.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        49920 | elapsed time per iteration (ms): 3210.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.477066E+00 | loss scale: 2048.0 | grad norm: 0.404 | num zeros: 464109696.0 | params norm: 201.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 3169.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.474791E+00 | loss scale: 2048.0 | grad norm: 0.581 | num zeros: 441372288.0 | params norm: 201.614 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:        50432 | elapsed time per iteration (ms): 2875.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.475076E+00 | loss scale: 2048.0 | grad norm: 0.465 | num zeros: 442395456.0 | params norm: 201.644 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 2934.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.469653E+00 | loss scale: 2048.0 | grad norm: 0.404 | num zeros: 431022912.0 | params norm: 201.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:        50944 | elapsed time per iteration (ms): 2814.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.471725E+00 | loss scale: 2048.0 | grad norm: 0.459 | num zeros: 444938496.0 | params norm: 201.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 2785.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.470269E+00 | loss scale: 2048.0 | grad norm: 0.414 | num zeros: 426000544.0 | params norm: 201.735 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-01-05 05:12:05 
