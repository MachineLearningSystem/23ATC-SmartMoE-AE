--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,408] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,408] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,408] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,408] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,408] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,409] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,409] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,409] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,440] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,440] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,440] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,440] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,440] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,440] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,440] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:31,440] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:32,256] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:32,256] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:32,256] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:32,257] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:32,257] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:32,257] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:32,257] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[2022-12-22 16:03:32,261] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
using world size: 32, data-parallel-size: 32, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 32
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /mnt/cache/zhaishuming/Megatron-DeepSpeed/examples/MoE/ds_config_gpt_gpt-L16_H2560_MoE32B-lr-1.0e-4-minlr-1.0e-5-gbs-512-mbs-2-gpus-32-mp-1-pp-1-ep-32-mlc-0.01-cap-2.4-drop-true.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2560
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 32
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 2.4
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [32]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /mnt/cache/zhaishuming/Auto-Megatron/deepspeed/output/tensorboard/gpt-L16_H2560_MoE32B-lr-1.0e-4-minlr-1.0e-5-gbs-512-mbs-2-gpus-32-mp-1-pp-1-ep-32-mlc-0.01-cap-2.4-drop-true_SH-IDC1-10-140-0-31_2022.12.22-16.02.40
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... 200
  train_samples ................................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 32
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
[2022-12-22 16:03:37,006] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:37,006] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:37,006] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:37,006] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:37,007] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:37,007] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:37,007] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:03:37,007] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-22 16:03:38,446] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,446] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,446] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=26, local_rank=2, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,446] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=24, local_rank=0, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,447] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=5, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=29, local_rank=5, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,446] [INFO] [comm.py:654:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=31, local_rank=7, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,447] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=22, local_rank=6, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=23, local_rank=7, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,446] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,446] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=21, local_rank=5, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,446] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=20, local_rank=4, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,446] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=25, local_rank=1, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=27, local_rank=3, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=28, local_rank=4, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,449] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=30, local_rank=6, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,447] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,447] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,447] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=4, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,447] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,447] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=6, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:03:38,447] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=7, world_size=32, master_addr=10.140.1.11, master_port=29500
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.166 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 8.632 seconds
time to initialize megatron (seconds): -26.441
[after megatron is initialized] datetime: 2022-12-22 16:03:49 
building GPT model ...
[2022-12-22 16:03:49,674] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-12-22 16:03:49,678] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-12-22 16:03:49,678] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 202.34 GB, percent = 20.1%
[2022-12-22 16:03:49,732] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:03:49,743] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:03:49,754] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:03:49,768] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:03:49,781] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:03:49,796] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:03:49,804] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:03:49,812] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:03:49,855] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-12-22 16:03:49,856] [INFO] [utils.py:828:see_memory_usage] MA 2.59 GB         Max_MA 2.69 GB         CA 2.7 GB         Max_CA 3 GB 
[2022-12-22 16:03:49,857] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 202.37 GB, percent = 20.1%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1390883840
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-12-22 16:03:49,861] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_32
[2022-12-22 16:03:50,157] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 32
[2022-12-22 16:03:50,161] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [0]
[2022-12-22 16:03:50,172] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [1]
[2022-12-22 16:03:50,182] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [2]
[2022-12-22 16:03:50,192] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [3]
[2022-12-22 16:03:50,202] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [4]
[2022-12-22 16:03:50,213] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [5]
[2022-12-22 16:03:50,223] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [6]
[2022-12-22 16:03:50,233] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [7]
[2022-12-22 16:03:50,243] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [8]
[2022-12-22 16:03:50,253] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [9]
[2022-12-22 16:03:50,264] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [10]
[2022-12-22 16:03:50,274] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [11]
[2022-12-22 16:03:50,284] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [12]
[2022-12-22 16:03:50,294] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [13]
[2022-12-22 16:03:50,304] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [14]
[2022-12-22 16:03:50,315] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [15]
[2022-12-22 16:03:50,325] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [16]
[2022-12-22 16:03:50,335] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [17]
[2022-12-22 16:03:50,345] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [18]
[2022-12-22 16:03:50,355] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [19]
[2022-12-22 16:03:50,366] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [20]
[2022-12-22 16:03:50,376] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [21]
[2022-12-22 16:03:50,386] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [22]
[2022-12-22 16:03:50,396] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [23]
[2022-12-22 16:03:50,406] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [24]
[2022-12-22 16:03:50,417] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [25]
[2022-12-22 16:03:50,427] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [26]
[2022-12-22 16:03:50,437] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [27]
[2022-12-22 16:03:50,447] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [28]
[2022-12-22 16:03:50,457] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [29]
[2022-12-22 16:03:50,468] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [30]
[2022-12-22 16:03:50,478] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [31]
[2022-12-22 16:03:50,488] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_32 with ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[2022-12-22 16:03:53,013] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-12-22 16:03:53,013] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-12-22 16:03:53,013] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-12-22 16:03:53,019] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2022-12-22 16:03:53,019] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2022-12-22 16:03:53,362] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2022-12-22 16:03:53,362] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-12-22 16:03:53,362] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7efe6a3764a0>
[2022-12-22 16:03:53,362] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:03:53,363] [INFO] [config.py:1020:print] DeepSpeedEngine configuration:
[2022-12-22 16:03:53,363] [INFO] [config.py:1024:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-12-22 16:03:53,363] [INFO] [config.py:1024:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-12-22 16:03:53,363] [INFO] [config.py:1024:print]   amp_enabled .................. False
[2022-12-22 16:03:53,363] [INFO] [config.py:1024:print]   amp_params ................... False
[2022-12-22 16:03:53,363] [INFO] [config.py:1024:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-12-22 16:03:53,363] [INFO] [config.py:1024:print]   bfloat16_enabled ............. False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   checkpoint_parallel_write_pipeline  False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   checkpoint_tag_validation_enabled  True
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   checkpoint_tag_validation_fail  False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7efe6a377a00>
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   communication_data_type ...... None
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   curriculum_enabled ........... False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 212296, 'difficulty_step': 8}}
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   dataloader_drop_last ......... False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   disable_allgather ............ False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   dump_state ................... False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   eigenvalue_enabled ........... False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   eigenvalue_gas_boundary_resolution  1
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   eigenvalue_layer_num ......... 0
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   eigenvalue_max_iter .......... 100
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   eigenvalue_stability ......... 1e-06
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   eigenvalue_tol ............... 0.01
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   eigenvalue_verbose ........... False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   elasticity_enabled ........... False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   fp16_auto_cast ............... False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   fp16_enabled ................. True
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   fp16_master_weights_and_gradients  False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   global_rank .................. 0
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   grad_accum_dtype ............. None
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   gradient_accumulation_steps .. 8
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   gradient_clipping ............ 1.0
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   gradient_predivide_factor .... 1.0
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   initial_dynamic_scale ........ 2048
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   load_universal_checkpoint .... False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   loss_scale ................... 0
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   memory_breakdown ............. False
[2022-12-22 16:03:53,364] [INFO] [config.py:1024:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7efe63219c30>
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   optimizer_legacy_fusion ...... False
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   optimizer_name ............... None
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   optimizer_params ............. None
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   pld_enabled .................. False
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   pld_params ................... False
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   prescale_gradients ........... True
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   scheduler_name ............... None
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   scheduler_params ............. None
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   sparse_attention ............. None
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   sparse_gradients_enabled ..... False
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   steps_per_print .............. 1
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   train_batch_size ............. 512
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   train_micro_batch_size_per_gpu  2
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   use_node_local_storage ....... False
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   wall_clock_breakdown ......... False
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   world_size ................... 32
[2022-12-22 16:03:53,365] [INFO] [config.py:1024:print]   zero_allow_untested_optimizer  False
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[2022-12-22 16:03:53,629] [INFO] [config.py:1024:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-12-22 16:03:53,629] [INFO] [config.py:1024:print]   zero_enabled ................. False
[2022-12-22 16:03:53,629] [INFO] [config.py:1024:print]   zero_optimization_stage ...... 0
[2022-12-22 16:03:53,629] [INFO] [config.py:1009:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 2.122960e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.2653059959411621 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-22 16:03:53 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: 51200000
    test:       51200000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.039540 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.030 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-22 16:03:56 
done with setup ...
training ...
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20341253280639648 seconds
time (ms) | model-and-optimizer-setup: 3968.94 | train/valid/test-data-iterators-setup: 2126.10
[before the start of training step] datetime: 2022-12-22 16:03:56 
 iteration        1/     200 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 11602.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.115550E+01 | moe loss: 1.259865E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 8029.87 | backward-compute: 3115.06 | backward-embedding-all-reduce: 0.01 | optimizer: 455.29 | batch-generator: 1028.68
[2022-12-22 16:04:07,741] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18573.685546875 | max allocated: 28828.94482421875 | reserved: 32866.0 | max reserved: 32866.0
[2022-12-22 16:04:13,557] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        2/     200 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 5828.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.040496E+01 | moe loss: 1.911414E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3157.31 | backward-compute: 2548.49 | backward-embedding-all-reduce: 0.01 | optimizer: 91.57 | batch-generator: 1002.46
[2022-12-22 16:04:20,008] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        3/     200 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 6451.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.261184E+01 | moe loss: 2.040399E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3552.19 | backward-compute: 2797.06 | backward-embedding-all-reduce: 0.01 | optimizer: 93.18 | batch-generator: 981.55
[2022-12-22 16:04:20,027] [INFO] [timer.py:197:stop] 0/3, RunningAvgSamplesPerSec=68.90363133725721, CurrSamplesPerSec=68.90363133725721, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:04:25,646] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:04:25,647] [INFO] [timer.py:197:stop] 0/4, RunningAvgSamplesPerSec=69.803842006715, CurrSamplesPerSec=70.7278861610607, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration        4/     200 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 5638.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.042922E+01 | moe loss: 1.818399E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3023.30 | backward-compute: 2497.37 | backward-embedding-all-reduce: 0.01 | optimizer: 92.96 | batch-generator: 959.13
 iteration        5/     200 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 6741.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.001978E+01 | moe loss: 1.616742E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3488.25 | backward-compute: 3132.09 | backward-embedding-all-reduce: 0.01 | optimizer: 114.23 | batch-generator: 1001.04
[2022-12-22 16:04:32,403] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:04:32,406] [INFO] [timer.py:197:stop] 0/5, RunningAvgSamplesPerSec=65.94541649405231, CurrSamplesPerSec=59.38083438076669, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration        6/     200 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 6733.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.609517E+00 | moe loss: 1.445188E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3624.28 | backward-compute: 2974.12 | backward-embedding-all-reduce: 0.01 | optimizer: 101.26 | batch-generator: 988.57
[2022-12-22 16:04:39,125] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:04:39,137] [INFO] [timer.py:197:stop] 0/6, RunningAvgSamplesPerSec=65.48719637038072, CurrSamplesPerSec=64.14996299437519, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:04:44,753] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:04:44,753] [INFO] [timer.py:197:stop] 0/7, RunningAvgSamplesPerSec=66.38295641416761, CurrSamplesPerSec=70.2252315439165, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration        7/     200 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 5630.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.184029E+00 | moe loss: 1.387931E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2983.16 | backward-compute: 2535.17 | backward-embedding-all-reduce: 0.01 | optimizer: 90.81 | batch-generator: 903.40
[2022-12-22 16:04:52,104] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:04:52,107] [INFO] [timer.py:197:stop] 0/8, RunningAvgSamplesPerSec=60.8531477350933, CurrSamplesPerSec=42.95996960551505, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration        8/     200 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 7359.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.737767E+00 | moe loss: 1.350270E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4355.46 | backward-compute: 2863.86 | backward-embedding-all-reduce: 0.01 | optimizer: 122.25 | batch-generator: 922.95
[2022-12-22 16:04:58,761] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:04:58,761] [INFO] [timer.py:197:stop] 0/9, RunningAvgSamplesPerSec=61.0037562286192, CurrSamplesPerSec=61.9232986087322, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration        9/     200 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 6648.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.327951E+00 | moe loss: 1.324679E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3736.26 | backward-compute: 2812.49 | backward-embedding-all-reduce: 0.01 | optimizer: 94.94 | batch-generator: 870.46
[2022-12-22 16:05:04,526] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:05:04,526] [INFO] [timer.py:197:stop] 0/10, RunningAvgSamplesPerSec=62.071010123129014, CurrSamplesPerSec=70.73330951274244, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       10/     200 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 5765.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.100610E+00 | moe loss: 1.325646E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3131.28 | backward-compute: 2534.95 | backward-embedding-all-reduce: 0.01 | optimizer: 90.18 | batch-generator: 925.02
 iteration       11/     200 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 7329.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.354155E+00 | moe loss: 1.279004E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4302.89 | backward-compute: 2905.91 | backward-embedding-all-reduce: 0.01 | optimizer: 106.46 | batch-generator: 936.89
[2022-12-22 16:05:11,888] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:05:11,890] [INFO] [timer.py:197:stop] 0/11, RunningAvgSamplesPerSec=62.51774489494209, CurrSamplesPerSec=66.33726213547126, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:05:17,833] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:05:17,834] [INFO] [timer.py:197:stop] 0/12, RunningAvgSamplesPerSec=62.764087513286796, CurrSamplesPerSec=65.07174424267488, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       12/     200 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 5976.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.459854E+00 | moe loss: 1.302588E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3231.85 | backward-compute: 2609.58 | backward-embedding-all-reduce: 0.01 | optimizer: 91.18 | batch-generator: 872.62
[2022-12-22 16:05:23,526] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:05:23,527] [INFO] [timer.py:197:stop] 0/13, RunningAvgSamplesPerSec=63.329143246746966, CurrSamplesPerSec=69.59464554974582, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       13/     200 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 5694.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.994111E+00 | moe loss: 1.332256E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3055.22 | backward-compute: 2539.35 | backward-embedding-all-reduce: 0.01 | optimizer: 89.94 | batch-generator: 853.22
 iteration       14/     200 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 7125.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.729022E+00 | moe loss: 1.334596E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3730.08 | backward-compute: 3261.77 | backward-embedding-all-reduce: 0.01 | optimizer: 124.68 | batch-generator: 888.25
[2022-12-22 16:05:30,661] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:05:30,677] [INFO] [timer.py:197:stop] 0/14, RunningAvgSamplesPerSec=63.40114521497064, CurrSamplesPerSec=64.20410957265898, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:05:36,295] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:05:36,295] [INFO] [timer.py:197:stop] 0/15, RunningAvgSamplesPerSec=63.88301265094423, CurrSamplesPerSec=70.29408217193641, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       15/     200 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 5642.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.655048E+00 | moe loss: 1.311627E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3021.31 | backward-compute: 2500.35 | backward-embedding-all-reduce: 0.01 | optimizer: 91.22 | batch-generator: 807.73
[2022-12-22 16:05:42,486] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:05:42,486] [INFO] [timer.py:197:stop] 0/16, RunningAvgSamplesPerSec=64.3811617877145, CurrSamplesPerSec=71.64382880881178, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       16/     200 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 6192.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.609449E+00 | moe loss: 1.304625E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3544.49 | backward-compute: 2542.09 | backward-embedding-all-reduce: 0.01 | optimizer: 92.44 | batch-generator: 1065.90
 iteration       17/     200 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 6836.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.565205E+00 | moe loss: 1.265909E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3608.85 | backward-compute: 3106.18 | backward-embedding-all-reduce: 0.01 | optimizer: 111.65 | batch-generator: 865.07
[2022-12-22 16:05:49,351] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:05:49,356] [INFO] [timer.py:197:stop] 0/17, RunningAvgSamplesPerSec=64.46727754766334, CurrSamplesPerSec=65.69754924707435, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:05:54,812] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:05:54,813] [INFO] [timer.py:197:stop] 0/18, RunningAvgSamplesPerSec=64.89717241999969, CurrSamplesPerSec=72.1100763744705, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       18/     200 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 5488.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.510222E+00 | moe loss: 1.298965E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2897.10 | backward-compute: 2462.74 | backward-embedding-all-reduce: 0.01 | optimizer: 89.49 | batch-generator: 824.97
[2022-12-22 16:06:01,062] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:01,063] [INFO] [timer.py:197:stop] 0/19, RunningAvgSamplesPerSec=65.15326759416286, CurrSamplesPerSec=69.54419747943355, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       19/     200 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 6251.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.467051E+00 | moe loss: 1.315661E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3594.95 | backward-compute: 2549.13 | backward-embedding-all-reduce: 0.01 | optimizer: 93.60 | batch-generator: 797.84
[2022-12-22 16:06:06,893] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:06,893] [INFO] [timer.py:197:stop] 0/20, RunningAvgSamplesPerSec=64.0393605026325, CurrSamplesPerSec=49.61813610172371, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       20/     200 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 5849.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.424592E+00 | moe loss: 1.318413E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2871.43 | backward-compute: 2810.33 | backward-embedding-all-reduce: 0.01 | optimizer: 151.96 | batch-generator: 793.15
 iteration       21/     200 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 6426.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.417032E+00 | moe loss: 1.284847E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3741.41 | backward-compute: 2547.37 | backward-embedding-all-reduce: 0.01 | optimizer: 106.40 | batch-generator: 781.15
[2022-12-22 16:06:13,371] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:13,387] [INFO] [timer.py:197:stop] 0/21, RunningAvgSamplesPerSec=64.10685390561677, CurrSamplesPerSec=65.34653334229174, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:06:19,578] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:19,578] [INFO] [timer.py:197:stop] 0/22, RunningAvgSamplesPerSec=64.45134723879562, CurrSamplesPerSec=71.7801741749579, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       22/     200 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 6240.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.401992E+00 | moe loss: 1.267717E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3146.39 | backward-compute: 2935.87 | backward-embedding-all-reduce: 0.01 | optimizer: 96.32 | batch-generator: 775.12
[2022-12-22 16:06:25,036] [INFO] [logging.py:68:log_dist] [Rank 0] step=23, skipped=0, lr=[9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:25,037] [INFO] [timer.py:197:stop] 0/23, RunningAvgSamplesPerSec=64.74176629200785, CurrSamplesPerSec=71.15421221564937, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       23/     200 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 5456.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.375953E+00 | moe loss: 1.259440E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2883.10 | backward-compute: 2460.41 | backward-embedding-all-reduce: 0.01 | optimizer: 92.58 | batch-generator: 812.37
 iteration       24/     200 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 6673.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.370189E+00 | moe loss: 1.251387E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3098.12 | backward-compute: 3441.61 | backward-embedding-all-reduce: 0.01 | optimizer: 123.84 | batch-generator: 785.56
[2022-12-22 16:06:31,717] [INFO] [logging.py:68:log_dist] [Rank 0] step=24, skipped=0, lr=[9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:31,728] [INFO] [timer.py:197:stop] 0/24, RunningAvgSamplesPerSec=64.82651041773866, CurrSamplesPerSec=66.65883354999559, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:06:37,200] [INFO] [logging.py:68:log_dist] [Rank 0] step=25, skipped=0, lr=[9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:37,200] [INFO] [timer.py:197:stop] 0/25, RunningAvgSamplesPerSec=64.57113757921488, CurrSamplesPerSec=59.42137248781956, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       25/     200 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 5490.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.374683E+00 | moe loss: 1.267555E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2737.88 | backward-compute: 2622.11 | backward-embedding-all-reduce: 0.01 | optimizer: 107.15 | batch-generator: 755.47
[2022-12-22 16:06:43,146] [INFO] [logging.py:68:log_dist] [Rank 0] step=26, skipped=0, lr=[9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:43,147] [INFO] [timer.py:197:stop] 0/26, RunningAvgSamplesPerSec=64.80930953059314, CurrSamplesPerSec=70.8171509992104, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       26/     200 | consumed samples:        13312 | consumed tokens:     13631488 | elapsed time per iteration (ms): 5944.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.359454E+00 | moe loss: 1.218441E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3309.10 | backward-compute: 2516.72 | backward-embedding-all-reduce: 0.01 | optimizer: 109.02 | batch-generator: 929.95
[2022-12-22 16:06:49,797] [INFO] [logging.py:68:log_dist] [Rank 0] step=27, skipped=0, lr=[9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:49,800] [INFO] [timer.py:197:stop] 0/27, RunningAvgSamplesPerSec=64.91897117195798, CurrSamplesPerSec=67.66689597437669, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       27/     200 | consumed samples:        13824 | consumed tokens:     14155776 | elapsed time per iteration (ms): 6653.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.349225E+00 | moe loss: 1.232622E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3396.04 | backward-compute: 3118.78 | backward-embedding-all-reduce: 0.01 | optimizer: 126.84 | batch-generator: 923.40
[2022-12-22 16:06:55,148] [INFO] [logging.py:68:log_dist] [Rank 0] step=28, skipped=0, lr=[9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:06:55,149] [INFO] [timer.py:197:stop] 0/28, RunningAvgSamplesPerSec=65.17575669658889, CurrSamplesPerSec=72.32805656147612, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       28/     200 | consumed samples:        14336 | consumed tokens:     14680064 | elapsed time per iteration (ms): 5349.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.346043E+00 | moe loss: 1.231897E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2783.43 | backward-compute: 2462.94 | backward-embedding-all-reduce: 0.01 | optimizer: 89.77 | batch-generator: 750.21
[2022-12-22 16:07:00,933] [INFO] [logging.py:68:log_dist] [Rank 0] step=29, skipped=0, lr=[9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:00,933] [INFO] [timer.py:197:stop] 0/29, RunningAvgSamplesPerSec=65.41469133195992, CurrSamplesPerSec=72.30668422556813, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       29/     200 | consumed samples:        14848 | consumed tokens:     15204352 | elapsed time per iteration (ms): 5785.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.336067E+00 | moe loss: 1.229348E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3166.66 | backward-compute: 2513.20 | backward-embedding-all-reduce: 0.01 | optimizer: 91.25 | batch-generator: 859.73
[2022-12-22 16:07:06,303] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:06,303] [INFO] [timer.py:197:stop] 0/30, RunningAvgSamplesPerSec=65.63004791359604, CurrSamplesPerSec=72.0329654877791, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       30/     200 | consumed samples:        15360 | consumed tokens:     15728640 | elapsed time per iteration (ms): 5368.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.329094E+00 | moe loss: 1.192190E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2811.80 | backward-compute: 2458.89 | backward-embedding-all-reduce: 0.01 | optimizer: 90.82 | batch-generator: 754.54
 iteration       31/     200 | consumed samples:        15872 | consumed tokens:     16252928 | elapsed time per iteration (ms): 6544.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.329066E+00 | moe loss: 1.163568E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3317.96 | backward-compute: 2842.04 | backward-embedding-all-reduce: 0.01 | optimizer: 119.51 | batch-generator: 859.93
[2022-12-22 16:07:12,857] [INFO] [logging.py:68:log_dist] [Rank 0] step=31, skipped=0, lr=[9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:12,872] [INFO] [timer.py:197:stop] 0/31, RunningAvgSamplesPerSec=65.65947744975486, CurrSamplesPerSec=66.4943564052806, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:07:18,926] [INFO] [logging.py:68:log_dist] [Rank 0] step=32, skipped=0, lr=[9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:18,927] [INFO] [timer.py:197:stop] 0/32, RunningAvgSamplesPerSec=65.79819628086501, CurrSamplesPerSec=70.09265578737678, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       32/     200 | consumed samples:        16384 | consumed tokens:     16777216 | elapsed time per iteration (ms): 6079.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.297861E+00 | moe loss: 1.140323E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3424.07 | backward-compute: 2524.97 | backward-embedding-all-reduce: 0.01 | optimizer: 91.23 | batch-generator: 738.41
[2022-12-22 16:07:24,239] [INFO] [logging.py:68:log_dist] [Rank 0] step=33, skipped=0, lr=[9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:24,240] [INFO] [timer.py:197:stop] 0/33, RunningAvgSamplesPerSec=65.97709242447719, CurrSamplesPerSec=71.83649561277215, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       33/     200 | consumed samples:        16896 | consumed tokens:     17301504 | elapsed time per iteration (ms): 5312.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.284882E+00 | moe loss: 1.140787E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2740.34 | backward-compute: 2465.84 | backward-embedding-all-reduce: 0.01 | optimizer: 90.76 | batch-generator: 687.83
[2022-12-22 16:07:30,629] [INFO] [logging.py:68:log_dist] [Rank 0] step=34, skipped=0, lr=[9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:30,636] [INFO] [timer.py:197:stop] 0/34, RunningAvgSamplesPerSec=65.99846610406298, CurrSamplesPerSec=66.66798859144478, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       34/     200 | consumed samples:        17408 | consumed tokens:     17825792 | elapsed time per iteration (ms): 6396.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.272983E+00 | moe loss: 1.138850E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3725.75 | backward-compute: 2540.77 | backward-embedding-all-reduce: 0.01 | optimizer: 120.69 | batch-generator: 749.14
[2022-12-22 16:07:35,895] [INFO] [logging.py:68:log_dist] [Rank 0] step=35, skipped=0, lr=[9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:35,896] [INFO] [timer.py:197:stop] 0/35, RunningAvgSamplesPerSec=66.17383918263947, CurrSamplesPerSec=72.32361350451613, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       35/     200 | consumed samples:        17920 | consumed tokens:     18350080 | elapsed time per iteration (ms): 5259.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.261762E+00 | moe loss: 1.125876E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2700.78 | backward-compute: 2454.43 | backward-embedding-all-reduce: 0.01 | optimizer: 91.35 | batch-generator: 670.17
[2022-12-22 16:07:41,816] [INFO] [logging.py:68:log_dist] [Rank 0] step=36, skipped=0, lr=[9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:41,817] [INFO] [timer.py:197:stop] 0/36, RunningAvgSamplesPerSec=66.32749537919624, CurrSamplesPerSec=71.83168986081371, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       36/     200 | consumed samples:        18432 | consumed tokens:     18874368 | elapsed time per iteration (ms): 5921.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.256675E+00 | moe loss: 1.126085E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2868.51 | backward-compute: 2943.14 | backward-embedding-all-reduce: 0.01 | optimizer: 91.32 | batch-generator: 702.52
 iteration       37/     200 | consumed samples:        18944 | consumed tokens:     19398656 | elapsed time per iteration (ms): 6196.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.247266E+00 | moe loss: 1.141417E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3319.99 | backward-compute: 2660.41 | backward-embedding-all-reduce: 0.01 | optimizer: 204.98 | batch-generator: 672.33
[2022-12-22 16:07:48,039] [INFO] [logging.py:68:log_dist] [Rank 0] step=37, skipped=0, lr=[9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:48,061] [INFO] [timer.py:197:stop] 0/37, RunningAvgSamplesPerSec=64.78779898914455, CurrSamplesPerSec=36.20925863376933, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:07:53,788] [INFO] [logging.py:68:log_dist] [Rank 0] step=38, skipped=0, lr=[9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:07:53,789] [INFO] [timer.py:197:stop] 0/38, RunningAvgSamplesPerSec=64.9738949089245, CurrSamplesPerSec=72.23605260049078, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       38/     200 | consumed samples:        19456 | consumed tokens:     19922944 | elapsed time per iteration (ms): 5773.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.232635E+00 | moe loss: 1.111349E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3143.31 | backward-compute: 2484.37 | backward-embedding-all-reduce: 0.01 | optimizer: 92.08 | batch-generator: 679.44
[2022-12-22 16:08:00,106] [INFO] [logging.py:68:log_dist] [Rank 0] step=39, skipped=0, lr=[9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:00,107] [INFO] [timer.py:197:stop] 0/39, RunningAvgSamplesPerSec=65.13876622257224, CurrSamplesPerSec=71.68741365731907, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       39/     200 | consumed samples:        19968 | consumed tokens:     20447232 | elapsed time per iteration (ms): 6320.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.233936E+00 | moe loss: 1.104656E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3641.47 | backward-compute: 2565.05 | backward-embedding-all-reduce: 0.01 | optimizer: 98.00 | batch-generator: 673.04
[2022-12-22 16:08:05,286] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:05,287] [INFO] [timer.py:197:stop] 0/40, RunningAvgSamplesPerSec=65.3041544760382, CurrSamplesPerSec=72.0751480178059, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       40/     200 | consumed samples:        20480 | consumed tokens:     20971520 | elapsed time per iteration (ms): 5178.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.218896E+00 | moe loss: 1.086861E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2621.98 | backward-compute: 2455.28 | backward-embedding-all-reduce: 0.01 | optimizer: 91.40 | batch-generator: 626.21
 iteration       41/     200 | consumed samples:        20992 | consumed tokens:     21495808 | elapsed time per iteration (ms): 6417.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.207982E+00 | moe loss: 1.059742E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3097.07 | backward-compute: 3185.74 | backward-embedding-all-reduce: 0.01 | optimizer: 117.51 | batch-generator: 705.91
[2022-12-22 16:08:11,724] [INFO] [logging.py:68:log_dist] [Rank 0] step=41, skipped=0, lr=[9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:11,728] [INFO] [timer.py:197:stop] 0/41, RunningAvgSamplesPerSec=65.28614595022866, CurrSamplesPerSec=64.60910540724113, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:08:17,240] [INFO] [logging.py:68:log_dist] [Rank 0] step=42, skipped=0, lr=[9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:17,240] [INFO] [timer.py:197:stop] 0/42, RunningAvgSamplesPerSec=65.04253265817246, CurrSamplesPerSec=56.77955562963467, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       42/     200 | consumed samples:        21504 | consumed tokens:     22020096 | elapsed time per iteration (ms): 5540.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.206998E+00 | moe loss: 1.058173E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2718.73 | backward-compute: 2685.70 | backward-embedding-all-reduce: 0.01 | optimizer: 96.05 | batch-generator: 636.08
[2022-12-22 16:08:22,959] [INFO] [logging.py:68:log_dist] [Rank 0] step=43, skipped=0, lr=[9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:22,959] [INFO] [timer.py:197:stop] 0/43, RunningAvgSamplesPerSec=65.18964739098986, CurrSamplesPerSec=71.67422546523352, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       43/     200 | consumed samples:        22016 | consumed tokens:     22544384 | elapsed time per iteration (ms): 5715.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.193131E+00 | moe loss: 1.071480E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3099.07 | backward-compute: 2515.31 | backward-embedding-all-reduce: 0.01 | optimizer: 91.96 | batch-generator: 653.23
 iteration       44/     200 | consumed samples:        22528 | consumed tokens:     23068672 | elapsed time per iteration (ms): 6637.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.180531E+00 | moe loss: 1.050268E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3450.36 | backward-compute: 3052.68 | backward-embedding-all-reduce: 0.01 | optimizer: 125.04 | batch-generator: 670.53
[2022-12-22 16:08:29,602] [INFO] [logging.py:68:log_dist] [Rank 0] step=44, skipped=0, lr=[9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:29,616] [INFO] [timer.py:197:stop] 0/44, RunningAvgSamplesPerSec=65.16898691269323, CurrSamplesPerSec=64.33303815303249, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:08:34,827] [INFO] [logging.py:68:log_dist] [Rank 0] step=45, skipped=0, lr=[9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:34,828] [INFO] [timer.py:197:stop] 0/45, RunningAvgSamplesPerSec=65.33194376299488, CurrSamplesPerSec=72.99839256145644, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       45/     200 | consumed samples:        23040 | consumed tokens:     23592960 | elapsed time per iteration (ms): 5228.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.168344E+00 | moe loss: 1.024670E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2660.37 | backward-compute: 2452.41 | backward-embedding-all-reduce: 0.01 | optimizer: 90.79 | batch-generator: 624.20
[2022-12-22 16:08:40,807] [INFO] [logging.py:68:log_dist] [Rank 0] step=46, skipped=0, lr=[9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:40,807] [INFO] [timer.py:197:stop] 0/46, RunningAvgSamplesPerSec=65.47571114924713, CurrSamplesPerSec=72.3188397753886, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       46/     200 | consumed samples:        23552 | consumed tokens:     24117248 | elapsed time per iteration (ms): 5981.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.143056E+00 | moe loss: 9.843225E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2932.14 | backward-compute: 2940.16 | backward-embedding-all-reduce: 0.01 | optimizer: 92.81 | batch-generator: 639.51
[2022-12-22 16:08:46,042] [INFO] [logging.py:68:log_dist] [Rank 0] step=47, skipped=0, lr=[9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:46,043] [INFO] [timer.py:197:stop] 0/47, RunningAvgSamplesPerSec=65.62444488987502, CurrSamplesPerSec=72.91198344649405, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       47/     200 | consumed samples:        24064 | consumed tokens:     24641536 | elapsed time per iteration (ms): 5233.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.123603E+00 | moe loss: 1.020752E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2677.13 | backward-compute: 2456.90 | backward-embedding-all-reduce: 0.01 | optimizer: 90.59 | batch-generator: 618.07
 iteration       48/     200 | consumed samples:        24576 | consumed tokens:     25165824 | elapsed time per iteration (ms): 6875.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.119169E+00 | moe loss: 1.017588E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3534.47 | backward-compute: 3204.48 | backward-embedding-all-reduce: 0.01 | optimizer: 125.92 | batch-generator: 659.78
[2022-12-22 16:08:52,922] [INFO] [logging.py:68:log_dist] [Rank 0] step=48, skipped=0, lr=[9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:52,937] [INFO] [timer.py:197:stop] 0/48, RunningAvgSamplesPerSec=65.64484447295852, CurrSamplesPerSec=66.57613838468038, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:08:58,385] [INFO] [logging.py:68:log_dist] [Rank 0] step=49, skipped=0, lr=[9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:08:58,385] [INFO] [timer.py:197:stop] 0/49, RunningAvgSamplesPerSec=65.60865270836635, CurrSamplesPerSec=63.98590398424114, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       49/     200 | consumed samples:        25088 | consumed tokens:     25690112 | elapsed time per iteration (ms): 5466.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.106984E+00 | moe loss: 9.894760E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2832.41 | backward-compute: 2517.34 | backward-embedding-all-reduce: 0.01 | optimizer: 91.41 | batch-generator: 606.05
[2022-12-22 16:09:03,690] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:09:03,690] [INFO] [timer.py:197:stop] 0/50, RunningAvgSamplesPerSec=65.7410415095005, CurrSamplesPerSec=72.62913196111987, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       50/     200 | consumed samples:        25600 | consumed tokens:     26214400 | elapsed time per iteration (ms): 5306.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.095407E+00 | moe loss: 9.652247E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2751.35 | backward-compute: 2456.90 | backward-embedding-all-reduce: 0.01 | optimizer: 89.83 | batch-generator: 578.50
 iteration       51/     200 | consumed samples:        26112 | consumed tokens:     26738688 | elapsed time per iteration (ms): 6579.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.087452E+00 | moe loss: 9.553146E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3181.43 | backward-compute: 3261.04 | backward-embedding-all-reduce: 0.01 | optimizer: 115.34 | batch-generator: 726.91
[2022-12-22 16:09:10,276] [INFO] [logging.py:68:log_dist] [Rank 0] step=51, skipped=0, lr=[9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:09:10,290] [INFO] [timer.py:197:stop] 0/51, RunningAvgSamplesPerSec=65.75444817333374, CurrSamplesPerSec=66.40446206750345, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:09:15,490] [INFO] [logging.py:68:log_dist] [Rank 0] step=52, skipped=0, lr=[9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:09:15,490] [INFO] [timer.py:197:stop] 0/52, RunningAvgSamplesPerSec=65.86413815937101, CurrSamplesPerSec=71.727160695715, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       52/     200 | consumed samples:        26624 | consumed tokens:     27262976 | elapsed time per iteration (ms): 5220.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.075360E+00 | moe loss: 9.411980E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2649.51 | backward-compute: 2454.76 | backward-embedding-all-reduce: 0.01 | optimizer: 89.77 | batch-generator: 558.39
[2022-12-22 16:09:20,920] [INFO] [logging.py:68:log_dist] [Rank 0] step=53, skipped=0, lr=[9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:09:20,920] [INFO] [timer.py:197:stop] 0/53, RunningAvgSamplesPerSec=65.95626215312365, CurrSamplesPerSec=70.9157453108402, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       53/     200 | consumed samples:        27136 | consumed tokens:     27787264 | elapsed time per iteration (ms): 5429.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.063790E+00 | moe loss: 9.376312E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2772.92 | backward-compute: 2543.47 | backward-embedding-all-reduce: 0.01 | optimizer: 104.84 | batch-generator: 582.51
[2022-12-22 16:09:26,170] [INFO] [logging.py:68:log_dist] [Rank 0] step=54, skipped=0, lr=[9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:09:26,171] [INFO] [timer.py:197:stop] 0/54, RunningAvgSamplesPerSec=66.05887563920598, CurrSamplesPerSec=71.75202738191815, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       54/     200 | consumed samples:        27648 | consumed tokens:     28311552 | elapsed time per iteration (ms): 5251.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.057410E+00 | moe loss: 9.342054E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2693.35 | backward-compute: 2455.49 | backward-embedding-all-reduce: 0.01 | optimizer: 90.31 | batch-generator: 576.52
[2022-12-22 16:09:32,987] [INFO] [logging.py:68:log_dist] [Rank 0] step=55, skipped=0, lr=[9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:09:32,995] [INFO] [timer.py:197:stop] 0/55, RunningAvgSamplesPerSec=66.08346495375118, CurrSamplesPerSec=67.38783278075412, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       55/     200 | consumed samples:        28160 | consumed tokens:     28835840 | elapsed time per iteration (ms): 6821.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.036604E+00 | moe loss: 9.087077E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3789.94 | backward-compute: 2900.01 | backward-embedding-all-reduce: 0.01 | optimizer: 113.10 | batch-generator: 633.18
[2022-12-22 16:09:38,392] [INFO] [logging.py:68:log_dist] [Rank 0] step=56, skipped=0, lr=[9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:09:38,393] [INFO] [timer.py:197:stop] 0/56, RunningAvgSamplesPerSec=65.97847224599509, CurrSamplesPerSec=60.85419388477996, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       56/     200 | consumed samples:        28672 | consumed tokens:     29360128 | elapsed time per iteration (ms): 5396.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.019830E+00 | moe loss: 8.993389E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2652.00 | backward-compute: 2632.55 | backward-embedding-all-reduce: 0.01 | optimizer: 101.06 | batch-generator: 538.94
[2022-12-22 16:09:43,696] [INFO] [logging.py:68:log_dist] [Rank 0] step=57, skipped=0, lr=[9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:09:43,697] [INFO] [timer.py:197:stop] 0/57, RunningAvgSamplesPerSec=66.08844666777044, CurrSamplesPerSec=72.62533954084338, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       57/     200 | consumed samples:        29184 | consumed tokens:     29884416 | elapsed time per iteration (ms): 5306.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.005474E+00 | moe loss: 8.978922E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2740.75 | backward-compute: 2461.30 | backward-embedding-all-reduce: 0.01 | optimizer: 89.77 | batch-generator: 550.68
[2022-12-22 16:09:50,042] [INFO] [logging.py:68:log_dist] [Rank 0] step=58, skipped=0, lr=[9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       58/     200 | consumed samples:        29696 | consumed tokens:     30408704 | elapsed time per iteration (ms): 6345.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.991648E+00 | moe loss: 8.817942E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3052.18 | backward-compute: 3145.73 | backward-embedding-all-reduce: 0.01 | optimizer: 136.31 | batch-generator: 626.66
[2022-12-22 16:09:50,054] [INFO] [timer.py:197:stop] 0/58, RunningAvgSamplesPerSec=66.03740910100522, CurrSamplesPerSec=63.34679298482096, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:09:55,162] [INFO] [logging.py:68:log_dist] [Rank 0] step=59, skipped=0, lr=[9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:09:55,162] [INFO] [timer.py:197:stop] 0/59, RunningAvgSamplesPerSec=66.13690616448562, CurrSamplesPerSec=72.23134873150359, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       59/     200 | consumed samples:        30208 | consumed tokens:     30932992 | elapsed time per iteration (ms): 5120.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.983246E+00 | moe loss: 8.736301E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2558.69 | backward-compute: 2452.79 | backward-embedding-all-reduce: 0.01 | optimizer: 90.30 | batch-generator: 543.03
[2022-12-22 16:10:01,111] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:01,112] [INFO] [timer.py:197:stop] 0/60, RunningAvgSamplesPerSec=66.21975025174702, CurrSamplesPerSec=71.31131086030886, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       60/     200 | consumed samples:        30720 | consumed tokens:     31457280 | elapsed time per iteration (ms): 5951.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.966951E+00 | moe loss: 8.853738E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3092.52 | backward-compute: 2747.45 | backward-embedding-all-reduce: 0.01 | optimizer: 92.49 | batch-generator: 563.95
[2022-12-22 16:10:06,425] [INFO] [logging.py:68:log_dist] [Rank 0] step=61, skipped=0, lr=[9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:06,425] [INFO] [timer.py:197:stop] 0/61, RunningAvgSamplesPerSec=66.26743132518901, CurrSamplesPerSec=69.15553947978923, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       61/     200 | consumed samples:        31232 | consumed tokens:     31981568 | elapsed time per iteration (ms): 5313.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.954932E+00 | moe loss: 8.614486E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2674.29 | backward-compute: 2540.76 | backward-embedding-all-reduce: 0.01 | optimizer: 89.47 | batch-generator: 533.69
[2022-12-22 16:10:13,317] [INFO] [logging.py:68:log_dist] [Rank 0] step=62, skipped=0, lr=[9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       62/     200 | consumed samples:        31744 | consumed tokens:     32505856 | elapsed time per iteration (ms): 6885.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.934612E+00 | moe loss: 8.647585E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3809.54 | backward-compute: 2942.59 | backward-embedding-all-reduce: 0.01 | optimizer: 121.23 | batch-generator: 595.08
[2022-12-22 16:10:13,325] [INFO] [timer.py:197:stop] 0/62, RunningAvgSamplesPerSec=66.17709600303526, CurrSamplesPerSec=61.25079228752673, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:10:19,266] [INFO] [logging.py:68:log_dist] [Rank 0] step=63, skipped=0, lr=[9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:19,266] [INFO] [timer.py:197:stop] 0/63, RunningAvgSamplesPerSec=66.2418772789837, CurrSamplesPerSec=70.37533518249553, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       63/     200 | consumed samples:        32256 | consumed tokens:     33030144 | elapsed time per iteration (ms): 5955.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.935200E+00 | moe loss: 8.755250E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3231.80 | backward-compute: 2603.30 | backward-embedding-all-reduce: 0.01 | optimizer: 91.50 | batch-generator: 539.25
[2022-12-22 16:10:24,522] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:24,522] [INFO] [timer.py:197:stop] 0/64, RunningAvgSamplesPerSec=66.30046193297015, CurrSamplesPerSec=70.0812553226262, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       64/     200 | consumed samples:        32768 | consumed tokens:     33554432 | elapsed time per iteration (ms): 5254.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.921213E+00 | moe loss: 8.494815E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2612.78 | backward-compute: 2540.22 | backward-embedding-all-reduce: 0.01 | optimizer: 90.85 | batch-generator: 518.51
 iteration       65/     200 | consumed samples:        33280 | consumed tokens:     34078720 | elapsed time per iteration (ms): 6588.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.910702E+00 | moe loss: 8.561495E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3545.18 | backward-compute: 2913.50 | backward-embedding-all-reduce: 0.01 | optimizer: 116.75 | batch-generator: 580.06
[2022-12-22 16:10:31,116] [INFO] [logging.py:68:log_dist] [Rank 0] step=65, skipped=0, lr=[9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:31,129] [INFO] [timer.py:197:stop] 0/65, RunningAvgSamplesPerSec=66.29046298853497, CurrSamplesPerSec=65.67636398689973, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:10:36,436] [INFO] [logging.py:68:log_dist] [Rank 0] step=66, skipped=0, lr=[9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:36,437] [INFO] [timer.py:197:stop] 0/66, RunningAvgSamplesPerSec=66.35182057051946, CurrSamplesPerSec=70.46051202067129, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       66/     200 | consumed samples:        33792 | consumed tokens:     34603008 | elapsed time per iteration (ms): 5327.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.886324E+00 | moe loss: 8.582945E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2664.30 | backward-compute: 2547.78 | backward-embedding-all-reduce: 0.01 | optimizer: 89.87 | batch-generator: 500.91
[2022-12-22 16:10:42,092] [INFO] [logging.py:68:log_dist] [Rank 0] step=67, skipped=0, lr=[9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:42,093] [INFO] [timer.py:197:stop] 0/67, RunningAvgSamplesPerSec=66.41087750378873, CurrSamplesPerSec=70.42239619118676, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       67/     200 | consumed samples:        34304 | consumed tokens:     35127296 | elapsed time per iteration (ms): 5655.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.888278E+00 | moe loss: 8.451925E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2846.03 | backward-compute: 2707.50 | backward-embedding-all-reduce: 0.01 | optimizer: 91.27 | batch-generator: 504.19
[2022-12-22 16:10:48,656] [INFO] [logging.py:68:log_dist] [Rank 0] step=68, skipped=0, lr=[9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:48,666] [INFO] [timer.py:197:stop] 0/68, RunningAvgSamplesPerSec=65.1559719417212, CurrSamplesPerSec=29.240930544860653, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       68/     200 | consumed samples:        34816 | consumed tokens:     35651584 | elapsed time per iteration (ms): 6563.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.871807E+00 | moe loss: 8.491871E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3530.56 | backward-compute: 2863.97 | backward-embedding-all-reduce: 0.01 | optimizer: 145.46 | batch-generator: 492.95
[2022-12-22 16:10:54,135] [INFO] [logging.py:68:log_dist] [Rank 0] step=69, skipped=0, lr=[9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:54,135] [INFO] [timer.py:197:stop] 0/69, RunningAvgSamplesPerSec=65.22616005306939, CurrSamplesPerSec=70.21850818981291, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       69/     200 | consumed samples:        35328 | consumed tokens:     36175872 | elapsed time per iteration (ms): 5478.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.858275E+00 | moe loss: 8.434875E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2819.03 | backward-compute: 2554.04 | backward-embedding-all-reduce: 0.01 | optimizer: 90.02 | batch-generator: 590.96
[2022-12-22 16:10:59,731] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:10:59,731] [INFO] [timer.py:197:stop] 0/70, RunningAvgSamplesPerSec=65.29148814959454, CurrSamplesPerSec=69.9880133981706, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       70/     200 | consumed samples:        35840 | consumed tokens:     36700160 | elapsed time per iteration (ms): 5595.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.860645E+00 | moe loss: 8.339020E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2907.18 | backward-compute: 2581.14 | backward-embedding-all-reduce: 0.01 | optimizer: 91.50 | batch-generator: 491.62
[2022-12-22 16:11:05,030] [INFO] [logging.py:68:log_dist] [Rank 0] step=71, skipped=0, lr=[9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:05,030] [INFO] [timer.py:197:stop] 0/71, RunningAvgSamplesPerSec=65.37254250619029, CurrSamplesPerSec=71.39989030765457, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       71/     200 | consumed samples:        36352 | consumed tokens:     37224448 | elapsed time per iteration (ms): 5300.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.831225E+00 | moe loss: 8.251418E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2653.84 | backward-compute: 2545.36 | backward-embedding-all-reduce: 0.01 | optimizer: 89.84 | batch-generator: 487.22
 iteration       72/     200 | consumed samples:        36864 | consumed tokens:     37748736 | elapsed time per iteration (ms): 6212.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.832634E+00 | moe loss: 8.477534E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2982.95 | backward-compute: 3104.32 | backward-embedding-all-reduce: 0.01 | optimizer: 113.57 | batch-generator: 529.47
[2022-12-22 16:11:11,256] [INFO] [logging.py:68:log_dist] [Rank 0] step=72, skipped=0, lr=[9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:11,268] [INFO] [timer.py:197:stop] 0/72, RunningAvgSamplesPerSec=65.38376633286089, CurrSamplesPerSec=66.16762947077818, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:11:16,452] [INFO] [logging.py:68:log_dist] [Rank 0] step=73, skipped=0, lr=[9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:16,452] [INFO] [timer.py:197:stop] 0/73, RunningAvgSamplesPerSec=65.44145445239917, CurrSamplesPerSec=69.74923887491981, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       73/     200 | consumed samples:        37376 | consumed tokens:     38273024 | elapsed time per iteration (ms): 5207.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.825310E+00 | moe loss: 8.304190E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2539.65 | backward-compute: 2546.68 | backward-embedding-all-reduce: 0.01 | optimizer: 89.93 | batch-generator: 472.34
[2022-12-22 16:11:22,044] [INFO] [logging.py:68:log_dist] [Rank 0] step=74, skipped=0, lr=[9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:22,044] [INFO] [timer.py:197:stop] 0/74, RunningAvgSamplesPerSec=65.48876895231535, CurrSamplesPerSec=69.03243555873527, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       74/     200 | consumed samples:        37888 | consumed tokens:     38797312 | elapsed time per iteration (ms): 5595.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.809187E+00 | moe loss: 8.184966E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2881.35 | backward-compute: 2608.53 | backward-embedding-all-reduce: 0.01 | optimizer: 91.53 | batch-generator: 517.48
[2022-12-22 16:11:27,793] [INFO] [logging.py:68:log_dist] [Rank 0] step=75, skipped=0, lr=[9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:27,794] [INFO] [timer.py:197:stop] 0/75, RunningAvgSamplesPerSec=64.99917659854434, CurrSamplesPerSec=42.25472138864298, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       75/     200 | consumed samples:        38400 | consumed tokens:     39321600 | elapsed time per iteration (ms): 5786.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.800438E+00 | moe loss: 8.174865E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2493.97 | backward-compute: 3118.63 | backward-embedding-all-reduce: 0.01 | optimizer: 165.08 | batch-generator: 493.99
 iteration       76/     200 | consumed samples:        38912 | consumed tokens:     39845888 | elapsed time per iteration (ms): 5656.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.790681E+00 | moe loss: 8.224513E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2941.48 | backward-compute: 2609.76 | backward-embedding-all-reduce: 0.01 | optimizer: 93.62 | batch-generator: 517.23
[2022-12-22 16:11:33,504] [INFO] [logging.py:68:log_dist] [Rank 0] step=76, skipped=0, lr=[9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:33,507] [INFO] [timer.py:197:stop] 0/76, RunningAvgSamplesPerSec=65.02167823342725, CurrSamplesPerSec=66.70746848525708, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:11:39,206] [INFO] [logging.py:68:log_dist] [Rank 0] step=77, skipped=0, lr=[9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:39,206] [INFO] [timer.py:197:stop] 0/77, RunningAvgSamplesPerSec=65.09146975767698, CurrSamplesPerSec=70.70767240225771, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       77/     200 | consumed samples:        39424 | consumed tokens:     40370176 | elapsed time per iteration (ms): 5715.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.774408E+00 | moe loss: 8.197198E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2998.56 | backward-compute: 2593.23 | backward-embedding-all-reduce: 0.01 | optimizer: 93.04 | batch-generator: 514.28
[2022-12-22 16:11:44,500] [INFO] [logging.py:68:log_dist] [Rank 0] step=78, skipped=0, lr=[9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:44,501] [INFO] [timer.py:197:stop] 0/78, RunningAvgSamplesPerSec=65.16002683771943, CurrSamplesPerSec=70.74868951674011, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       78/     200 | consumed samples:        39936 | consumed tokens:     40894464 | elapsed time per iteration (ms): 5293.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.769988E+00 | moe loss: 8.221954E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2644.99 | backward-compute: 2550.18 | backward-embedding-all-reduce: 0.01 | optimizer: 89.70 | batch-generator: 493.85
 iteration       79/     200 | consumed samples:        40448 | consumed tokens:     41418752 | elapsed time per iteration (ms): 6164.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.764968E+00 | moe loss: 8.496504E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2884.69 | backward-compute: 3144.93 | backward-embedding-all-reduce: 0.01 | optimizer: 120.59 | batch-generator: 558.13
[2022-12-22 16:11:50,682] [INFO] [logging.py:68:log_dist] [Rank 0] step=79, skipped=0, lr=[9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:50,695] [INFO] [timer.py:197:stop] 0/79, RunningAvgSamplesPerSec=65.14629341532627, CurrSamplesPerSec=64.11922498599667, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:11:55,940] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:11:55,940] [INFO] [timer.py:197:stop] 0/80, RunningAvgSamplesPerSec=65.2164933175286, CurrSamplesPerSec=71.1173206617806, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       80/     200 | consumed samples:        40960 | consumed tokens:     41943040 | elapsed time per iteration (ms): 5276.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.750030E+00 | moe loss: 8.244737E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2603.54 | backward-compute: 2545.83 | backward-embedding-all-reduce: 0.01 | optimizer: 89.50 | batch-generator: 440.38
[2022-12-22 16:12:01,479] [INFO] [logging.py:68:log_dist] [Rank 0] step=81, skipped=0, lr=[9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:01,480] [INFO] [timer.py:197:stop] 0/81, RunningAvgSamplesPerSec=65.28072260545403, CurrSamplesPerSec=70.71283186714909, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       81/     200 | consumed samples:        41472 | consumed tokens:     42467328 | elapsed time per iteration (ms): 5539.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.749987E+00 | moe loss: 8.233413E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2797.62 | backward-compute: 2632.54 | backward-embedding-all-reduce: 0.01 | optimizer: 92.35 | batch-generator: 491.60
[2022-12-22 16:12:06,666] [INFO] [logging.py:68:log_dist] [Rank 0] step=82, skipped=0, lr=[9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:06,667] [INFO] [timer.py:197:stop] 0/82, RunningAvgSamplesPerSec=65.33794912720249, CurrSamplesPerSec=70.19948403394967, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       82/     200 | consumed samples:        41984 | consumed tokens:     42991616 | elapsed time per iteration (ms): 5186.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.742073E+00 | moe loss: 8.323985E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2543.99 | backward-compute: 2542.83 | backward-embedding-all-reduce: 0.01 | optimizer: 89.95 | batch-generator: 520.70
 iteration       83/     200 | consumed samples:        42496 | consumed tokens:     43515904 | elapsed time per iteration (ms): 6398.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.721488E+00 | moe loss: 8.261541E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3493.89 | backward-compute: 2783.82 | backward-embedding-all-reduce: 0.01 | optimizer: 107.83 | batch-generator: 514.90
[2022-12-22 16:12:13,075] [INFO] [logging.py:68:log_dist] [Rank 0] step=83, skipped=0, lr=[9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:13,083] [INFO] [timer.py:197:stop] 0/83, RunningAvgSamplesPerSec=65.3237904169224, CurrSamplesPerSec=64.21063665144138, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:12:18,682] [INFO] [logging.py:68:log_dist] [Rank 0] step=84, skipped=0, lr=[9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:18,683] [INFO] [timer.py:197:stop] 0/84, RunningAvgSamplesPerSec=65.31148050745357, CurrSamplesPerSec=64.32955387212044, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       84/     200 | consumed samples:        43008 | consumed tokens:     44040192 | elapsed time per iteration (ms): 5616.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.701659E+00 | moe loss: 8.274312E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2898.28 | backward-compute: 2591.72 | backward-embedding-all-reduce: 0.01 | optimizer: 92.85 | batch-generator: 482.29
[2022-12-22 16:12:23,933] [INFO] [logging.py:68:log_dist] [Rank 0] step=85, skipped=0, lr=[9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:23,934] [INFO] [timer.py:197:stop] 0/85, RunningAvgSamplesPerSec=65.36655720449659, CurrSamplesPerSec=70.22243917496917, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       85/     200 | consumed samples:        43520 | consumed tokens:     44564480 | elapsed time per iteration (ms): 5250.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.698497E+00 | moe loss: 8.223125E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2600.98 | backward-compute: 2544.56 | backward-embedding-all-reduce: 0.01 | optimizer: 90.45 | batch-generator: 459.12
 iteration       86/     200 | consumed samples:        44032 | consumed tokens:     45088768 | elapsed time per iteration (ms): 6292.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.689163E+00 | moe loss: 8.231379E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3015.21 | backward-compute: 3147.40 | backward-embedding-all-reduce: 0.01 | optimizer: 115.79 | batch-generator: 552.85
[2022-12-22 16:12:30,235] [INFO] [logging.py:68:log_dist] [Rank 0] step=86, skipped=0, lr=[9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:30,251] [INFO] [timer.py:197:stop] 0/86, RunningAvgSamplesPerSec=65.36293234294585, CurrSamplesPerSec=65.06346388582143, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:12:35,401] [INFO] [logging.py:68:log_dist] [Rank 0] step=87, skipped=0, lr=[9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:35,401] [INFO] [timer.py:197:stop] 0/87, RunningAvgSamplesPerSec=65.41733131503915, CurrSamplesPerSec=70.33439956400528, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       87/     200 | consumed samples:        44544 | consumed tokens:     45613056 | elapsed time per iteration (ms): 5175.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.667786E+00 | moe loss: 8.347724E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2511.57 | backward-compute: 2542.53 | backward-embedding-all-reduce: 0.01 | optimizer: 89.73 | batch-generator: 446.99
[2022-12-22 16:12:41,182] [INFO] [logging.py:68:log_dist] [Rank 0] step=88, skipped=0, lr=[9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:41,183] [INFO] [timer.py:197:stop] 0/88, RunningAvgSamplesPerSec=65.451093586682, CurrSamplesPerSec=68.45410699718187, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       88/     200 | consumed samples:        45056 | consumed tokens:     46137344 | elapsed time per iteration (ms): 5782.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.659755E+00 | moe loss: 8.185886E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3087.80 | backward-compute: 2585.07 | backward-embedding-all-reduce: 0.01 | optimizer: 90.77 | batch-generator: 492.10
[2022-12-22 16:12:46,283] [INFO] [logging.py:68:log_dist] [Rank 0] step=89, skipped=0, lr=[9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:46,283] [INFO] [timer.py:197:stop] 0/89, RunningAvgSamplesPerSec=65.50453520591924, CurrSamplesPerSec=70.45165408292671, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       89/     200 | consumed samples:        45568 | consumed tokens:     46661632 | elapsed time per iteration (ms): 5099.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.647070E+00 | moe loss: 8.186207E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2452.34 | backward-compute: 2547.26 | backward-embedding-all-reduce: 0.01 | optimizer: 89.91 | batch-generator: 439.91
[2022-12-22 16:12:52,535] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:52,538] [INFO] [timer.py:197:stop] 0/90, RunningAvgSamplesPerSec=65.49777794133053, CurrSamplesPerSec=64.91518514828404, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       90/     200 | consumed samples:        46080 | consumed tokens:     47185920 | elapsed time per iteration (ms): 6253.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.626564E+00 | moe loss: 8.131929E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2833.38 | backward-compute: 3287.40 | backward-embedding-all-reduce: 0.01 | optimizer: 118.06 | batch-generator: 518.10
 iteration       91/     200 | consumed samples:        46592 | consumed tokens:     47710208 | elapsed time per iteration (ms): 5715.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.635084E+00 | moe loss: 8.342547E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2913.03 | backward-compute: 2661.22 | backward-embedding-all-reduce: 0.01 | optimizer: 115.91 | batch-generator: 421.66
[2022-12-22 16:12:58,265] [INFO] [logging.py:68:log_dist] [Rank 0] step=91, skipped=0, lr=[9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:12:58,266] [INFO] [timer.py:197:stop] 0/91, RunningAvgSamplesPerSec=65.09665897999982, CurrSamplesPerSec=42.30005244261567, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:13:03,709] [INFO] [logging.py:68:log_dist] [Rank 0] step=92, skipped=0, lr=[9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:03,709] [INFO] [timer.py:197:stop] 0/92, RunningAvgSamplesPerSec=65.13898562212849, CurrSamplesPerSec=69.14004289516339, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       92/     200 | consumed samples:        47104 | consumed tokens:     48234496 | elapsed time per iteration (ms): 5457.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.620828E+00 | moe loss: 8.274847E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2742.59 | backward-compute: 2592.22 | backward-embedding-all-reduce: 0.01 | optimizer: 91.17 | batch-generator: 507.66
[2022-12-22 16:13:09,888] [INFO] [logging.py:68:log_dist] [Rank 0] step=93, skipped=0, lr=[9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:09,892] [INFO] [timer.py:197:stop] 0/93, RunningAvgSamplesPerSec=65.15742385799292, CurrSamplesPerSec=66.86072712645195, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       93/     200 | consumed samples:        47616 | consumed tokens:     48758784 | elapsed time per iteration (ms): 6172.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.605855E+00 | moe loss: 8.318425E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3362.69 | backward-compute: 2677.60 | backward-embedding-all-reduce: 0.01 | optimizer: 115.54 | batch-generator: 468.41
[2022-12-22 16:13:15,053] [INFO] [logging.py:68:log_dist] [Rank 0] step=94, skipped=0, lr=[9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:15,053] [INFO] [timer.py:197:stop] 0/94, RunningAvgSamplesPerSec=65.21728089561968, CurrSamplesPerSec=71.16662482247148, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       94/     200 | consumed samples:        48128 | consumed tokens:     49283072 | elapsed time per iteration (ms): 5171.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.610519E+00 | moe loss: 8.238754E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2523.03 | backward-compute: 2541.82 | backward-embedding-all-reduce: 0.01 | optimizer: 89.64 | batch-generator: 400.94
[2022-12-22 16:13:20,694] [INFO] [logging.py:68:log_dist] [Rank 0] step=95, skipped=0, lr=[9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:20,694] [INFO] [timer.py:197:stop] 0/95, RunningAvgSamplesPerSec=65.26731751174297, CurrSamplesPerSec=70.2240925245026, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       95/     200 | consumed samples:        48640 | consumed tokens:     49807360 | elapsed time per iteration (ms): 5645.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.598476E+00 | moe loss: 8.285230E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2942.54 | backward-compute: 2589.89 | backward-embedding-all-reduce: 0.01 | optimizer: 91.75 | batch-generator: 450.51
 iteration       96/     200 | consumed samples:        49152 | consumed tokens:     50331648 | elapsed time per iteration (ms): 5339.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.599708E+00 | moe loss: 8.305544E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2673.76 | backward-compute: 2567.61 | backward-embedding-all-reduce: 0.01 | optimizer: 90.47 | batch-generator: 423.93
[2022-12-22 16:13:26,050] [INFO] [logging.py:68:log_dist] [Rank 0] step=96, skipped=0, lr=[9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:26,050] [INFO] [timer.py:197:stop] 0/96, RunningAvgSamplesPerSec=65.2885855894727, CurrSamplesPerSec=67.32899615292197, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:13:32,261] [INFO] [logging.py:68:log_dist] [Rank 0] step=97, skipped=0, lr=[9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:32,276] [INFO] [timer.py:197:stop] 0/97, RunningAvgSamplesPerSec=65.27980244309633, CurrSamplesPerSec=64.46460642519648, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       97/     200 | consumed samples:        49664 | consumed tokens:     50855936 | elapsed time per iteration (ms): 6239.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.575881E+00 | moe loss: 8.519102E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3457.48 | backward-compute: 2634.79 | backward-embedding-all-reduce: 0.01 | optimizer: 125.21 | batch-generator: 453.89
[2022-12-22 16:13:37,498] [INFO] [logging.py:68:log_dist] [Rank 0] step=98, skipped=0, lr=[9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:37,499] [INFO] [timer.py:197:stop] 0/98, RunningAvgSamplesPerSec=65.25087192996108, CurrSamplesPerSec=62.61467971013512, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration       98/     200 | consumed samples:        50176 | consumed tokens:     51380224 | elapsed time per iteration (ms): 5238.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.569312E+00 | moe loss: 8.164559E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2450.98 | backward-compute: 2642.63 | backward-embedding-all-reduce: 0.01 | optimizer: 118.17 | batch-generator: 402.89
 iteration       99/     200 | consumed samples:        50688 | consumed tokens:     51904512 | elapsed time per iteration (ms): 5395.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.559567E+00 | moe loss: 8.417088E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2656.01 | backward-compute: 2639.14 | backward-embedding-all-reduce: 0.01 | optimizer: 90.17 | batch-generator: 457.45
[2022-12-22 16:13:42,924] [INFO] [logging.py:68:log_dist] [Rank 0] step=99, skipped=0, lr=[9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:42,924] [INFO] [timer.py:197:stop] 0/99, RunningAvgSamplesPerSec=65.28457800078465, CurrSamplesPerSec=68.69095405376635, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:13:49,257] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:49,266] [INFO] [timer.py:197:stop] 0/100, RunningAvgSamplesPerSec=65.04185685253783, CurrSamplesPerSec=47.80254954808161, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      100/     200 | consumed samples:        51200 | consumed tokens:     52428800 | elapsed time per iteration (ms): 6344.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.552710E+00 | moe loss: 8.376376E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2851.41 | backward-compute: 3352.33 | backward-embedding-all-reduce: 0.01 | optimizer: 115.46 | batch-generator: 470.07
[2022-12-22 16:13:54,521] [INFO] [logging.py:68:log_dist] [Rank 0] step=101, skipped=0, lr=[9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:13:54,521] [INFO] [timer.py:197:stop] 0/101, RunningAvgSamplesPerSec=65.09710105206106, CurrSamplesPerSec=71.00760770886282, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      101/     200 | consumed samples:        51712 | consumed tokens:     52953088 | elapsed time per iteration (ms): 5264.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.543967E+00 | moe loss: 8.373033E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2607.31 | backward-compute: 2549.48 | backward-embedding-all-reduce: 0.01 | optimizer: 91.10 | batch-generator: 411.13
 iteration      102/     200 | consumed samples:        52224 | consumed tokens:     53477376 | elapsed time per iteration (ms): 5704.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.535837E+00 | moe loss: 8.569511E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2827.31 | backward-compute: 2771.91 | backward-embedding-all-reduce: 0.01 | optimizer: 93.60 | batch-generator: 403.26
[2022-12-22 16:14:00,237] [INFO] [logging.py:68:log_dist] [Rank 0] step=102, skipped=0, lr=[9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:00,238] [INFO] [timer.py:197:stop] 0/102, RunningAvgSamplesPerSec=65.10026056066135, CurrSamplesPerSec=65.41457738480062, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:14:05,412] [INFO] [logging.py:68:log_dist] [Rank 0] step=103, skipped=0, lr=[9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:05,412] [INFO] [timer.py:197:stop] 0/103, RunningAvgSamplesPerSec=65.14071250909231, CurrSamplesPerSec=69.45660130915128, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      103/     200 | consumed samples:        52736 | consumed tokens:     54001664 | elapsed time per iteration (ms): 5186.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.517990E+00 | moe loss: 8.424827E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2509.77 | backward-compute: 2558.44 | backward-embedding-all-reduce: 0.01 | optimizer: 95.93 | batch-generator: 413.79
[2022-12-22 16:14:11,851] [INFO] [logging.py:68:log_dist] [Rank 0] step=104, skipped=0, lr=[9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:11,857] [INFO] [timer.py:197:stop] 0/104, RunningAvgSamplesPerSec=65.16195386474381, CurrSamplesPerSec=67.38111740487439, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      104/     200 | consumed samples:        53248 | consumed tokens:     54525952 | elapsed time per iteration (ms): 6437.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.523323E+00 | moe loss: 8.367170E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3041.10 | backward-compute: 3258.17 | backward-embedding-all-reduce: 0.01 | optimizer: 116.68 | batch-generator: 499.53
 iteration      105/     200 | consumed samples:        53760 | consumed tokens:     55050240 | elapsed time per iteration (ms): 5386.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.515301E+00 | moe loss: 8.283882E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2625.74 | backward-compute: 2633.26 | backward-embedding-all-reduce: 0.01 | optimizer: 114.19 | batch-generator: 401.32
[2022-12-22 16:14:17,253] [INFO] [logging.py:68:log_dist] [Rank 0] step=105, skipped=0, lr=[9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:17,253] [INFO] [timer.py:197:stop] 0/105, RunningAvgSamplesPerSec=65.13776595349317, CurrSamplesPerSec=62.76148571698449, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:14:22,580] [INFO] [logging.py:68:log_dist] [Rank 0] step=106, skipped=0, lr=[9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:22,581] [INFO] [timer.py:197:stop] 0/106, RunningAvgSamplesPerSec=65.18837407055335, CurrSamplesPerSec=70.85883769371749, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      106/     200 | consumed samples:        54272 | consumed tokens:     55574528 | elapsed time per iteration (ms): 5344.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.507515E+00 | moe loss: 8.370645E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2649.15 | backward-compute: 2578.32 | backward-embedding-all-reduce: 0.01 | optimizer: 91.07 | batch-generator: 394.18
 iteration      107/     200 | consumed samples:        54784 | consumed tokens:     56098816 | elapsed time per iteration (ms): 6014.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.508967E+00 | moe loss: 8.274766E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2942.52 | backward-compute: 2881.43 | backward-embedding-all-reduce: 0.01 | optimizer: 175.09 | batch-generator: 390.37
[2022-12-22 16:14:28,613] [INFO] [logging.py:68:log_dist] [Rank 0] step=107, skipped=0, lr=[9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:28,632] [INFO] [timer.py:197:stop] 0/107, RunningAvgSamplesPerSec=64.65066648551641, CurrSamplesPerSec=34.79872117331389, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:14:34,049] [INFO] [logging.py:68:log_dist] [Rank 0] step=108, skipped=0, lr=[9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:34,049] [INFO] [timer.py:197:stop] 0/108, RunningAvgSamplesPerSec=64.69786392098106, CurrSamplesPerSec=70.06892576911632, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      108/     200 | consumed samples:        55296 | consumed tokens:     56623104 | elapsed time per iteration (ms): 5454.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.484068E+00 | moe loss: 8.273448E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2755.72 | backward-compute: 2563.85 | backward-embedding-all-reduce: 0.01 | optimizer: 90.34 | batch-generator: 453.71
[2022-12-22 16:14:39,777] [INFO] [logging.py:68:log_dist] [Rank 0] step=109, skipped=0, lr=[9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:39,778] [INFO] [timer.py:197:stop] 0/109, RunningAvgSamplesPerSec=64.74259520748755, CurrSamplesPerSec=69.86262148543607, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      109/     200 | consumed samples:        55808 | consumed tokens:     57147392 | elapsed time per iteration (ms): 5729.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.483453E+00 | moe loss: 8.317996E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2965.53 | backward-compute: 2652.56 | backward-embedding-all-reduce: 0.01 | optimizer: 94.40 | batch-generator: 398.44
[2022-12-22 16:14:44,893] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:44,893] [INFO] [timer.py:197:stop] 0/110, RunningAvgSamplesPerSec=64.79452418588895, CurrSamplesPerSec=70.87743493484442, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      110/     200 | consumed samples:        56320 | consumed tokens:     57671680 | elapsed time per iteration (ms): 5115.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.478310E+00 | moe loss: 8.448546E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2457.32 | backward-compute: 2549.95 | backward-embedding-all-reduce: 0.01 | optimizer: 93.40 | batch-generator: 371.54
 iteration      111/     200 | consumed samples:        56832 | consumed tokens:     58195968 | elapsed time per iteration (ms): 6197.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.477713E+00 | moe loss: 8.304075E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2682.99 | backward-compute: 3389.07 | backward-embedding-all-reduce: 0.01 | optimizer: 114.21 | batch-generator: 440.69
[2022-12-22 16:14:51,105] [INFO] [logging.py:68:log_dist] [Rank 0] step=111, skipped=0, lr=[9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:51,113] [INFO] [timer.py:197:stop] 0/111, RunningAvgSamplesPerSec=64.78477440851327, CurrSamplesPerSec=63.7487926376545, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:14:56,199] [INFO] [logging.py:68:log_dist] [Rank 0] step=112, skipped=0, lr=[9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:14:56,199] [INFO] [timer.py:197:stop] 0/112, RunningAvgSamplesPerSec=64.82996177311476, CurrSamplesPerSec=70.1643825304354, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      112/     200 | consumed samples:        57344 | consumed tokens:     58720256 | elapsed time per iteration (ms): 5106.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.465337E+00 | moe loss: 8.308963E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2435.04 | backward-compute: 2547.96 | backward-embedding-all-reduce: 0.01 | optimizer: 97.57 | batch-generator: 382.58
[2022-12-22 16:15:01,960] [INFO] [logging.py:68:log_dist] [Rank 0] step=113, skipped=0, lr=[9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:01,961] [INFO] [timer.py:197:stop] 0/113, RunningAvgSamplesPerSec=64.87324475744424, CurrSamplesPerSec=70.01517647153949, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      113/     200 | consumed samples:        57856 | consumed tokens:     59244544 | elapsed time per iteration (ms): 5768.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.461159E+00 | moe loss: 8.273739E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2821.79 | backward-compute: 2821.08 | backward-embedding-all-reduce: 0.01 | optimizer: 95.19 | batch-generator: 392.40
[2022-12-22 16:15:07,091] [INFO] [logging.py:68:log_dist] [Rank 0] step=114, skipped=0, lr=[9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:07,091] [INFO] [timer.py:197:stop] 0/114, RunningAvgSamplesPerSec=64.91618775274254, CurrSamplesPerSec=70.06428044561679, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      114/     200 | consumed samples:        58368 | consumed tokens:     59768832 | elapsed time per iteration (ms): 5123.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.468874E+00 | moe loss: 8.225453E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2481.00 | backward-compute: 2538.95 | backward-embedding-all-reduce: 0.01 | optimizer: 90.46 | batch-generator: 386.85
 iteration      115/     200 | consumed samples:        58880 | consumed tokens:     60293120 | elapsed time per iteration (ms): 6300.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.472367E+00 | moe loss: 8.291955E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3205.11 | backward-compute: 2879.11 | backward-embedding-all-reduce: 0.01 | optimizer: 113.69 | batch-generator: 464.81
[2022-12-22 16:15:13,408] [INFO] [logging.py:68:log_dist] [Rank 0] step=115, skipped=0, lr=[9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:13,424] [INFO] [timer.py:197:stop] 0/115, RunningAvgSamplesPerSec=64.90137913496724, CurrSamplesPerSec=63.2845024135228, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:15:19,119] [INFO] [logging.py:68:log_dist] [Rank 0] step=116, skipped=0, lr=[9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:19,120] [INFO] [timer.py:197:stop] 0/116, RunningAvgSamplesPerSec=64.90264627867495, CurrSamplesPerSec=65.04615292162586, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      116/     200 | consumed samples:        59392 | consumed tokens:     60817408 | elapsed time per iteration (ms): 5728.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.443078E+00 | moe loss: 8.240535E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3002.39 | backward-compute: 2586.60 | backward-embedding-all-reduce: 0.01 | optimizer: 92.07 | batch-generator: 406.11
[2022-12-22 16:15:24,533] [INFO] [logging.py:68:log_dist] [Rank 0] step=117, skipped=0, lr=[9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:24,534] [INFO] [timer.py:197:stop] 0/117, RunningAvgSamplesPerSec=64.94593275180901, CurrSamplesPerSec=70.29021678420743, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      117/     200 | consumed samples:        59904 | consumed tokens:     61341696 | elapsed time per iteration (ms): 5412.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.447131E+00 | moe loss: 8.244952E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2744.46 | backward-compute: 2567.37 | backward-embedding-all-reduce: 0.01 | optimizer: 90.70 | batch-generator: 379.75
[2022-12-22 16:15:29,797] [INFO] [logging.py:68:log_dist] [Rank 0] step=118, skipped=0, lr=[9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:29,798] [INFO] [timer.py:197:stop] 0/118, RunningAvgSamplesPerSec=64.97248766985926, CurrSamplesPerSec=68.17829163806172, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      118/     200 | consumed samples:        60416 | consumed tokens:     61865984 | elapsed time per iteration (ms): 5263.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.446212E+00 | moe loss: 8.328974E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2532.05 | backward-compute: 2588.36 | backward-embedding-all-reduce: 0.01 | optimizer: 104.64 | batch-generator: 358.91
[2022-12-22 16:15:34,869] [INFO] [logging.py:68:log_dist] [Rank 0] step=119, skipped=0, lr=[9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:34,870] [INFO] [timer.py:197:stop] 0/119, RunningAvgSamplesPerSec=65.01653318692696, CurrSamplesPerSec=70.56564564163538, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      119/     200 | consumed samples:        60928 | consumed tokens:     62390272 | elapsed time per iteration (ms): 5072.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.434619E+00 | moe loss: 8.361236E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2420.28 | backward-compute: 2550.26 | backward-embedding-all-reduce: 0.01 | optimizer: 90.76 | batch-generator: 366.59
 iteration      120/     200 | consumed samples:        61440 | consumed tokens:     62914560 | elapsed time per iteration (ms): 7165.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.433749E+00 | moe loss: 8.237655E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3513.10 | backward-compute: 3525.38 | backward-embedding-all-reduce: 0.01 | optimizer: 107.64 | batch-generator: 494.32
[2022-12-22 16:15:42,052] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:42,058] [INFO] [timer.py:197:stop] 0/120, RunningAvgSamplesPerSec=65.01962689979403, CurrSamplesPerSec=65.38363505941355, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:15:47,144] [INFO] [logging.py:68:log_dist] [Rank 0] step=121, skipped=0, lr=[9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:47,144] [INFO] [timer.py:197:stop] 0/121, RunningAvgSamplesPerSec=65.06246587900513, CurrSamplesPerSec=70.54721163759606, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      121/     200 | consumed samples:        61952 | consumed tokens:     63438848 | elapsed time per iteration (ms): 5109.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.413042E+00 | moe loss: 8.191988E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2450.05 | backward-compute: 2537.91 | backward-embedding-all-reduce: 0.01 | optimizer: 91.10 | batch-generator: 369.33
[2022-12-22 16:15:52,538] [INFO] [logging.py:68:log_dist] [Rank 0] step=122, skipped=0, lr=[9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:52,539] [INFO] [timer.py:197:stop] 0/122, RunningAvgSamplesPerSec=65.02576497376876, CurrSamplesPerSec=60.935392725992436, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      122/     200 | consumed samples:        62464 | consumed tokens:     63963136 | elapsed time per iteration (ms): 5427.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.429314E+00 | moe loss: 8.275658E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2586.42 | backward-compute: 2669.56 | backward-embedding-all-reduce: 0.01 | optimizer: 122.59 | batch-generator: 436.62
 iteration      123/     200 | consumed samples:        62976 | consumed tokens:     64487424 | elapsed time per iteration (ms): 6265.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.411593E+00 | moe loss: 8.224988E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2499.66 | backward-compute: 3521.25 | backward-embedding-all-reduce: 0.01 | optimizer: 216.68 | batch-generator: 401.90
[2022-12-22 16:15:58,870] [INFO] [logging.py:68:log_dist] [Rank 0] step=123, skipped=0, lr=[9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:15:58,891] [INFO] [timer.py:197:stop] 0/123, RunningAvgSamplesPerSec=64.44040250961227, CurrSamplesPerSec=30.977373924950722, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:16:04,579] [INFO] [logging.py:68:log_dist] [Rank 0] step=124, skipped=0, lr=[9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:04,580] [INFO] [timer.py:197:stop] 0/124, RunningAvgSamplesPerSec=64.47447094006233, CurrSamplesPerSec=68.88080550132008, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      124/     200 | consumed samples:        63488 | consumed tokens:     65011712 | elapsed time per iteration (ms): 5741.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.422170E+00 | moe loss: 8.290030E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2960.65 | backward-compute: 2632.10 | backward-embedding-all-reduce: 0.01 | optimizer: 89.69 | batch-generator: 393.38
[2022-12-22 16:16:10,002] [INFO] [logging.py:68:log_dist] [Rank 0] step=125, skipped=0, lr=[9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:10,002] [INFO] [timer.py:197:stop] 0/125, RunningAvgSamplesPerSec=64.50639001256508, CurrSamplesPerSec=68.6528845364458, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      125/     200 | consumed samples:        64000 | consumed tokens:     65536000 | elapsed time per iteration (ms): 5432.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.402488E+00 | moe loss: 8.214439E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2542.08 | backward-compute: 2768.17 | backward-embedding-all-reduce: 0.01 | optimizer: 111.35 | batch-generator: 388.26
[2022-12-22 16:16:15,095] [INFO] [logging.py:68:log_dist] [Rank 0] step=126, skipped=0, lr=[9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:15,095] [INFO] [timer.py:197:stop] 0/126, RunningAvgSamplesPerSec=64.55150778489306, CurrSamplesPerSec=70.62760174661827, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      126/     200 | consumed samples:        64512 | consumed tokens:     66060288 | elapsed time per iteration (ms): 5083.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.397605E+00 | moe loss: 8.248772E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2436.16 | backward-compute: 2550.05 | backward-embedding-all-reduce: 0.01 | optimizer: 90.39 | batch-generator: 394.00
[2022-12-22 16:16:21,491] [INFO] [logging.py:68:log_dist] [Rank 0] step=127, skipped=0, lr=[9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:21,492] [INFO] [timer.py:197:stop] 0/127, RunningAvgSamplesPerSec=64.5703234461438, CurrSamplesPerSec=66.99166208717848, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      127/     200 | consumed samples:        65024 | consumed tokens:     66584576 | elapsed time per iteration (ms): 6397.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.392610E+00 | moe loss: 8.266450E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3328.44 | backward-compute: 2922.78 | backward-embedding-all-reduce: 0.01 | optimizer: 127.41 | batch-generator: 388.96
[2022-12-22 16:16:26,560] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:26,561] [INFO] [timer.py:197:stop] 0/128, RunningAvgSamplesPerSec=64.61498913569673, CurrSamplesPerSec=70.73088660680455, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      128/     200 | consumed samples:        65536 | consumed tokens:     67108864 | elapsed time per iteration (ms): 5068.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.393943E+00 | moe loss: 8.198979E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2412.54 | backward-compute: 2536.59 | backward-embedding-all-reduce: 0.01 | optimizer: 90.16 | batch-generator: 364.04
[2022-12-22 16:16:32,235] [INFO] [logging.py:68:log_dist] [Rank 0] step=129, skipped=0, lr=[9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:32,236] [INFO] [timer.py:197:stop] 0/129, RunningAvgSamplesPerSec=64.64436527241881, CurrSamplesPerSec=68.5724573873615, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      129/     200 | consumed samples:        66048 | consumed tokens:     67633152 | elapsed time per iteration (ms): 5687.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.383916E+00 | moe loss: 8.181755E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2977.79 | backward-compute: 2581.72 | backward-embedding-all-reduce: 0.01 | optimizer: 110.83 | batch-generator: 417.98
[2022-12-22 16:16:37,733] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:37,733] [INFO] [timer.py:197:stop] 0/130, RunningAvgSamplesPerSec=64.46320009153693, CurrSamplesPerSec=47.54217056954515, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      130/     200 | consumed samples:        66560 | consumed tokens:     68157440 | elapsed time per iteration (ms): 5509.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.379520E+00 | moe loss: 8.135834E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2374.31 | backward-compute: 2879.71 | backward-embedding-all-reduce: 0.01 | optimizer: 195.89 | batch-generator: 366.73
 iteration      131/     200 | consumed samples:        67072 | consumed tokens:     68681728 | elapsed time per iteration (ms): 6011.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.383296E+00 | moe loss: 8.163667E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2874.34 | backward-compute: 2992.83 | backward-embedding-all-reduce: 0.01 | optimizer: 104.20 | batch-generator: 362.68
[2022-12-22 16:16:43,790] [INFO] [logging.py:68:log_dist] [Rank 0] step=131, skipped=0, lr=[9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:43,803] [INFO] [timer.py:197:stop] 0/131, RunningAvgSamplesPerSec=64.46922445448243, CurrSamplesPerSec=65.2497517604046, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:16:49,109] [INFO] [logging.py:68:log_dist] [Rank 0] step=132, skipped=0, lr=[9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:49,109] [INFO] [timer.py:197:stop] 0/132, RunningAvgSamplesPerSec=64.45759709140246, CurrSamplesPerSec=62.99203528214428, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      132/     200 | consumed samples:        67584 | consumed tokens:     69206016 | elapsed time per iteration (ms): 5353.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.364725E+00 | moe loss: 8.204232E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2604.75 | backward-compute: 2601.72 | backward-embedding-all-reduce: 0.01 | optimizer: 107.09 | batch-generator: 420.64
[2022-12-22 16:16:54,207] [INFO] [logging.py:68:log_dist] [Rank 0] step=133, skipped=0, lr=[9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:16:54,208] [INFO] [timer.py:197:stop] 0/133, RunningAvgSamplesPerSec=64.49845720675323, CurrSamplesPerSec=70.29098982774435, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      133/     200 | consumed samples:        68096 | consumed tokens:     69730304 | elapsed time per iteration (ms): 5083.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.360205E+00 | moe loss: 8.192774E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2442.17 | backward-compute: 2545.05 | backward-embedding-all-reduce: 0.01 | optimizer: 90.18 | batch-generator: 446.57
 iteration      134/     200 | consumed samples:        68608 | consumed tokens:     70254592 | elapsed time per iteration (ms): 6581.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.368202E+00 | moe loss: 8.266155E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3440.60 | backward-compute: 3021.26 | backward-embedding-all-reduce: 0.01 | optimizer: 106.42 | batch-generator: 365.93
[2022-12-22 16:17:00,806] [INFO] [logging.py:68:log_dist] [Rank 0] step=134, skipped=0, lr=[9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:00,814] [INFO] [timer.py:197:stop] 0/134, RunningAvgSamplesPerSec=64.51795713180432, CurrSamplesPerSec=67.17859538200652, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:17:05,803] [INFO] [logging.py:68:log_dist] [Rank 0] step=135, skipped=0, lr=[9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:05,804] [INFO] [timer.py:197:stop] 0/135, RunningAvgSamplesPerSec=64.5523402747586, CurrSamplesPerSec=69.43694592290309, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      135/     200 | consumed samples:        69120 | consumed tokens:     70778880 | elapsed time per iteration (ms): 5014.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.366922E+00 | moe loss: 8.172780E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2349.39 | backward-compute: 2538.80 | backward-embedding-all-reduce: 0.01 | optimizer: 96.00 | batch-generator: 360.56
[2022-12-22 16:17:11,474] [INFO] [logging.py:68:log_dist] [Rank 0] step=136, skipped=0, lr=[9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:11,475] [INFO] [timer.py:197:stop] 0/136, RunningAvgSamplesPerSec=64.55883819017129, CurrSamplesPerSec=65.4348762776276, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      136/     200 | consumed samples:        69632 | consumed tokens:     71303168 | elapsed time per iteration (ms): 5693.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.369885E+00 | moe loss: 8.213165E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2602.13 | backward-compute: 2953.39 | backward-embedding-all-reduce: 0.01 | optimizer: 119.02 | batch-generator: 472.34
[2022-12-22 16:17:16,525] [INFO] [logging.py:68:log_dist] [Rank 0] step=137, skipped=0, lr=[9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:16,526] [INFO] [timer.py:197:stop] 0/137, RunningAvgSamplesPerSec=64.59608870954912, CurrSamplesPerSec=70.00905929015391, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      137/     200 | consumed samples:        70144 | consumed tokens:     71827456 | elapsed time per iteration (ms): 5028.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.354228E+00 | moe loss: 8.291676E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2371.87 | backward-compute: 2554.62 | backward-embedding-all-reduce: 0.01 | optimizer: 91.83 | batch-generator: 350.12
 iteration      138/     200 | consumed samples:        70656 | consumed tokens:     72351744 | elapsed time per iteration (ms): 6650.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.348197E+00 | moe loss: 8.099943E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3851.04 | backward-compute: 2679.44 | backward-embedding-all-reduce: 0.01 | optimizer: 107.50 | batch-generator: 353.77
[2022-12-22 16:17:23,188] [INFO] [logging.py:68:log_dist] [Rank 0] step=138, skipped=0, lr=[9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:23,195] [INFO] [timer.py:197:stop] 0/138, RunningAvgSamplesPerSec=64.58878097415, CurrSamplesPerSec=63.61718697972743, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:17:28,953] [INFO] [logging.py:68:log_dist] [Rank 0] step=139, skipped=0, lr=[9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:28,954] [INFO] [timer.py:197:stop] 0/139, RunningAvgSamplesPerSec=64.28623682077699, CurrSamplesPerSec=39.26966548390471, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      139/     200 | consumed samples:        71168 | consumed tokens:     72876032 | elapsed time per iteration (ms): 5790.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.342224E+00 | moe loss: 8.268707E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2992.78 | backward-compute: 2641.75 | backward-embedding-all-reduce: 0.01 | optimizer: 110.97 | batch-generator: 347.20
[2022-12-22 16:17:34,037] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:34,037] [INFO] [timer.py:197:stop] 0/140, RunningAvgSamplesPerSec=64.32338189698284, CurrSamplesPerSec=69.85291346278703, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      140/     200 | consumed samples:        71680 | consumed tokens:     73400320 | elapsed time per iteration (ms): 5069.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.342658E+00 | moe loss: 8.256657E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2421.50 | backward-compute: 2547.90 | backward-embedding-all-reduce: 0.01 | optimizer: 90.00 | batch-generator: 399.39
 iteration      141/     200 | consumed samples:        72192 | consumed tokens:     73924608 | elapsed time per iteration (ms): 6722.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.319913E+00 | moe loss: 8.145498E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3398.60 | backward-compute: 3203.22 | backward-embedding-all-reduce: 0.01 | optimizer: 107.99 | batch-generator: 382.35
[2022-12-22 16:17:40,775] [INFO] [logging.py:68:log_dist] [Rank 0] step=141, skipped=0, lr=[9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:40,790] [INFO] [timer.py:197:stop] 0/141, RunningAvgSamplesPerSec=64.33541150448842, CurrSamplesPerSec=66.03979528399388, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:17:45,769] [INFO] [logging.py:68:log_dist] [Rank 0] step=142, skipped=0, lr=[9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:45,769] [INFO] [timer.py:197:stop] 0/142, RunningAvgSamplesPerSec=64.37339994001643, CurrSamplesPerSec=70.12933443407564, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      142/     200 | consumed samples:        72704 | consumed tokens:     74448896 | elapsed time per iteration (ms): 5010.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.331755E+00 | moe loss: 8.178750E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2348.41 | backward-compute: 2533.34 | backward-embedding-all-reduce: 0.01 | optimizer: 90.45 | batch-generator: 342.68
[2022-12-22 16:17:51,396] [INFO] [logging.py:68:log_dist] [Rank 0] step=143, skipped=0, lr=[9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:51,397] [INFO] [timer.py:197:stop] 0/143, RunningAvgSamplesPerSec=64.37683617386699, CurrSamplesPerSec=64.86155698883992, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      143/     200 | consumed samples:        73216 | consumed tokens:     74973184 | elapsed time per iteration (ms): 5636.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.314921E+00 | moe loss: 8.154479E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2513.31 | backward-compute: 3005.21 | backward-embedding-all-reduce: 0.01 | optimizer: 102.20 | batch-generator: 379.84
[2022-12-22 16:17:56,505] [INFO] [logging.py:68:log_dist] [Rank 0] step=144, skipped=0, lr=[9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:17:56,506] [INFO] [timer.py:197:stop] 0/144, RunningAvgSamplesPerSec=64.4176810932208, CurrSamplesPerSec=70.7466571085201, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      144/     200 | consumed samples:        73728 | consumed tokens:     75497472 | elapsed time per iteration (ms): 5100.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.311074E+00 | moe loss: 8.198597E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2453.38 | backward-compute: 2544.25 | backward-embedding-all-reduce: 0.01 | optimizer: 91.51 | batch-generator: 366.88
 iteration      145/     200 | consumed samples:        74240 | consumed tokens:     76021760 | elapsed time per iteration (ms): 6565.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.315077E+00 | moe loss: 8.189234E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3795.17 | backward-compute: 2659.97 | backward-embedding-all-reduce: 0.01 | optimizer: 99.79 | batch-generator: 368.79
[2022-12-22 16:18:03,080] [INFO] [logging.py:68:log_dist] [Rank 0] step=145, skipped=0, lr=[9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:18:03,088] [INFO] [timer.py:197:stop] 0/145, RunningAvgSamplesPerSec=64.43285943393367, CurrSamplesPerSec=66.66331970276656, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:18:08,803] [INFO] [logging.py:68:log_dist] [Rank 0] step=146, skipped=0, lr=[9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:18:08,804] [INFO] [timer.py:197:stop] 0/146, RunningAvgSamplesPerSec=64.18927628299124, CurrSamplesPerSec=41.66512062508052, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      146/     200 | consumed samples:        74752 | consumed tokens:     76546048 | elapsed time per iteration (ms): 5741.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.305929E+00 | moe loss: 8.186705E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2924.14 | backward-compute: 2658.00 | backward-embedding-all-reduce: 0.01 | optimizer: 110.44 | batch-generator: 334.14
[2022-12-22 16:18:14,220] [INFO] [logging.py:68:log_dist] [Rank 0] step=147, skipped=0, lr=[9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:18:14,221] [INFO] [timer.py:197:stop] 0/147, RunningAvgSamplesPerSec=64.22813704039092, CurrSamplesPerSec=70.36223797853239, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      147/     200 | consumed samples:        75264 | consumed tokens:     77070336 | elapsed time per iteration (ms): 5406.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.323581E+00 | moe loss: 8.256547E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2715.01 | backward-compute: 2570.60 | backward-embedding-all-reduce: 0.01 | optimizer: 91.31 | batch-generator: 398.21
 iteration      148/     200 | consumed samples:        75776 | consumed tokens:     77594624 | elapsed time per iteration (ms): 6156.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.295923E+00 | moe loss: 8.264300E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3411.03 | backward-compute: 2630.44 | backward-embedding-all-reduce: 0.01 | optimizer: 103.34 | batch-generator: 353.16
[2022-12-22 16:18:20,389] [INFO] [logging.py:68:log_dist] [Rank 0] step=148, skipped=0, lr=[9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:18:20,392] [INFO] [timer.py:197:stop] 0/148, RunningAvgSamplesPerSec=64.25128543960957, CurrSamplesPerSec=67.79416127929171, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:18:25,411] [INFO] [logging.py:68:log_dist] [Rank 0] step=149, skipped=0, lr=[9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:18:25,412] [INFO] [timer.py:197:stop] 0/149, RunningAvgSamplesPerSec=64.29062225108184, CurrSamplesPerSec=70.60140984280899, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      149/     200 | consumed samples:        76288 | consumed tokens:     78118912 | elapsed time per iteration (ms): 5034.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.309938E+00 | moe loss: 8.214185E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2374.55 | backward-compute: 2544.79 | backward-embedding-all-reduce: 0.01 | optimizer: 90.80 | batch-generator: 317.02
 iteration      150/     200 | consumed samples:        76800 | consumed tokens:     78643200 | elapsed time per iteration (ms): 5968.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.295860E+00 | moe loss: 8.280335E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3071.18 | backward-compute: 2773.87 | backward-embedding-all-reduce: 0.01 | optimizer: 112.42 | batch-generator: 356.64
[2022-12-22 16:18:31,378] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:18:31,397] [INFO] [timer.py:197:stop] 0/150, RunningAvgSamplesPerSec=64.29072138709049, CurrSamplesPerSec=64.30529770689904, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:18:36,432] [INFO] [logging.py:68:log_dist] [Rank 0] step=151, skipped=0, lr=[9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:18:36,433] [INFO] [timer.py:197:stop] 0/151, RunningAvgSamplesPerSec=64.32939101807148, CurrSamplesPerSec=70.61552505073176, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      151/     200 | consumed samples:        77312 | consumed tokens:     79167488 | elapsed time per iteration (ms): 5052.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.288314E+00 | moe loss: 8.340148E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2385.10 | backward-compute: 2550.00 | backward-embedding-all-reduce: 0.01 | optimizer: 89.94 | batch-generator: 331.15
[2022-12-22 16:18:42,708] [INFO] [logging.py:68:log_dist] [Rank 0] step=152, skipped=0, lr=[9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      152/     200 | consumed samples:        77824 | consumed tokens:     79691776 | elapsed time per iteration (ms): 6271.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.288290E+00 | moe loss: 8.289429E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3020.35 | backward-compute: 3127.81 | backward-embedding-all-reduce: 0.01 | optimizer: 102.42 | batch-generator: 402.24
[2022-12-22 16:18:42,711] [INFO] [timer.py:197:stop] 0/152, RunningAvgSamplesPerSec=64.33345495586131, CurrSamplesPerSec=64.94477423300772, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:18:47,995] [INFO] [logging.py:68:log_dist] [Rank 0] step=153, skipped=0, lr=[9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:18:47,996] [INFO] [timer.py:197:stop] 0/153, RunningAvgSamplesPerSec=64.297149441997, CurrSamplesPerSec=59.27916778094868, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      153/     200 | consumed samples:        78336 | consumed tokens:     80216064 | elapsed time per iteration (ms): 5311.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.290447E+00 | moe loss: 8.223183E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2423.68 | backward-compute: 2709.00 | backward-embedding-all-reduce: 0.01 | optimizer: 121.70 | batch-generator: 294.47
 iteration      154/     200 | consumed samples:        78848 | consumed tokens:     80740352 | elapsed time per iteration (ms): 5616.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.277853E+00 | moe loss: 8.181991E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2630.82 | backward-compute: 2872.99 | backward-embedding-all-reduce: 0.01 | optimizer: 102.42 | batch-generator: 362.71
[2022-12-22 16:18:53,634] [INFO] [logging.py:68:log_dist] [Rank 0] step=154, skipped=0, lr=[9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:18:53,649] [INFO] [timer.py:197:stop] 0/154, RunningAvgSamplesPerSec=64.28847661377914, CurrSamplesPerSec=63.00519395596181, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      155/     200 | consumed samples:        79360 | consumed tokens:     81264640 | elapsed time per iteration (ms): 6362.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.271325E+00 | moe loss: 8.265182E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3529.98 | backward-compute: 2687.06 | backward-embedding-all-reduce: 0.01 | optimizer: 104.79 | batch-generator: 375.54
[2022-12-22 16:19:00,003] [INFO] [logging.py:68:log_dist] [Rank 0] step=155, skipped=0, lr=[9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:00,006] [INFO] [timer.py:197:stop] 0/155, RunningAvgSamplesPerSec=64.20091542641148, CurrSamplesPerSec=53.18939800288699, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:19:05,224] [INFO] [logging.py:68:log_dist] [Rank 0] step=156, skipped=0, lr=[9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:05,225] [INFO] [timer.py:197:stop] 0/156, RunningAvgSamplesPerSec=64.21123347914502, CurrSamplesPerSec=65.82995259594233, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      156/     200 | consumed samples:        79872 | consumed tokens:     81788928 | elapsed time per iteration (ms): 5243.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.272010E+00 | moe loss: 8.203312E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2534.82 | backward-compute: 2581.47 | backward-embedding-all-reduce: 0.01 | optimizer: 100.75 | batch-generator: 346.70
[2022-12-22 16:19:10,512] [INFO] [logging.py:68:log_dist] [Rank 0] step=157, skipped=0, lr=[9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:10,513] [INFO] [timer.py:197:stop] 0/157, RunningAvgSamplesPerSec=64.22907192707125, CurrSamplesPerSec=67.09977198089659, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      157/     200 | consumed samples:        80384 | consumed tokens:     82313216 | elapsed time per iteration (ms): 5275.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.261939E+00 | moe loss: 8.214636E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2548.61 | backward-compute: 2625.69 | backward-embedding-all-reduce: 0.01 | optimizer: 90.97 | batch-generator: 348.74
[2022-12-22 16:19:15,573] [INFO] [logging.py:68:log_dist] [Rank 0] step=158, skipped=0, lr=[9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:15,573] [INFO] [timer.py:197:stop] 0/158, RunningAvgSamplesPerSec=64.26640824231147, CurrSamplesPerSec=70.63029634231277, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      158/     200 | consumed samples:        80896 | consumed tokens:     82837504 | elapsed time per iteration (ms): 5061.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.262538E+00 | moe loss: 8.206135E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2396.05 | backward-compute: 2549.90 | backward-embedding-all-reduce: 0.01 | optimizer: 90.44 | batch-generator: 319.34
[2022-12-22 16:19:22,112] [INFO] [logging.py:68:log_dist] [Rank 0] step=159, skipped=0, lr=[9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:22,113] [INFO] [timer.py:197:stop] 0/159, RunningAvgSamplesPerSec=64.2731809945918, CurrSamplesPerSec=65.34750372156488, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      159/     200 | consumed samples:        81408 | consumed tokens:     83361792 | elapsed time per iteration (ms): 6541.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.269950E+00 | moe loss: 8.163501E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3755.85 | backward-compute: 2650.65 | backward-embedding-all-reduce: 0.01 | optimizer: 115.72 | batch-generator: 389.24
[2022-12-22 16:19:27,101] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:27,102] [INFO] [timer.py:197:stop] 0/160, RunningAvgSamplesPerSec=64.30662136953404, CurrSamplesPerSec=70.02673812415739, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      160/     200 | consumed samples:        81920 | consumed tokens:     83886080 | elapsed time per iteration (ms): 4986.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.265162E+00 | moe loss: 8.169720E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2328.99 | backward-compute: 2555.25 | backward-embedding-all-reduce: 0.01 | optimizer: 89.70 | batch-generator: 312.77
[2022-12-22 16:19:32,388] [INFO] [logging.py:68:log_dist] [Rank 0] step=161, skipped=0, lr=[9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:32,389] [INFO] [timer.py:197:stop] 0/161, RunningAvgSamplesPerSec=64.33158850514603, CurrSamplesPerSec=68.53583141186867, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      161/     200 | consumed samples:        82432 | consumed tokens:     84410368 | elapsed time per iteration (ms): 5282.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.258062E+00 | moe loss: 8.189046E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2492.60 | backward-compute: 2686.99 | backward-embedding-all-reduce: 0.01 | optimizer: 95.53 | batch-generator: 319.80
[2022-12-22 16:19:38,289] [INFO] [logging.py:68:log_dist] [Rank 0] step=162, skipped=0, lr=[9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:38,289] [INFO] [timer.py:197:stop] 0/162, RunningAvgSamplesPerSec=64.00962936303314, CurrSamplesPerSec=35.6451776147576, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      162/     200 | consumed samples:        82944 | consumed tokens:     84934656 | elapsed time per iteration (ms): 5907.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.260072E+00 | moe loss: 8.163177E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3030.97 | backward-compute: 2723.98 | backward-embedding-all-reduce: 0.01 | optimizer: 122.08 | batch-generator: 330.40
 iteration      163/     200 | consumed samples:        83456 | consumed tokens:     85458944 | elapsed time per iteration (ms): 5297.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.242352E+00 | moe loss: 8.133796E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2622.27 | backward-compute: 2559.67 | backward-embedding-all-reduce: 0.01 | optimizer: 100.81 | batch-generator: 362.38
[2022-12-22 16:19:43,592] [INFO] [logging.py:68:log_dist] [Rank 0] step=163, skipped=0, lr=[9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:43,607] [INFO] [timer.py:197:stop] 0/163, RunningAvgSamplesPerSec=64.03523041279927, CurrSamplesPerSec=68.41319578422413, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      164/     200 | consumed samples:        83968 | consumed tokens:     85983232 | elapsed time per iteration (ms): 5612.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.250004E+00 | moe loss: 8.186054E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2383.61 | backward-compute: 3052.90 | backward-embedding-all-reduce: 0.01 | optimizer: 148.56 | batch-generator: 319.98
[2022-12-22 16:19:49,228] [INFO] [logging.py:68:log_dist] [Rank 0] step=164, skipped=0, lr=[9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:49,229] [INFO] [timer.py:197:stop] 0/164, RunningAvgSamplesPerSec=63.8714076520532, CurrSamplesPerSec=45.23823548346107, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:19:54,631] [INFO] [logging.py:68:log_dist] [Rank 0] step=165, skipped=0, lr=[9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:19:54,632] [INFO] [timer.py:197:stop] 0/165, RunningAvgSamplesPerSec=63.90803114239901, CurrSamplesPerSec=70.45233822838922, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      165/     200 | consumed samples:        84480 | consumed tokens:     86507520 | elapsed time per iteration (ms): 5428.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.240784E+00 | moe loss: 8.243032E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2675.09 | backward-compute: 2610.09 | backward-embedding-all-reduce: 0.01 | optimizer: 90.90 | batch-generator: 299.34
[2022-12-22 16:20:00,679] [INFO] [logging.py:68:log_dist] [Rank 0] step=166, skipped=0, lr=[9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:00,682] [INFO] [timer.py:197:stop] 0/166, RunningAvgSamplesPerSec=63.93379479825499, CurrSamplesPerSec=68.43044405849061, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      166/     200 | consumed samples:        84992 | consumed tokens:     87031808 | elapsed time per iteration (ms): 6062.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.243427E+00 | moe loss: 8.162188E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3261.28 | backward-compute: 2666.52 | backward-embedding-all-reduce: 0.01 | optimizer: 111.28 | batch-generator: 404.84
[2022-12-22 16:20:05,677] [INFO] [logging.py:68:log_dist] [Rank 0] step=167, skipped=0, lr=[9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:05,677] [INFO] [timer.py:197:stop] 0/167, RunningAvgSamplesPerSec=63.96955757434474, CurrSamplesPerSec=70.43065543283419, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      167/     200 | consumed samples:        85504 | consumed tokens:     87556096 | elapsed time per iteration (ms): 4983.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.245255E+00 | moe loss: 8.299638E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2329.55 | backward-compute: 2552.72 | backward-embedding-all-reduce: 0.01 | optimizer: 91.34 | batch-generator: 314.14
 iteration      168/     200 | consumed samples:        86016 | consumed tokens:     88080384 | elapsed time per iteration (ms): 6157.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.240234E+00 | moe loss: 8.105967E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3020.66 | backward-compute: 3025.80 | backward-embedding-all-reduce: 0.01 | optimizer: 98.14 | batch-generator: 322.31
[2022-12-22 16:20:11,846] [INFO] [logging.py:68:log_dist] [Rank 0] step=168, skipped=0, lr=[9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:11,849] [INFO] [timer.py:197:stop] 0/168, RunningAvgSamplesPerSec=63.987916769290564, CurrSamplesPerSec=67.16867770749853, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:20:16,817] [INFO] [logging.py:68:log_dist] [Rank 0] step=169, skipped=0, lr=[9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:16,817] [INFO] [timer.py:197:stop] 0/169, RunningAvgSamplesPerSec=64.0217437582953, CurrSamplesPerSec=70.18045183548361, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      169/     200 | consumed samples:        86528 | consumed tokens:     88604672 | elapsed time per iteration (ms): 4982.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.224625E+00 | moe loss: 8.163101E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2321.52 | backward-compute: 2546.23 | backward-embedding-all-reduce: 0.01 | optimizer: 90.48 | batch-generator: 310.47
[2022-12-22 16:20:22,557] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:22,557] [INFO] [timer.py:197:stop] 0/170, RunningAvgSamplesPerSec=64.04556045916694, CurrSamplesPerSec=68.28799277117996, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      170/     200 | consumed samples:        87040 | consumed tokens:     89128960 | elapsed time per iteration (ms): 5756.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.214902E+00 | moe loss: 8.153806E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3012.41 | backward-compute: 2615.19 | backward-embedding-all-reduce: 0.01 | optimizer: 111.25 | batch-generator: 369.61
[2022-12-22 16:20:27,551] [INFO] [logging.py:68:log_dist] [Rank 0] step=171, skipped=0, lr=[9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:27,551] [INFO] [timer.py:197:stop] 0/171, RunningAvgSamplesPerSec=64.0768622362885, CurrSamplesPerSec=69.8087705579049, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      171/     200 | consumed samples:        87552 | consumed tokens:     89653248 | elapsed time per iteration (ms): 4980.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.217462E+00 | moe loss: 8.062729E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2332.50 | backward-compute: 2547.75 | backward-embedding-all-reduce: 0.01 | optimizer: 89.55 | batch-generator: 334.82
 iteration      172/     200 | consumed samples:        88064 | consumed tokens:     90177536 | elapsed time per iteration (ms): 6401.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.227863E+00 | moe loss: 8.089823E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3625.46 | backward-compute: 2667.80 | backward-embedding-all-reduce: 0.01 | optimizer: 101.08 | batch-generator: 299.33
[2022-12-22 16:20:33,974] [INFO] [logging.py:68:log_dist] [Rank 0] step=172, skipped=0, lr=[9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:33,977] [INFO] [timer.py:197:stop] 0/172, RunningAvgSamplesPerSec=64.09374799812497, CurrSamplesPerSec=67.08124329807202, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:20:39,735] [INFO] [logging.py:68:log_dist] [Rank 0] step=173, skipped=0, lr=[9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:39,740] [INFO] [timer.py:197:stop] 0/173, RunningAvgSamplesPerSec=64.10174030866683, CurrSamplesPerSec=65.49003226495074, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      173/     200 | consumed samples:        88576 | consumed tokens:     90701824 | elapsed time per iteration (ms): 5798.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.217512E+00 | moe loss: 8.110377E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3028.26 | backward-compute: 2614.51 | backward-embedding-all-reduce: 0.01 | optimizer: 114.72 | batch-generator: 374.87
[2022-12-22 16:20:44,751] [INFO] [logging.py:68:log_dist] [Rank 0] step=174, skipped=0, lr=[9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:44,751] [INFO] [timer.py:197:stop] 0/174, RunningAvgSamplesPerSec=64.13645300083553, CurrSamplesPerSec=70.68162581023648, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      174/     200 | consumed samples:        89088 | consumed tokens:     91226112 | elapsed time per iteration (ms): 4997.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.202677E+00 | moe loss: 8.075230E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2344.56 | backward-compute: 2552.35 | backward-embedding-all-reduce: 0.01 | optimizer: 91.21 | batch-generator: 296.13
 iteration      175/     200 | consumed samples:        89600 | consumed tokens:     91750400 | elapsed time per iteration (ms): 6408.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.210189E+00 | moe loss: 8.138790E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3517.70 | backward-compute: 2775.99 | backward-embedding-all-reduce: 0.01 | optimizer: 101.21 | batch-generator: 316.71
[2022-12-22 16:20:51,175] [INFO] [logging.py:68:log_dist] [Rank 0] step=175, skipped=0, lr=[9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:51,178] [INFO] [timer.py:197:stop] 0/175, RunningAvgSamplesPerSec=64.14718078385899, CurrSamplesPerSec=66.04733472447457, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:20:56,232] [INFO] [logging.py:68:log_dist] [Rank 0] step=176, skipped=0, lr=[9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:20:56,232] [INFO] [timer.py:197:stop] 0/176, RunningAvgSamplesPerSec=64.18171205938881, CurrSamplesPerSec=70.77263970543066, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      176/     200 | consumed samples:        90112 | consumed tokens:     92274688 | elapsed time per iteration (ms): 5073.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.211156E+00 | moe loss: 8.129714E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2400.56 | backward-compute: 2549.71 | backward-embedding-all-reduce: 0.01 | optimizer: 89.65 | batch-generator: 316.36
[2022-12-22 16:21:02,107] [INFO] [logging.py:68:log_dist] [Rank 0] step=177, skipped=0, lr=[9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:02,107] [INFO] [timer.py:197:stop] 0/177, RunningAvgSamplesPerSec=64.20500656693423, CurrSamplesPerSec=68.53304929745032, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      177/     200 | consumed samples:        90624 | consumed tokens:     92798976 | elapsed time per iteration (ms): 5895.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.206042E+00 | moe loss: 8.091062E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3123.15 | backward-compute: 2633.09 | backward-embedding-all-reduce: 0.01 | optimizer: 117.15 | batch-generator: 364.84
[2022-12-22 16:21:07,139] [INFO] [logging.py:68:log_dist] [Rank 0] step=178, skipped=0, lr=[9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:07,139] [INFO] [timer.py:197:stop] 0/178, RunningAvgSamplesPerSec=64.24053951455612, CurrSamplesPerSec=71.12943768265251, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      178/     200 | consumed samples:        91136 | consumed tokens:     93323264 | elapsed time per iteration (ms): 5011.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.200642E+00 | moe loss: 8.173129E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2371.96 | backward-compute: 2541.25 | backward-embedding-all-reduce: 0.01 | optimizer: 89.63 | batch-generator: 274.45
[2022-12-22 16:21:13,843] [INFO] [logging.py:68:log_dist] [Rank 0] step=179, skipped=0, lr=[9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      179/     200 | consumed samples:        91648 | consumed tokens:     93847552 | elapsed time per iteration (ms): 6695.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.206207E+00 | moe loss: 8.148449E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3319.46 | backward-compute: 3258.83 | backward-embedding-all-reduce: 0.01 | optimizer: 101.59 | batch-generator: 328.41
[2022-12-22 16:21:13,847] [INFO] [timer.py:197:stop] 0/179, RunningAvgSamplesPerSec=64.25394583788443, CurrSamplesPerSec=66.70393774591344, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      180/     200 | consumed samples:        92160 | consumed tokens:     94371840 | elapsed time per iteration (ms): 6021.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.187470E+00 | moe loss: 8.089777E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2826.85 | backward-compute: 3049.74 | backward-embedding-all-reduce: 0.01 | optimizer: 106.70 | batch-generator: 414.25
[2022-12-22 16:21:19,851] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:19,864] [INFO] [timer.py:197:stop] 0/180, RunningAvgSamplesPerSec=64.13396559835374, CurrSamplesPerSec=48.202582124806646, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:21:24,888] [INFO] [logging.py:68:log_dist] [Rank 0] step=181, skipped=0, lr=[9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:24,889] [INFO] [timer.py:197:stop] 0/181, RunningAvgSamplesPerSec=64.16721011190353, CurrSamplesPerSec=70.68961089508508, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      181/     200 | consumed samples:        92672 | consumed tokens:     94896128 | elapsed time per iteration (ms): 5033.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.191577E+00 | moe loss: 8.076913E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2370.55 | backward-compute: 2556.75 | backward-embedding-all-reduce: 0.01 | optimizer: 90.79 | batch-generator: 297.55
 iteration      182/     200 | consumed samples:        93184 | consumed tokens:     95420416 | elapsed time per iteration (ms): 6486.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.193611E+00 | moe loss: 8.106956E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2872.00 | backward-compute: 3500.15 | backward-embedding-all-reduce: 0.01 | optimizer: 102.18 | batch-generator: 285.45
[2022-12-22 16:21:31,394] [INFO] [logging.py:68:log_dist] [Rank 0] step=182, skipped=0, lr=[9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:31,397] [INFO] [timer.py:197:stop] 0/182, RunningAvgSamplesPerSec=64.1787411438009, CurrSamplesPerSec=66.31177982851628, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:21:36,380] [INFO] [logging.py:68:log_dist] [Rank 0] step=183, skipped=0, lr=[9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:36,380] [INFO] [timer.py:197:stop] 0/183, RunningAvgSamplesPerSec=64.20831424236573, CurrSamplesPerSec=70.01559649634397, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      183/     200 | consumed samples:        93696 | consumed tokens:     95944704 | elapsed time per iteration (ms): 5004.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.194969E+00 | moe loss: 8.109320E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2318.64 | backward-compute: 2554.86 | backward-embedding-all-reduce: 0.01 | optimizer: 90.93 | batch-generator: 275.45
[2022-12-22 16:21:42,234] [INFO] [logging.py:68:log_dist] [Rank 0] step=184, skipped=0, lr=[9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:42,235] [INFO] [timer.py:197:stop] 0/184, RunningAvgSamplesPerSec=64.22871789133885, CurrSamplesPerSec=68.14839968885377, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      184/     200 | consumed samples:        94208 | consumed tokens:     96468992 | elapsed time per iteration (ms): 5866.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.180210E+00 | moe loss: 8.145550E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3093.46 | backward-compute: 2645.27 | backward-embedding-all-reduce: 0.01 | optimizer: 105.73 | batch-generator: 351.45
[2022-12-22 16:21:47,209] [INFO] [logging.py:68:log_dist] [Rank 0] step=185, skipped=0, lr=[9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:47,210] [INFO] [timer.py:197:stop] 0/185, RunningAvgSamplesPerSec=64.25978039890497, CurrSamplesPerSec=70.46178819107217, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      185/     200 | consumed samples:        94720 | consumed tokens:     96993280 | elapsed time per iteration (ms): 4964.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.183915E+00 | moe loss: 8.155565E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2324.68 | backward-compute: 2540.66 | backward-embedding-all-reduce: 0.01 | optimizer: 90.42 | batch-generator: 284.47
 iteration      186/     200 | consumed samples:        95232 | consumed tokens:     97517568 | elapsed time per iteration (ms): 6694.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.175971E+00 | moe loss: 8.142885E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2932.69 | backward-compute: 3642.16 | backward-embedding-all-reduce: 0.01 | optimizer: 109.08 | batch-generator: 277.41
[2022-12-22 16:21:53,925] [INFO] [logging.py:68:log_dist] [Rank 0] step=186, skipped=0, lr=[9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:53,944] [INFO] [timer.py:197:stop] 0/186, RunningAvgSamplesPerSec=64.26035942516994, CurrSamplesPerSec=64.36649720340051, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:21:59,948] [INFO] [logging.py:68:log_dist] [Rank 0] step=187, skipped=0, lr=[9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:21:59,948] [INFO] [timer.py:197:stop] 0/187, RunningAvgSamplesPerSec=64.1815577231934, CurrSamplesPerSec=52.36587912336583, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      187/     200 | consumed samples:        95744 | consumed tokens:     98041856 | elapsed time per iteration (ms): 6055.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.179804E+00 | moe loss: 8.185785E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2774.80 | backward-compute: 3121.81 | backward-embedding-all-reduce: 0.01 | optimizer: 106.69 | batch-generator: 394.62
[2022-12-22 16:22:04,971] [INFO] [logging.py:68:log_dist] [Rank 0] step=188, skipped=0, lr=[9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:04,971] [INFO] [timer.py:197:stop] 0/188, RunningAvgSamplesPerSec=64.21339642370172, CurrSamplesPerSec=70.70195503011942, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      188/     200 | consumed samples:        96256 | consumed tokens:     98566144 | elapsed time per iteration (ms): 5010.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.171316E+00 | moe loss: 8.149282E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2356.82 | backward-compute: 2553.40 | backward-embedding-all-reduce: 0.01 | optimizer: 90.77 | batch-generator: 287.95
 iteration      189/     200 | consumed samples:        96768 | consumed tokens:     99090432 | elapsed time per iteration (ms): 5929.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.178779E+00 | moe loss: 8.156746E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2761.06 | backward-compute: 3055.55 | backward-embedding-all-reduce: 0.01 | optimizer: 97.72 | batch-generator: 298.53
[2022-12-22 16:22:10,913] [INFO] [logging.py:68:log_dist] [Rank 0] step=189, skipped=0, lr=[9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:10,913] [INFO] [timer.py:197:stop] 0/189, RunningAvgSamplesPerSec=64.23008095939288, CurrSamplesPerSec=67.49184648286703, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:22:15,929] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:15,930] [INFO] [timer.py:197:stop] 0/190, RunningAvgSamplesPerSec=64.25955589831031, CurrSamplesPerSec=70.29152360582452, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      190/     200 | consumed samples:        97280 | consumed tokens:     99614720 | elapsed time per iteration (ms): 5030.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.173271E+00 | moe loss: 8.091789E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2355.74 | backward-compute: 2557.89 | backward-embedding-all-reduce: 0.01 | optimizer: 89.61 | batch-generator: 297.64
[2022-12-22 16:22:21,744] [INFO] [logging.py:68:log_dist] [Rank 0] step=191, skipped=0, lr=[9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:21,744] [INFO] [timer.py:197:stop] 0/191, RunningAvgSamplesPerSec=64.28067198818798, CurrSamplesPerSec=68.51328354270004, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      191/     200 | consumed samples:        97792 | consumed tokens:    100139008 | elapsed time per iteration (ms): 5824.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.170287E+00 | moe loss: 8.131239E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3065.08 | backward-compute: 2635.11 | backward-embedding-all-reduce: 0.01 | optimizer: 105.51 | batch-generator: 345.36
 iteration      192/     200 | consumed samples:        98304 | consumed tokens:    100663296 | elapsed time per iteration (ms): 5708.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.163350E+00 | moe loss: 8.206805E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2919.63 | backward-compute: 2672.71 | backward-embedding-all-reduce: 0.01 | optimizer: 106.17 | batch-generator: 272.24
[2022-12-22 16:22:27,500] [INFO] [logging.py:68:log_dist] [Rank 0] step=192, skipped=0, lr=[9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:27,512] [INFO] [timer.py:197:stop] 0/192, RunningAvgSamplesPerSec=64.02644066760071, CurrSamplesPerSec=36.638905760638686, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:22:33,353] [INFO] [logging.py:68:log_dist] [Rank 0] step=193, skipped=0, lr=[9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:33,354] [INFO] [timer.py:197:stop] 0/193, RunningAvgSamplesPerSec=64.04014315626267, CurrSamplesPerSec=66.75454755024022, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      193/     200 | consumed samples:        98816 | consumed tokens:    101187584 | elapsed time per iteration (ms): 5891.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.154844E+00 | moe loss: 8.096977E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3124.71 | backward-compute: 2610.31 | backward-embedding-all-reduce: 0.01 | optimizer: 101.55 | batch-generator: 294.10
 iteration      194/     200 | consumed samples:        99328 | consumed tokens:    101711872 | elapsed time per iteration (ms): 5998.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.146536E+00 | moe loss: 8.108557E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2297.06 | backward-compute: 3463.89 | backward-embedding-all-reduce: 0.01 | optimizer: 200.95 | batch-generator: 284.31
[2022-12-22 16:22:39,376] [INFO] [logging.py:68:log_dist] [Rank 0] step=194, skipped=0, lr=[9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:39,396] [INFO] [timer.py:197:stop] 0/194, RunningAvgSamplesPerSec=63.71384450164786, CurrSamplesPerSec=32.28981356057997, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:22:44,781] [INFO] [logging.py:68:log_dist] [Rank 0] step=195, skipped=0, lr=[9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:44,782] [INFO] [timer.py:197:stop] 0/195, RunningAvgSamplesPerSec=63.746581051959, CurrSamplesPerSec=70.72350708538025, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      195/     200 | consumed samples:        99840 | consumed tokens:    102236160 | elapsed time per iteration (ms): 5427.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.152815E+00 | moe loss: 8.148528E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2697.64 | backward-compute: 2590.04 | backward-embedding-all-reduce: 0.01 | optimizer: 89.99 | batch-generator: 373.90
[2022-12-22 16:22:50,306] [INFO] [logging.py:68:log_dist] [Rank 0] step=196, skipped=0, lr=[9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:50,307] [INFO] [timer.py:197:stop] 0/196, RunningAvgSamplesPerSec=63.76878082568209, CurrSamplesPerSec=68.36366192369393, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      196/     200 | consumed samples:       100352 | consumed tokens:    102760448 | elapsed time per iteration (ms): 5526.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.140553E+00 | moe loss: 8.110724E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2820.27 | backward-compute: 2591.39 | backward-embedding-all-reduce: 0.01 | optimizer: 101.15 | batch-generator: 291.24
[2022-12-22 16:22:55,268] [INFO] [logging.py:68:log_dist] [Rank 0] step=197, skipped=0, lr=[9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:22:55,269] [INFO] [timer.py:197:stop] 0/197, RunningAvgSamplesPerSec=63.799757635602475, CurrSamplesPerSec=70.43773368449585, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      197/     200 | consumed samples:       100864 | consumed tokens:    103284736 | elapsed time per iteration (ms): 4961.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.163591E+00 | moe loss: 8.135685E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2300.77 | backward-compute: 2552.10 | backward-embedding-all-reduce: 0.01 | optimizer: 91.78 | batch-generator: 284.45
 iteration      198/     200 | consumed samples:       101376 | consumed tokens:    103809024 | elapsed time per iteration (ms): 6652.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.139576E+00 | moe loss: 8.105761E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3405.72 | backward-compute: 3105.07 | backward-embedding-all-reduce: 0.01 | optimizer: 127.57 | batch-generator: 377.83
[2022-12-22 16:23:01,923] [INFO] [logging.py:68:log_dist] [Rank 0] step=198, skipped=0, lr=[9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:23:01,937] [INFO] [timer.py:197:stop] 0/198, RunningAvgSamplesPerSec=63.79522139824936, CurrSamplesPerSec=62.922813703453755, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
[2022-12-22 16:23:06,810] [INFO] [logging.py:68:log_dist] [Rank 0] step=199, skipped=0, lr=[9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:23:06,810] [INFO] [timer.py:197:stop] 0/199, RunningAvgSamplesPerSec=63.828492738434754, CurrSamplesPerSec=71.09596104328811, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      199/     200 | consumed samples:       101888 | consumed tokens:    104333312 | elapsed time per iteration (ms): 4889.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.143534E+00 | moe loss: 8.109515E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2257.16 | backward-compute: 2518.80 | backward-embedding-all-reduce: 0.01 | optimizer: 91.24 | batch-generator: 271.55
[2022-12-22 16:23:12,174] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:23:12,174] [INFO] [timer.py:197:stop] 0/200, RunningAvgSamplesPerSec=63.84313366718073, CurrSamplesPerSec=66.86459093317339, MemAllocated=18.14GB, MaxMemAllocated=28.15GB
 iteration      200/     200 | consumed samples:       102400 | consumed tokens:    104857600 | elapsed time per iteration (ms): 5365.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.136000E+00 | moe loss: 8.097541E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2625.30 | backward-compute: 2626.63 | backward-embedding-all-reduce: 0.01 | optimizer: 91.96 | batch-generator: 409.11
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.25525999069213867 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.25072455406188965 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2459583282470703 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24307703971862793 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24343466758728027 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24310302734375 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24300575256347656 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24330973625183105 seconds
[after training is done] datetime: 2022-12-22 16:23:12 
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24662327766418457 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24397730827331543 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2509901523590088 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24385285377502441 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24357032775878906 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24363231658935547 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24337291717529297 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24361419677734375 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.21825695037841797 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2039945125579834 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20413494110107422 seconds
Loading extension module utils...
Time to load utils op: 0.2181248664855957 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20264744758605957 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2039790153503418 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2040996551513672 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.25285935401916504 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2547314167022705 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.25337815284729004 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2601954936981201 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.25481319427490234 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2652931213378906 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.26047515869140625 seconds
