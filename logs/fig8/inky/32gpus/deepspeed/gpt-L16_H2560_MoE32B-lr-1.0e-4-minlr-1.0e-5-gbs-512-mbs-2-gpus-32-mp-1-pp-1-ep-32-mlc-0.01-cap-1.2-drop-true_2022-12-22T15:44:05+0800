--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:30,408] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:30,408] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:30,408] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:30,408] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:30,490] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:30,490] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:30,490] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:30,491] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[2022-12-22 15:44:30,674] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[2022-12-22 15:44:30,674] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[2022-12-22 15:44:30,674] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[2022-12-22 15:44:30,674] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[2022-12-22 15:44:30,675] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[2022-12-22 15:44:30,675] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
[2022-12-22 15:44:30,675] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[2022-12-22 15:44:30,683] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,561] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,561] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,561] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,561] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,562] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,561] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,561] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,562] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
using world size: 32, data-parallel-size: 32, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 32
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /mnt/cache/zhaishuming/Megatron-DeepSpeed/examples/MoE/ds_config_gpt_gpt-L16_H2560_MoE32B-lr-1.0e-4-minlr-1.0e-5-gbs-512-mbs-2-gpus-32-mp-1-pp-1-ep-32-mlc-0.01-cap-1.2-drop-true.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2560
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 32
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.2
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [32]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /mnt/cache/zhaishuming/Auto-Megatron/deepspeed/output/tensorboard/gpt-L16_H2560_MoE32B-lr-1.0e-4-minlr-1.0e-5-gbs-512-mbs-2-gpus-32-mp-1-pp-1-ep-32-mlc-0.01-cap-1.2-drop-true_SH-IDC1-10-140-0-31_2022.12.22-15.44.05
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... 200
  train_samples ................................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 32
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,714] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,714] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,715] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,715] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,715] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,716] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
> initializing torch distributed ...
[2022-12-22 15:44:34,717] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 15:44:34,717] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=24, local_rank=0, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,228] [INFO] [comm.py:654:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,229] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=6, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,229] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=28, local_rank=4, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,229] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=30, local_rank=6, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,229] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=4, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=26, local_rank=2, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=27, local_rank=3, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=29, local_rank=5, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=31, local_rank=7, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,229] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=7, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=25, local_rank=1, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,228] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,229] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=5, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,226] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=20, local_rank=4, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=23, local_rank=7, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=22, local_rank=6, world_size=32, master_addr=10.140.1.76, master_port=29500
[2022-12-22 15:44:36,227] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=21, local_rank=5, world_size=32, master_addr=10.140.1.76, master_port=29500
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.244 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 9.356 seconds
time to initialize megatron (seconds): -15.875
[after megatron is initialized] datetime: 2022-12-22 15:44:48 
building GPT model ...
[2022-12-22 15:44:48,212] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-12-22 15:44:48,215] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-12-22 15:44:48,216] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 29.64 GB, percent = 2.9%
[2022-12-22 15:44:48,265] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 15:44:48,280] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 15:44:48,291] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 15:44:48,309] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 15:44:48,324] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 15:44:48,338] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 15:44:48,347] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 15:44:48,354] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 15:44:48,390] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-12-22 15:44:48,391] [INFO] [utils.py:828:see_memory_usage] MA 2.59 GB         Max_MA 2.69 GB         CA 2.7 GB         Max_CA 3 GB 
[2022-12-22 15:44:48,391] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 29.69 GB, percent = 2.9%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1390883840
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-12-22 15:44:48,395] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_32
[2022-12-22 15:44:48,714] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 32
[2022-12-22 15:44:48,720] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [0]
[2022-12-22 15:44:48,730] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [1]
[2022-12-22 15:44:48,740] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [2]
[2022-12-22 15:44:48,750] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [3]
[2022-12-22 15:44:48,761] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [4]
[2022-12-22 15:44:48,771] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [5]
[2022-12-22 15:44:48,781] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [6]
[2022-12-22 15:44:48,791] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [7]
[2022-12-22 15:44:48,802] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [8]
[2022-12-22 15:44:48,812] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [9]
[2022-12-22 15:44:48,822] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [10]
[2022-12-22 15:44:48,832] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [11]
[2022-12-22 15:44:48,842] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [12]
[2022-12-22 15:44:48,853] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [13]
[2022-12-22 15:44:48,863] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [14]
[2022-12-22 15:44:48,873] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [15]
[2022-12-22 15:44:48,883] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [16]
[2022-12-22 15:44:48,893] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [17]
[2022-12-22 15:44:48,904] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [18]
[2022-12-22 15:44:48,914] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [19]
[2022-12-22 15:44:48,924] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [20]
[2022-12-22 15:44:48,924] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [21]
[2022-12-22 15:44:48,934] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [22]
[2022-12-22 15:44:48,945] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [23]
[2022-12-22 15:44:48,955] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [24]
[2022-12-22 15:44:48,965] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [25]
[2022-12-22 15:44:48,975] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [26]
[2022-12-22 15:44:48,986] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [27]
[2022-12-22 15:44:48,996] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [28]
[2022-12-22 15:44:49,006] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [29]
[2022-12-22 15:44:49,016] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [30]
[2022-12-22 15:44:49,026] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [31]
[2022-12-22 15:44:49,037] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_32 with ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[2022-12-22 15:44:51,576] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-12-22 15:44:51,577] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-12-22 15:44:51,577] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-12-22 15:44:51,582] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2022-12-22 15:44:51,582] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2022-12-22 15:44:51,875] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2022-12-22 15:44:51,875] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-12-22 15:44:51,875] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7fac5a8c40a0>
[2022-12-22 15:44:51,875] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:44:51,876] [INFO] [config.py:1020:print] DeepSpeedEngine configuration:
[2022-12-22 15:44:51,876] [INFO] [config.py:1024:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-12-22 15:44:51,876] [INFO] [config.py:1024:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-12-22 15:44:51,876] [INFO] [config.py:1024:print]   amp_enabled .................. False
[2022-12-22 15:44:51,876] [INFO] [config.py:1024:print]   amp_params ................... False
[2022-12-22 15:44:51,876] [INFO] [config.py:1024:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-12-22 15:44:51,876] [INFO] [config.py:1024:print]   bfloat16_enabled ............. False
[2022-12-22 15:44:51,876] [INFO] [config.py:1024:print]   checkpoint_parallel_write_pipeline  False
[2022-12-22 15:44:51,876] [INFO] [config.py:1024:print]   checkpoint_tag_validation_enabled  True
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   checkpoint_tag_validation_fail  False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fac5a8c77c0>
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   communication_data_type ...... None
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   curriculum_enabled ........... False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 212296, 'difficulty_step': 8}}
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   dataloader_drop_last ......... False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   disable_allgather ............ False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   dump_state ................... False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   eigenvalue_enabled ........... False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   eigenvalue_gas_boundary_resolution  1
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   eigenvalue_layer_num ......... 0
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   eigenvalue_max_iter .......... 100
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   eigenvalue_stability ......... 1e-06
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   eigenvalue_tol ............... 0.01
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   eigenvalue_verbose ........... False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   elasticity_enabled ........... False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   fp16_auto_cast ............... False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   fp16_enabled ................. True
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   fp16_master_weights_and_gradients  False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   global_rank .................. 0
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   grad_accum_dtype ............. None
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   gradient_accumulation_steps .. 8
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   gradient_clipping ............ 1.0
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   gradient_predivide_factor .... 1.0
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   initial_dynamic_scale ........ 2048
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   load_universal_checkpoint .... False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   loss_scale ................... 0
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   memory_breakdown ............. False
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fac535b9c30>
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-12-22 15:44:51,877] [INFO] [config.py:1024:print]   optimizer_legacy_fusion ...... False
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   optimizer_name ............... None
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   optimizer_params ............. None
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   pld_enabled .................. False
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   pld_params ................... False
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   prescale_gradients ........... True
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   scheduler_name ............... None
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   scheduler_params ............. None
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   sparse_attention ............. None
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   sparse_gradients_enabled ..... False
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   steps_per_print .............. 1
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   train_batch_size ............. 512
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   train_micro_batch_size_per_gpu  2
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   use_node_local_storage ....... False
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   wall_clock_breakdown ......... False
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   world_size ................... 32
[2022-12-22 15:44:51,878] [INFO] [config.py:1024:print]   zero_allow_untested_optimizer  False
[2022-12-22 15:44:51,918] [INFO] [config.py:1024:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-12-22 15:44:51,918] [INFO] [config.py:1024:print]   zero_enabled ................. False
[2022-12-22 15:44:51,918] [INFO] [config.py:1024:print]   zero_optimization_stage ...... 0
[2022-12-22 15:44:51,918] [INFO] [config.py:1009:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 2.122960e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4041626453399658 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-22 15:44:52 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: 51200000
    test:       51200000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.042547 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.035 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-22 15:44:54 
done with setup ...
training ...
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.42918872833251953 seconds
time (ms) | model-and-optimizer-setup: 4211.07 | train/valid/test-data-iterators-setup: 2158.79
[before the start of training step] datetime: 2022-12-22 15:44:54 
 iteration        1/     200 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 10302.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.115491E+01 | moe loss: 1.258180E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 7656.77 | backward-compute: 2195.15 | backward-embedding-all-reduce: 0.01 | optimizer: 446.89 | batch-generator: 945.15
[2022-12-22 15:45:04,958] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18573.685546875 | max allocated: 28636.591796875 | reserved: 32198.0 | max reserved: 32198.0
[2022-12-22 15:45:09,724] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        2/     200 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 4791.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.039745E+01 | moe loss: 1.900380E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2591.45 | backward-compute: 1969.05 | backward-embedding-all-reduce: 0.01 | optimizer: 130.43 | batch-generator: 1036.33
[2022-12-22 15:45:14,361] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:45:14,361] [INFO] [timer.py:197:stop] 0/3, RunningAvgSamplesPerSec=86.93764570010215, CurrSamplesPerSec=86.93764570010215, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration        3/     200 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 4623.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.261833E+01 | moe loss: 2.026972E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2660.47 | backward-compute: 1863.08 | backward-embedding-all-reduce: 0.01 | optimizer: 92.85 | batch-generator: 1031.89
[2022-12-22 15:45:19,789] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:45:19,789] [INFO] [timer.py:197:stop] 0/4, RunningAvgSamplesPerSec=75.26172171252193, CurrSamplesPerSec=66.3506748386191, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration        4/     200 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 5438.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.042649E+01 | moe loss: 1.812396E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3117.54 | backward-compute: 2152.51 | backward-embedding-all-reduce: 0.01 | optimizer: 157.92 | batch-generator: 982.24
 iteration        5/     200 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 5358.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.001731E+01 | moe loss: 1.634435E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3205.55 | backward-compute: 2010.75 | backward-embedding-all-reduce: 0.01 | optimizer: 100.72 | batch-generator: 957.61
[2022-12-22 15:45:25,186] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:45:25,203] [INFO] [timer.py:197:stop] 0/5, RunningAvgSamplesPerSec=76.63235648865763, CurrSamplesPerSec=79.52905582385989, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:45:30,105] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:45:30,105] [INFO] [timer.py:197:stop] 0/6, RunningAvgSamplesPerSec=73.35722119978689, CurrSamplesPerSec=65.02061322035516, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration        6/     200 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 4974.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.605782E+00 | moe loss: 1.421971E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2994.69 | backward-compute: 1805.28 | backward-embedding-all-reduce: 0.01 | optimizer: 122.63 | batch-generator: 914.92
[2022-12-22 15:45:34,743] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:45:34,744] [INFO] [timer.py:197:stop] 0/7, RunningAvgSamplesPerSec=75.87302545616447, CurrSamplesPerSec=87.93620160544268, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration        7/     200 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 4609.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.179758E+00 | moe loss: 1.381683E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2685.70 | backward-compute: 1823.49 | backward-embedding-all-reduce: 0.01 | optimizer: 92.41 | batch-generator: 911.51
 iteration        8/     200 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 6707.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.733674E+00 | moe loss: 1.355731E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4615.27 | backward-compute: 1955.81 | backward-embedding-all-reduce: 0.01 | optimizer: 112.79 | batch-generator: 1141.37
[2022-12-22 15:45:41,462] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:45:41,462] [INFO] [timer.py:197:stop] 0/8, RunningAvgSamplesPerSec=71.71271896450253, CurrSamplesPerSec=56.28224290757093, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:45:46,979] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:45:46,982] [INFO] [timer.py:197:stop] 0/9, RunningAvgSamplesPerSec=73.50588133677599, CurrSamplesPerSec=86.4804391234284, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration        9/     200 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 5532.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.325595E+00 | moe loss: 1.306508E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3257.80 | backward-compute: 2146.13 | backward-embedding-all-reduce: 0.01 | optimizer: 97.51 | batch-generator: 887.90
[2022-12-22 15:45:51,511] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:45:51,512] [INFO] [timer.py:197:stop] 0/10, RunningAvgSamplesPerSec=75.07717268437075, CurrSamplesPerSec=88.28815499826177, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       10/     200 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 4532.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.099517E+00 | moe loss: 1.299803E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2691.73 | backward-compute: 1739.97 | backward-embedding-all-reduce: 0.01 | optimizer: 91.64 | batch-generator: 915.04
 iteration       11/     200 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 5784.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.364979E+00 | moe loss: 1.273549E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3649.78 | backward-compute: 2009.41 | backward-embedding-all-reduce: 0.01 | optimizer: 113.93 | batch-generator: 873.30
[2022-12-22 15:45:57,305] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:45:57,305] [INFO] [timer.py:197:stop] 0/11, RunningAvgSamplesPerSec=67.80512088379525, CurrSamplesPerSec=38.202474550109294, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:46:02,483] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:02,483] [INFO] [timer.py:197:stop] 0/12, RunningAvgSamplesPerSec=69.1538131371232, CurrSamplesPerSec=84.23288257121116, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       12/     200 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 5186.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.481689E+00 | moe loss: 1.300581E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2895.74 | backward-compute: 2165.26 | backward-embedding-all-reduce: 0.02 | optimizer: 105.97 | batch-generator: 912.43
[2022-12-22 15:46:07,028] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:07,029] [INFO] [timer.py:197:stop] 0/13, RunningAvgSamplesPerSec=70.52256653332742, CurrSamplesPerSec=87.9255731685547, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       13/     200 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 4546.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.008080E+00 | moe loss: 1.343928E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2669.27 | backward-compute: 1758.93 | backward-embedding-all-reduce: 0.01 | optimizer: 92.60 | batch-generator: 853.23
[2022-12-22 15:46:11,536] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:11,537] [INFO] [timer.py:197:stop] 0/14, RunningAvgSamplesPerSec=71.68122729965921, CurrSamplesPerSec=87.49360540120168, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       14/     200 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 4506.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.735820E+00 | moe loss: 1.347640E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2672.47 | backward-compute: 1732.92 | backward-embedding-all-reduce: 0.01 | optimizer: 93.44 | batch-generator: 836.86
[2022-12-22 15:46:17,164] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:17,165] [INFO] [timer.py:197:stop] 0/15, RunningAvgSamplesPerSec=67.07294514800891, CurrSamplesPerSec=37.86303207001197, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       15/     200 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 5657.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.659799E+00 | moe loss: 1.340713E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3362.27 | backward-compute: 2067.74 | backward-embedding-all-reduce: 0.01 | optimizer: 191.30 | batch-generator: 862.49
[2022-12-22 15:46:22,386] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:22,387] [INFO] [timer.py:197:stop] 0/16, RunningAvgSamplesPerSec=68.09035485180979, CurrSamplesPerSec=84.81536484589435, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       16/     200 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 5193.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.614525E+00 | moe loss: 1.330599E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2885.15 | backward-compute: 2186.58 | backward-embedding-all-reduce: 0.01 | optimizer: 103.17 | batch-generator: 931.98
 iteration       17/     200 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 4687.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.571282E+00 | moe loss: 1.282872E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2540.49 | backward-compute: 2009.99 | backward-embedding-all-reduce: 0.01 | optimizer: 100.51 | batch-generator: 820.08
[2022-12-22 15:46:27,094] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:27,104] [INFO] [timer.py:197:stop] 0/17, RunningAvgSamplesPerSec=68.98794618006148, CurrSamplesPerSec=84.60138470770363, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:46:31,541] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:31,541] [INFO] [timer.py:197:stop] 0/18, RunningAvgSamplesPerSec=69.95263536955957, CurrSamplesPerSec=88.51981572898839, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       18/     200 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 4465.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.517085E+00 | moe loss: 1.287266E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2588.50 | backward-compute: 1749.65 | backward-embedding-all-reduce: 0.01 | optimizer: 93.22 | batch-generator: 822.63
[2022-12-22 15:46:37,324] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:37,325] [INFO] [timer.py:197:stop] 0/19, RunningAvgSamplesPerSec=66.33473471929342, CurrSamplesPerSec=36.29790477601918, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       19/     200 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 5822.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.472691E+00 | moe loss: 1.314600E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3456.43 | backward-compute: 2118.52 | backward-embedding-all-reduce: 0.01 | optimizer: 208.06 | batch-generator: 832.00
 iteration       20/     200 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 5944.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.426884E+00 | moe loss: 1.277612E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3631.54 | backward-compute: 2120.83 | backward-embedding-all-reduce: 0.01 | optimizer: 172.65 | batch-generator: 909.48
[2022-12-22 15:46:43,332] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:43,346] [INFO] [timer.py:197:stop] 0/20, RunningAvgSamplesPerSec=65.7862954035093, CurrSamplesPerSec=57.67935631120908, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:46:47,967] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:47,968] [INFO] [timer.py:197:stop] 0/21, RunningAvgSamplesPerSec=66.67621781433856, CurrSamplesPerSec=88.13712640655201, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       21/     200 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 4659.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.416511E+00 | moe loss: 1.266703E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2765.61 | backward-compute: 1756.41 | backward-embedding-all-reduce: 0.01 | optimizer: 93.80 | batch-generator: 713.10
[2022-12-22 15:46:52,596] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:52,597] [INFO] [timer.py:197:stop] 0/22, RunningAvgSamplesPerSec=67.53523249492943, CurrSamplesPerSec=89.425068758928, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       22/     200 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 4628.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.407910E+00 | moe loss: 1.266392E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2759.26 | backward-compute: 1762.04 | backward-embedding-all-reduce: 0.01 | optimizer: 91.75 | batch-generator: 806.98
[2022-12-22 15:46:58,722] [INFO] [logging.py:68:log_dist] [Rank 0] step=23, skipped=0, lr=[9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:46:58,723] [INFO] [timer.py:197:stop] 0/23, RunningAvgSamplesPerSec=67.9925797476902, CurrSamplesPerSec=78.64410625044312, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       23/     200 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 6128.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.382207E+00 | moe loss: 1.259321E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3675.53 | backward-compute: 2286.07 | backward-embedding-all-reduce: 0.01 | optimizer: 149.80 | batch-generator: 848.29
[2022-12-22 15:47:04,050] [INFO] [logging.py:68:log_dist] [Rank 0] step=24, skipped=0, lr=[9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:04,051] [INFO] [timer.py:197:stop] 0/24, RunningAvgSamplesPerSec=68.59318147369213, CurrSamplesPerSec=84.21509786045355, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       24/     200 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 5324.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.371861E+00 | moe loss: 1.260199E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3353.35 | backward-compute: 1859.16 | backward-embedding-all-reduce: 0.01 | optimizer: 98.88 | batch-generator: 741.99
[2022-12-22 15:47:08,286] [INFO] [logging.py:68:log_dist] [Rank 0] step=25, skipped=0, lr=[9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:08,286] [INFO] [timer.py:197:stop] 0/25, RunningAvgSamplesPerSec=69.31846698833618, CurrSamplesPerSec=90.33157674410465, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       25/     200 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 4237.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.375607E+00 | moe loss: 1.228944E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2375.41 | backward-compute: 1740.25 | backward-embedding-all-reduce: 0.01 | optimizer: 93.76 | batch-generator: 765.13
[2022-12-22 15:47:12,584] [INFO] [logging.py:68:log_dist] [Rank 0] step=26, skipped=0, lr=[9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:12,585] [INFO] [timer.py:197:stop] 0/26, RunningAvgSamplesPerSec=69.98425077853659, CurrSamplesPerSec=89.82800610508824, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       26/     200 | consumed samples:        13312 | consumed tokens:     13631488 | elapsed time per iteration (ms): 4297.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.361634E+00 | moe loss: 1.205209E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2457.81 | backward-compute: 1742.41 | backward-embedding-all-reduce: 0.01 | optimizer: 89.91 | batch-generator: 765.77
[2022-12-22 15:47:18,319] [INFO] [logging.py:68:log_dist] [Rank 0] step=27, skipped=0, lr=[9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:18,320] [INFO] [timer.py:197:stop] 0/27, RunningAvgSamplesPerSec=70.34111711341247, CurrSamplesPerSec=80.15001240608474, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       27/     200 | consumed samples:        13824 | consumed tokens:     14155776 | elapsed time per iteration (ms): 5734.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.352719E+00 | moe loss: 1.238271E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3722.19 | backward-compute: 1878.89 | backward-embedding-all-reduce: 0.01 | optimizer: 117.37 | batch-generator: 826.24
[2022-12-22 15:47:22,823] [INFO] [logging.py:68:log_dist] [Rank 0] step=28, skipped=0, lr=[9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:22,823] [INFO] [timer.py:197:stop] 0/28, RunningAvgSamplesPerSec=70.81048338045326, CurrSamplesPerSec=84.98800257083245, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       28/     200 | consumed samples:        14336 | consumed tokens:     14680064 | elapsed time per iteration (ms): 4509.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.347326E+00 | moe loss: 1.225126E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2635.13 | backward-compute: 1767.12 | backward-embedding-all-reduce: 0.01 | optimizer: 95.55 | batch-generator: 735.91
 iteration       29/     200 | consumed samples:        14848 | consumed tokens:     15204352 | elapsed time per iteration (ms): 4922.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.335009E+00 | moe loss: 1.242016E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2998.66 | backward-compute: 1803.49 | backward-embedding-all-reduce: 0.01 | optimizer: 103.62 | batch-generator: 729.52
[2022-12-22 15:47:27,756] [INFO] [logging.py:68:log_dist] [Rank 0] step=29, skipped=0, lr=[9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:27,768] [INFO] [timer.py:197:stop] 0/29, RunningAvgSamplesPerSec=71.26751485846661, CurrSamplesPerSec=85.6386754939844, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:47:32,127] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:32,128] [INFO] [timer.py:197:stop] 0/30, RunningAvgSamplesPerSec=71.80366591684901, CurrSamplesPerSec=90.10634635473312, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       30/     200 | consumed samples:        15360 | consumed tokens:     15728640 | elapsed time per iteration (ms): 4377.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.329513E+00 | moe loss: 1.206526E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2517.30 | backward-compute: 1742.80 | backward-embedding-all-reduce: 0.01 | optimizer: 91.49 | batch-generator: 746.15
[2022-12-22 15:47:37,323] [INFO] [logging.py:68:log_dist] [Rank 0] step=31, skipped=0, lr=[9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:37,324] [INFO] [timer.py:197:stop] 0/31, RunningAvgSamplesPerSec=71.4256203760483, CurrSamplesPerSec=62.24890522255914, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       31/     200 | consumed samples:        15872 | consumed tokens:     16252928 | elapsed time per iteration (ms): 5232.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.330683E+00 | moe loss: 1.210199E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2827.17 | backward-compute: 2208.39 | backward-embedding-all-reduce: 0.01 | optimizer: 171.72 | batch-generator: 714.89
 iteration       32/     200 | consumed samples:        16384 | consumed tokens:     16777216 | elapsed time per iteration (ms): 6022.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.298423E+00 | moe loss: 1.194678E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3688.70 | backward-compute: 2113.28 | backward-embedding-all-reduce: 0.01 | optimizer: 206.48 | batch-generator: 763.38
[2022-12-22 15:47:43,427] [INFO] [logging.py:68:log_dist] [Rank 0] step=32, skipped=0, lr=[9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:43,440] [INFO] [timer.py:197:stop] 0/32, RunningAvgSamplesPerSec=70.7955549112241, CurrSamplesPerSec=56.374094778713655, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:47:48,041] [INFO] [logging.py:68:log_dist] [Rank 0] step=33, skipped=0, lr=[9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:48,041] [INFO] [timer.py:197:stop] 0/33, RunningAvgSamplesPerSec=71.21426531722602, CurrSamplesPerSec=86.57540983236454, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       33/     200 | consumed samples:        16896 | consumed tokens:     17301504 | elapsed time per iteration (ms): 4657.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.289127E+00 | moe loss: 1.193147E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2692.22 | backward-compute: 1809.22 | backward-embedding-all-reduce: 0.01 | optimizer: 93.67 | batch-generator: 658.46
[2022-12-22 15:47:52,345] [INFO] [logging.py:68:log_dist] [Rank 0] step=34, skipped=0, lr=[9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:52,345] [INFO] [timer.py:197:stop] 0/34, RunningAvgSamplesPerSec=71.66769477636059, CurrSamplesPerSec=89.29225146086978, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       34/     200 | consumed samples:        17408 | consumed tokens:     17825792 | elapsed time per iteration (ms): 4304.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.276861E+00 | moe loss: 1.167020E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2442.27 | backward-compute: 1762.36 | backward-embedding-all-reduce: 0.01 | optimizer: 90.45 | batch-generator: 717.57
 iteration       35/     200 | consumed samples:        17920 | consumed tokens:     18350080 | elapsed time per iteration (ms): 5586.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.264884E+00 | moe loss: 1.178027E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3621.92 | backward-compute: 1827.71 | backward-embedding-all-reduce: 0.01 | optimizer: 116.19 | batch-generator: 685.73
[2022-12-22 15:47:57,944] [INFO] [logging.py:68:log_dist] [Rank 0] step=35, skipped=0, lr=[9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:47:57,945] [INFO] [timer.py:197:stop] 0/35, RunningAvgSamplesPerSec=70.74389317552956, CurrSamplesPerSec=50.0848019234203, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       36/     200 | consumed samples:        18432 | consumed tokens:     18874368 | elapsed time per iteration (ms): 5424.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.260858E+00 | moe loss: 1.163791E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3210.45 | backward-compute: 2031.42 | backward-embedding-all-reduce: 0.01 | optimizer: 158.49 | batch-generator: 715.32
[2022-12-22 15:48:03,380] [INFO] [logging.py:68:log_dist] [Rank 0] step=36, skipped=0, lr=[9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:03,383] [INFO] [timer.py:197:stop] 0/36, RunningAvgSamplesPerSec=70.47501336501747, CurrSamplesPerSec=62.62081456492797, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:48:08,068] [INFO] [logging.py:68:log_dist] [Rank 0] step=37, skipped=0, lr=[9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:08,068] [INFO] [timer.py:197:stop] 0/37, RunningAvgSamplesPerSec=70.88255503215888, CurrSamplesPerSec=88.22982740594198, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       37/     200 | consumed samples:        18944 | consumed tokens:     19398656 | elapsed time per iteration (ms): 4714.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.250848E+00 | moe loss: 1.121074E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2765.78 | backward-compute: 1812.03 | backward-embedding-all-reduce: 0.01 | optimizer: 91.84 | batch-generator: 645.05
[2022-12-22 15:48:12,307] [INFO] [logging.py:68:log_dist] [Rank 0] step=38, skipped=0, lr=[9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:12,308] [INFO] [timer.py:197:stop] 0/38, RunningAvgSamplesPerSec=71.29451865049299, CurrSamplesPerSec=89.5004427752942, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       38/     200 | consumed samples:        19456 | consumed tokens:     19922944 | elapsed time per iteration (ms): 4238.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.234586E+00 | moe loss: 1.079764E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2404.06 | backward-compute: 1732.47 | backward-embedding-all-reduce: 0.01 | optimizer: 91.21 | batch-generator: 635.28
[2022-12-22 15:48:17,836] [INFO] [logging.py:68:log_dist] [Rank 0] step=39, skipped=0, lr=[9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:17,837] [INFO] [timer.py:197:stop] 0/39, RunningAvgSamplesPerSec=70.59978785898348, CurrSamplesPerSec=52.265063356204394, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       39/     200 | consumed samples:        19968 | consumed tokens:     20447232 | elapsed time per iteration (ms): 5535.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.237309E+00 | moe loss: 1.082406E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3531.99 | backward-compute: 1856.01 | backward-embedding-all-reduce: 0.01 | optimizer: 129.93 | batch-generator: 647.39
[2022-12-22 15:48:22,708] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:22,709] [INFO] [timer.py:197:stop] 0/40, RunningAvgSamplesPerSec=70.92070984623163, CurrSamplesPerSec=85.26061432259003, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       40/     200 | consumed samples:        20480 | consumed tokens:     20971520 | elapsed time per iteration (ms): 4865.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.221267E+00 | moe loss: 1.078869E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2933.84 | backward-compute: 1821.02 | backward-embedding-all-reduce: 0.01 | optimizer: 98.05 | batch-generator: 691.84
[2022-12-22 15:48:26,768] [INFO] [logging.py:68:log_dist] [Rank 0] step=41, skipped=0, lr=[9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:26,768] [INFO] [timer.py:197:stop] 0/41, RunningAvgSamplesPerSec=71.28539922888667, CurrSamplesPerSec=88.597764489135, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       41/     200 | consumed samples:        20992 | consumed tokens:     21495808 | elapsed time per iteration (ms): 4063.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.211452E+00 | moe loss: 1.066918E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2214.74 | backward-compute: 1731.71 | backward-embedding-all-reduce: 0.01 | optimizer: 91.42 | batch-generator: 608.22
[2022-12-22 15:48:30,863] [INFO] [logging.py:68:log_dist] [Rank 0] step=42, skipped=0, lr=[9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:30,863] [INFO] [timer.py:197:stop] 0/42, RunningAvgSamplesPerSec=71.60689257884655, CurrSamplesPerSec=86.88975004693498, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       42/     200 | consumed samples:        21504 | consumed tokens:     22020096 | elapsed time per iteration (ms): 4090.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.208836E+00 | moe loss: 1.044405E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2262.70 | backward-compute: 1729.60 | backward-embedding-all-reduce: 0.01 | optimizer: 91.97 | batch-generator: 654.11
 iteration       43/     200 | consumed samples:        22016 | consumed tokens:     22544384 | elapsed time per iteration (ms): 5478.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.197780E+00 | moe loss: 1.028836E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3041.09 | backward-compute: 2233.17 | backward-embedding-all-reduce: 0.01 | optimizer: 193.53 | batch-generator: 640.88
[2022-12-22 15:48:36,372] [INFO] [logging.py:68:log_dist] [Rank 0] step=43, skipped=0, lr=[9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:36,387] [INFO] [timer.py:197:stop] 0/43, RunningAvgSamplesPerSec=71.19849744461212, CurrSamplesPerSec=57.97301285085886, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:48:42,508] [INFO] [logging.py:68:log_dist] [Rank 0] step=44, skipped=0, lr=[9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:42,508] [INFO] [timer.py:197:stop] 0/44, RunningAvgSamplesPerSec=71.43007982648398, CurrSamplesPerSec=82.42165327542882, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       44/     200 | consumed samples:        22528 | consumed tokens:     23068672 | elapsed time per iteration (ms): 6184.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.186376E+00 | moe loss: 1.014868E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3540.10 | backward-compute: 2439.94 | backward-embedding-all-reduce: 0.01 | optimizer: 124.73 | batch-generator: 671.37
[2022-12-22 15:48:46,741] [INFO] [logging.py:68:log_dist] [Rank 0] step=45, skipped=0, lr=[9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:46,742] [INFO] [timer.py:197:stop] 0/45, RunningAvgSamplesPerSec=71.77446414308643, CurrSamplesPerSec=89.99861734249473, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       45/     200 | consumed samples:        23040 | consumed tokens:     23592960 | elapsed time per iteration (ms): 4214.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.177597E+00 | moe loss: 1.009642E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2380.68 | backward-compute: 1729.28 | backward-embedding-all-reduce: 0.01 | optimizer: 91.70 | batch-generator: 620.18
[2022-12-22 15:48:50,867] [INFO] [logging.py:68:log_dist] [Rank 0] step=46, skipped=0, lr=[9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:50,867] [INFO] [timer.py:197:stop] 0/46, RunningAvgSamplesPerSec=72.08938660063171, CurrSamplesPerSec=88.85331342010944, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       46/     200 | consumed samples:        23552 | consumed tokens:     24117248 | elapsed time per iteration (ms): 4126.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.157489E+00 | moe loss: 9.908795E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2294.37 | backward-compute: 1726.63 | backward-embedding-all-reduce: 0.01 | optimizer: 91.74 | batch-generator: 570.77
 iteration       47/     200 | consumed samples:        24064 | consumed tokens:     24641536 | elapsed time per iteration (ms): 5105.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.137909E+00 | moe loss: 1.000033E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3048.69 | backward-compute: 1828.51 | backward-embedding-all-reduce: 0.01 | optimizer: 220.10 | batch-generator: 623.81
[2022-12-22 15:48:55,992] [INFO] [logging.py:68:log_dist] [Rank 0] step=47, skipped=0, lr=[9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:48:56,010] [INFO] [timer.py:197:stop] 0/47, RunningAvgSamplesPerSec=72.06863077324635, CurrSamplesPerSec=71.16705877612009, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:49:02,112] [INFO] [logging.py:68:log_dist] [Rank 0] step=48, skipped=0, lr=[9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:02,113] [INFO] [timer.py:197:stop] 0/48, RunningAvgSamplesPerSec=72.33472683167655, CurrSamplesPerSec=86.74806119131625, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       48/     200 | consumed samples:        24576 | consumed tokens:     25165824 | elapsed time per iteration (ms): 6155.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.128592E+00 | moe loss: 9.759973E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3823.07 | backward-compute: 2130.95 | backward-embedding-all-reduce: 0.01 | optimizer: 119.34 | batch-generator: 636.47
[2022-12-22 15:49:06,305] [INFO] [logging.py:68:log_dist] [Rank 0] step=49, skipped=0, lr=[9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:06,306] [INFO] [timer.py:197:stop] 0/49, RunningAvgSamplesPerSec=72.63252461269876, CurrSamplesPerSec=89.60111939875043, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       49/     200 | consumed samples:        25088 | consumed tokens:     25690112 | elapsed time per iteration (ms): 4177.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.111155E+00 | moe loss: 9.664796E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2333.04 | backward-compute: 1733.75 | backward-embedding-all-reduce: 0.01 | optimizer: 91.30 | batch-generator: 589.29
[2022-12-22 15:49:10,546] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:10,546] [INFO] [timer.py:197:stop] 0/50, RunningAvgSamplesPerSec=72.61536299512875, CurrSamplesPerSec=71.81781445840896, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       50/     200 | consumed samples:        25600 | consumed tokens:     26214400 | elapsed time per iteration (ms): 4240.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.100065E+00 | moe loss: 9.527311E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2399.53 | backward-compute: 1738.41 | backward-embedding-all-reduce: 0.01 | optimizer: 91.96 | batch-generator: 577.93
 iteration       51/     200 | consumed samples:        26112 | consumed tokens:     26738688 | elapsed time per iteration (ms): 4846.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.092197E+00 | moe loss: 9.423941E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2873.19 | backward-compute: 1859.57 | backward-embedding-all-reduce: 0.01 | optimizer: 100.72 | batch-generator: 607.76
[2022-12-22 15:49:15,406] [INFO] [logging.py:68:log_dist] [Rank 0] step=51, skipped=0, lr=[9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:15,410] [INFO] [timer.py:197:stop] 0/51, RunningAvgSamplesPerSec=72.79203611242433, CurrSamplesPerSec=82.41702233224717, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:49:21,092] [INFO] [logging.py:68:log_dist] [Rank 0] step=52, skipped=0, lr=[9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:21,092] [INFO] [timer.py:197:stop] 0/52, RunningAvgSamplesPerSec=71.71331644684648, CurrSamplesPerSec=41.54546290297568, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       52/     200 | consumed samples:        26624 | consumed tokens:     27262976 | elapsed time per iteration (ms): 5724.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.079979E+00 | moe loss: 9.423579E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3126.98 | backward-compute: 2442.71 | backward-embedding-all-reduce: 0.01 | optimizer: 129.33 | batch-generator: 630.47
[2022-12-22 15:49:25,225] [INFO] [logging.py:68:log_dist] [Rank 0] step=53, skipped=0, lr=[9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:25,225] [INFO] [timer.py:197:stop] 0/53, RunningAvgSamplesPerSec=71.9948816992999, CurrSamplesPerSec=89.58075669722933, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       53/     200 | consumed samples:        27136 | consumed tokens:     27787264 | elapsed time per iteration (ms): 4107.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.069448E+00 | moe loss: 9.236018E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2252.44 | backward-compute: 1732.99 | backward-embedding-all-reduce: 0.01 | optimizer: 92.59 | batch-generator: 549.24
[2022-12-22 15:49:29,234] [INFO] [logging.py:68:log_dist] [Rank 0] step=54, skipped=0, lr=[9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:29,235] [INFO] [timer.py:197:stop] 0/54, RunningAvgSamplesPerSec=72.26794920432195, CurrSamplesPerSec=89.5998333743333, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       54/     200 | consumed samples:        27648 | consumed tokens:     28311552 | elapsed time per iteration (ms): 4010.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.064251E+00 | moe loss: 9.200735E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2180.06 | backward-compute: 1730.44 | backward-embedding-all-reduce: 0.01 | optimizer: 90.06 | batch-generator: 556.51
 iteration       55/     200 | consumed samples:        28160 | consumed tokens:     28835840 | elapsed time per iteration (ms): 4614.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.041877E+00 | moe loss: 8.957974E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2261.63 | backward-compute: 2122.57 | backward-embedding-all-reduce: 0.01 | optimizer: 223.17 | batch-generator: 564.12
[2022-12-22 15:49:33,907] [INFO] [logging.py:68:log_dist] [Rank 0] step=55, skipped=0, lr=[9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:33,931] [INFO] [timer.py:197:stop] 0/55, RunningAvgSamplesPerSec=71.71563231939157, CurrSamplesPerSec=51.32015436072883, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:49:40,019] [INFO] [logging.py:68:log_dist] [Rank 0] step=56, skipped=0, lr=[9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:40,020] [INFO] [timer.py:197:stop] 0/56, RunningAvgSamplesPerSec=71.58025238157735, CurrSamplesPerSec=65.07000913623216, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       56/     200 | consumed samples:        28672 | consumed tokens:     29360128 | elapsed time per iteration (ms): 6184.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.027464E+00 | moe loss: 9.032109E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3184.36 | backward-compute: 2754.54 | backward-embedding-all-reduce: 0.01 | optimizer: 151.79 | batch-generator: 640.76
[2022-12-22 15:49:44,576] [INFO] [logging.py:68:log_dist] [Rank 0] step=57, skipped=0, lr=[9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:44,577] [INFO] [timer.py:197:stop] 0/57, RunningAvgSamplesPerSec=71.83029683722408, CurrSamplesPerSec=88.5299751792627, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       57/     200 | consumed samples:        29184 | consumed tokens:     29884416 | elapsed time per iteration (ms): 4542.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.012605E+00 | moe loss: 9.040998E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2632.92 | backward-compute: 1796.49 | backward-embedding-all-reduce: 0.01 | optimizer: 92.36 | batch-generator: 545.16
[2022-12-22 15:49:48,585] [INFO] [logging.py:68:log_dist] [Rank 0] step=58, skipped=0, lr=[9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:48,585] [INFO] [timer.py:197:stop] 0/58, RunningAvgSamplesPerSec=72.07739191059909, CurrSamplesPerSec=88.89653895714024, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       58/     200 | consumed samples:        29696 | consumed tokens:     30408704 | elapsed time per iteration (ms): 4007.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.998234E+00 | moe loss: 9.018743E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2168.71 | backward-compute: 1736.86 | backward-embedding-all-reduce: 0.01 | optimizer: 91.36 | batch-generator: 552.77
[2022-12-22 15:49:52,668] [INFO] [logging.py:68:log_dist] [Rank 0] step=59, skipped=0, lr=[9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:52,669] [INFO] [timer.py:197:stop] 0/59, RunningAvgSamplesPerSec=72.32344531086748, CurrSamplesPerSec=89.41729409840532, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       59/     200 | consumed samples:        30208 | consumed tokens:     30932992 | elapsed time per iteration (ms): 4083.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.993427E+00 | moe loss: 8.909475E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2249.49 | backward-compute: 1732.49 | backward-embedding-all-reduce: 0.01 | optimizer: 92.18 | batch-generator: 561.47
 iteration       60/     200 | consumed samples:        30720 | consumed tokens:     31457280 | elapsed time per iteration (ms): 6188.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.976300E+00 | moe loss: 8.699112E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3847.16 | backward-compute: 2204.57 | backward-embedding-all-reduce: 0.01 | optimizer: 117.71 | batch-generator: 835.04
[2022-12-22 15:49:58,863] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:49:58,871] [INFO] [timer.py:197:stop] 0/60, RunningAvgSamplesPerSec=72.45120203168644, CurrSamplesPerSec=80.562958659014, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:50:03,624] [INFO] [logging.py:68:log_dist] [Rank 0] step=61, skipped=0, lr=[9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:03,624] [INFO] [timer.py:197:stop] 0/61, RunningAvgSamplesPerSec=72.60181919732493, CurrSamplesPerSec=82.5559996506288, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       61/     200 | consumed samples:        31232 | consumed tokens:     31981568 | elapsed time per iteration (ms): 4766.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.958229E+00 | moe loss: 8.577009E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2370.00 | backward-compute: 2272.39 | backward-embedding-all-reduce: 0.01 | optimizer: 100.25 | batch-generator: 531.04
[2022-12-22 15:50:07,484] [INFO] [logging.py:68:log_dist] [Rank 0] step=62, skipped=0, lr=[9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:07,484] [INFO] [timer.py:197:stop] 0/62, RunningAvgSamplesPerSec=72.82556955794085, CurrSamplesPerSec=89.01043877610233, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       62/     200 | consumed samples:        31744 | consumed tokens:     32505856 | elapsed time per iteration (ms): 3860.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.939960E+00 | moe loss: 8.455671E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2015.95 | backward-compute: 1726.22 | backward-embedding-all-reduce: 0.01 | optimizer: 92.23 | batch-generator: 527.33
[2022-12-22 15:50:11,548] [INFO] [logging.py:68:log_dist] [Rank 0] step=63, skipped=0, lr=[9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:11,549] [INFO] [timer.py:197:stop] 0/63, RunningAvgSamplesPerSec=73.05704494107103, CurrSamplesPerSec=90.27294745288368, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       63/     200 | consumed samples:        32256 | consumed tokens:     33030144 | elapsed time per iteration (ms): 4065.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.938704E+00 | moe loss: 8.519562E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2176.13 | backward-compute: 1786.37 | backward-embedding-all-reduce: 0.01 | optimizer: 92.56 | batch-generator: 526.49
 iteration       64/     200 | consumed samples:        32768 | consumed tokens:     33554432 | elapsed time per iteration (ms): 4350.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.927126E+00 | moe loss: 8.531659E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2198.98 | backward-compute: 2048.99 | backward-embedding-all-reduce: 0.01 | optimizer: 94.02 | batch-generator: 511.11
[2022-12-22 15:50:15,915] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:15,918] [INFO] [timer.py:197:stop] 0/64, RunningAvgSamplesPerSec=73.21346017133685, CurrSamplesPerSec=84.21158408862256, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:50:22,022] [INFO] [logging.py:68:log_dist] [Rank 0] step=65, skipped=0, lr=[9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:22,023] [INFO] [timer.py:197:stop] 0/65, RunningAvgSamplesPerSec=73.36928963082124, CurrSamplesPerSec=84.52317341965062, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       65/     200 | consumed samples:        33280 | consumed tokens:     34078720 | elapsed time per iteration (ms): 6143.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.922515E+00 | moe loss: 8.293161E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3919.00 | backward-compute: 1919.59 | backward-embedding-all-reduce: 0.01 | optimizer: 120.63 | batch-generator: 627.20
[2022-12-22 15:50:26,152] [INFO] [logging.py:68:log_dist] [Rank 0] step=66, skipped=0, lr=[9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:26,152] [INFO] [timer.py:197:stop] 0/66, RunningAvgSamplesPerSec=73.55814158778428, CurrSamplesPerSec=87.7951307611624, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       66/     200 | consumed samples:        33792 | consumed tokens:     34603008 | elapsed time per iteration (ms): 4111.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.899665E+00 | moe loss: 8.546044E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2248.17 | backward-compute: 1757.31 | backward-embedding-all-reduce: 0.01 | optimizer: 90.30 | batch-generator: 495.76
[2022-12-22 15:50:30,029] [INFO] [logging.py:68:log_dist] [Rank 0] step=67, skipped=0, lr=[9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:30,030] [INFO] [timer.py:197:stop] 0/67, RunningAvgSamplesPerSec=73.64712705771126, CurrSamplesPerSec=79.8275951693436, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       67/     200 | consumed samples:        34304 | consumed tokens:     35127296 | elapsed time per iteration (ms): 3874.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.897217E+00 | moe loss: 8.488853E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1958.35 | backward-compute: 1795.53 | backward-embedding-all-reduce: 0.01 | optimizer: 114.61 | batch-generator: 512.24
 iteration       68/     200 | consumed samples:        34816 | consumed tokens:     35651584 | elapsed time per iteration (ms): 4357.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.878878E+00 | moe loss: 8.259722E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2403.74 | backward-compute: 1829.19 | backward-embedding-all-reduce: 0.01 | optimizer: 107.86 | batch-generator: 493.22
[2022-12-22 15:50:34,462] [INFO] [logging.py:68:log_dist] [Rank 0] step=68, skipped=0, lr=[9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:34,469] [INFO] [timer.py:197:stop] 0/68, RunningAvgSamplesPerSec=73.06876817236963, CurrSamplesPerSec=48.37543025739394, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:50:39,312] [INFO] [logging.py:68:log_dist] [Rank 0] step=69, skipped=0, lr=[9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:39,313] [INFO] [timer.py:197:stop] 0/69, RunningAvgSamplesPerSec=73.2168821281216, CurrSamplesPerSec=84.52508967796543, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       69/     200 | consumed samples:        35328 | consumed tokens:     36175872 | elapsed time per iteration (ms): 4941.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.869393E+00 | moe loss: 8.153397E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2905.77 | backward-compute: 1819.57 | backward-embedding-all-reduce: 0.01 | optimizer: 115.07 | batch-generator: 552.37
[2022-12-22 15:50:43,898] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:43,898] [INFO] [timer.py:197:stop] 0/70, RunningAvgSamplesPerSec=73.22407548031478, CurrSamplesPerSec=73.70927125877378, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       70/     200 | consumed samples:        35840 | consumed tokens:     36700160 | elapsed time per iteration (ms): 4571.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.869379E+00 | moe loss: 8.256775E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2272.82 | backward-compute: 2180.75 | backward-embedding-all-reduce: 0.01 | optimizer: 102.56 | batch-generator: 500.82
[2022-12-22 15:50:47,809] [INFO] [logging.py:68:log_dist] [Rank 0] step=71, skipped=0, lr=[9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:47,809] [INFO] [timer.py:197:stop] 0/71, RunningAvgSamplesPerSec=73.42418859755894, CurrSamplesPerSec=90.1835705490762, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       71/     200 | consumed samples:        36352 | consumed tokens:     37224448 | elapsed time per iteration (ms): 3909.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.837843E+00 | moe loss: 8.318314E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2048.80 | backward-compute: 1745.79 | backward-embedding-all-reduce: 0.01 | optimizer: 91.84 | batch-generator: 480.34
[2022-12-22 15:50:51,997] [INFO] [logging.py:68:log_dist] [Rank 0] step=72, skipped=0, lr=[9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:51,998] [INFO] [timer.py:197:stop] 0/72, RunningAvgSamplesPerSec=73.61365351810882, CurrSamplesPerSec=89.55962635481079, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       72/     200 | consumed samples:        36864 | consumed tokens:     37748736 | elapsed time per iteration (ms): 4187.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.842601E+00 | moe loss: 8.303523E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2309.91 | backward-compute: 1776.91 | backward-embedding-all-reduce: 0.01 | optimizer: 92.04 | batch-generator: 522.37
 iteration       73/     200 | consumed samples:        37376 | consumed tokens:     38273024 | elapsed time per iteration (ms): 4553.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.832738E+00 | moe loss: 8.274049E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2193.82 | backward-compute: 2154.29 | backward-embedding-all-reduce: 0.01 | optimizer: 187.93 | batch-generator: 464.20
[2022-12-22 15:50:56,561] [INFO] [logging.py:68:log_dist] [Rank 0] step=73, skipped=0, lr=[9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:50:56,576] [INFO] [timer.py:197:stop] 0/73, RunningAvgSamplesPerSec=73.37194545076528, CurrSamplesPerSec=59.65961528631277, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:51:01,956] [INFO] [logging.py:68:log_dist] [Rank 0] step=74, skipped=0, lr=[9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:01,956] [INFO] [timer.py:197:stop] 0/74, RunningAvgSamplesPerSec=73.49803881667572, CurrSamplesPerSec=83.71237681810244, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       74/     200 | consumed samples:        37888 | consumed tokens:     38797312 | elapsed time per iteration (ms): 5419.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.816388E+00 | moe loss: 8.286394E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3401.67 | backward-compute: 1817.59 | backward-embedding-all-reduce: 0.01 | optimizer: 114.66 | batch-generator: 555.34
[2022-12-22 15:51:05,912] [INFO] [logging.py:68:log_dist] [Rank 0] step=75, skipped=0, lr=[9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:05,913] [INFO] [timer.py:197:stop] 0/75, RunningAvgSamplesPerSec=73.68129479391999, CurrSamplesPerSec=89.80276311092689, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       75/     200 | consumed samples:        38400 | consumed tokens:     39321600 | elapsed time per iteration (ms): 3942.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.811800E+00 | moe loss: 8.211108E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2093.10 | backward-compute: 1742.84 | backward-embedding-all-reduce: 0.01 | optimizer: 94.15 | batch-generator: 485.42
[2022-12-22 15:51:09,565] [INFO] [logging.py:68:log_dist] [Rank 0] step=76, skipped=0, lr=[9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:09,565] [INFO] [timer.py:197:stop] 0/76, RunningAvgSamplesPerSec=73.86343535674835, CurrSamplesPerSec=90.12752367381637, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       76/     200 | consumed samples:        38912 | consumed tokens:     39845888 | elapsed time per iteration (ms): 3652.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.796649E+00 | moe loss: 8.141770E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1817.09 | backward-compute: 1728.26 | backward-embedding-all-reduce: 0.01 | optimizer: 91.53 | batch-generator: 475.31
 iteration       77/     200 | consumed samples:        39424 | consumed tokens:     40370176 | elapsed time per iteration (ms): 4214.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.781309E+00 | moe loss: 8.091377E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2072.64 | backward-compute: 1971.43 | backward-embedding-all-reduce: 0.01 | optimizer: 156.71 | batch-generator: 482.24
[2022-12-22 15:51:13,818] [INFO] [logging.py:68:log_dist] [Rank 0] step=77, skipped=0, lr=[9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:13,822] [INFO] [timer.py:197:stop] 0/77, RunningAvgSamplesPerSec=73.65414553532862, CurrSamplesPerSec=60.887459497835536, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:51:19,082] [INFO] [logging.py:68:log_dist] [Rank 0] step=78, skipped=0, lr=[9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:19,082] [INFO] [timer.py:197:stop] 0/78, RunningAvgSamplesPerSec=73.80347634502658, CurrSamplesPerSec=87.03850588502318, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       78/     200 | consumed samples:        39936 | consumed tokens:     40894464 | elapsed time per iteration (ms): 5328.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.779187E+00 | moe loss: 8.160114E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3301.21 | backward-compute: 1821.28 | backward-embedding-all-reduce: 0.01 | optimizer: 117.95 | batch-generator: 658.81
[2022-12-22 15:51:23,369] [INFO] [logging.py:68:log_dist] [Rank 0] step=79, skipped=0, lr=[9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:23,369] [INFO] [timer.py:197:stop] 0/79, RunningAvgSamplesPerSec=73.9274415602271, CurrSamplesPerSec=84.7456123734982, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       79/     200 | consumed samples:        40448 | consumed tokens:     41418752 | elapsed time per iteration (ms): 4257.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.768726E+00 | moe loss: 8.192730E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2097.19 | backward-compute: 2039.86 | backward-embedding-all-reduce: 0.01 | optimizer: 112.58 | batch-generator: 479.69
[2022-12-22 15:51:27,294] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:27,295] [INFO] [timer.py:197:stop] 0/80, RunningAvgSamplesPerSec=73.98639503223897, CurrSamplesPerSec=78.82664129512332, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       80/     200 | consumed samples:        40960 | consumed tokens:     41943040 | elapsed time per iteration (ms): 3947.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.758185E+00 | moe loss: 8.170334E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1970.62 | backward-compute: 1819.99 | backward-embedding-all-reduce: 0.01 | optimizer: 116.81 | batch-generator: 446.75
[2022-12-22 15:51:31,150] [INFO] [logging.py:68:log_dist] [Rank 0] step=81, skipped=0, lr=[9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:31,150] [INFO] [timer.py:197:stop] 0/81, RunningAvgSamplesPerSec=74.15029627668633, CurrSamplesPerSec=89.63929823436578, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       81/     200 | consumed samples:        41472 | consumed tokens:     42467328 | elapsed time per iteration (ms): 3840.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.758382E+00 | moe loss: 8.234145E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1919.79 | backward-compute: 1820.90 | backward-embedding-all-reduce: 0.01 | optimizer: 90.50 | batch-generator: 506.25
 iteration       82/     200 | consumed samples:        41984 | consumed tokens:     42991616 | elapsed time per iteration (ms): 3974.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.748828E+00 | moe loss: 8.288238E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2073.94 | backward-compute: 1799.24 | backward-embedding-all-reduce: 0.01 | optimizer: 94.93 | batch-generator: 464.56
[2022-12-22 15:51:35,140] [INFO] [logging.py:68:log_dist] [Rank 0] step=82, skipped=0, lr=[9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:35,141] [INFO] [timer.py:197:stop] 0/82, RunningAvgSamplesPerSec=74.26319527059877, CurrSamplesPerSec=84.41714297035507, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:51:39,352] [INFO] [logging.py:68:log_dist] [Rank 0] step=83, skipped=0, lr=[9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:39,353] [INFO] [timer.py:197:stop] 0/83, RunningAvgSamplesPerSec=74.39707799387514, CurrSamplesPerSec=86.93533694328575, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       83/     200 | consumed samples:        42496 | consumed tokens:     43515904 | elapsed time per iteration (ms): 4234.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.730929E+00 | moe loss: 8.194005E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2337.12 | backward-compute: 1767.16 | backward-embedding-all-reduce: 0.01 | optimizer: 106.74 | batch-generator: 489.30
 iteration       84/     200 | consumed samples:        43008 | consumed tokens:     44040192 | elapsed time per iteration (ms): 3956.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.705078E+00 | moe loss: 8.085263E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2100.23 | backward-compute: 1749.86 | backward-embedding-all-reduce: 0.01 | optimizer: 93.70 | batch-generator: 445.85
[2022-12-22 15:51:43,337] [INFO] [logging.py:68:log_dist] [Rank 0] step=84, skipped=0, lr=[9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:43,337] [INFO] [timer.py:197:stop] 0/84, RunningAvgSamplesPerSec=74.5124844260338, CurrSamplesPerSec=85.22033754098148, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:51:47,203] [INFO] [logging.py:68:log_dist] [Rank 0] step=85, skipped=0, lr=[9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:47,204] [INFO] [timer.py:197:stop] 0/85, RunningAvgSamplesPerSec=74.6410604534542, CurrSamplesPerSec=86.94319284879809, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       85/     200 | consumed samples:        43520 | consumed tokens:     44564480 | elapsed time per iteration (ms): 3886.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.705883E+00 | moe loss: 8.066335E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1990.65 | backward-compute: 1755.69 | backward-embedding-all-reduce: 0.01 | optimizer: 96.92 | batch-generator: 438.62
[2022-12-22 15:51:51,174] [INFO] [logging.py:68:log_dist] [Rank 0] step=86, skipped=0, lr=[9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:51,174] [INFO] [timer.py:197:stop] 0/86, RunningAvgSamplesPerSec=74.75438471558297, CurrSamplesPerSec=85.53282749026492, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       86/     200 | consumed samples:        44032 | consumed tokens:     45088768 | elapsed time per iteration (ms): 3969.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.697085E+00 | moe loss: 8.107921E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2098.54 | backward-compute: 1768.44 | backward-embedding-all-reduce: 0.01 | optimizer: 92.28 | batch-generator: 464.65
 iteration       87/     200 | consumed samples:        44544 | consumed tokens:     45613056 | elapsed time per iteration (ms): 4437.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.675298E+00 | moe loss: 8.096677E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2511.86 | backward-compute: 1808.64 | backward-embedding-all-reduce: 0.01 | optimizer: 104.67 | batch-generator: 454.02
[2022-12-22 15:51:55,639] [INFO] [logging.py:68:log_dist] [Rank 0] step=87, skipped=0, lr=[9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:51:55,653] [INFO] [timer.py:197:stop] 0/87, RunningAvgSamplesPerSec=74.81612624182948, CurrSamplesPerSec=80.39365405285933, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:52:00,371] [INFO] [logging.py:68:log_dist] [Rank 0] step=88, skipped=0, lr=[9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:00,371] [INFO] [timer.py:197:stop] 0/88, RunningAvgSamplesPerSec=74.76996381129676, CurrSamplesPerSec=71.04399064164046, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       88/     200 | consumed samples:        45056 | consumed tokens:     46137344 | elapsed time per iteration (ms): 4775.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.669649E+00 | moe loss: 8.172753E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2618.65 | backward-compute: 1969.85 | backward-embedding-all-reduce: 0.01 | optimizer: 132.43 | batch-generator: 497.34
[2022-12-22 15:52:04,538] [INFO] [logging.py:68:log_dist] [Rank 0] step=89, skipped=0, lr=[9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:04,538] [INFO] [timer.py:197:stop] 0/89, RunningAvgSamplesPerSec=74.88404898391393, CurrSamplesPerSec=86.19450348216404, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       89/     200 | consumed samples:        45568 | consumed tokens:     46661632 | elapsed time per iteration (ms): 4151.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.654567E+00 | moe loss: 8.166494E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2230.72 | backward-compute: 1801.34 | backward-embedding-all-reduce: 0.01 | optimizer: 93.45 | batch-generator: 444.83
[2022-12-22 15:52:08,243] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:08,244] [INFO] [timer.py:197:stop] 0/90, RunningAvgSamplesPerSec=75.03056838985871, CurrSamplesPerSec=90.42286170093793, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       90/     200 | consumed samples:        46080 | consumed tokens:     47185920 | elapsed time per iteration (ms): 3704.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.635041E+00 | moe loss: 8.159725E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1869.76 | backward-compute: 1733.63 | backward-embedding-all-reduce: 0.01 | optimizer: 91.34 | batch-generator: 428.18
[2022-12-22 15:52:12,336] [INFO] [logging.py:68:log_dist] [Rank 0] step=91, skipped=0, lr=[9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:12,337] [INFO] [timer.py:197:stop] 0/91, RunningAvgSamplesPerSec=75.16640489511944, CurrSamplesPerSec=89.41106938701553, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       91/     200 | consumed samples:        46592 | consumed tokens:     47710208 | elapsed time per iteration (ms): 4095.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.643583E+00 | moe loss: 8.335669E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2205.77 | backward-compute: 1783.68 | backward-embedding-all-reduce: 0.01 | optimizer: 94.15 | batch-generator: 421.44
 iteration       92/     200 | consumed samples:        47104 | consumed tokens:     48234496 | elapsed time per iteration (ms): 4282.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.627463E+00 | moe loss: 8.341146E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2142.72 | backward-compute: 1976.12 | backward-embedding-all-reduce: 0.01 | optimizer: 154.59 | batch-generator: 425.17
[2022-12-22 15:52:16,636] [INFO] [logging.py:68:log_dist] [Rank 0] step=92, skipped=0, lr=[9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:16,640] [INFO] [timer.py:197:stop] 0/92, RunningAvgSamplesPerSec=75.01949606487521, CurrSamplesPerSec=63.90370954410583, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:52:21,342] [INFO] [logging.py:68:log_dist] [Rank 0] step=93, skipped=0, lr=[9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:21,342] [INFO] [timer.py:197:stop] 0/93, RunningAvgSamplesPerSec=75.07208002867559, CurrSamplesPerSec=80.12682962939968, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       93/     200 | consumed samples:        47616 | consumed tokens:     48758784 | elapsed time per iteration (ms): 4735.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.614981E+00 | moe loss: 8.071027E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2753.08 | backward-compute: 1806.89 | backward-embedding-all-reduce: 0.01 | optimizer: 117.25 | batch-generator: 500.80
[2022-12-22 15:52:25,056] [INFO] [logging.py:68:log_dist] [Rank 0] step=94, skipped=0, lr=[9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:25,056] [INFO] [timer.py:197:stop] 0/94, RunningAvgSamplesPerSec=75.20199157370742, CurrSamplesPerSec=89.25783995296959, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       94/     200 | consumed samples:        48128 | consumed tokens:     49283072 | elapsed time per iteration (ms): 3700.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.617567E+00 | moe loss: 8.305082E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1854.69 | backward-compute: 1730.34 | backward-embedding-all-reduce: 0.01 | optimizer: 91.00 | batch-generator: 431.31
[2022-12-22 15:52:28,615] [INFO] [logging.py:68:log_dist] [Rank 0] step=95, skipped=0, lr=[9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:28,616] [INFO] [timer.py:197:stop] 0/95, RunningAvgSamplesPerSec=75.32641952123538, CurrSamplesPerSec=88.8515488049655, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       95/     200 | consumed samples:        48640 | consumed tokens:     49807360 | elapsed time per iteration (ms): 3558.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.604569E+00 | moe loss: 8.162101E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1723.51 | backward-compute: 1733.70 | backward-embedding-all-reduce: 0.01 | optimizer: 90.27 | batch-generator: 420.58
 iteration       96/     200 | consumed samples:        49152 | consumed tokens:     50331648 | elapsed time per iteration (ms): 4011.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.592726E+00 | moe loss: 8.109656E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2151.82 | backward-compute: 1754.57 | backward-embedding-all-reduce: 0.01 | optimizer: 91.64 | batch-generator: 427.18
[2022-12-22 15:52:32,638] [INFO] [logging.py:68:log_dist] [Rank 0] step=96, skipped=0, lr=[9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:32,638] [INFO] [timer.py:197:stop] 0/96, RunningAvgSamplesPerSec=75.42362026515067, CurrSamplesPerSec=85.70930439355426, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:52:37,692] [INFO] [logging.py:68:log_dist] [Rank 0] step=97, skipped=0, lr=[9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:37,696] [INFO] [timer.py:197:stop] 0/97, RunningAvgSamplesPerSec=74.80211025435383, CurrSamplesPerSec=42.15190929680148, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       97/     200 | consumed samples:        49664 | consumed tokens:     50855936 | elapsed time per iteration (ms): 5093.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.576572E+00 | moe loss: 8.214673E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2605.52 | backward-compute: 2306.52 | backward-embedding-all-reduce: 0.01 | optimizer: 150.59 | batch-generator: 404.63
[2022-12-22 15:52:42,525] [INFO] [logging.py:68:log_dist] [Rank 0] step=98, skipped=0, lr=[9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:42,526] [INFO] [timer.py:197:stop] 0/98, RunningAvgSamplesPerSec=74.89353869544077, CurrSamplesPerSec=84.73229079664979, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration       98/     200 | consumed samples:        50176 | consumed tokens:     51380224 | elapsed time per iteration (ms): 4807.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.567259E+00 | moe loss: 8.359990E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2435.68 | backward-compute: 2234.09 | backward-embedding-all-reduce: 0.01 | optimizer: 119.43 | batch-generator: 470.63
 iteration       99/     200 | consumed samples:        50688 | consumed tokens:     51904512 | elapsed time per iteration (ms): 4204.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.560373E+00 | moe loss: 8.422162E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2327.13 | backward-compute: 1755.21 | backward-embedding-all-reduce: 0.01 | optimizer: 93.28 | batch-generator: 420.67
[2022-12-22 15:52:46,748] [INFO] [logging.py:68:log_dist] [Rank 0] step=99, skipped=0, lr=[9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:46,749] [INFO] [timer.py:197:stop] 0/99, RunningAvgSamplesPerSec=74.99502240737247, CurrSamplesPerSec=86.20947934939068, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:52:50,822] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:50,823] [INFO] [timer.py:197:stop] 0/100, RunningAvgSamplesPerSec=75.00176483983662, CurrSamplesPerSec=75.6615938191177, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      100/     200 | consumed samples:        51200 | consumed tokens:     52428800 | elapsed time per iteration (ms): 4090.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.553061E+00 | moe loss: 8.271080E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2196.73 | backward-compute: 1767.62 | backward-embedding-all-reduce: 0.01 | optimizer: 92.50 | batch-generator: 539.70
[2022-12-22 15:52:55,176] [INFO] [logging.py:68:log_dist] [Rank 0] step=101, skipped=0, lr=[9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:55,179] [INFO] [timer.py:197:stop] 0/101, RunningAvgSamplesPerSec=75.10260320821901, CurrSamplesPerSec=86.49972319877809, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      101/     200 | consumed samples:        51712 | consumed tokens:     52953088 | elapsed time per iteration (ms): 4357.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.544106E+00 | moe loss: 8.225696E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2436.76 | backward-compute: 1808.33 | backward-embedding-all-reduce: 0.01 | optimizer: 95.62 | batch-generator: 396.72
[2022-12-22 15:52:59,343] [INFO] [logging.py:68:log_dist] [Rank 0] step=102, skipped=0, lr=[9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:52:59,343] [INFO] [timer.py:197:stop] 0/102, RunningAvgSamplesPerSec=75.17643285243658, CurrSamplesPerSec=83.28155997652043, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      102/     200 | consumed samples:        52224 | consumed tokens:     53477376 | elapsed time per iteration (ms): 4174.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.534470E+00 | moe loss: 8.299605E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2118.03 | backward-compute: 1943.85 | backward-embedding-all-reduce: 0.01 | optimizer: 103.67 | batch-generator: 457.96
 iteration      103/     200 | consumed samples:        52736 | consumed tokens:     54001664 | elapsed time per iteration (ms): 4102.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.518230E+00 | moe loss: 8.196614E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2233.03 | backward-compute: 1763.43 | backward-embedding-all-reduce: 0.01 | optimizer: 94.36 | batch-generator: 523.08
[2022-12-22 15:53:03,501] [INFO] [logging.py:68:log_dist] [Rank 0] step=103, skipped=0, lr=[9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:03,506] [INFO] [timer.py:197:stop] 0/103, RunningAvgSamplesPerSec=75.23265957923941, CurrSamplesPerSec=81.31441013642295, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:53:07,463] [INFO] [logging.py:68:log_dist] [Rank 0] step=104, skipped=0, lr=[9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:07,464] [INFO] [timer.py:197:stop] 0/104, RunningAvgSamplesPerSec=75.31671679466238, CurrSamplesPerSec=84.8970983196137, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      104/     200 | consumed samples:        53248 | consumed tokens:     54525952 | elapsed time per iteration (ms): 4008.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.520771E+00 | moe loss: 8.306227E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2053.52 | backward-compute: 1784.34 | backward-embedding-all-reduce: 0.01 | optimizer: 95.91 | batch-generator: 409.41
[2022-12-22 15:53:11,766] [INFO] [logging.py:68:log_dist] [Rank 0] step=105, skipped=0, lr=[9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:11,769] [INFO] [timer.py:197:stop] 0/105, RunningAvgSamplesPerSec=75.37481264185416, CurrSamplesPerSec=81.81159364635849, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      105/     200 | consumed samples:        53760 | consumed tokens:     55050240 | elapsed time per iteration (ms): 4318.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.517759E+00 | moe loss: 8.338105E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2342.86 | backward-compute: 1853.87 | backward-embedding-all-reduce: 0.01 | optimizer: 106.63 | batch-generator: 423.95
[2022-12-22 15:53:15,413] [INFO] [logging.py:68:log_dist] [Rank 0] step=106, skipped=0, lr=[9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:15,413] [INFO] [timer.py:197:stop] 0/106, RunningAvgSamplesPerSec=75.48481860778176, CurrSamplesPerSec=88.83949247710964, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      106/     200 | consumed samples:        54272 | consumed tokens:     55574528 | elapsed time per iteration (ms): 3630.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.504183E+00 | moe loss: 8.465350E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1794.11 | backward-compute: 1737.03 | backward-embedding-all-reduce: 0.01 | optimizer: 92.96 | batch-generator: 389.42
[2022-12-22 15:53:19,572] [INFO] [logging.py:68:log_dist] [Rank 0] step=107, skipped=0, lr=[9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:19,573] [INFO] [timer.py:197:stop] 0/107, RunningAvgSamplesPerSec=75.58270810458988, CurrSamplesPerSec=87.36554951823483, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      107/     200 | consumed samples:        54784 | consumed tokens:     56098816 | elapsed time per iteration (ms): 4164.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.512733E+00 | moe loss: 8.190714E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2007.41 | backward-compute: 2039.98 | backward-embedding-all-reduce: 0.01 | optimizer: 105.80 | batch-generator: 434.95
 iteration      108/     200 | consumed samples:        55296 | consumed tokens:     56623104 | elapsed time per iteration (ms): 4516.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.488142E+00 | moe loss: 8.341308E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2138.94 | backward-compute: 2148.43 | backward-embedding-all-reduce: 0.01 | optimizer: 210.88 | batch-generator: 379.76
[2022-12-22 15:53:24,161] [INFO] [logging.py:68:log_dist] [Rank 0] step=108, skipped=0, lr=[9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:24,173] [INFO] [timer.py:197:stop] 0/108, RunningAvgSamplesPerSec=75.30692047030819, CurrSamplesPerSec=54.44689765680065, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:53:29,008] [INFO] [logging.py:68:log_dist] [Rank 0] step=109, skipped=0, lr=[9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:29,008] [INFO] [timer.py:197:stop] 0/109, RunningAvgSamplesPerSec=75.40510132346422, CurrSamplesPerSec=87.49688504643014, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      109/     200 | consumed samples:        55808 | consumed tokens:     57147392 | elapsed time per iteration (ms): 4921.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.488282E+00 | moe loss: 8.242224E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2593.80 | backward-compute: 2133.98 | backward-embedding-all-reduce: 0.01 | optimizer: 100.16 | batch-generator: 434.82
[2022-12-22 15:53:32,737] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:32,737] [INFO] [timer.py:197:stop] 0/110, RunningAvgSamplesPerSec=75.52308750386919, CurrSamplesPerSec=90.70999445471537, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      110/     200 | consumed samples:        56320 | consumed tokens:     57671680 | elapsed time per iteration (ms): 3719.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.476068E+00 | moe loss: 8.247214E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1877.37 | backward-compute: 1737.18 | backward-embedding-all-reduce: 0.01 | optimizer: 91.51 | batch-generator: 372.62
[2022-12-22 15:53:36,518] [INFO] [logging.py:68:log_dist] [Rank 0] step=111, skipped=0, lr=[9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:36,518] [INFO] [timer.py:197:stop] 0/111, RunningAvgSamplesPerSec=75.51467100756818, CurrSamplesPerSec=74.61659972258744, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      111/     200 | consumed samples:        56832 | consumed tokens:     58195968 | elapsed time per iteration (ms): 3782.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.477128E+00 | moe loss: 8.327166E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1793.86 | backward-compute: 1847.48 | backward-embedding-all-reduce: 0.01 | optimizer: 119.95 | batch-generator: 377.78
[2022-12-22 15:53:41,182] [INFO] [logging.py:68:log_dist] [Rank 0] step=112, skipped=0, lr=[9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:41,183] [INFO] [timer.py:197:stop] 0/112, RunningAvgSamplesPerSec=75.12254491802368, CurrSamplesPerSec=47.97079529147702, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      112/     200 | consumed samples:        57344 | consumed tokens:     58720256 | elapsed time per iteration (ms): 4698.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.462247E+00 | moe loss: 8.255122E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2388.75 | backward-compute: 2139.18 | backward-embedding-all-reduce: 0.01 | optimizer: 143.68 | batch-generator: 394.10
 iteration      113/     200 | consumed samples:        57856 | consumed tokens:     59244544 | elapsed time per iteration (ms): 4685.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.458426E+00 | moe loss: 8.140462E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2745.31 | backward-compute: 1825.02 | backward-embedding-all-reduce: 0.01 | optimizer: 100.79 | batch-generator: 376.20
[2022-12-22 15:53:45,917] [INFO] [logging.py:68:log_dist] [Rank 0] step=113, skipped=0, lr=[9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:45,922] [INFO] [timer.py:197:stop] 0/113, RunningAvgSamplesPerSec=75.18899593529778, CurrSamplesPerSec=83.29367973217978, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:53:50,116] [INFO] [logging.py:68:log_dist] [Rank 0] step=114, skipped=0, lr=[9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:50,117] [INFO] [timer.py:197:stop] 0/114, RunningAvgSamplesPerSec=75.23492267093572, CurrSamplesPerSec=80.70690875942145, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      114/     200 | consumed samples:        58368 | consumed tokens:     59768832 | elapsed time per iteration (ms): 4221.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.460658E+00 | moe loss: 8.168406E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2070.80 | backward-compute: 1994.10 | backward-embedding-all-reduce: 0.01 | optimizer: 123.20 | batch-generator: 404.89
[2022-12-22 15:53:53,758] [INFO] [logging.py:68:log_dist] [Rank 0] step=115, skipped=0, lr=[9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:53,758] [INFO] [timer.py:197:stop] 0/115, RunningAvgSamplesPerSec=75.32990483483671, CurrSamplesPerSec=87.73544524269668, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      115/     200 | consumed samples:        58880 | consumed tokens:     60293120 | elapsed time per iteration (ms): 3634.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.464198E+00 | moe loss: 8.163737E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1792.90 | backward-compute: 1738.00 | backward-embedding-all-reduce: 0.01 | optimizer: 92.03 | batch-generator: 395.99
[2022-12-22 15:53:57,668] [INFO] [logging.py:68:log_dist] [Rank 0] step=116, skipped=0, lr=[9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:53:57,668] [INFO] [timer.py:197:stop] 0/116, RunningAvgSamplesPerSec=75.35697474587877, CurrSamplesPerSec=78.54648963463735, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      116/     200 | consumed samples:        59392 | consumed tokens:     60817408 | elapsed time per iteration (ms): 3924.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.439128E+00 | moe loss: 8.214694E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1976.19 | backward-compute: 1806.55 | backward-embedding-all-reduce: 0.01 | optimizer: 122.38 | batch-generator: 375.34
 iteration      117/     200 | consumed samples:        59904 | consumed tokens:     61341696 | elapsed time per iteration (ms): 4624.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.438853E+00 | moe loss: 8.146443E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2688.33 | backward-compute: 1821.86 | backward-embedding-all-reduce: 0.01 | optimizer: 98.91 | batch-generator: 405.58
[2022-12-22 15:54:02,319] [INFO] [logging.py:68:log_dist] [Rank 0] step=117, skipped=0, lr=[9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:02,320] [INFO] [timer.py:197:stop] 0/117, RunningAvgSamplesPerSec=75.40032996440114, CurrSamplesPerSec=80.69278895941395, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:54:05,946] [INFO] [logging.py:68:log_dist] [Rank 0] step=118, skipped=0, lr=[9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:05,951] [INFO] [timer.py:197:stop] 0/118, RunningAvgSamplesPerSec=75.49724986073196, CurrSamplesPerSec=88.59323222585517, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      118/     200 | consumed samples:        60416 | consumed tokens:     61865984 | elapsed time per iteration (ms): 3638.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.437580E+00 | moe loss: 8.183177E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1774.50 | backward-compute: 1731.17 | backward-embedding-all-reduce: 0.01 | optimizer: 94.56 | batch-generator: 365.73
[2022-12-22 15:54:09,857] [INFO] [logging.py:68:log_dist] [Rank 0] step=119, skipped=0, lr=[9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:09,857] [INFO] [timer.py:197:stop] 0/119, RunningAvgSamplesPerSec=75.59881530128604, CurrSamplesPerSec=89.57770757408957, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      119/     200 | consumed samples:        60928 | consumed tokens:     62390272 | elapsed time per iteration (ms): 3912.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.430669E+00 | moe loss: 8.295867E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1942.59 | backward-compute: 1853.48 | backward-embedding-all-reduce: 0.01 | optimizer: 93.68 | batch-generator: 448.52
 iteration      120/     200 | consumed samples:        61440 | consumed tokens:     62914560 | elapsed time per iteration (ms): 3816.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.424725E+00 | moe loss: 8.211540E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1899.24 | backward-compute: 1729.01 | backward-embedding-all-reduce: 0.01 | optimizer: 178.70 | batch-generator: 351.70
[2022-12-22 15:54:13,707] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:13,709] [INFO] [timer.py:197:stop] 0/120, RunningAvgSamplesPerSec=75.61226226736501, CurrSamplesPerSec=77.21928098323637, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:54:18,262] [INFO] [logging.py:68:log_dist] [Rank 0] step=121, skipped=0, lr=[9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:18,263] [INFO] [timer.py:197:stop] 0/121, RunningAvgSamplesPerSec=75.56111455914396, CurrSamplesPerSec=69.97560718109324, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      121/     200 | consumed samples:        61952 | consumed tokens:     63438848 | elapsed time per iteration (ms): 4603.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.401461E+00 | moe loss: 8.176398E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2623.01 | backward-compute: 1811.76 | backward-embedding-all-reduce: 0.01 | optimizer: 111.05 | batch-generator: 446.56
[2022-12-22 15:54:22,698] [INFO] [logging.py:68:log_dist] [Rank 0] step=122, skipped=0, lr=[9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:22,698] [INFO] [timer.py:197:stop] 0/122, RunningAvgSamplesPerSec=75.64111566053178, CurrSamplesPerSec=86.54515277360889, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      122/     200 | consumed samples:        62464 | consumed tokens:     63963136 | elapsed time per iteration (ms): 4419.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.418905E+00 | moe loss: 8.154609E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2516.91 | backward-compute: 1796.79 | backward-embedding-all-reduce: 0.01 | optimizer: 94.44 | batch-generator: 414.18
 iteration      123/     200 | consumed samples:        62976 | consumed tokens:     64487424 | elapsed time per iteration (ms): 3958.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.399663E+00 | moe loss: 8.133035E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1997.81 | backward-compute: 1814.03 | backward-embedding-all-reduce: 0.01 | optimizer: 122.65 | batch-generator: 350.52
[2022-12-22 15:54:26,664] [INFO] [logging.py:68:log_dist] [Rank 0] step=123, skipped=0, lr=[9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:26,679] [INFO] [timer.py:197:stop] 0/123, RunningAvgSamplesPerSec=75.63739856973743, CurrSamplesPerSec=75.19398437942064, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:54:30,765] [INFO] [logging.py:68:log_dist] [Rank 0] step=124, skipped=0, lr=[9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:30,765] [INFO] [timer.py:197:stop] 0/124, RunningAvgSamplesPerSec=75.60406364015343, CurrSamplesPerSec=71.77643150868623, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      124/     200 | consumed samples:        63488 | consumed tokens:     65011712 | elapsed time per iteration (ms): 4108.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.410293E+00 | moe loss: 8.208877E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2052.88 | backward-compute: 1907.14 | backward-embedding-all-reduce: 0.01 | optimizer: 120.13 | batch-generator: 350.58
 iteration      125/     200 | consumed samples:        64000 | consumed tokens:     65536000 | elapsed time per iteration (ms): 4214.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.391455E+00 | moe loss: 8.160220E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2013.82 | backward-compute: 2081.02 | backward-embedding-all-reduce: 0.01 | optimizer: 95.36 | batch-generator: 365.34
[2022-12-22 15:54:34,991] [INFO] [logging.py:68:log_dist] [Rank 0] step=125, skipped=0, lr=[9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:34,992] [INFO] [timer.py:197:stop] 0/125, RunningAvgSamplesPerSec=75.58225997184607, CurrSamplesPerSec=73.01336343422118, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:54:39,250] [INFO] [logging.py:68:log_dist] [Rank 0] step=126, skipped=0, lr=[9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:39,251] [INFO] [timer.py:197:stop] 0/126, RunningAvgSamplesPerSec=75.66396091638454, CurrSamplesPerSec=87.26671055433374, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      126/     200 | consumed samples:        64512 | consumed tokens:     66060288 | elapsed time per iteration (ms): 4283.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.385773E+00 | moe loss: 8.142096E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2256.95 | backward-compute: 1892.41 | backward-embedding-all-reduce: 0.01 | optimizer: 108.14 | batch-generator: 432.32
 iteration      127/     200 | consumed samples:        65024 | consumed tokens:     66584576 | elapsed time per iteration (ms): 4536.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.383268E+00 | moe loss: 8.070475E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2484.68 | backward-compute: 1903.93 | backward-embedding-all-reduce: 0.01 | optimizer: 131.30 | batch-generator: 345.38
[2022-12-22 15:54:43,810] [INFO] [logging.py:68:log_dist] [Rank 0] step=127, skipped=0, lr=[9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:43,828] [INFO] [timer.py:197:stop] 0/127, RunningAvgSamplesPerSec=75.66137544065943, CurrSamplesPerSec=75.3421400516323, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:54:47,610] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:47,610] [INFO] [timer.py:197:stop] 0/128, RunningAvgSamplesPerSec=75.70362496846184, CurrSamplesPerSec=81.38427647595631, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      128/     200 | consumed samples:        65536 | consumed tokens:     67108864 | elapsed time per iteration (ms): 3822.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.385484E+00 | moe loss: 8.164846E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1867.64 | backward-compute: 1795.96 | backward-embedding-all-reduce: 0.01 | optimizer: 110.93 | batch-generator: 358.87
[2022-12-22 15:54:51,467] [INFO] [logging.py:68:log_dist] [Rank 0] step=129, skipped=0, lr=[9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:51,467] [INFO] [timer.py:197:stop] 0/129, RunningAvgSamplesPerSec=75.79610153416007, CurrSamplesPerSec=89.58467303243445, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      129/     200 | consumed samples:        66048 | consumed tokens:     67633152 | elapsed time per iteration (ms): 3845.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.379757E+00 | moe loss: 8.143923E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1926.48 | backward-compute: 1820.23 | backward-embedding-all-reduce: 0.01 | optimizer: 92.51 | batch-generator: 412.99
 iteration      130/     200 | consumed samples:        66560 | consumed tokens:     68157440 | elapsed time per iteration (ms): 4576.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.387694E+00 | moe loss: 8.203816E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2622.62 | backward-compute: 1836.43 | backward-embedding-all-reduce: 0.01 | optimizer: 100.40 | batch-generator: 359.26
[2022-12-22 15:54:56,059] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:54:56,062] [INFO] [timer.py:197:stop] 0/130, RunningAvgSamplesPerSec=75.83729144141861, CurrSamplesPerSec=81.45925602685494, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:55:00,339] [INFO] [logging.py:68:log_dist] [Rank 0] step=131, skipped=0, lr=[9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:00,339] [INFO] [timer.py:197:stop] 0/131, RunningAvgSamplesPerSec=75.80358815363373, CurrSamplesPerSec=71.72357686648378, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      131/     200 | consumed samples:        67072 | consumed tokens:     68681728 | elapsed time per iteration (ms): 4307.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.374022E+00 | moe loss: 8.133923E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2091.24 | backward-compute: 1919.87 | backward-embedding-all-reduce: 0.01 | optimizer: 264.58 | batch-generator: 394.73
[2022-12-22 15:55:04,744] [INFO] [logging.py:68:log_dist] [Rank 0] step=132, skipped=0, lr=[9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:04,746] [INFO] [timer.py:197:stop] 0/132, RunningAvgSamplesPerSec=75.87353124194867, CurrSamplesPerSec=86.12467563134781, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      132/     200 | consumed samples:        67584 | consumed tokens:     69206016 | elapsed time per iteration (ms): 4392.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.358087E+00 | moe loss: 8.117615E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2380.25 | backward-compute: 1802.02 | backward-embedding-all-reduce: 0.01 | optimizer: 91.01 | batch-generator: 367.96
[2022-12-22 15:55:08,628] [INFO] [logging.py:68:log_dist] [Rank 0] step=133, skipped=0, lr=[9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:08,629] [INFO] [timer.py:197:stop] 0/133, RunningAvgSamplesPerSec=75.95761950821088, CurrSamplesPerSec=88.74330620364032, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      133/     200 | consumed samples:        68096 | consumed tokens:     69730304 | elapsed time per iteration (ms): 3889.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.358895E+00 | moe loss: 8.196704E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1989.95 | backward-compute: 1790.80 | backward-embedding-all-reduce: 0.01 | optimizer: 94.27 | batch-generator: 343.83
[2022-12-22 15:55:12,408] [INFO] [logging.py:68:log_dist] [Rank 0] step=134, skipped=0, lr=[9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:12,408] [INFO] [timer.py:197:stop] 0/134, RunningAvgSamplesPerSec=76.04206925991554, CurrSamplesPerSec=89.00533298319468, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      134/     200 | consumed samples:        68608 | consumed tokens:     70254592 | elapsed time per iteration (ms): 3775.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.357025E+00 | moe loss: 8.138368E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1831.75 | backward-compute: 1834.14 | backward-embedding-all-reduce: 0.01 | optimizer: 100.24 | batch-generator: 343.80
 iteration      135/     200 | consumed samples:        69120 | consumed tokens:     70778880 | elapsed time per iteration (ms): 3937.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.361413E+00 | moe loss: 8.027826E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2071.85 | backward-compute: 1760.98 | backward-embedding-all-reduce: 0.01 | optimizer: 92.67 | batch-generator: 358.66
[2022-12-22 15:55:16,359] [INFO] [logging.py:68:log_dist] [Rank 0] step=135, skipped=0, lr=[9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:16,360] [INFO] [timer.py:197:stop] 0/135, RunningAvgSamplesPerSec=76.10129936912935, CurrSamplesPerSec=84.82244021897984, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:55:21,289] [INFO] [logging.py:68:log_dist] [Rank 0] step=136, skipped=0, lr=[9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:21,289] [INFO] [timer.py:197:stop] 0/136, RunningAvgSamplesPerSec=75.91536037012769, CurrSamplesPerSec=57.296336400331526, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      136/     200 | consumed samples:        69632 | consumed tokens:     71303168 | elapsed time per iteration (ms): 4957.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.354027E+00 | moe loss: 8.047454E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2623.66 | backward-compute: 2190.85 | backward-embedding-all-reduce: 0.01 | optimizer: 110.45 | batch-generator: 457.72
[2022-12-22 15:55:24,915] [INFO] [logging.py:68:log_dist] [Rank 0] step=137, skipped=0, lr=[9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:24,915] [INFO] [timer.py:197:stop] 0/137, RunningAvgSamplesPerSec=76.00334503935196, CurrSamplesPerSec=89.97713863565211, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      137/     200 | consumed samples:        70144 | consumed tokens:     71827456 | elapsed time per iteration (ms): 3610.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.342223E+00 | moe loss: 8.182928E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1751.25 | backward-compute: 1756.56 | backward-embedding-all-reduce: 0.01 | optimizer: 89.72 | batch-generator: 353.95
[2022-12-22 15:55:28,637] [INFO] [logging.py:68:log_dist] [Rank 0] step=138, skipped=0, lr=[9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:28,637] [INFO] [timer.py:197:stop] 0/138, RunningAvgSamplesPerSec=76.08126110324406, CurrSamplesPerSec=88.30203729051185, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      138/     200 | consumed samples:        70656 | consumed tokens:     72351744 | elapsed time per iteration (ms): 3726.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.332427E+00 | moe loss: 8.151460E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1834.88 | backward-compute: 1783.27 | backward-embedding-all-reduce: 0.01 | optimizer: 93.64 | batch-generator: 337.60
[2022-12-22 15:55:32,352] [INFO] [logging.py:68:log_dist] [Rank 0] step=139, skipped=0, lr=[9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:32,352] [INFO] [timer.py:197:stop] 0/139, RunningAvgSamplesPerSec=76.16077249777577, CurrSamplesPerSec=88.77908371185137, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      139/     200 | consumed samples:        71168 | consumed tokens:     72876032 | elapsed time per iteration (ms): 3711.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.330568E+00 | moe loss: 8.030473E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1768.33 | backward-compute: 1834.07 | backward-embedding-all-reduce: 0.01 | optimizer: 100.27 | batch-generator: 339.36
 iteration      140/     200 | consumed samples:        71680 | consumed tokens:     73400320 | elapsed time per iteration (ms): 3797.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.330685E+00 | moe loss: 8.079486E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1935.28 | backward-compute: 1755.29 | backward-embedding-all-reduce: 0.01 | optimizer: 95.84 | batch-generator: 325.36
[2022-12-22 15:55:36,170] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:36,171] [INFO] [timer.py:197:stop] 0/140, RunningAvgSamplesPerSec=76.21634309833611, CurrSamplesPerSec=84.68123681368046, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:55:40,937] [INFO] [logging.py:68:log_dist] [Rank 0] step=141, skipped=0, lr=[9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:40,938] [INFO] [timer.py:197:stop] 0/141, RunningAvgSamplesPerSec=76.06471910201783, CurrSamplesPerSec=59.680333525423485, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      141/     200 | consumed samples:        72192 | consumed tokens:     73924608 | elapsed time per iteration (ms): 4802.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.313318E+00 | moe loss: 8.168560E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2439.41 | backward-compute: 2175.91 | backward-embedding-all-reduce: 0.01 | optimizer: 135.09 | batch-generator: 393.31
[2022-12-22 15:55:45,038] [INFO] [logging.py:68:log_dist] [Rank 0] step=142, skipped=0, lr=[9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:45,039] [INFO] [timer.py:197:stop] 0/142, RunningAvgSamplesPerSec=76.13249425618562, CurrSamplesPerSec=86.89453161164181, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      142/     200 | consumed samples:        72704 | consumed tokens:     74448896 | elapsed time per iteration (ms): 4084.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.316938E+00 | moe loss: 8.111651E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2166.13 | backward-compute: 1793.23 | backward-embedding-all-reduce: 0.01 | optimizer: 96.60 | batch-generator: 320.05
[2022-12-22 15:55:48,761] [INFO] [logging.py:68:log_dist] [Rank 0] step=143, skipped=0, lr=[9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:48,761] [INFO] [timer.py:197:stop] 0/143, RunningAvgSamplesPerSec=76.21462450567815, CurrSamplesPerSec=89.77297060122889, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      143/     200 | consumed samples:        73216 | consumed tokens:     74973184 | elapsed time per iteration (ms): 3722.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.303686E+00 | moe loss: 8.053421E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1811.11 | backward-compute: 1804.15 | backward-embedding-all-reduce: 0.01 | optimizer: 91.17 | batch-generator: 326.40
[2022-12-22 15:55:52,523] [INFO] [logging.py:68:log_dist] [Rank 0] step=144, skipped=0, lr=[9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:52,523] [INFO] [timer.py:197:stop] 0/144, RunningAvgSamplesPerSec=76.29317670663346, CurrSamplesPerSec=89.2656759618403, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      144/     200 | consumed samples:        73728 | consumed tokens:     75497472 | elapsed time per iteration (ms): 3764.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.300510E+00 | moe loss: 8.104032E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1817.23 | backward-compute: 1832.26 | backward-embedding-all-reduce: 0.01 | optimizer: 100.68 | batch-generator: 357.28
 iteration      145/     200 | consumed samples:        74240 | consumed tokens:     76021760 | elapsed time per iteration (ms): 4705.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.299809E+00 | moe loss: 8.122363E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2586.11 | backward-compute: 2003.64 | backward-embedding-all-reduce: 0.01 | optimizer: 105.04 | batch-generator: 332.96
[2022-12-22 15:55:57,245] [INFO] [logging.py:68:log_dist] [Rank 0] step=145, skipped=0, lr=[9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:55:57,247] [INFO] [timer.py:197:stop] 0/145, RunningAvgSamplesPerSec=76.19114952560874, CurrSamplesPerSec=64.03170820664053, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:56:02,150] [INFO] [logging.py:68:log_dist] [Rank 0] step=146, skipped=0, lr=[9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:02,150] [INFO] [timer.py:197:stop] 0/146, RunningAvgSamplesPerSec=76.23150170545988, CurrSamplesPerSec=82.47800011737122, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      146/     200 | consumed samples:        74752 | consumed tokens:     76546048 | elapsed time per iteration (ms): 4933.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.297200E+00 | moe loss: 8.108363E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2945.87 | backward-compute: 1844.77 | backward-embedding-all-reduce: 0.01 | optimizer: 113.27 | batch-generator: 401.01
 iteration      147/     200 | consumed samples:        75264 | consumed tokens:     77070336 | elapsed time per iteration (ms): 3840.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.310859E+00 | moe loss: 8.111438E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1934.74 | backward-compute: 1793.59 | backward-embedding-all-reduce: 0.01 | optimizer: 97.14 | batch-generator: 343.85
[2022-12-22 15:56:06,010] [INFO] [logging.py:68:log_dist] [Rank 0] step=147, skipped=0, lr=[9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:06,022] [INFO] [timer.py:197:stop] 0/147, RunningAvgSamplesPerSec=76.28994388739444, CurrSamplesPerSec=85.75722219080762, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:56:09,577] [INFO] [logging.py:68:log_dist] [Rank 0] step=148, skipped=0, lr=[9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:09,577] [INFO] [timer.py:197:stop] 0/148, RunningAvgSamplesPerSec=76.36877117276273, CurrSamplesPerSec=89.82686385364572, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      148/     200 | consumed samples:        75776 | consumed tokens:     77594624 | elapsed time per iteration (ms): 3575.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.281815E+00 | moe loss: 8.099040E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1696.84 | backward-compute: 1759.60 | backward-embedding-all-reduce: 0.01 | optimizer: 92.39 | batch-generator: 351.47
[2022-12-22 15:56:13,127] [INFO] [logging.py:68:log_dist] [Rank 0] step=149, skipped=0, lr=[9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:13,127] [INFO] [timer.py:197:stop] 0/149, RunningAvgSamplesPerSec=76.45079947624205, CurrSamplesPerSec=90.66958140100454, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      149/     200 | consumed samples:        76288 | consumed tokens:     78118912 | elapsed time per iteration (ms): 3547.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.294775E+00 | moe loss: 8.092739E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1692.06 | backward-compute: 1757.59 | backward-embedding-all-reduce: 0.01 | optimizer: 89.63 | batch-generator: 316.89
 iteration      150/     200 | consumed samples:        76800 | consumed tokens:     78643200 | elapsed time per iteration (ms): 3914.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.280588E+00 | moe loss: 8.128548E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1781.53 | backward-compute: 1961.58 | backward-embedding-all-reduce: 0.01 | optimizer: 160.23 | batch-generator: 327.36
[2022-12-22 15:56:17,066] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:17,071] [INFO] [timer.py:197:stop] 0/150, RunningAvgSamplesPerSec=76.34586745096941, CurrSamplesPerSec=63.52817767554027, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:56:21,333] [INFO] [logging.py:68:log_dist] [Rank 0] step=151, skipped=0, lr=[9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:21,333] [INFO] [timer.py:197:stop] 0/151, RunningAvgSamplesPerSec=76.36930859174343, CurrSamplesPerSec=80.00486880368284, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      151/     200 | consumed samples:        77312 | consumed tokens:     79167488 | elapsed time per iteration (ms): 4331.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.275195E+00 | moe loss: 8.126555E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2219.92 | backward-compute: 1904.64 | backward-embedding-all-reduce: 0.01 | optimizer: 147.46 | batch-generator: 372.47
 iteration      152/     200 | consumed samples:        77824 | consumed tokens:     79691776 | elapsed time per iteration (ms): 4192.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.274471E+00 | moe loss: 8.119976E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2285.63 | backward-compute: 1796.93 | backward-embedding-all-reduce: 0.01 | optimizer: 99.54 | batch-generator: 320.16
[2022-12-22 15:56:25,570] [INFO] [logging.py:68:log_dist] [Rank 0] step=152, skipped=0, lr=[9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:25,578] [INFO] [timer.py:197:stop] 0/152, RunningAvgSamplesPerSec=76.43218730098769, CurrSamplesPerSec=87.12000168764594, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:56:29,219] [INFO] [logging.py:68:log_dist] [Rank 0] step=153, skipped=0, lr=[9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:29,219] [INFO] [timer.py:197:stop] 0/153, RunningAvgSamplesPerSec=76.50629627765787, CurrSamplesPerSec=89.52718812367262, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      153/     200 | consumed samples:        78336 | consumed tokens:     80216064 | elapsed time per iteration (ms): 3653.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.277085E+00 | moe loss: 8.126335E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1719.21 | backward-compute: 1824.09 | backward-embedding-all-reduce: 0.01 | optimizer: 92.28 | batch-generator: 313.66
[2022-12-22 15:56:32,945] [INFO] [logging.py:68:log_dist] [Rank 0] step=154, skipped=0, lr=[9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:32,945] [INFO] [timer.py:197:stop] 0/154, RunningAvgSamplesPerSec=76.58004977826535, CurrSamplesPerSec=89.62672794134832, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      154/     200 | consumed samples:        78848 | consumed tokens:     80740352 | elapsed time per iteration (ms): 3725.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.265842E+00 | moe loss: 8.071264E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1799.69 | backward-compute: 1822.84 | backward-embedding-all-reduce: 0.01 | optimizer: 92.47 | batch-generator: 322.28
 iteration      155/     200 | consumed samples:        79360 | consumed tokens:     81264640 | elapsed time per iteration (ms): 4046.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.258206E+00 | moe loss: 8.105328E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1976.87 | backward-compute: 1898.67 | backward-embedding-all-reduce: 0.01 | optimizer: 156.80 | batch-generator: 316.85
[2022-12-22 15:56:37,005] [INFO] [logging.py:68:log_dist] [Rank 0] step=155, skipped=0, lr=[9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:37,009] [INFO] [timer.py:197:stop] 0/155, RunningAvgSamplesPerSec=76.50897522132739, CurrSamplesPerSec=67.05006109641845, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:56:41,911] [INFO] [logging.py:68:log_dist] [Rank 0] step=156, skipped=0, lr=[9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:41,911] [INFO] [timer.py:197:stop] 0/156, RunningAvgSamplesPerSec=76.56428227594529, CurrSamplesPerSec=86.08542818155725, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      156/     200 | consumed samples:        79872 | consumed tokens:     81788928 | elapsed time per iteration (ms): 4935.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.260845E+00 | moe loss: 8.099996E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2667.82 | backward-compute: 2118.70 | backward-embedding-all-reduce: 0.01 | optimizer: 111.72 | batch-generator: 363.33
[2022-12-22 15:56:45,445] [INFO] [logging.py:68:log_dist] [Rank 0] step=157, skipped=0, lr=[9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:45,446] [INFO] [timer.py:197:stop] 0/157, RunningAvgSamplesPerSec=76.62747690091796, CurrSamplesPerSec=87.78579954647815, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      157/     200 | consumed samples:        80384 | consumed tokens:     82313216 | elapsed time per iteration (ms): 3518.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.256193E+00 | moe loss: 8.133706E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1679.35 | backward-compute: 1734.80 | backward-embedding-all-reduce: 0.01 | optimizer: 90.37 | batch-generator: 308.33
[2022-12-22 15:56:49,069] [INFO] [logging.py:68:log_dist] [Rank 0] step=158, skipped=0, lr=[9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:49,069] [INFO] [timer.py:197:stop] 0/158, RunningAvgSamplesPerSec=76.70111306693126, CurrSamplesPerSec=90.12519368146026, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      158/     200 | consumed samples:        80896 | consumed tokens:     82837504 | elapsed time per iteration (ms): 3624.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.261733E+00 | moe loss: 8.130795E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1729.15 | backward-compute: 1789.84 | backward-embedding-all-reduce: 0.01 | optimizer: 93.64 | batch-generator: 295.41
[2022-12-22 15:56:52,769] [INFO] [logging.py:68:log_dist] [Rank 0] step=159, skipped=0, lr=[9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:52,769] [INFO] [timer.py:197:stop] 0/159, RunningAvgSamplesPerSec=76.77075005071691, CurrSamplesPerSec=89.43808912974306, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      159/     200 | consumed samples:        81408 | consumed tokens:     83361792 | elapsed time per iteration (ms): 3701.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.261197E+00 | moe loss: 8.129349E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1850.65 | backward-compute: 1742.97 | backward-embedding-all-reduce: 0.01 | optimizer: 93.94 | batch-generator: 297.50
[2022-12-22 15:56:57,989] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:56:58,004] [INFO] [timer.py:197:stop] 0/160, RunningAvgSamplesPerSec=76.43608407273916, CurrSamplesPerSec=45.37858917261787, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      160/     200 | consumed samples:        81920 | consumed tokens:     83886080 | elapsed time per iteration (ms): 5241.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.262357E+00 | moe loss: 8.125523E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2467.65 | backward-compute: 2573.95 | backward-embedding-all-reduce: 0.01 | optimizer: 165.56 | batch-generator: 303.93
[2022-12-22 15:57:02,261] [INFO] [logging.py:68:log_dist] [Rank 0] step=161, skipped=0, lr=[9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:02,262] [INFO] [timer.py:197:stop] 0/161, RunningAvgSamplesPerSec=76.4981236583128, CurrSamplesPerSec=87.75147775022376, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      161/     200 | consumed samples:        82432 | consumed tokens:     84410368 | elapsed time per iteration (ms): 4253.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.255485E+00 | moe loss: 8.090980E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2327.62 | backward-compute: 1815.67 | backward-embedding-all-reduce: 0.01 | optimizer: 93.07 | batch-generator: 405.76
[2022-12-22 15:57:06,005] [INFO] [logging.py:68:log_dist] [Rank 0] step=162, skipped=0, lr=[9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:06,006] [INFO] [timer.py:197:stop] 0/162, RunningAvgSamplesPerSec=76.40517214546355, CurrSamplesPerSec=64.03393827552473, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      162/     200 | consumed samples:        82944 | consumed tokens:     84934656 | elapsed time per iteration (ms): 3743.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.255174E+00 | moe loss: 8.078179E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1880.93 | backward-compute: 1748.22 | backward-embedding-all-reduce: 0.01 | optimizer: 94.59 | batch-generator: 304.44
[2022-12-22 15:57:09,606] [INFO] [logging.py:68:log_dist] [Rank 0] step=163, skipped=0, lr=[9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:09,606] [INFO] [timer.py:197:stop] 0/163, RunningAvgSamplesPerSec=76.4732279513615, CurrSamplesPerSec=89.18322938857091, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      163/     200 | consumed samples:        83456 | consumed tokens:     85458944 | elapsed time per iteration (ms): 3601.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.239072E+00 | moe loss: 8.062126E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1699.03 | backward-compute: 1800.22 | backward-embedding-all-reduce: 0.01 | optimizer: 89.63 | batch-generator: 307.08
[2022-12-22 15:57:13,302] [INFO] [logging.py:68:log_dist] [Rank 0] step=164, skipped=0, lr=[9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:13,303] [INFO] [timer.py:197:stop] 0/164, RunningAvgSamplesPerSec=76.53763816863264, CurrSamplesPerSec=88.5446053803864, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      164/     200 | consumed samples:        83968 | consumed tokens:     85983232 | elapsed time per iteration (ms): 3693.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.242799E+00 | moe loss: 8.055855E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1721.33 | backward-compute: 1871.97 | backward-embedding-all-reduce: 0.01 | optimizer: 92.76 | batch-generator: 293.24
 iteration      165/     200 | consumed samples:        84480 | consumed tokens:     86507520 | elapsed time per iteration (ms): 3783.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.235533E+00 | moe loss: 8.092380E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1758.52 | backward-compute: 1888.75 | backward-embedding-all-reduce: 0.01 | optimizer: 125.77 | batch-generator: 297.94
[2022-12-22 15:57:17,103] [INFO] [logging.py:68:log_dist] [Rank 0] step=165, skipped=0, lr=[9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:17,103] [INFO] [timer.py:197:stop] 0/165, RunningAvgSamplesPerSec=76.46295356004462, CurrSamplesPerSec=66.02574467153579, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:57:21,999] [INFO] [logging.py:68:log_dist] [Rank 0] step=166, skipped=0, lr=[9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:21,999] [INFO] [timer.py:197:stop] 0/166, RunningAvgSamplesPerSec=76.48913534827652, CurrSamplesPerSec=81.01058400493726, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      166/     200 | consumed samples:        84992 | consumed tokens:     87031808 | elapsed time per iteration (ms): 4931.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.235065E+00 | moe loss: 8.132265E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2601.69 | backward-compute: 2164.74 | backward-embedding-all-reduce: 0.01 | optimizer: 120.58 | batch-generator: 421.76
[2022-12-22 15:57:26,023] [INFO] [logging.py:68:log_dist] [Rank 0] step=167, skipped=0, lr=[9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:26,025] [INFO] [timer.py:197:stop] 0/167, RunningAvgSamplesPerSec=76.54199079610036, CurrSamplesPerSec=86.32494863475702, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      167/     200 | consumed samples:        85504 | consumed tokens:     87556096 | elapsed time per iteration (ms): 4005.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.229564E+00 | moe loss: 8.121339E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1833.88 | backward-compute: 2058.14 | backward-embedding-all-reduce: 0.01 | optimizer: 97.40 | batch-generator: 300.39
[2022-12-22 15:57:29,583] [INFO] [logging.py:68:log_dist] [Rank 0] step=168, skipped=0, lr=[9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:29,583] [INFO] [timer.py:197:stop] 0/168, RunningAvgSamplesPerSec=76.6040526117111, CurrSamplesPerSec=88.43541904418024, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      168/     200 | consumed samples:        86016 | consumed tokens:     88080384 | elapsed time per iteration (ms): 3558.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.224275E+00 | moe loss: 8.100996E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1709.97 | backward-compute: 1735.91 | backward-embedding-all-reduce: 0.01 | optimizer: 97.29 | batch-generator: 308.12
[2022-12-22 15:57:33,376] [INFO] [logging.py:68:log_dist] [Rank 0] step=169, skipped=0, lr=[9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:33,376] [INFO] [timer.py:197:stop] 0/169, RunningAvgSamplesPerSec=76.66315314697079, CurrSamplesPerSec=87.92352842528929, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      169/     200 | consumed samples:        86528 | consumed tokens:     88604672 | elapsed time per iteration (ms): 3792.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.209671E+00 | moe loss: 8.145104E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1787.32 | backward-compute: 1893.19 | backward-embedding-all-reduce: 0.01 | optimizer: 97.87 | batch-generator: 307.96
 iteration      170/     200 | consumed samples:        87040 | consumed tokens:     89128960 | elapsed time per iteration (ms): 3835.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.206281E+00 | moe loss: 8.154951E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1749.92 | backward-compute: 1905.55 | backward-embedding-all-reduce: 0.01 | optimizer: 163.07 | batch-generator: 288.86
[2022-12-22 15:57:37,227] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:37,227] [INFO] [timer.py:197:stop] 0/170, RunningAvgSamplesPerSec=76.58832420291432, CurrSamplesPerSec=65.85385417407751, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:57:41,999] [INFO] [logging.py:68:log_dist] [Rank 0] step=171, skipped=0, lr=[9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:41,999] [INFO] [timer.py:197:stop] 0/171, RunningAvgSamplesPerSec=76.63193729282894, CurrSamplesPerSec=84.73865681083804, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      171/     200 | consumed samples:        87552 | consumed tokens:     89653248 | elapsed time per iteration (ms): 4804.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.205437E+00 | moe loss: 8.088149E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2598.50 | backward-compute: 2039.10 | backward-embedding-all-reduce: 0.01 | optimizer: 118.47 | batch-generator: 339.77
 iteration      172/     200 | consumed samples:        88064 | consumed tokens:     90177536 | elapsed time per iteration (ms): 4178.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.218292E+00 | moe loss: 8.059967E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1810.62 | backward-compute: 2241.09 | backward-embedding-all-reduce: 0.01 | optimizer: 113.25 | batch-generator: 283.72
[2022-12-22 15:57:46,218] [INFO] [logging.py:68:log_dist] [Rank 0] step=172, skipped=0, lr=[9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:46,233] [INFO] [timer.py:197:stop] 0/172, RunningAvgSamplesPerSec=76.66319249371134, CurrSamplesPerSec=82.33867891481894, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:57:49,776] [INFO] [logging.py:68:log_dist] [Rank 0] step=173, skipped=0, lr=[9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:49,777] [INFO] [timer.py:197:stop] 0/173, RunningAvgSamplesPerSec=76.73147592523324, CurrSamplesPerSec=90.42316629264806, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      173/     200 | consumed samples:        88576 | consumed tokens:     90701824 | elapsed time per iteration (ms): 3583.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.207722E+00 | moe loss: 8.097971E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1700.81 | backward-compute: 1747.10 | backward-embedding-all-reduce: 0.01 | optimizer: 89.91 | batch-generator: 287.30
[2022-12-22 15:57:53,422] [INFO] [logging.py:68:log_dist] [Rank 0] step=174, skipped=0, lr=[9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:53,422] [INFO] [timer.py:197:stop] 0/174, RunningAvgSamplesPerSec=76.79512523278953, CurrSamplesPerSec=89.48871687662957, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      174/     200 | consumed samples:        89088 | consumed tokens:     91226112 | elapsed time per iteration (ms): 3643.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.194947E+00 | moe loss: 8.040601E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1704.36 | backward-compute: 1838.89 | backward-embedding-all-reduce: 0.01 | optimizer: 91.53 | batch-generator: 315.72
 iteration      175/     200 | consumed samples:        89600 | consumed tokens:     91750400 | elapsed time per iteration (ms): 3678.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.201690E+00 | moe loss: 8.105188E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1755.51 | backward-compute: 1792.26 | backward-embedding-all-reduce: 0.01 | optimizer: 117.86 | batch-generator: 301.76
[2022-12-22 15:57:57,112] [INFO] [logging.py:68:log_dist] [Rank 0] step=175, skipped=0, lr=[9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:57:57,113] [INFO] [timer.py:197:stop] 0/175, RunningAvgSamplesPerSec=76.80374603373002, CurrSamplesPerSec=78.31588701011991, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:58:01,504] [INFO] [logging.py:68:log_dist] [Rank 0] step=176, skipped=0, lr=[9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:01,505] [INFO] [timer.py:197:stop] 0/176, RunningAvgSamplesPerSec=76.63504969707758, CurrSamplesPerSec=55.53314626127584, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      176/     200 | consumed samples:        90112 | consumed tokens:     92274688 | elapsed time per iteration (ms): 4429.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.202857E+00 | moe loss: 8.064075E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2270.62 | backward-compute: 1988.16 | backward-embedding-all-reduce: 0.01 | optimizer: 124.81 | batch-generator: 365.63
[2022-12-22 15:58:05,454] [INFO] [logging.py:68:log_dist] [Rank 0] step=177, skipped=0, lr=[9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:05,455] [INFO] [timer.py:197:stop] 0/177, RunningAvgSamplesPerSec=76.69616863621948, CurrSamplesPerSec=89.05429006499043, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      177/     200 | consumed samples:        90624 | consumed tokens:     92798976 | elapsed time per iteration (ms): 3924.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.198105E+00 | moe loss: 8.099921E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1869.11 | backward-compute: 1950.43 | backward-embedding-all-reduce: 0.01 | optimizer: 96.66 | batch-generator: 309.69
[2022-12-22 15:58:09,340] [INFO] [logging.py:68:log_dist] [Rank 0] step=178, skipped=0, lr=[9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:09,341] [INFO] [timer.py:197:stop] 0/178, RunningAvgSamplesPerSec=76.75881171873004, CurrSamplesPerSec=89.56004468068922, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      178/     200 | consumed samples:        91136 | consumed tokens:     93323264 | elapsed time per iteration (ms): 3888.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.189641E+00 | moe loss: 8.135413E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1937.59 | backward-compute: 1840.54 | backward-embedding-all-reduce: 0.01 | optimizer: 96.84 | batch-generator: 297.87
[2022-12-22 15:58:13,293] [INFO] [logging.py:68:log_dist] [Rank 0] step=179, skipped=0, lr=[9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:13,293] [INFO] [timer.py:197:stop] 0/179, RunningAvgSamplesPerSec=76.81944153475445, CurrSamplesPerSec=89.22303977047048, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      179/     200 | consumed samples:        91648 | consumed tokens:     93847552 | elapsed time per iteration (ms): 3949.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.194868E+00 | moe loss: 8.138708E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1828.40 | backward-compute: 2016.25 | backward-embedding-all-reduce: 0.01 | optimizer: 93.99 | batch-generator: 298.51
 iteration      180/     200 | consumed samples:        92160 | consumed tokens:     94371840 | elapsed time per iteration (ms): 3777.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.172023E+00 | moe loss: 8.145919E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1665.44 | backward-compute: 1955.21 | backward-embedding-all-reduce: 0.01 | optimizer: 132.99 | batch-generator: 295.74
[2022-12-22 15:58:17,078] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:17,079] [INFO] [timer.py:197:stop] 0/180, RunningAvgSamplesPerSec=76.77344107801696, CurrSamplesPerSec=69.41604509067889, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:58:21,505] [INFO] [logging.py:68:log_dist] [Rank 0] step=181, skipped=0, lr=[9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:21,505] [INFO] [timer.py:197:stop] 0/181, RunningAvgSamplesPerSec=76.61265557843323, CurrSamplesPerSec=55.80828602002055, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      181/     200 | consumed samples:        92672 | consumed tokens:     94896128 | elapsed time per iteration (ms): 4444.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.179806E+00 | moe loss: 8.118121E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2366.17 | backward-compute: 1931.06 | backward-embedding-all-reduce: 0.01 | optimizer: 114.18 | batch-generator: 378.26
 iteration      182/     200 | consumed samples:        93184 | consumed tokens:     95420416 | elapsed time per iteration (ms): 3981.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.186835E+00 | moe loss: 8.160854E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2124.30 | backward-compute: 1736.08 | backward-embedding-all-reduce: 0.01 | optimizer: 103.07 | batch-generator: 324.71
[2022-12-22 15:58:25,501] [INFO] [logging.py:68:log_dist] [Rank 0] step=182, skipped=0, lr=[9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:25,515] [INFO] [timer.py:197:stop] 0/182, RunningAvgSamplesPerSec=76.65578048859413, CurrSamplesPerSec=85.24491051260547, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:58:29,378] [INFO] [logging.py:68:log_dist] [Rank 0] step=183, skipped=0, lr=[9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:29,379] [INFO] [timer.py:197:stop] 0/183, RunningAvgSamplesPerSec=76.71499546796888, CurrSamplesPerSec=89.1046615764662, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      183/     200 | consumed samples:        93696 | consumed tokens:     95944704 | elapsed time per iteration (ms): 3881.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.192585E+00 | moe loss: 8.067604E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1986.76 | backward-compute: 1772.71 | backward-embedding-all-reduce: 0.01 | optimizer: 96.66 | batch-generator: 326.64
[2022-12-22 15:58:33,222] [INFO] [logging.py:68:log_dist] [Rank 0] step=184, skipped=0, lr=[9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      184/     200 | consumed samples:        94208 | consumed tokens:     96468992 | elapsed time per iteration (ms): 3843.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.175939E+00 | moe loss: 8.156516E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1807.80 | backward-compute: 1923.91 | backward-embedding-all-reduce: 0.01 | optimizer: 100.89 | batch-generator: 274.66
[2022-12-22 15:58:33,223] [INFO] [timer.py:197:stop] 0/184, RunningAvgSamplesPerSec=76.77237807532282, CurrSamplesPerSec=88.79397259857095, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      185/     200 | consumed samples:        94720 | consumed tokens:     96993280 | elapsed time per iteration (ms): 3822.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.176223E+00 | moe loss: 8.124065E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1755.26 | backward-compute: 1925.33 | backward-embedding-all-reduce: 0.01 | optimizer: 127.64 | batch-generator: 282.93
[2022-12-22 15:58:37,056] [INFO] [logging.py:68:log_dist] [Rank 0] step=185, skipped=0, lr=[9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:37,058] [INFO] [timer.py:197:stop] 0/185, RunningAvgSamplesPerSec=76.72821693090692, CurrSamplesPerSec=69.45676305408968, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:58:41,945] [INFO] [logging.py:68:log_dist] [Rank 0] step=186, skipped=0, lr=[9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:41,946] [INFO] [timer.py:197:stop] 0/186, RunningAvgSamplesPerSec=76.65514307105235, CurrSamplesPerSec=65.27818622593337, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      186/     200 | consumed samples:        95232 | consumed tokens:     97517568 | elapsed time per iteration (ms): 4937.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.170460E+00 | moe loss: 8.063406E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2585.88 | backward-compute: 2165.81 | backward-embedding-all-reduce: 0.01 | optimizer: 131.89 | batch-generator: 516.99
 iteration      187/     200 | consumed samples:        95744 | consumed tokens:     98041856 | elapsed time per iteration (ms): 4218.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.172190E+00 | moe loss: 8.125374E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2284.96 | backward-compute: 1808.72 | backward-embedding-all-reduce: 0.01 | optimizer: 116.37 | batch-generator: 326.32
[2022-12-22 15:58:46,206] [INFO] [logging.py:68:log_dist] [Rank 0] step=187, skipped=0, lr=[9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:46,216] [INFO] [timer.py:197:stop] 0/187, RunningAvgSamplesPerSec=76.69407758725954, CurrSamplesPerSec=84.60058481412077, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:58:50,006] [INFO] [logging.py:68:log_dist] [Rank 0] step=188, skipped=0, lr=[9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:50,006] [INFO] [timer.py:197:stop] 0/188, RunningAvgSamplesPerSec=76.75430470572668, CurrSamplesPerSec=89.80038979729336, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      188/     200 | consumed samples:        96256 | consumed tokens:     98566144 | elapsed time per iteration (ms): 3811.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.165181E+00 | moe loss: 8.129371E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1805.40 | backward-compute: 1880.87 | backward-embedding-all-reduce: 0.01 | optimizer: 94.85 | batch-generator: 316.76
[2022-12-22 15:58:53,907] [INFO] [logging.py:68:log_dist] [Rank 0] step=189, skipped=0, lr=[9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:53,907] [INFO] [timer.py:197:stop] 0/189, RunningAvgSamplesPerSec=76.81146430925622, CurrSamplesPerSec=89.16178268124953, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      189/     200 | consumed samples:        96768 | consumed tokens:     99090432 | elapsed time per iteration (ms): 3894.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.172660E+00 | moe loss: 8.133979E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2009.08 | backward-compute: 1780.72 | backward-embedding-all-reduce: 0.01 | optimizer: 92.82 | batch-generator: 280.08
[2022-12-22 15:58:58,005] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:58:58,005] [INFO] [timer.py:197:stop] 0/190, RunningAvgSamplesPerSec=76.71822052475989, CurrSamplesPerSec=62.52478148914637, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      190/     200 | consumed samples:        97280 | consumed tokens:     99614720 | elapsed time per iteration (ms): 4146.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.165637E+00 | moe loss: 8.100715E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1933.54 | backward-compute: 1912.86 | backward-embedding-all-reduce: 0.01 | optimizer: 270.00 | batch-generator: 276.97
[2022-12-22 15:59:02,184] [INFO] [logging.py:68:log_dist] [Rank 0] step=191, skipped=0, lr=[9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:02,184] [INFO] [timer.py:197:stop] 0/191, RunningAvgSamplesPerSec=76.7659105529114, CurrSamplesPerSec=86.92438609665531, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      191/     200 | consumed samples:        97792 | consumed tokens:    100139008 | elapsed time per iteration (ms): 4133.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.161208E+00 | moe loss: 8.108779E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2038.72 | backward-compute: 1986.19 | backward-embedding-all-reduce: 0.01 | optimizer: 95.10 | batch-generator: 370.27
 iteration      192/     200 | consumed samples:        98304 | consumed tokens:    100663296 | elapsed time per iteration (ms): 4441.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.153559E+00 | moe loss: 8.092365E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2107.79 | backward-compute: 2199.15 | backward-embedding-all-reduce: 0.01 | optimizer: 108.70 | batch-generator: 259.74
[2022-12-22 15:59:06,634] [INFO] [logging.py:68:log_dist] [Rank 0] step=192, skipped=0, lr=[9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:06,646] [INFO] [timer.py:197:stop] 0/192, RunningAvgSamplesPerSec=76.73454832370007, CurrSamplesPerSec=71.23421550545994, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:59:11,014] [INFO] [logging.py:68:log_dist] [Rank 0] step=193, skipped=0, lr=[9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:11,014] [INFO] [timer.py:197:stop] 0/193, RunningAvgSamplesPerSec=76.696852816668, CurrSamplesPerSec=70.14934715368204, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      193/     200 | consumed samples:        98816 | consumed tokens:    101187584 | elapsed time per iteration (ms): 4390.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.145269E+00 | moe loss: 8.093958E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2274.94 | backward-compute: 1935.19 | backward-embedding-all-reduce: 0.01 | optimizer: 150.99 | batch-generator: 279.80
 iteration      194/     200 | consumed samples:        99328 | consumed tokens:    101711872 | elapsed time per iteration (ms): 3751.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.137647E+00 | moe loss: 8.106197E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1829.59 | backward-compute: 1782.61 | backward-embedding-all-reduce: 0.01 | optimizer: 93.03 | batch-generator: 265.60
[2022-12-22 15:59:14,784] [INFO] [logging.py:68:log_dist] [Rank 0] step=194, skipped=0, lr=[9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:14,784] [INFO] [timer.py:197:stop] 0/194, RunningAvgSamplesPerSec=76.71663649234291, CurrSamplesPerSec=80.69215829325562, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:59:19,114] [INFO] [logging.py:68:log_dist] [Rank 0] step=195, skipped=0, lr=[9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:19,114] [INFO] [timer.py:197:stop] 0/195, RunningAvgSamplesPerSec=76.49039770436751, CurrSamplesPerSec=48.837844839882514, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      195/     200 | consumed samples:        99840 | consumed tokens:    102236160 | elapsed time per iteration (ms): 4363.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.145151E+00 | moe loss: 8.095457E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2395.26 | backward-compute: 1818.03 | backward-embedding-all-reduce: 0.01 | optimizer: 116.91 | batch-generator: 273.30
[2022-12-22 15:59:22,924] [INFO] [logging.py:68:log_dist] [Rank 0] step=196, skipped=0, lr=[9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:22,924] [INFO] [timer.py:197:stop] 0/196, RunningAvgSamplesPerSec=76.53764295425897, CurrSamplesPerSec=86.8964725145275, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      196/     200 | consumed samples:       100352 | consumed tokens:    102760448 | elapsed time per iteration (ms): 3793.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.133281E+00 | moe loss: 8.094523E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1756.67 | backward-compute: 1929.63 | backward-embedding-all-reduce: 0.01 | optimizer: 93.51 | batch-generator: 328.32
 iteration      197/     200 | consumed samples:       100864 | consumed tokens:    103284736 | elapsed time per iteration (ms): 3830.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.155793E+00 | moe loss: 8.100311E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1733.52 | backward-compute: 1972.22 | backward-embedding-all-reduce: 0.01 | optimizer: 96.36 | batch-generator: 258.23
[2022-12-22 15:59:26,771] [INFO] [logging.py:68:log_dist] [Rank 0] step=197, skipped=0, lr=[9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:26,774] [INFO] [timer.py:197:stop] 0/197, RunningAvgSamplesPerSec=76.56633796759542, CurrSamplesPerSec=82.572074453932, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:59:30,442] [INFO] [logging.py:68:log_dist] [Rank 0] step=198, skipped=0, lr=[9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:30,442] [INFO] [timer.py:197:stop] 0/198, RunningAvgSamplesPerSec=76.61200490510444, CurrSamplesPerSec=86.69508193121054, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      198/     200 | consumed samples:       101376 | consumed tokens:    103809024 | elapsed time per iteration (ms): 3686.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.138684E+00 | moe loss: 8.105025E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1814.64 | backward-compute: 1749.33 | backward-embedding-all-reduce: 0.01 | optimizer: 96.28 | batch-generator: 272.31
 iteration      199/     200 | consumed samples:       101888 | consumed tokens:    104333312 | elapsed time per iteration (ms): 3634.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.143381E+00 | moe loss: 8.135433E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1778.90 | backward-compute: 1743.11 | backward-embedding-all-reduce: 0.01 | optimizer: 102.43 | batch-generator: 268.24
[2022-12-22 15:59:34,163] [INFO] [logging.py:68:log_dist] [Rank 0] step=199, skipped=0, lr=[9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:34,165] [INFO] [timer.py:197:stop] 0/199, RunningAvgSamplesPerSec=76.62522323171403, CurrSamplesPerSec=79.30715735280258, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
[2022-12-22 15:59:38,192] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 15:59:38,193] [INFO] [timer.py:197:stop] 0/200, RunningAvgSamplesPerSec=76.47744133182948, CurrSamplesPerSec=55.42082073505472, MemAllocated=18.14GB, MaxMemAllocated=27.97GB
 iteration      200/     200 | consumed samples:       102400 | consumed tokens:    104857600 | elapsed time per iteration (ms): 4159.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.133363E+00 | moe loss: 8.166053E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1725.22 | backward-compute: 2138.06 | backward-embedding-all-reduce: 0.01 | optimizer: 157.54 | batch-generator: 251.32
[after training is done] datetime: 2022-12-22 15:59:38 
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.46266722679138184 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.459758996963501 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.43089747428894043 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.428600549697876 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.46415138244628906 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4288022518157959 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.42903828620910645 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4039032459259033 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40422797203063965 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4321577548980713 seconds
Loading extension module utils...
Time to load utils op: 0.43969273567199707 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40398263931274414 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40366578102111816 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4338972568511963 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3755819797515869 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3740510940551758 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.38058996200561523 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3737480640411377 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3799283504486084 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3824295997619629 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.37377095222473145 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.37529897689819336 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4731287956237793 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4527449607849121 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4531104564666748 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4527595043182373 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.45239925384521484 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.45273780822753906 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.45868778228759766 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4526071548461914 seconds
