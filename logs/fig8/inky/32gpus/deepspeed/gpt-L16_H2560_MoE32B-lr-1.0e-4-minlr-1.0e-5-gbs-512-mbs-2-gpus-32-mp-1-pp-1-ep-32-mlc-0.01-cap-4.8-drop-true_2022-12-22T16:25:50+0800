--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:24,917] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:24,917] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:24,917] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:24,917] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:24,917] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:24,917] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:24,918] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:24,918] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,427] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,427] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,427] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,427] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,427] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,427] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,427] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[2022-12-22 16:26:25,432] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,642] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,642] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,642] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,642] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,642] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,643] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,643] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:25,643] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
using world size: 32, data-parallel-size: 32, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 32
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /mnt/cache/zhaishuming/Megatron-DeepSpeed/examples/MoE/ds_config_gpt_gpt-L16_H2560_MoE32B-lr-1.0e-4-minlr-1.0e-5-gbs-512-mbs-2-gpus-32-mp-1-pp-1-ep-32-mlc-0.01-cap-4.8-drop-true.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2560
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 32
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 4.8
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [32]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /mnt/cache/zhaishuming/Auto-Megatron/deepspeed/output/tensorboard/gpt-L16_H2560_MoE32B-lr-1.0e-4-minlr-1.0e-5-gbs-512-mbs-2-gpus-32-mp-1-pp-1-ep-32-mlc-0.01-cap-4.8-drop-true_SH-IDC1-10-140-0-31_2022.12.22-16.25.50
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... 200
  train_samples ................................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 32
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:32,488] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:32,488] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:32,488] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:32,489] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:32,489] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-22 16:26:32,489] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:32,490] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-22 16:26:32,490] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-22 16:26:33,893] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,880] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=20, local_rank=4, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,893] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,893] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,893] [INFO] [comm.py:654:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2022-12-22 16:26:33,893] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,880] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=23, local_rank=7, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,880] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=16, local_rank=0, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,883] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=28, local_rank=4, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,891] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,893] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,883] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=25, local_rank=1, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,893] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,883] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=26, local_rank=2, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,891] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,883] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=27, local_rank=3, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,883] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=29, local_rank=5, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,893] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,883] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=24, local_rank=0, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,880] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=17, local_rank=1, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,891] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,880] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=18, local_rank=2, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,880] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=19, local_rank=3, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,880] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=21, local_rank=5, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,883] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=31, local_rank=7, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,880] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=22, local_rank=6, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,891] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=4, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,883] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=30, local_rank=6, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,891] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=6, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,894] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,891] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=7, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,891] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=5, world_size=32, master_addr=10.140.1.11, master_port=29500
[2022-12-22 16:26:33,891] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=32, master_addr=10.140.1.11, master_port=29500
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.234 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.308 seconds
time to initialize megatron (seconds): 67.788
[after megatron is initialized] datetime: 2022-12-22 16:26:43 
building GPT model ...
[2022-12-22 16:26:43,926] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-12-22 16:26:43,930] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-12-22 16:26:43,930] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 202.29 GB, percent = 20.1%
[2022-12-22 16:26:43,978] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:26:43,989] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:26:44,003] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:26:44,018] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:26:44,032] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:26:44,040] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:26:44,048] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:26:44,056] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 1 | expert_parallel_size: 32
[2022-12-22 16:26:44,090] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-12-22 16:26:44,091] [INFO] [utils.py:828:see_memory_usage] MA 2.59 GB         Max_MA 2.69 GB         CA 2.7 GB         Max_CA 3 GB 
[2022-12-22 16:26:44,092] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 202.33 GB, percent = 20.1%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1390883840
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-12-22 16:26:44,096] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_32
[2022-12-22 16:26:44,393] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 32
[2022-12-22 16:26:44,458] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [0]
[2022-12-22 16:26:44,468] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [1]
[2022-12-22 16:26:44,478] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [2]
[2022-12-22 16:26:44,488] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [3]
[2022-12-22 16:26:44,489] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [4]
[2022-12-22 16:26:44,499] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [5]
[2022-12-22 16:26:44,509] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [6]
[2022-12-22 16:26:44,519] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [7]
[2022-12-22 16:26:44,529] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [8]
[2022-12-22 16:26:44,540] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [9]
[2022-12-22 16:26:44,550] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [10]
[2022-12-22 16:26:44,560] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [11]
[2022-12-22 16:26:44,570] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [12]
[2022-12-22 16:26:44,581] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [13]
[2022-12-22 16:26:44,591] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [14]
[2022-12-22 16:26:44,601] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [15]
[2022-12-22 16:26:44,611] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [16]
[2022-12-22 16:26:44,621] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [17]
[2022-12-22 16:26:44,632] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [18]
[2022-12-22 16:26:44,642] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [19]
[2022-12-22 16:26:44,652] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [20]
[2022-12-22 16:26:44,662] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [21]
[2022-12-22 16:26:44,673] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [22]
[2022-12-22 16:26:44,683] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [23]
[2022-12-22 16:26:44,693] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [24]
[2022-12-22 16:26:44,703] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [25]
[2022-12-22 16:26:44,713] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [26]
[2022-12-22 16:26:44,724] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [27]
[2022-12-22 16:26:44,734] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [28]
[2022-12-22 16:26:44,744] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [29]
[2022-12-22 16:26:44,754] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [30]
[2022-12-22 16:26:44,764] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_32 with ranks: [31]
[2022-12-22 16:26:44,775] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_32 with ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[2022-12-22 16:26:47,294] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-12-22 16:26:47,294] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-12-22 16:26:47,294] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-12-22 16:26:47,300] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2022-12-22 16:26:47,300] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2022-12-22 16:26:47,584] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2022-12-22 16:26:47,585] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-12-22 16:26:47,585] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f1570db2470>
[2022-12-22 16:26:47,585] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:26:47,585] [INFO] [config.py:1020:print] DeepSpeedEngine configuration:
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   amp_enabled .................. False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   amp_params ................... False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   bfloat16_enabled ............. False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   checkpoint_parallel_write_pipeline  False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   checkpoint_tag_validation_enabled  True
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   checkpoint_tag_validation_fail  False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1570db3a00>
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   communication_data_type ...... None
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   curriculum_enabled ........... False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 212296, 'difficulty_step': 8}}
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   dataloader_drop_last ......... False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   disable_allgather ............ False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   dump_state ................... False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   eigenvalue_enabled ........... False
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   eigenvalue_gas_boundary_resolution  1
[2022-12-22 16:26:47,586] [INFO] [config.py:1024:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   eigenvalue_layer_num ......... 0
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   eigenvalue_max_iter .......... 100
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   eigenvalue_stability ......... 1e-06
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   eigenvalue_tol ............... 0.01
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   eigenvalue_verbose ........... False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   elasticity_enabled ........... False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   fp16_auto_cast ............... False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   fp16_enabled ................. True
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   fp16_master_weights_and_gradients  False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   global_rank .................. 0
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   grad_accum_dtype ............. None
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   gradient_accumulation_steps .. 8
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   gradient_clipping ............ 1.0
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   gradient_predivide_factor .... 1.0
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   initial_dynamic_scale ........ 2048
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   load_universal_checkpoint .... False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   loss_scale ................... 0
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   memory_breakdown ............. False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f1569ba1c30>
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   optimizer_legacy_fusion ...... False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   optimizer_name ............... None
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   optimizer_params ............. None
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   pld_enabled .................. False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   pld_params ................... False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   prescale_gradients ........... True
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   scheduler_name ............... None
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   scheduler_params ............. None
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   sparse_attention ............. None
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   sparse_gradients_enabled ..... False
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   steps_per_print .............. 1
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   train_batch_size ............. 512
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   train_micro_batch_size_per_gpu  2
[2022-12-22 16:26:47,587] [INFO] [config.py:1024:print]   use_node_local_storage ....... False
[2022-12-22 16:26:47,588] [INFO] [config.py:1024:print]   wall_clock_breakdown ......... False
[2022-12-22 16:26:47,588] [INFO] [config.py:1024:print]   world_size ................... 32
[2022-12-22 16:26:47,588] [INFO] [config.py:1024:print]   zero_allow_untested_optimizer  False
[2022-12-22 16:26:47,598] [INFO] [config.py:1024:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-12-22 16:26:47,598] [INFO] [config.py:1024:print]   zero_enabled ................. False
[2022-12-22 16:26:47,598] [INFO] [config.py:1024:print]   zero_optimization_stage ...... 0
[2022-12-22 16:26:47,598] [INFO] [config.py:1009:print_user_config]   json = {
    "train_batch_size": 512, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 2.122960e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.35807132720947266 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-22 16:26:48 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: 51200000
    test:       51200000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.037866 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.033 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24034667015075684 seconds
time (ms) | model-and-optimizer-setup: 4160.74 | train/valid/test-data-iterators-setup: 2537.54
[after dataloaders are built] datetime: 2022-12-22 16:26:50 
done with setup ...
training ...
[before the start of training step] datetime: 2022-12-22 16:26:50 
 iteration        1/     200 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 15627.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.115610E+01 | moe loss: 1.262557E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 10738.66 | backward-compute: 4454.89 | backward-embedding-all-reduce: 0.01 | optimizer: 425.13 | batch-generator: 1007.48
[2022-12-22 16:27:06,212] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18573.685546875 | max allocated: 31076.18896484375 | reserved: 35172.0 | max reserved: 35172.0
[2022-12-22 16:27:15,209] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        2/     200 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 9023.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.041300E+01 | moe loss: 1.916315E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4684.35 | backward-compute: 4199.58 | backward-embedding-all-reduce: 0.01 | optimizer: 96.95 | batch-generator: 998.92
[2022-12-22 16:27:25,157] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:27:25,161] [INFO] [timer.py:197:stop] 0/3, RunningAvgSamplesPerSec=44.02059905040234, CurrSamplesPerSec=44.02059905040234, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration        3/     200 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 9944.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.260628E+01 | moe loss: 2.042389E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5207.56 | backward-compute: 4615.21 | backward-embedding-all-reduce: 0.01 | optimizer: 112.73 | batch-generator: 1023.29
[2022-12-22 16:27:34,473] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:27:34,474] [INFO] [timer.py:197:stop] 0/4, RunningAvgSamplesPerSec=47.18282132350195, CurrSamplesPerSec=50.83452215611409, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration        4/     200 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 9324.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.042931E+01 | moe loss: 1.836618E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4746.47 | backward-compute: 4457.30 | backward-embedding-all-reduce: 0.01 | optimizer: 96.39 | batch-generator: 1014.45
 iteration        5/     200 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 9613.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.001966E+01 | moe loss: 1.620923E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[2022-12-22 16:27:44,103] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
time (ms) | forward-compute: 4952.21 | backward-compute: 4528.90 | backward-embedding-all-reduce: 0.01 | optimizer: 112.91 | batch-generator: 1019.28
[2022-12-22 16:27:44,107] [INFO] [timer.py:197:stop] 0/5, RunningAvgSamplesPerSec=47.560454866412464, CurrSamplesPerSec=48.33415158029459, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:27:53,794] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:27:53,795] [INFO] [timer.py:197:stop] 0/6, RunningAvgSamplesPerSec=48.083153522987125, CurrSamplesPerSec=49.72253452147826, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration        6/     200 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 9702.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.610435E+00 | moe loss: 1.464586E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5530.22 | backward-compute: 4046.50 | backward-embedding-all-reduce: 0.01 | optimizer: 97.77 | batch-generator: 987.44
[2022-12-22 16:28:03,156] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:28:03,159] [INFO] [timer.py:197:stop] 0/7, RunningAvgSamplesPerSec=48.11717414976317, CurrSamplesPerSec=48.253739440495934, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration        7/     200 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 9358.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.181341E+00 | moe loss: 1.383658E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4989.09 | backward-compute: 4227.08 | backward-embedding-all-reduce: 0.01 | optimizer: 128.74 | batch-generator: 984.30
[2022-12-22 16:28:13,019] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:28:13,019] [INFO] [timer.py:197:stop] 0/8, RunningAvgSamplesPerSec=47.23905332465599, CurrSamplesPerSec=43.289010317195256, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration        8/     200 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 9865.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.736387E+00 | moe loss: 1.355633E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5667.16 | backward-compute: 4061.25 | backward-embedding-all-reduce: 0.01 | optimizer: 116.70 | batch-generator: 988.22
 iteration        9/     200 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 9452.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.326957E+00 | moe loss: 1.309434E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4971.09 | backward-compute: 4341.96 | backward-embedding-all-reduce: 0.01 | optimizer: 128.16 | batch-generator: 963.67
[2022-12-22 16:28:22,494] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:28:22,508] [INFO] [timer.py:197:stop] 0/9, RunningAvgSamplesPerSec=47.13091556516131, CurrSamplesPerSec=46.49234501769732, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:28:32,660] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:28:32,661] [INFO] [timer.py:197:stop] 0/10, RunningAvgSamplesPerSec=47.4051241411136, CurrSamplesPerSec=49.417716746490406, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       10/     200 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 10192.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.100228E+00 | moe loss: 1.289828E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5204.05 | backward-compute: 4835.60 | backward-embedding-all-reduce: 0.01 | optimizer: 102.85 | batch-generator: 969.18
[2022-12-22 16:28:41,932] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:28:41,934] [INFO] [timer.py:197:stop] 0/11, RunningAvgSamplesPerSec=47.48011961853242, CurrSamplesPerSec=48.08873528641443, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       11/     200 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 9268.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.373970E+00 | moe loss: 1.292600E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4913.26 | backward-compute: 4226.70 | backward-embedding-all-reduce: 0.01 | optimizer: 113.20 | batch-generator: 940.79
[2022-12-22 16:28:52,065] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:28:52,065] [INFO] [timer.py:197:stop] 0/12, RunningAvgSamplesPerSec=47.037200407221064, CurrSamplesPerSec=43.393985715500435, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       12/     200 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 10134.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.474257E+00 | moe loss: 1.310616E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5560.32 | backward-compute: 4446.85 | backward-embedding-all-reduce: 0.01 | optimizer: 110.55 | batch-generator: 910.35
 iteration       13/     200 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 8971.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.998334E+00 | moe loss: 1.341450E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4786.08 | backward-compute: 4050.04 | backward-embedding-all-reduce: 0.01 | optimizer: 119.83 | batch-generator: 898.95
[2022-12-22 16:29:01,053] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:29:01,068] [INFO] [timer.py:197:stop] 0/13, RunningAvgSamplesPerSec=46.18948398860557, CurrSamplesPerSec=39.13624934720786, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:29:11,022] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:29:11,022] [INFO] [timer.py:197:stop] 0/14, RunningAvgSamplesPerSec=42.518882414823224, CurrSamplesPerSec=22.68700113572151, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       14/     200 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 9993.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.730844E+00 | moe loss: 1.366783E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5781.17 | backward-compute: 4054.82 | backward-embedding-all-reduce: 0.01 | optimizer: 111.35 | batch-generator: 885.60
 iteration       15/     200 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 8996.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.654725E+00 | moe loss: 1.330944E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4635.37 | backward-compute: 4152.60 | backward-embedding-all-reduce: 0.01 | optimizer: 192.99 | batch-generator: 861.78
[2022-12-22 16:29:20,043] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:29:20,064] [INFO] [timer.py:197:stop] 0/15, RunningAvgSamplesPerSec=41.84937586564071, CurrSamplesPerSec=35.19850580836316, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:29:29,213] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:29:29,214] [INFO] [timer.py:197:stop] 0/16, RunningAvgSamplesPerSec=41.501847110608495, CurrSamplesPerSec=37.458042277622596, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       16/     200 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 9224.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.608960E+00 | moe loss: 1.323234E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4602.06 | backward-compute: 4376.34 | backward-embedding-all-reduce: 0.01 | optimizer: 177.51 | batch-generator: 893.89
[2022-12-22 16:29:38,597] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:29:38,598] [INFO] [timer.py:197:stop] 0/17, RunningAvgSamplesPerSec=41.97991410705378, CurrSamplesPerSec=50.051668351927134, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       17/     200 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 9345.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.565856E+00 | moe loss: 1.272342E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5136.66 | backward-compute: 4093.14 | backward-embedding-all-reduce: 0.01 | optimizer: 97.30 | batch-generator: 895.42
[2022-12-22 16:29:47,683] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:29:47,688] [INFO] [timer.py:197:stop] 0/18, RunningAvgSamplesPerSec=42.33484652402242, CurrSamplesPerSec=48.48365121090168, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       18/     200 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 9084.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.509483E+00 | moe loss: 1.286520E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4735.27 | backward-compute: 4211.61 | backward-embedding-all-reduce: 0.01 | optimizer: 117.02 | batch-generator: 886.17
[2022-12-22 16:29:57,468] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:29:57,469] [INFO] [timer.py:197:stop] 0/19, RunningAvgSamplesPerSec=42.68459858972731, CurrSamplesPerSec=49.18628990498625, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       19/     200 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 9788.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.464299E+00 | moe loss: 1.321327E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4962.05 | backward-compute: 4702.88 | backward-embedding-all-reduce: 0.01 | optimizer: 104.47 | batch-generator: 882.24
 iteration       20/     200 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 9275.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.423217E+00 | moe loss: 1.279564E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4878.87 | backward-compute: 4258.25 | backward-embedding-all-reduce: 0.01 | optimizer: 123.44 | batch-generator: 839.36
[2022-12-22 16:30:06,755] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:30:06,760] [INFO] [timer.py:197:stop] 0/20, RunningAvgSamplesPerSec=42.947655374589736, CurrSamplesPerSec=47.97375303035583, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:30:16,336] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:30:16,336] [INFO] [timer.py:197:stop] 0/21, RunningAvgSamplesPerSec=43.28431190318698, CurrSamplesPerSec=50.394921974221255, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       21/     200 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 9594.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.414582E+00 | moe loss: 1.289234E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4734.40 | backward-compute: 4728.08 | backward-embedding-all-reduce: 0.01 | optimizer: 103.63 | batch-generator: 812.59
 iteration       22/     200 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 9283.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.398920E+00 | moe loss: 1.266659E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4848.03 | backward-compute: 4290.31 | backward-embedding-all-reduce: 0.01 | optimizer: 133.35 | batch-generator: 817.50
[2022-12-22 16:30:25,627] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:30:25,641] [INFO] [timer.py:197:stop] 0/22, RunningAvgSamplesPerSec=43.49471250956911, CurrSamplesPerSec=47.92050958979821, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:30:34,700] [INFO] [logging.py:68:log_dist] [Rank 0] step=23, skipped=0, lr=[9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:30:34,701] [INFO] [timer.py:197:stop] 0/23, RunningAvgSamplesPerSec=43.77761251210202, CurrSamplesPerSec=50.32399875218967, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       23/     200 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 9093.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.376513E+00 | moe loss: 1.226585E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4448.74 | backward-compute: 4510.20 | backward-embedding-all-reduce: 0.01 | optimizer: 103.08 | batch-generator: 825.14
 iteration       24/     200 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 9458.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.372606E+00 | moe loss: 1.228743E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4830.59 | backward-compute: 4488.17 | backward-embedding-all-reduce: 0.01 | optimizer: 121.30 | batch-generator: 925.64
[2022-12-22 16:30:44,170] [INFO] [logging.py:68:log_dist] [Rank 0] step=24, skipped=0, lr=[9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:30:44,184] [INFO] [timer.py:197:stop] 0/24, RunningAvgSamplesPerSec=43.94697950449449, CurrSamplesPerSec=47.83318047388153, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:30:53,294] [INFO] [logging.py:68:log_dist] [Rank 0] step=25, skipped=0, lr=[9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:30:53,295] [INFO] [timer.py:197:stop] 0/25, RunningAvgSamplesPerSec=44.182894540110716, CurrSamplesPerSec=50.09965528375168, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       25/     200 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 9137.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.373190E+00 | moe loss: 1.243947E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4880.48 | backward-compute: 4115.85 | backward-embedding-all-reduce: 0.01 | optimizer: 109.05 | batch-generator: 818.04
 iteration       26/     200 | consumed samples:        13312 | consumed tokens:     13631488 | elapsed time per iteration (ms): 9425.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.357773E+00 | moe loss: 1.254702E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4867.55 | backward-compute: 4412.72 | backward-embedding-all-reduce: 0.01 | optimizer: 124.10 | batch-generator: 853.96
[2022-12-22 16:31:02,720] [INFO] [logging.py:68:log_dist] [Rank 0] step=26, skipped=0, lr=[9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:31:02,731] [INFO] [timer.py:197:stop] 0/26, RunningAvgSamplesPerSec=44.3420005437675, CurrSamplesPerSec=48.346269174985515, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:31:11,854] [INFO] [logging.py:68:log_dist] [Rank 0] step=27, skipped=0, lr=[9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:31:11,855] [INFO] [timer.py:197:stop] 0/27, RunningAvgSamplesPerSec=44.44163422635136, CurrSamplesPerSec=46.97482139151002, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       27/     200 | consumed samples:        13824 | consumed tokens:     14155776 | elapsed time per iteration (ms): 9133.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.350197E+00 | moe loss: 1.220233E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4797.35 | backward-compute: 4222.65 | backward-embedding-all-reduce: 0.01 | optimizer: 98.50 | batch-generator: 764.91
 iteration       28/     200 | consumed samples:        14336 | consumed tokens:     14680064 | elapsed time per iteration (ms): 9182.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.345651E+00 | moe loss: 1.198286E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4470.88 | backward-compute: 4550.22 | backward-embedding-all-reduce: 0.01 | optimizer: 142.17 | batch-generator: 901.40
[2022-12-22 16:31:21,053] [INFO] [logging.py:68:log_dist] [Rank 0] step=28, skipped=0, lr=[9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:31:21,068] [INFO] [timer.py:197:stop] 0/28, RunningAvgSamplesPerSec=43.58157250523364, CurrSamplesPerSec=29.371291844459506, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:31:30,020] [INFO] [logging.py:68:log_dist] [Rank 0] step=29, skipped=0, lr=[9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:31:30,032] [INFO] [timer.py:197:stop] 0/29, RunningAvgSamplesPerSec=43.168021288360215, CurrSamplesPerSec=34.62534635284217, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       29/     200 | consumed samples:        14848 | consumed tokens:     15204352 | elapsed time per iteration (ms): 8979.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.332870E+00 | moe loss: 1.173621E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4273.53 | backward-compute: 4536.27 | backward-embedding-all-reduce: 0.01 | optimizer: 129.23 | batch-generator: 735.72
[2022-12-22 16:31:38,524] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:31:38,524] [INFO] [timer.py:197:stop] 0/30, RunningAvgSamplesPerSec=43.39932969502764, CurrSamplesPerSec=50.740153807040876, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       30/     200 | consumed samples:        15360 | consumed tokens:     15728640 | elapsed time per iteration (ms): 8505.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.322596E+00 | moe loss: 1.198188E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4372.80 | backward-compute: 4004.74 | backward-embedding-all-reduce: 0.01 | optimizer: 90.72 | batch-generator: 786.33
 iteration       31/     200 | consumed samples:        15872 | consumed tokens:     16252928 | elapsed time per iteration (ms): 9267.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.324184E+00 | moe loss: 1.190092E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4795.44 | backward-compute: 4341.08 | backward-embedding-all-reduce: 0.01 | optimizer: 115.33 | batch-generator: 781.12
[2022-12-22 16:31:47,820] [INFO] [logging.py:68:log_dist] [Rank 0] step=31, skipped=0, lr=[9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:31:47,831] [INFO] [timer.py:197:stop] 0/31, RunningAvgSamplesPerSec=43.51739282356151, CurrSamplesPerSec=47.1054598264909, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:31:56,942] [INFO] [logging.py:68:log_dist] [Rank 0] step=32, skipped=0, lr=[9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:31:56,944] [INFO] [timer.py:197:stop] 0/32, RunningAvgSamplesPerSec=43.71808819233991, CurrSamplesPerSec=50.46782912842441, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       32/     200 | consumed samples:        16384 | consumed tokens:     16777216 | elapsed time per iteration (ms): 9151.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.291905E+00 | moe loss: 1.152458E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4516.34 | backward-compute: 4487.45 | backward-embedding-all-reduce: 0.01 | optimizer: 101.47 | batch-generator: 730.80
 iteration       33/     200 | consumed samples:        16896 | consumed tokens:     17301504 | elapsed time per iteration (ms): 9167.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.284331E+00 | moe loss: 1.117333E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4414.75 | backward-compute: 4623.57 | backward-embedding-all-reduce: 0.01 | optimizer: 117.13 | batch-generator: 765.48
[2022-12-22 16:32:06,117] [INFO] [logging.py:68:log_dist] [Rank 0] step=33, skipped=0, lr=[9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:32:06,129] [INFO] [timer.py:197:stop] 0/33, RunningAvgSamplesPerSec=43.83619420522801, CurrSamplesPerSec=47.702277589519255, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:32:15,431] [INFO] [logging.py:68:log_dist] [Rank 0] step=34, skipped=0, lr=[9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:32:15,432] [INFO] [timer.py:197:stop] 0/34, RunningAvgSamplesPerSec=43.98620569796219, CurrSamplesPerSec=49.20624279486805, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       34/     200 | consumed samples:        17408 | consumed tokens:     17825792 | elapsed time per iteration (ms): 9322.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.271104E+00 | moe loss: 1.101837E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4362.34 | backward-compute: 4822.89 | backward-embedding-all-reduce: 0.01 | optimizer: 106.12 | batch-generator: 851.79
 iteration       35/     200 | consumed samples:        17920 | consumed tokens:     18350080 | elapsed time per iteration (ms): 8701.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.259644E+00 | moe loss: 1.124543E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4561.44 | backward-compute: 4010.36 | backward-embedding-all-reduce: 0.01 | optimizer: 114.57 | batch-generator: 737.09
[2022-12-22 16:32:24,138] [INFO] [logging.py:68:log_dist] [Rank 0] step=35, skipped=0, lr=[9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:32:24,150] [INFO] [timer.py:197:stop] 0/35, RunningAvgSamplesPerSec=44.10338105656194, CurrSamplesPerSec=48.21333486599397, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:32:33,951] [INFO] [logging.py:68:log_dist] [Rank 0] step=36, skipped=0, lr=[9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:32:33,954] [INFO] [timer.py:197:stop] 0/36, RunningAvgSamplesPerSec=44.26572332768796, CurrSamplesPerSec=50.38620050946409, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       36/     200 | consumed samples:        18432 | consumed tokens:     18874368 | elapsed time per iteration (ms): 9819.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.252585E+00 | moe loss: 1.128293E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4509.76 | backward-compute: 5174.53 | backward-embedding-all-reduce: 0.01 | optimizer: 106.38 | batch-generator: 754.44
[2022-12-22 16:32:42,611] [INFO] [logging.py:68:log_dist] [Rank 0] step=37, skipped=0, lr=[9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       37/     200 | consumed samples:        18944 | consumed tokens:     19398656 | elapsed time per iteration (ms): 8652.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.245469E+00 | moe loss: 1.077667E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4301.86 | backward-compute: 4206.33 | backward-embedding-all-reduce: 0.01 | optimizer: 130.82 | batch-generator: 700.09
[2022-12-22 16:32:42,615] [INFO] [timer.py:197:stop] 0/37, RunningAvgSamplesPerSec=44.35752043372734, CurrSamplesPerSec=47.722342430847746, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:32:52,442] [INFO] [logging.py:68:log_dist] [Rank 0] step=38, skipped=0, lr=[9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:32:52,445] [INFO] [timer.py:197:stop] 0/38, RunningAvgSamplesPerSec=44.30726671345525, CurrSamplesPerSec=42.61738689904363, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       38/     200 | consumed samples:        19456 | consumed tokens:     19922944 | elapsed time per iteration (ms): 9835.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.229300E+00 | moe loss: 1.071128E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5206.78 | backward-compute: 4500.69 | backward-embedding-all-reduce: 0.01 | optimizer: 105.45 | batch-generator: 664.53
 iteration       39/     200 | consumed samples:        19968 | consumed tokens:     20447232 | elapsed time per iteration (ms): 8579.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.231521E+00 | moe loss: 1.049887E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4440.53 | backward-compute: 4002.08 | backward-embedding-all-reduce: 0.01 | optimizer: 124.63 | batch-generator: 701.61
[2022-12-22 16:33:01,036] [INFO] [logging.py:68:log_dist] [Rank 0] step=39, skipped=0, lr=[9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:33:01,036] [INFO] [timer.py:197:stop] 0/39, RunningAvgSamplesPerSec=44.37549989228384, CurrSamplesPerSec=46.98007478366031, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       40/     200 | consumed samples:        20480 | consumed tokens:     20971520 | elapsed time per iteration (ms): 8966.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.214764E+00 | moe loss: 1.015447E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4068.27 | backward-compute: 4685.10 | backward-embedding-all-reduce: 0.01 | optimizer: 169.88 | batch-generator: 647.66
[2022-12-22 16:33:10,144] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:33:10,148] [INFO] [timer.py:197:stop] 0/40, RunningAvgSamplesPerSec=43.75440170541732, CurrSamplesPerSec=28.82623106863968, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       41/     200 | consumed samples:        20992 | consumed tokens:     21495808 | elapsed time per iteration (ms): 8894.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.202850E+00 | moe loss: 1.001010E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4519.46 | backward-compute: 4047.33 | backward-embedding-all-reduce: 0.01 | optimizer: 116.68 | batch-generator: 674.66
[2022-12-22 16:33:18,904] [INFO] [logging.py:68:log_dist] [Rank 0] step=41, skipped=0, lr=[9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:33:18,904] [INFO] [timer.py:197:stop] 0/41, RunningAvgSamplesPerSec=43.8847753340614, CurrSamplesPerSec=49.488195932622354, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:33:27,356] [INFO] [logging.py:68:log_dist] [Rank 0] step=42, skipped=0, lr=[9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:33:27,362] [INFO] [timer.py:197:stop] 0/42, RunningAvgSamplesPerSec=43.987012157635725, CurrSamplesPerSec=48.38293749818633, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       42/     200 | consumed samples:        21504 | consumed tokens:     22020096 | elapsed time per iteration (ms): 8483.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.201342E+00 | moe loss: 1.004707E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4313.55 | backward-compute: 4023.54 | backward-embedding-all-reduce: 0.01 | optimizer: 120.49 | batch-generator: 676.13
[2022-12-22 16:33:37,482] [INFO] [logging.py:68:log_dist] [Rank 0] step=43, skipped=0, lr=[9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       43/     200 | consumed samples:        22016 | consumed tokens:     22544384 | elapsed time per iteration (ms): 10103.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.188833E+00 | moe loss: 9.985890E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5394.08 | backward-compute: 4581.92 | backward-embedding-all-reduce: 0.01 | optimizer: 104.49 | batch-generator: 650.41
[2022-12-22 16:33:37,486] [INFO] [timer.py:197:stop] 0/43, RunningAvgSamplesPerSec=44.10759041526995, CurrSamplesPerSec=49.539550833064354, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:33:46,057] [INFO] [logging.py:68:log_dist] [Rank 0] step=44, skipped=0, lr=[9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       44/     200 | consumed samples:        22528 | consumed tokens:     23068672 | elapsed time per iteration (ms): 8590.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.175431E+00 | moe loss: 9.661806E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4433.77 | backward-compute: 4023.77 | backward-embedding-all-reduce: 0.01 | optimizer: 112.70 | batch-generator: 658.15
[2022-12-22 16:33:46,070] [INFO] [timer.py:197:stop] 0/44, RunningAvgSamplesPerSec=44.19849720495778, CurrSamplesPerSec=48.278091116278, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:33:55,951] [INFO] [logging.py:68:log_dist] [Rank 0] step=45, skipped=0, lr=[9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       45/     200 | consumed samples:        23040 | consumed tokens:     23592960 | elapsed time per iteration (ms): 9879.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.166182E+00 | moe loss: 9.794176E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4978.15 | backward-compute: 4777.57 | backward-embedding-all-reduce: 0.01 | optimizer: 105.17 | batch-generator: 670.37
[2022-12-22 16:33:55,954] [INFO] [timer.py:197:stop] 0/45, RunningAvgSamplesPerSec=44.30629987933913, CurrSamplesPerSec=49.36307353777073, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:34:04,714] [INFO] [logging.py:68:log_dist] [Rank 0] step=46, skipped=0, lr=[9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:34:04,715] [INFO] [timer.py:197:stop] 0/46, RunningAvgSamplesPerSec=44.426681441908265, CurrSamplesPerSec=50.3037891327809, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       46/     200 | consumed samples:        23552 | consumed tokens:     24117248 | elapsed time per iteration (ms): 8776.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.143950E+00 | moe loss: 9.860746E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4546.87 | backward-compute: 4097.85 | backward-embedding-all-reduce: 0.01 | optimizer: 103.04 | batch-generator: 647.25
 iteration       47/     200 | consumed samples:        24064 | consumed tokens:     24641536 | elapsed time per iteration (ms): 9396.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.119967E+00 | moe loss: 9.621523E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4979.86 | backward-compute: 4293.03 | backward-embedding-all-reduce: 0.01 | optimizer: 107.13 | batch-generator: 688.39
[2022-12-22 16:34:14,122] [INFO] [logging.py:68:log_dist] [Rank 0] step=47, skipped=0, lr=[9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:34:14,125] [INFO] [timer.py:197:stop] 0/47, RunningAvgSamplesPerSec=44.528247216425974, CurrSamplesPerSec=49.50830325695418, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:34:23,336] [INFO] [logging.py:68:log_dist] [Rank 0] step=48, skipped=0, lr=[9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:34:23,337] [INFO] [timer.py:197:stop] 0/48, RunningAvgSamplesPerSec=44.60360439854878, CurrSamplesPerSec=48.280426905192755, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       48/     200 | consumed samples:        24576 | consumed tokens:     25165824 | elapsed time per iteration (ms): 9230.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.116197E+00 | moe loss: 9.469120E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4761.44 | backward-compute: 4342.45 | backward-embedding-all-reduce: 0.01 | optimizer: 104.33 | batch-generator: 722.75
[2022-12-22 16:34:33,033] [INFO] [logging.py:68:log_dist] [Rank 0] step=49, skipped=0, lr=[9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       49/     200 | consumed samples:        25088 | consumed tokens:     25690112 | elapsed time per iteration (ms): 9679.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.102251E+00 | moe loss: 9.342936E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[2022-12-22 16:34:33,036] [INFO] [timer.py:197:stop] 0/49, RunningAvgSamplesPerSec=44.69941707114199, CurrSamplesPerSec=49.60056053795088, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
time (ms) | forward-compute: 4793.74 | backward-compute: 4764.27 | backward-embedding-all-reduce: 0.01 | optimizer: 103.72 | batch-generator: 657.19
[2022-12-22 16:34:41,753] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:34:41,757] [INFO] [timer.py:197:stop] 0/50, RunningAvgSamplesPerSec=44.775521566604326, CurrSamplesPerSec=48.67017889367569, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       50/     200 | consumed samples:        25600 | consumed tokens:     26214400 | elapsed time per iteration (ms): 8733.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.092131E+00 | moe loss: 9.380841E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4392.09 | backward-compute: 4203.39 | backward-embedding-all-reduce: 0.01 | optimizer: 113.08 | batch-generator: 638.77
[2022-12-22 16:34:50,719] [INFO] [logging.py:68:log_dist] [Rank 0] step=51, skipped=0, lr=[9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:34:50,720] [INFO] [timer.py:197:stop] 0/51, RunningAvgSamplesPerSec=44.34481021061397, CurrSamplesPerSec=30.337236344452577, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       51/     200 | consumed samples:        26112 | consumed tokens:     26738688 | elapsed time per iteration (ms): 8951.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.083984E+00 | moe loss: 9.272169E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4551.80 | backward-compute: 4266.57 | backward-embedding-all-reduce: 0.01 | optimizer: 113.85 | batch-generator: 590.09
[2022-12-22 16:34:58,947] [INFO] [logging.py:68:log_dist] [Rank 0] step=52, skipped=0, lr=[9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:34:58,947] [INFO] [timer.py:197:stop] 0/52, RunningAvgSamplesPerSec=44.46131852149429, CurrSamplesPerSec=51.03099913825274, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       52/     200 | consumed samples:        26624 | consumed tokens:     27262976 | elapsed time per iteration (ms): 8235.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.071353E+00 | moe loss: 9.320012E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4139.13 | backward-compute: 3984.88 | backward-embedding-all-reduce: 0.01 | optimizer: 91.50 | batch-generator: 584.19
[2022-12-22 16:35:07,782] [INFO] [logging.py:68:log_dist] [Rank 0] step=53, skipped=0, lr=[9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:35:07,792] [INFO] [timer.py:197:stop] 0/53, RunningAvgSamplesPerSec=44.52380405555991, CurrSamplesPerSec=47.888938028231834, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       53/     200 | consumed samples:        27136 | consumed tokens:     27787264 | elapsed time per iteration (ms): 8845.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.061090E+00 | moe loss: 8.967955E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4451.31 | backward-compute: 4264.63 | backward-embedding-all-reduce: 0.01 | optimizer: 114.05 | batch-generator: 616.19
 iteration       54/     200 | consumed samples:        27648 | consumed tokens:     28311552 | elapsed time per iteration (ms): 9259.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.054588E+00 | moe loss: 9.047423E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4877.28 | backward-compute: 4272.06 | backward-embedding-all-reduce: 0.01 | optimizer: 101.60 | batch-generator: 607.47
[2022-12-22 16:35:17,062] [INFO] [logging.py:68:log_dist] [Rank 0] step=54, skipped=0, lr=[9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:35:17,065] [INFO] [timer.py:197:stop] 0/54, RunningAvgSamplesPerSec=44.60429180994578, CurrSamplesPerSec=49.13422543004615, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:35:26,031] [INFO] [logging.py:68:log_dist] [Rank 0] step=55, skipped=0, lr=[9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:35:26,044] [INFO] [timer.py:197:stop] 0/55, RunningAvgSamplesPerSec=44.67568546304368, CurrSamplesPerSec=48.731683282992385, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       55/     200 | consumed samples:        28160 | consumed tokens:     28835840 | elapsed time per iteration (ms): 8986.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.033490E+00 | moe loss: 8.899651E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4487.35 | backward-compute: 4360.97 | backward-embedding-all-reduce: 0.01 | optimizer: 115.31 | batch-generator: 610.02
 iteration       56/     200 | consumed samples:        28672 | consumed tokens:     29360128 | elapsed time per iteration (ms): 9370.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.017842E+00 | moe loss: 8.947244E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5194.79 | backward-compute: 4043.50 | backward-embedding-all-reduce: 0.01 | optimizer: 100.56 | batch-generator: 594.38
[2022-12-22 16:35:35,421] [INFO] [logging.py:68:log_dist] [Rank 0] step=56, skipped=0, lr=[9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:35:35,423] [INFO] [timer.py:197:stop] 0/56, RunningAvgSamplesPerSec=44.7641248883824, CurrSamplesPerSec=50.01120743973085, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:35:43,979] [INFO] [logging.py:68:log_dist] [Rank 0] step=57, skipped=0, lr=[9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:35:43,992] [INFO] [timer.py:197:stop] 0/57, RunningAvgSamplesPerSec=44.81903086356243, CurrSamplesPerSec=47.998157571943025, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       57/     200 | consumed samples:        29184 | consumed tokens:     29884416 | elapsed time per iteration (ms): 8578.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.005598E+00 | moe loss: 8.897827E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4120.05 | backward-compute: 4323.12 | backward-embedding-all-reduce: 0.01 | optimizer: 110.93 | batch-generator: 626.05
[2022-12-22 16:35:53,680] [INFO] [logging.py:68:log_dist] [Rank 0] step=58, skipped=0, lr=[9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:35:53,680] [INFO] [timer.py:197:stop] 0/58, RunningAvgSamplesPerSec=44.902968646054674, CurrSamplesPerSec=50.05932215874877, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       58/     200 | consumed samples:        29696 | consumed tokens:     30408704 | elapsed time per iteration (ms): 9689.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.988799E+00 | moe loss: 8.696882E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5527.16 | backward-compute: 4035.92 | backward-embedding-all-reduce: 0.01 | optimizer: 106.44 | batch-generator: 650.57
[2022-12-22 16:36:02,151] [INFO] [logging.py:68:log_dist] [Rank 0] step=59, skipped=0, lr=[9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:36:02,152] [INFO] [timer.py:197:stop] 0/59, RunningAvgSamplesPerSec=44.97068508642333, CurrSamplesPerSec=49.118842383010495, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       59/     200 | consumed samples:        30208 | consumed tokens:     30932992 | elapsed time per iteration (ms): 8483.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.981049E+00 | moe loss: 8.604974E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4076.53 | backward-compute: 4289.14 | backward-embedding-all-reduce: 0.01 | optimizer: 106.40 | batch-generator: 576.94
[2022-12-22 16:36:11,539] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       60/     200 | consumed samples:        30720 | consumed tokens:     31457280 | elapsed time per iteration (ms): 9372.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.964532E+00 | moe loss: 8.500722E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4762.00 | backward-compute: 4478.45 | backward-embedding-all-reduce: 0.01 | optimizer: 114.40 | batch-generator: 556.38
[2022-12-22 16:36:11,540] [INFO] [timer.py:197:stop] 0/60, RunningAvgSamplesPerSec=44.333154538132796, CurrSamplesPerSec=24.519667172127708, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:36:19,962] [INFO] [logging.py:68:log_dist] [Rank 0] step=61, skipped=0, lr=[9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:36:19,963] [INFO] [timer.py:197:stop] 0/61, RunningAvgSamplesPerSec=44.31294128107412, CurrSamplesPerSec=43.17129713095145, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       61/     200 | consumed samples:        31232 | consumed tokens:     31981568 | elapsed time per iteration (ms): 8444.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.950108E+00 | moe loss: 8.810189E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4120.42 | backward-compute: 4198.67 | backward-embedding-all-reduce: 0.01 | optimizer: 107.08 | batch-generator: 556.41
 iteration       62/     200 | consumed samples:        31744 | consumed tokens:     32505856 | elapsed time per iteration (ms): 8315.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.928881E+00 | moe loss: 8.595639E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4165.24 | backward-compute: 4038.37 | backward-embedding-all-reduce: 0.01 | optimizer: 98.32 | batch-generator: 565.71
[2022-12-22 16:36:28,300] [INFO] [logging.py:68:log_dist] [Rank 0] step=62, skipped=0, lr=[9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:36:28,305] [INFO] [timer.py:197:stop] 0/62, RunningAvgSamplesPerSec=44.36767399589539, CurrSamplesPerSec=47.85502768860618, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:36:37,578] [INFO] [logging.py:68:log_dist] [Rank 0] step=63, skipped=0, lr=[9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:36:37,581] [INFO] [timer.py:197:stop] 0/63, RunningAvgSamplesPerSec=44.44408271318091, CurrSamplesPerSec=49.565721300178666, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       63/     200 | consumed samples:        32256 | consumed tokens:     33030144 | elapsed time per iteration (ms): 9273.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.933743E+00 | moe loss: 8.408318E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4332.05 | backward-compute: 4808.08 | backward-embedding-all-reduce: 0.01 | optimizer: 111.83 | batch-generator: 558.42
[2022-12-22 16:36:46,531] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:36:46,532] [INFO] [timer.py:197:stop] 0/64, RunningAvgSamplesPerSec=44.52280426945247, CurrSamplesPerSec=49.916043898746295, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       64/     200 | consumed samples:        32768 | consumed tokens:     33554432 | elapsed time per iteration (ms): 8972.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.916904E+00 | moe loss: 8.484356E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4358.49 | backward-compute: 4485.35 | backward-embedding-all-reduce: 0.01 | optimizer: 103.30 | batch-generator: 601.12
[2022-12-22 16:36:55,924] [INFO] [logging.py:68:log_dist] [Rank 0] step=65, skipped=0, lr=[9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:36:55,925] [INFO] [timer.py:197:stop] 0/65, RunningAvgSamplesPerSec=44.58995714163636, CurrSamplesPerSec=49.189868152283005, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       65/     200 | consumed samples:        33280 | consumed tokens:     34078720 | elapsed time per iteration (ms): 9373.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.913127E+00 | moe loss: 8.497272E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4685.91 | backward-compute: 4576.07 | backward-embedding-all-reduce: 0.01 | optimizer: 101.83 | batch-generator: 576.31
[2022-12-22 16:37:04,963] [INFO] [logging.py:68:log_dist] [Rank 0] step=66, skipped=0, lr=[9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:37:04,964] [INFO] [timer.py:197:stop] 0/66, RunningAvgSamplesPerSec=44.67079816278581, CurrSamplesPerSec=50.43092758481666, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       66/     200 | consumed samples:        33792 | consumed tokens:     34603008 | elapsed time per iteration (ms): 9060.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.891338E+00 | moe loss: 8.383087E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4886.28 | backward-compute: 4039.37 | backward-embedding-all-reduce: 0.01 | optimizer: 105.21 | batch-generator: 560.41
 iteration       67/     200 | consumed samples:        34304 | consumed tokens:     35127296 | elapsed time per iteration (ms): 8989.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.889023E+00 | moe loss: 8.262769E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4361.49 | backward-compute: 4515.91 | backward-embedding-all-reduce: 0.01 | optimizer: 98.73 | batch-generator: 491.60
[2022-12-22 16:37:13,978] [INFO] [logging.py:68:log_dist] [Rank 0] step=67, skipped=0, lr=[9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:37:13,981] [INFO] [timer.py:197:stop] 0/67, RunningAvgSamplesPerSec=44.745297967528536, CurrSamplesPerSec=50.091914359365376, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:37:23,277] [INFO] [logging.py:68:log_dist] [Rank 0] step=68, skipped=0, lr=[9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:37:23,283] [INFO] [timer.py:197:stop] 0/68, RunningAvgSamplesPerSec=44.81235810276206, CurrSamplesPerSec=49.64896235537303, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       68/     200 | consumed samples:        34816 | consumed tokens:     35651584 | elapsed time per iteration (ms): 9320.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.869833E+00 | moe loss: 8.210526E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4766.27 | backward-compute: 4398.48 | backward-embedding-all-reduce: 0.01 | optimizer: 117.58 | batch-generator: 590.15
 iteration       69/     200 | consumed samples:        35328 | consumed tokens:     36175872 | elapsed time per iteration (ms): 8583.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.864161E+00 | moe loss: 8.317188E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4456.83 | backward-compute: 4017.73 | backward-embedding-all-reduce: 0.01 | optimizer: 97.09 | batch-generator: 497.59
[2022-12-22 16:37:31,885] [INFO] [logging.py:68:log_dist] [Rank 0] step=69, skipped=0, lr=[9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:37:31,890] [INFO] [timer.py:197:stop] 0/69, RunningAvgSamplesPerSec=44.66607630039734, CurrSamplesPerSec=36.74874298662654, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:37:41,273] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:37:41,284] [INFO] [timer.py:197:stop] 0/70, RunningAvgSamplesPerSec=44.13958711868263, CurrSamplesPerSec=24.662511466001835, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       70/     200 | consumed samples:        35840 | consumed tokens:     36700160 | elapsed time per iteration (ms): 9415.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.861857E+00 | moe loss: 8.306913E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3981.33 | backward-compute: 5249.44 | backward-embedding-all-reduce: 0.01 | optimizer: 144.11 | batch-generator: 499.07
[2022-12-22 16:37:49,501] [INFO] [logging.py:68:log_dist] [Rank 0] step=71, skipped=0, lr=[9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:37:49,502] [INFO] [timer.py:197:stop] 0/71, RunningAvgSamplesPerSec=44.22665192505941, CurrSamplesPerSec=51.077656324744005, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       71/     200 | consumed samples:        36352 | consumed tokens:     37224448 | elapsed time per iteration (ms): 8217.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.832431E+00 | moe loss: 8.200267E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4128.29 | backward-compute: 3984.17 | backward-embedding-all-reduce: 0.01 | optimizer: 90.39 | batch-generator: 567.09
 iteration       72/     200 | consumed samples:        36864 | consumed tokens:     37748736 | elapsed time per iteration (ms): 8459.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.835099E+00 | moe loss: 8.279482E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4101.80 | backward-compute: 4250.59 | backward-embedding-all-reduce: 0.01 | optimizer: 97.70 | batch-generator: 473.50
[2022-12-22 16:37:57,967] [INFO] [logging.py:68:log_dist] [Rank 0] step=72, skipped=0, lr=[9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:37:57,968] [INFO] [timer.py:197:stop] 0/72, RunningAvgSamplesPerSec=44.30238131475995, CurrSamplesPerSec=50.23793541123227, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:38:07,395] [INFO] [logging.py:68:log_dist] [Rank 0] step=73, skipped=0, lr=[9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:38:07,396] [INFO] [timer.py:197:stop] 0/73, RunningAvgSamplesPerSec=44.354013689571126, CurrSamplesPerSec=48.2939163819609, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       73/     200 | consumed samples:        37376 | consumed tokens:     38273024 | elapsed time per iteration (ms): 9457.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.826669E+00 | moe loss: 8.344959E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4975.01 | backward-compute: 4326.22 | backward-embedding-all-reduce: 0.01 | optimizer: 131.75 | batch-generator: 561.18
 iteration       74/     200 | consumed samples:        37888 | consumed tokens:     38797312 | elapsed time per iteration (ms): 8399.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.810327E+00 | moe loss: 8.336122E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4261.02 | backward-compute: 4013.03 | backward-embedding-all-reduce: 0.01 | optimizer: 96.23 | batch-generator: 496.98
[2022-12-22 16:38:15,830] [INFO] [logging.py:68:log_dist] [Rank 0] step=74, skipped=0, lr=[9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:38:15,833] [INFO] [timer.py:197:stop] 0/74, RunningAvgSamplesPerSec=44.425701389016474, CurrSamplesPerSec=50.184616483319466, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:38:24,976] [INFO] [logging.py:68:log_dist] [Rank 0] step=75, skipped=0, lr=[9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:38:24,990] [INFO] [timer.py:197:stop] 0/75, RunningAvgSamplesPerSec=44.48241279281524, CurrSamplesPerSec=48.984652712020875, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       75/     200 | consumed samples:        38400 | consumed tokens:     39321600 | elapsed time per iteration (ms): 9166.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.802409E+00 | moe loss: 8.279866E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4299.98 | backward-compute: 4718.26 | backward-embedding-all-reduce: 0.01 | optimizer: 117.97 | batch-generator: 634.87
 iteration       76/     200 | consumed samples:        38912 | consumed tokens:     39845888 | elapsed time per iteration (ms): 8852.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.788068E+00 | moe loss: 8.262560E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4296.13 | backward-compute: 4444.47 | backward-embedding-all-reduce: 0.01 | optimizer: 98.83 | batch-generator: 482.17
[2022-12-22 16:38:33,852] [INFO] [logging.py:68:log_dist] [Rank 0] step=76, skipped=0, lr=[9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:38:33,855] [INFO] [timer.py:197:stop] 0/76, RunningAvgSamplesPerSec=44.54630179799264, CurrSamplesPerSec=49.7639594125069, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:38:43,346] [INFO] [logging.py:68:log_dist] [Rank 0] step=77, skipped=0, lr=[9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:38:43,353] [INFO] [timer.py:197:stop] 0/77, RunningAvgSamplesPerSec=44.60120797392889, CurrSamplesPerSec=49.07755990958782, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       77/     200 | consumed samples:        39424 | consumed tokens:     40370176 | elapsed time per iteration (ms): 9511.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.778501E+00 | moe loss: 8.412974E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4197.81 | backward-compute: 5167.89 | backward-embedding-all-reduce: 0.01 | optimizer: 115.30 | batch-generator: 600.94
 iteration       78/     200 | consumed samples:        39936 | consumed tokens:     40894464 | elapsed time per iteration (ms): 8681.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.777847E+00 | moe loss: 8.183624E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4536.61 | backward-compute: 4038.52 | backward-embedding-all-reduce: 0.01 | optimizer: 94.91 | batch-generator: 486.75
[2022-12-22 16:38:52,047] [INFO] [logging.py:68:log_dist] [Rank 0] step=78, skipped=0, lr=[9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:38:52,047] [INFO] [timer.py:197:stop] 0/78, RunningAvgSamplesPerSec=44.512000994850936, CurrSamplesPerSec=38.7058295622724, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:39:00,954] [INFO] [logging.py:68:log_dist] [Rank 0] step=79, skipped=0, lr=[9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:39:00,975] [INFO] [timer.py:197:stop] 0/79, RunningAvgSamplesPerSec=44.253743183695434, CurrSamplesPerSec=30.711492200608795, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       79/     200 | consumed samples:        40448 | consumed tokens:     41418752 | elapsed time per iteration (ms): 8933.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.780015E+00 | moe loss: 8.468676E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4004.01 | backward-compute: 4750.62 | backward-embedding-all-reduce: 0.01 | optimizer: 148.81 | batch-generator: 481.20
[2022-12-22 16:39:09,199] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:39:09,199] [INFO] [timer.py:197:stop] 0/80, RunningAvgSamplesPerSec=44.33155641232224, CurrSamplesPerSec=51.273611540616265, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       80/     200 | consumed samples:        40960 | consumed tokens:     41943040 | elapsed time per iteration (ms): 8233.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.771682E+00 | moe loss: 8.428360E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4147.44 | backward-compute: 3978.42 | backward-embedding-all-reduce: 0.01 | optimizer: 91.07 | batch-generator: 517.69
 iteration       81/     200 | consumed samples:        41472 | consumed tokens:     42467328 | elapsed time per iteration (ms): 9291.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.766287E+00 | moe loss: 8.480565E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4809.12 | backward-compute: 4363.71 | backward-embedding-all-reduce: 0.01 | optimizer: 99.04 | batch-generator: 491.65
[2022-12-22 16:39:18,497] [INFO] [logging.py:68:log_dist] [Rank 0] step=81, skipped=0, lr=[9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:39:18,500] [INFO] [timer.py:197:stop] 0/81, RunningAvgSamplesPerSec=44.38839064633604, CurrSamplesPerSec=49.32032849908519, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:39:27,330] [INFO] [logging.py:68:log_dist] [Rank 0] step=82, skipped=0, lr=[9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:39:27,338] [INFO] [timer.py:197:stop] 0/82, RunningAvgSamplesPerSec=44.43959830584987, CurrSamplesPerSec=48.89579249355641, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       82/     200 | consumed samples:        41984 | consumed tokens:     42991616 | elapsed time per iteration (ms): 8851.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.754488E+00 | moe loss: 8.323781E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4147.28 | backward-compute: 4567.89 | backward-embedding-all-reduce: 0.01 | optimizer: 112.25 | batch-generator: 537.29
 iteration       83/     200 | consumed samples:        42496 | consumed tokens:     43515904 | elapsed time per iteration (ms): 9169.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.732113E+00 | moe loss: 8.338313E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4981.13 | backward-compute: 4073.44 | backward-embedding-all-reduce: 0.01 | optimizer: 98.93 | batch-generator: 470.81
[2022-12-22 16:39:36,516] [INFO] [logging.py:68:log_dist] [Rank 0] step=83, skipped=0, lr=[9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:39:36,518] [INFO] [timer.py:197:stop] 0/83, RunningAvgSamplesPerSec=44.50318103707538, CurrSamplesPerSec=50.25549521989959, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:39:45,816] [INFO] [logging.py:68:log_dist] [Rank 0] step=84, skipped=0, lr=[9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:39:45,822] [INFO] [timer.py:197:stop] 0/84, RunningAvgSamplesPerSec=44.54530246956899, CurrSamplesPerSec=48.24392241015217, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       84/     200 | consumed samples:        43008 | consumed tokens:     44040192 | elapsed time per iteration (ms): 9312.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.712944E+00 | moe loss: 8.459086E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5068.63 | backward-compute: 4102.23 | backward-embedding-all-reduce: 0.01 | optimizer: 117.41 | batch-generator: 508.16
 iteration       85/     200 | consumed samples:        43520 | consumed tokens:     44564480 | elapsed time per iteration (ms): 8797.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.711922E+00 | moe loss: 8.518086E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4125.63 | backward-compute: 4560.98 | backward-embedding-all-reduce: 0.01 | optimizer: 96.36 | batch-generator: 441.80
[2022-12-22 16:39:54,636] [INFO] [logging.py:68:log_dist] [Rank 0] step=85, skipped=0, lr=[9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:39:54,639] [INFO] [timer.py:197:stop] 0/85, RunningAvgSamplesPerSec=44.59696268408102, CurrSamplesPerSec=49.28370999302702, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:40:03,978] [INFO] [logging.py:68:log_dist] [Rank 0] step=86, skipped=0, lr=[9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:40:03,990] [INFO] [timer.py:197:stop] 0/86, RunningAvgSamplesPerSec=44.647616632922194, CurrSamplesPerSec=49.29476949475495, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       86/     200 | consumed samples:        44032 | consumed tokens:     45088768 | elapsed time per iteration (ms): 9364.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.697908E+00 | moe loss: 8.385620E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5143.60 | backward-compute: 4073.44 | backward-embedding-all-reduce: 0.01 | optimizer: 114.29 | batch-generator: 516.63
 iteration       87/     200 | consumed samples:        44544 | consumed tokens:     45613056 | elapsed time per iteration (ms): 8188.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.674543E+00 | moe loss: 8.267294E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4087.52 | backward-compute: 3995.53 | backward-embedding-all-reduce: 0.01 | optimizer: 91.41 | batch-generator: 439.47
[2022-12-22 16:40:12,191] [INFO] [logging.py:68:log_dist] [Rank 0] step=87, skipped=0, lr=[9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:40:12,194] [INFO] [timer.py:197:stop] 0/87, RunningAvgSamplesPerSec=44.64819309476162, CurrSamplesPerSec=44.696669089359055, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:40:21,341] [INFO] [logging.py:68:log_dist] [Rank 0] step=88, skipped=0, lr=[9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:40:21,352] [INFO] [timer.py:197:stop] 0/88, RunningAvgSamplesPerSec=44.25076880644608, CurrSamplesPerSec=25.191069387630996, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       88/     200 | consumed samples:        45056 | consumed tokens:     46137344 | elapsed time per iteration (ms): 9170.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.670745E+00 | moe loss: 8.338050E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3787.45 | backward-compute: 5154.97 | backward-embedding-all-reduce: 0.01 | optimizer: 188.00 | batch-generator: 453.42
[2022-12-22 16:40:29,527] [INFO] [logging.py:68:log_dist] [Rank 0] step=89, skipped=0, lr=[9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:40:29,528] [INFO] [timer.py:197:stop] 0/89, RunningAvgSamplesPerSec=44.32159375089239, CurrSamplesPerSec=51.39606041111418, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       89/     200 | consumed samples:        45568 | consumed tokens:     46661632 | elapsed time per iteration (ms): 8183.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.657017E+00 | moe loss: 8.300984E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4054.89 | backward-compute: 4019.81 | backward-embedding-all-reduce: 0.01 | optimizer: 90.08 | batch-generator: 495.22
 iteration       90/     200 | consumed samples:        46080 | consumed tokens:     47185920 | elapsed time per iteration (ms): 8348.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.639053E+00 | moe loss: 8.309287E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4238.30 | backward-compute: 4007.87 | backward-embedding-all-reduce: 0.01 | optimizer: 92.90 | batch-generator: 442.81
[2022-12-22 16:40:37,881] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:40:37,881] [INFO] [timer.py:197:stop] 0/90, RunningAvgSamplesPerSec=44.37820545716255, CurrSamplesPerSec=49.92623758960288, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:40:47,369] [INFO] [logging.py:68:log_dist] [Rank 0] step=91, skipped=0, lr=[9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       91/     200 | consumed samples:        46592 | consumed tokens:     47710208 | elapsed time per iteration (ms): 9499.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.645028E+00 | moe loss: 8.412172E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4495.52 | backward-compute: 4868.48 | backward-embedding-all-reduce: 0.01 | optimizer: 119.93 | batch-generator: 490.69
[2022-12-22 16:40:47,381] [INFO] [timer.py:197:stop] 0/91, RunningAvgSamplesPerSec=44.425790263882554, CurrSamplesPerSec=49.054501782468, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       92/     200 | consumed samples:        47104 | consumed tokens:     48234496 | elapsed time per iteration (ms): 8742.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.629587E+00 | moe loss: 8.288157E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4561.69 | backward-compute: 4061.03 | backward-embedding-all-reduce: 0.01 | optimizer: 95.92 | batch-generator: 469.28
[2022-12-22 16:40:56,130] [INFO] [logging.py:68:log_dist] [Rank 0] step=92, skipped=0, lr=[9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:40:56,134] [INFO] [timer.py:197:stop] 0/92, RunningAvgSamplesPerSec=44.483030379176455, CurrSamplesPerSec=50.244658794171414, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:41:05,040] [INFO] [logging.py:68:log_dist] [Rank 0] step=93, skipped=0, lr=[9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:41:05,046] [INFO] [timer.py:197:stop] 0/93, RunningAvgSamplesPerSec=44.52230350263071, CurrSamplesPerSec=48.36537194076277, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       93/     200 | consumed samples:        47616 | consumed tokens:     48758784 | elapsed time per iteration (ms): 8925.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.622627E+00 | moe loss: 8.416006E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4138.24 | backward-compute: 4650.37 | backward-embedding-all-reduce: 0.01 | optimizer: 113.49 | batch-generator: 511.15
 iteration       94/     200 | consumed samples:        48128 | consumed tokens:     49283072 | elapsed time per iteration (ms): 8821.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.619866E+00 | moe loss: 8.273470E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4213.10 | backward-compute: 4491.66 | backward-embedding-all-reduce: 0.01 | optimizer: 97.98 | batch-generator: 472.32
[2022-12-22 16:41:13,878] [INFO] [logging.py:68:log_dist] [Rank 0] step=94, skipped=0, lr=[9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:41:13,880] [INFO] [timer.py:197:stop] 0/94, RunningAvgSamplesPerSec=44.56875176971348, CurrSamplesPerSec=49.24378507743498, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:41:22,893] [INFO] [logging.py:68:log_dist] [Rank 0] step=95, skipped=0, lr=[9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:41:22,895] [INFO] [timer.py:197:stop] 0/95, RunningAvgSamplesPerSec=44.60448619888776, CurrSamplesPerSec=48.15671613140957, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       95/     200 | consumed samples:        48640 | consumed tokens:     49807360 | elapsed time per iteration (ms): 9041.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.595528E+00 | moe loss: 8.386286E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4880.34 | backward-compute: 4020.46 | backward-embedding-all-reduce: 0.01 | optimizer: 117.19 | batch-generator: 506.81
 iteration       96/     200 | consumed samples:        49152 | consumed tokens:     50331648 | elapsed time per iteration (ms): 9108.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.593368E+00 | moe loss: 8.292645E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4544.29 | backward-compute: 4446.26 | backward-embedding-all-reduce: 0.01 | optimizer: 99.07 | batch-generator: 461.89
[2022-12-22 16:41:32,029] [INFO] [logging.py:68:log_dist] [Rank 0] step=96, skipped=0, lr=[9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:41:32,032] [INFO] [timer.py:197:stop] 0/96, RunningAvgSamplesPerSec=44.530979722756584, CurrSamplesPerSec=38.613117174468535, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:41:40,318] [INFO] [logging.py:68:log_dist] [Rank 0] step=97, skipped=0, lr=[9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:41:40,319] [INFO] [timer.py:197:stop] 0/97, RunningAvgSamplesPerSec=44.465624693274485, CurrSamplesPerSec=39.07495082068393, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       97/     200 | consumed samples:        49664 | consumed tokens:     50855936 | elapsed time per iteration (ms): 8346.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.573074E+00 | moe loss: 8.206248E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3829.48 | backward-compute: 4295.07 | backward-embedding-all-reduce: 0.01 | optimizer: 183.80 | batch-generator: 436.70
 iteration       98/     200 | consumed samples:        50176 | consumed tokens:     51380224 | elapsed time per iteration (ms): 8496.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.566442E+00 | moe loss: 8.356235E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4348.60 | backward-compute: 4028.97 | backward-embedding-all-reduce: 0.01 | optimizer: 97.63 | batch-generator: 473.40
[2022-12-22 16:41:48,872] [INFO] [logging.py:68:log_dist] [Rank 0] step=98, skipped=0, lr=[9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:41:48,884] [INFO] [timer.py:197:stop] 0/98, RunningAvgSamplesPerSec=44.50876745973328, CurrSamplesPerSec=49.02784739011423, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:41:57,826] [INFO] [logging.py:68:log_dist] [Rank 0] step=99, skipped=0, lr=[9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:41:57,830] [INFO] [timer.py:197:stop] 0/99, RunningAvgSamplesPerSec=44.563081360449736, CurrSamplesPerSec=50.47630362187336, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration       99/     200 | consumed samples:        50688 | consumed tokens:     51904512 | elapsed time per iteration (ms): 8969.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.560071E+00 | moe loss: 8.384392E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4191.12 | backward-compute: 4637.06 | backward-embedding-all-reduce: 0.01 | optimizer: 98.38 | batch-generator: 474.65
 iteration      100/     200 | consumed samples:        51200 | consumed tokens:     52428800 | elapsed time per iteration (ms): 9219.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.558688E+00 | moe loss: 8.272881E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4307.12 | backward-compute: 4785.96 | backward-embedding-all-reduce: 0.01 | optimizer: 113.32 | batch-generator: 494.74
[2022-12-22 16:42:07,060] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:42:07,067] [INFO] [timer.py:197:stop] 0/100, RunningAvgSamplesPerSec=44.59899233682286, CurrSamplesPerSec=48.38077489491999, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:42:15,876] [INFO] [logging.py:68:log_dist] [Rank 0] step=101, skipped=0, lr=[9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05, 9.999999321765968e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:42:15,880] [INFO] [timer.py:197:stop] 0/101, RunningAvgSamplesPerSec=44.654485446253624, CurrSamplesPerSec=50.855748270772985, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      101/     200 | consumed samples:        51712 | consumed tokens:     52953088 | elapsed time per iteration (ms): 8830.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.554441E+00 | moe loss: 8.435603E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4151.55 | backward-compute: 4553.82 | backward-embedding-all-reduce: 0.01 | optimizer: 94.34 | batch-generator: 441.77
 iteration      102/     200 | consumed samples:        52224 | consumed tokens:     53477376 | elapsed time per iteration (ms): 9128.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.541465E+00 | moe loss: 8.291196E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4896.95 | backward-compute: 4080.34 | backward-embedding-all-reduce: 0.01 | optimizer: 140.16 | batch-generator: 451.24
[2022-12-22 16:42:25,013] [INFO] [logging.py:68:log_dist] [Rank 0] step=102, skipped=0, lr=[9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05, 9.999999308133464e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:42:25,025] [INFO] [timer.py:197:stop] 0/102, RunningAvgSamplesPerSec=44.68325313551818, CurrSamplesPerSec=47.72722974869883, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:42:33,205] [INFO] [logging.py:68:log_dist] [Rank 0] step=103, skipped=0, lr=[9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05, 9.999999294365314e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:42:33,206] [INFO] [timer.py:197:stop] 0/103, RunningAvgSamplesPerSec=44.735523712518834, CurrSamplesPerSec=50.66197356677542, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      103/     200 | consumed samples:        52736 | consumed tokens:     54001664 | elapsed time per iteration (ms): 8199.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.526598E+00 | moe loss: 8.281801E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4061.00 | backward-compute: 4019.06 | backward-embedding-all-reduce: 0.01 | optimizer: 93.44 | batch-generator: 417.09
 iteration      104/     200 | consumed samples:        53248 | consumed tokens:     54525952 | elapsed time per iteration (ms): 9346.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.527838E+00 | moe loss: 8.378078E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4375.40 | backward-compute: 4827.21 | backward-embedding-all-reduce: 0.01 | optimizer: 121.79 | batch-generator: 491.32
[2022-12-22 16:42:42,560] [INFO] [logging.py:68:log_dist] [Rank 0] step=104, skipped=0, lr=[9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05, 9.999999280461516e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:42:42,568] [INFO] [timer.py:197:stop] 0/104, RunningAvgSamplesPerSec=44.54326822597522, CurrSamplesPerSec=31.06099868285899, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:42:50,669] [INFO] [logging.py:68:log_dist] [Rank 0] step=105, skipped=0, lr=[9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05, 9.999999266422072e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:42:50,670] [INFO] [timer.py:197:stop] 0/105, RunningAvgSamplesPerSec=44.57411396665171, CurrSamplesPerSec=47.96184713769557, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      105/     200 | consumed samples:        53760 | consumed tokens:     55050240 | elapsed time per iteration (ms): 8118.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.523948E+00 | moe loss: 8.335380E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3926.74 | backward-compute: 4066.18 | backward-embedding-all-reduce: 0.01 | optimizer: 96.93 | batch-generator: 412.07
[2022-12-22 16:42:58,994] [INFO] [logging.py:68:log_dist] [Rank 0] step=106, skipped=0, lr=[9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05, 9.999999252246981e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:42:58,997] [INFO] [timer.py:197:stop] 0/106, RunningAvgSamplesPerSec=44.62604341535258, CurrSamplesPerSec=50.711205526111215, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      106/     200 | consumed samples:        54272 | consumed tokens:     55574528 | elapsed time per iteration (ms): 8324.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.513720E+00 | moe loss: 8.387546E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4176.97 | backward-compute: 4040.95 | backward-embedding-all-reduce: 0.01 | optimizer: 95.50 | batch-generator: 385.16
 iteration      107/     200 | consumed samples:        54784 | consumed tokens:     56098816 | elapsed time per iteration (ms): 9273.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.516015E+00 | moe loss: 8.260813E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4364.96 | backward-compute: 4771.15 | backward-embedding-all-reduce: 0.01 | optimizer: 118.54 | batch-generator: 459.00
[2022-12-22 16:43:08,277] [INFO] [logging.py:68:log_dist] [Rank 0] step=107, skipped=0, lr=[9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05, 9.999999237936243e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:43:08,290] [INFO] [timer.py:197:stop] 0/107, RunningAvgSamplesPerSec=44.65713888648388, CurrSamplesPerSec=48.146161335058046, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:43:16,612] [INFO] [logging.py:68:log_dist] [Rank 0] step=108, skipped=0, lr=[9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05, 9.999999223489858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:43:16,615] [INFO] [timer.py:197:stop] 0/108, RunningAvgSamplesPerSec=44.707577891970274, CurrSamplesPerSec=50.72305882379618, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      108/     200 | consumed samples:        55296 | consumed tokens:     56623104 | elapsed time per iteration (ms): 8345.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.498253E+00 | moe loss: 8.304986E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3933.81 | backward-compute: 4287.94 | backward-embedding-all-reduce: 0.01 | optimizer: 92.35 | batch-generator: 401.28
 iteration      109/     200 | consumed samples:        55808 | consumed tokens:     57147392 | elapsed time per iteration (ms): 9205.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.496080E+00 | moe loss: 8.266182E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4754.79 | backward-compute: 4324.61 | backward-embedding-all-reduce: 0.01 | optimizer: 114.90 | batch-generator: 433.58
[2022-12-22 16:43:25,826] [INFO] [logging.py:68:log_dist] [Rank 0] step=109, skipped=0, lr=[9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05, 9.999999208907828e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:43:25,839] [INFO] [timer.py:197:stop] 0/109, RunningAvgSamplesPerSec=44.74509995358271, CurrSamplesPerSec=49.114492643313405, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:43:34,585] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05, 9.999999194190149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:43:34,588] [INFO] [timer.py:197:stop] 0/110, RunningAvgSamplesPerSec=44.797339700512644, CurrSamplesPerSec=51.19240347101203, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      110/     200 | consumed samples:        56320 | consumed tokens:     57671680 | elapsed time per iteration (ms): 8765.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.487968E+00 | moe loss: 8.393847E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4425.49 | backward-compute: 4219.66 | backward-embedding-all-reduce: 0.01 | optimizer: 93.84 | batch-generator: 388.07
[2022-12-22 16:43:43,615] [INFO] [logging.py:68:log_dist] [Rank 0] step=111, skipped=0, lr=[9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05, 9.999999179336825e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:43:43,621] [INFO] [timer.py:197:stop] 0/111, RunningAvgSamplesPerSec=44.82415510613042, CurrSamplesPerSec=47.92223769432364, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      111/     200 | consumed samples:        56832 | consumed tokens:     58195968 | elapsed time per iteration (ms): 9028.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.486800E+00 | moe loss: 8.326364E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4320.55 | backward-compute: 4575.70 | backward-embedding-all-reduce: 0.01 | optimizer: 120.02 | batch-generator: 455.63
 iteration      112/     200 | consumed samples:        57344 | consumed tokens:     58720256 | elapsed time per iteration (ms): 8474.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.476985E+00 | moe loss: 8.304501E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4024.06 | backward-compute: 4312.64 | backward-embedding-all-reduce: 0.01 | optimizer: 105.23 | batch-generator: 427.84
[2022-12-22 16:43:52,093] [INFO] [logging.py:68:log_dist] [Rank 0] step=112, skipped=0, lr=[9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05, 9.999999164347853e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:43:52,094] [INFO] [timer.py:197:stop] 0/112, RunningAvgSamplesPerSec=44.762752403340656, CurrSamplesPerSec=38.94734981896372, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:44:00,456] [INFO] [logging.py:68:log_dist] [Rank 0] step=113, skipped=0, lr=[9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05, 9.999999149223235e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:44:00,457] [INFO] [timer.py:197:stop] 0/113, RunningAvgSamplesPerSec=44.68453189197018, CurrSamplesPerSec=37.48013403381954, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      113/     200 | consumed samples:        57856 | consumed tokens:     59244544 | elapsed time per iteration (ms): 8404.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.469692E+00 | moe loss: 8.291475E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3827.76 | backward-compute: 4083.97 | backward-embedding-all-reduce: 0.01 | optimizer: 466.65 | batch-generator: 371.77
 iteration      114/     200 | consumed samples:        58368 | consumed tokens:     59768832 | elapsed time per iteration (ms): 8568.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.472644E+00 | moe loss: 8.306089E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4405.16 | backward-compute: 4042.82 | backward-embedding-all-reduce: 0.01 | optimizer: 102.81 | batch-generator: 492.47
[2022-12-22 16:44:09,078] [INFO] [logging.py:68:log_dist] [Rank 0] step=114, skipped=0, lr=[9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05, 9.99999913396297e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:44:09,093] [INFO] [timer.py:197:stop] 0/114, RunningAvgSamplesPerSec=44.71298190733152, CurrSamplesPerSec=48.113248358827235, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:44:18,026] [INFO] [logging.py:68:log_dist] [Rank 0] step=115, skipped=0, lr=[9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05, 9.999999118567058e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:44:18,029] [INFO] [timer.py:197:stop] 0/115, RunningAvgSamplesPerSec=44.759684705147336, CurrSamplesPerSec=50.68956384219198, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      115/     200 | consumed samples:        58880 | consumed tokens:     60293120 | elapsed time per iteration (ms): 8966.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.474790E+00 | moe loss: 8.325197E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4123.92 | backward-compute: 4703.50 | backward-embedding-all-reduce: 0.01 | optimizer: 93.78 | batch-generator: 451.37
[2022-12-22 16:44:26,899] [INFO] [logging.py:68:log_dist] [Rank 0] step=116, skipped=0, lr=[9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05, 9.9999991030355e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      116/     200 | consumed samples:        59392 | consumed tokens:     60817408 | elapsed time per iteration (ms): 8868.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.449083E+00 | moe loss: 8.255102E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4396.65 | backward-compute: 4344.50 | backward-embedding-all-reduce: 0.01 | optimizer: 117.39 | batch-generator: 542.08
[2022-12-22 16:44:26,900] [INFO] [timer.py:197:stop] 0/116, RunningAvgSamplesPerSec=44.79473561634853, CurrSamplesPerSec=49.143400504806685, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:44:36,064] [INFO] [logging.py:68:log_dist] [Rank 0] step=117, skipped=0, lr=[9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05, 9.999999087368294e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:44:36,068] [INFO] [timer.py:197:stop] 0/117, RunningAvgSamplesPerSec=44.8427877180163, CurrSamplesPerSec=51.090653948220634, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      117/     200 | consumed samples:        59904 | consumed tokens:     61341696 | elapsed time per iteration (ms): 9169.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.449602E+00 | moe loss: 8.148322E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4794.36 | backward-compute: 4258.32 | backward-embedding-all-reduce: 0.01 | optimizer: 92.28 | batch-generator: 402.16
[2022-12-22 16:44:44,297] [INFO] [logging.py:68:log_dist] [Rank 0] step=118, skipped=0, lr=[9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05, 9.999999071565441e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:44:44,298] [INFO] [timer.py:197:stop] 0/118, RunningAvgSamplesPerSec=44.86687648874386, CurrSamplesPerSec=47.821071627714744, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      118/     200 | consumed samples:        60416 | consumed tokens:     61865984 | elapsed time per iteration (ms): 8258.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.450960E+00 | moe loss: 8.313021E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4064.56 | backward-compute: 4049.14 | backward-embedding-all-reduce: 0.01 | optimizer: 127.21 | batch-generator: 391.27
 iteration      119/     200 | consumed samples:        60928 | consumed tokens:     62390272 | elapsed time per iteration (ms): 9347.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.442411E+00 | moe loss: 8.289792E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4906.85 | backward-compute: 4329.03 | backward-embedding-all-reduce: 0.01 | optimizer: 102.49 | batch-generator: 382.89
[2022-12-22 16:44:53,676] [INFO] [logging.py:68:log_dist] [Rank 0] step=119, skipped=0, lr=[9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05, 9.999999055626943e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:44:53,690] [INFO] [timer.py:197:stop] 0/119, RunningAvgSamplesPerSec=44.90344323677522, CurrSamplesPerSec=49.59189194681584, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:45:02,138] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05, 9.999999039552796e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:45:02,139] [INFO] [timer.py:197:stop] 0/120, RunningAvgSamplesPerSec=44.87198890379521, CurrSamplesPerSec=41.47298262289074, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      120/     200 | consumed samples:        61440 | consumed tokens:     62914560 | elapsed time per iteration (ms): 8462.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.437478E+00 | moe loss: 8.233386E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3973.76 | backward-compute: 4345.98 | backward-embedding-all-reduce: 0.01 | optimizer: 111.88 | batch-generator: 462.04
[2022-12-22 16:45:10,052] [INFO] [logging.py:68:log_dist] [Rank 0] step=121, skipped=0, lr=[9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05, 9.999999023343003e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:45:10,053] [INFO] [timer.py:197:stop] 0/121, RunningAvgSamplesPerSec=44.90700801569183, CurrSamplesPerSec=49.46195230979287, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      121/     200 | consumed samples:        61952 | consumed tokens:     63438848 | elapsed time per iteration (ms): 7920.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.416032E+00 | moe loss: 8.312128E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3794.79 | backward-compute: 3976.05 | backward-embedding-all-reduce: 0.01 | optimizer: 133.99 | batch-generator: 371.87
 iteration      122/     200 | consumed samples:        62464 | consumed tokens:     63963136 | elapsed time per iteration (ms): 9094.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.427898E+00 | moe loss: 8.282720E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4889.73 | backward-compute: 4070.00 | backward-embedding-all-reduce: 0.01 | optimizer: 103.43 | batch-generator: 385.51
[2022-12-22 16:45:19,149] [INFO] [logging.py:68:log_dist] [Rank 0] step=122, skipped=0, lr=[9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05, 9.999999006997565e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:45:19,163] [INFO] [timer.py:197:stop] 0/122, RunningAvgSamplesPerSec=44.94293478347312, CurrSamplesPerSec=49.67184753571173, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:45:27,894] [INFO] [logging.py:68:log_dist] [Rank 0] step=123, skipped=0, lr=[9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05, 9.999998990516477e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:45:27,895] [INFO] [timer.py:197:stop] 0/123, RunningAvgSamplesPerSec=44.97121652834091, CurrSamplesPerSec=48.644548643106894, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      123/     200 | consumed samples:        62976 | consumed tokens:     64487424 | elapsed time per iteration (ms): 8751.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.412394E+00 | moe loss: 8.254985E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4052.95 | backward-compute: 4554.67 | backward-embedding-all-reduce: 0.01 | optimizer: 112.11 | batch-generator: 415.85
 iteration      124/     200 | consumed samples:        63488 | consumed tokens:     65011712 | elapsed time per iteration (ms): 9263.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.421540E+00 | moe loss: 8.438490E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4782.83 | backward-compute: 4359.92 | backward-embedding-all-reduce: 0.01 | optimizer: 103.23 | batch-generator: 439.04
[2022-12-22 16:45:37,166] [INFO] [logging.py:68:log_dist] [Rank 0] step=124, skipped=0, lr=[9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05, 9.999998973899745e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:45:37,181] [INFO] [timer.py:197:stop] 0/124, RunningAvgSamplesPerSec=45.006297245848906, CurrSamplesPerSec=49.69713670109769, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:45:45,441] [INFO] [logging.py:68:log_dist] [Rank 0] step=125, skipped=0, lr=[9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05, 9.999998957147365e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:45:45,442] [INFO] [timer.py:197:stop] 0/125, RunningAvgSamplesPerSec=45.03815762333781, CurrSamplesPerSec=49.295566115280565, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      125/     200 | consumed samples:        64000 | consumed tokens:     65536000 | elapsed time per iteration (ms): 8281.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.406878E+00 | moe loss: 8.276758E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4052.82 | backward-compute: 4070.68 | backward-embedding-all-reduce: 0.01 | optimizer: 127.16 | batch-generator: 392.27
 iteration      126/     200 | consumed samples:        64512 | consumed tokens:     66060288 | elapsed time per iteration (ms): 9266.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.403766E+00 | moe loss: 8.346231E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4960.61 | backward-compute: 4178.11 | backward-embedding-all-reduce: 0.01 | optimizer: 107.21 | batch-generator: 417.19
[2022-12-22 16:45:54,715] [INFO] [logging.py:68:log_dist] [Rank 0] step=126, skipped=0, lr=[9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05, 9.99999894025934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:45:54,728] [INFO] [timer.py:197:stop] 0/126, RunningAvgSamplesPerSec=45.07169042538038, CurrSamplesPerSec=49.6154031412161, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:46:03,131] [INFO] [logging.py:68:log_dist] [Rank 0] step=127, skipped=0, lr=[9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05, 9.999998923235666e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:46:03,132] [INFO] [timer.py:197:stop] 0/127, RunningAvgSamplesPerSec=45.09066657633419, CurrSamplesPerSec=47.57436562451096, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      127/     200 | consumed samples:        65024 | consumed tokens:     66584576 | elapsed time per iteration (ms): 8422.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.402074E+00 | moe loss: 8.341197E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3890.32 | backward-compute: 4396.25 | backward-embedding-all-reduce: 0.01 | optimizer: 109.75 | batch-generator: 455.42
 iteration      128/     200 | consumed samples:        65536 | consumed tokens:     67108864 | elapsed time per iteration (ms): 9247.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.404732E+00 | moe loss: 8.342932E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4328.41 | backward-compute: 4740.46 | backward-embedding-all-reduce: 0.01 | optimizer: 155.83 | batch-generator: 356.91
[2022-12-22 16:46:12,409] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05, 9.999998906076345e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:46:12,428] [INFO] [timer.py:197:stop] 0/128, RunningAvgSamplesPerSec=44.75426304980834, CurrSamplesPerSec=23.15783690892339, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:46:20,817] [INFO] [logging.py:68:log_dist] [Rank 0] step=129, skipped=0, lr=[9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05, 9.999998888781379e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:46:20,818] [INFO] [timer.py:197:stop] 0/129, RunningAvgSamplesPerSec=44.718011822250645, CurrSamplesPerSec=40.57672038290658, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      129/     200 | consumed samples:        66048 | consumed tokens:     67633152 | elapsed time per iteration (ms): 8448.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.390220E+00 | moe loss: 8.217765E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3977.60 | backward-compute: 4296.10 | backward-embedding-all-reduce: 0.01 | optimizer: 119.47 | batch-generator: 402.38
 iteration      130/     200 | consumed samples:        66560 | consumed tokens:     68157440 | elapsed time per iteration (ms): 8047.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.382187E+00 | moe loss: 8.331064E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3951.68 | backward-compute: 3993.16 | backward-embedding-all-reduce: 0.01 | optimizer: 90.70 | batch-generator: 387.43
[2022-12-22 16:46:28,891] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05, 9.999998871350765e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:46:28,892] [INFO] [timer.py:197:stop] 0/130, RunningAvgSamplesPerSec=44.746493288225594, CurrSamplesPerSec=48.68447864885408, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      131/     200 | consumed samples:        67072 | consumed tokens:     68681728 | elapsed time per iteration (ms): 9542.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.385223E+00 | moe loss: 8.275476E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4871.46 | backward-compute: 4534.47 | backward-embedding-all-reduce: 0.01 | optimizer: 106.96 | batch-generator: 377.52
[2022-12-22 16:46:38,424] [INFO] [logging.py:68:log_dist] [Rank 0] step=131, skipped=0, lr=[9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05, 9.999998853784504e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:46:38,438] [INFO] [timer.py:197:stop] 0/131, RunningAvgSamplesPerSec=44.78286540127445, CurrSamplesPerSec=49.98336392020402, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:46:46,981] [INFO] [logging.py:68:log_dist] [Rank 0] step=132, skipped=0, lr=[9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05, 9.999998836082598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:46:46,982] [INFO] [timer.py:197:stop] 0/132, RunningAvgSamplesPerSec=44.81432706075768, CurrSamplesPerSec=49.280489009369035, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      132/     200 | consumed samples:        67584 | consumed tokens:     69206016 | elapsed time per iteration (ms): 8564.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.364512E+00 | moe loss: 8.210332E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4003.26 | backward-compute: 4415.77 | backward-embedding-all-reduce: 0.01 | optimizer: 119.10 | batch-generator: 416.39
 iteration      133/     200 | consumed samples:        68096 | consumed tokens:     69730304 | elapsed time per iteration (ms): 8957.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.356751E+00 | moe loss: 8.294876E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4537.66 | backward-compute: 4302.29 | backward-embedding-all-reduce: 0.01 | optimizer: 104.68 | batch-generator: 364.66
[2022-12-22 16:46:55,948] [INFO] [logging.py:68:log_dist] [Rank 0] step=133, skipped=0, lr=[9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05, 9.999998818245043e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:46:55,963] [INFO] [timer.py:197:stop] 0/133, RunningAvgSamplesPerSec=44.84626188033705, CurrSamplesPerSec=49.42490487330741, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:47:04,908] [INFO] [logging.py:68:log_dist] [Rank 0] step=134, skipped=0, lr=[9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05, 9.999998800271843e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:47:04,909] [INFO] [timer.py:197:stop] 0/134, RunningAvgSamplesPerSec=44.8798793786847, CurrSamplesPerSec=49.76698556095224, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      134/     200 | consumed samples:        68608 | consumed tokens:     70254592 | elapsed time per iteration (ms): 8968.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.364002E+00 | moe loss: 8.275813E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4315.28 | backward-compute: 4505.48 | backward-embedding-all-reduce: 0.01 | optimizer: 114.77 | batch-generator: 419.77
[2022-12-22 16:47:13,753] [INFO] [logging.py:68:log_dist] [Rank 0] step=135, skipped=0, lr=[9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05, 9.999998782162996e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:47:13,756] [INFO] [timer.py:197:stop] 0/135, RunningAvgSamplesPerSec=44.91380806131241, CurrSamplesPerSec=49.89261790871229, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      135/     200 | consumed samples:        69120 | consumed tokens:     70778880 | elapsed time per iteration (ms): 8841.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.362266E+00 | moe loss: 8.170851E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4188.12 | backward-compute: 4538.15 | backward-embedding-all-reduce: 0.01 | optimizer: 102.63 | batch-generator: 394.84
 iteration      136/     200 | consumed samples:        69632 | consumed tokens:     71303168 | elapsed time per iteration (ms): 9065.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.363417E+00 | moe loss: 8.209603E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4250.69 | backward-compute: 4683.17 | backward-embedding-all-reduce: 0.01 | optimizer: 111.18 | batch-generator: 457.99
[2022-12-22 16:47:22,849] [INFO] [logging.py:68:log_dist] [Rank 0] step=136, skipped=0, lr=[9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05, 9.999998763918501e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:47:22,849] [INFO] [timer.py:197:stop] 0/136, RunningAvgSamplesPerSec=44.73154872901277, CurrSamplesPerSec=29.051905889885525, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:47:31,185] [INFO] [logging.py:68:log_dist] [Rank 0] step=137, skipped=0, lr=[9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05, 9.99999874553836e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:47:31,186] [INFO] [timer.py:197:stop] 0/137, RunningAvgSamplesPerSec=44.6590629979224, CurrSamplesPerSec=36.69175099597292, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      137/     200 | consumed samples:        70144 | consumed tokens:     71827456 | elapsed time per iteration (ms): 8373.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.347181E+00 | moe loss: 8.276952E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3765.90 | backward-compute: 4455.90 | backward-embedding-all-reduce: 0.01 | optimizer: 108.16 | batch-generator: 356.81
 iteration      138/     200 | consumed samples:        70656 | consumed tokens:     72351744 | elapsed time per iteration (ms): 8345.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.342916E+00 | moe loss: 8.238272E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4207.20 | backward-compute: 4026.74 | backward-embedding-all-reduce: 0.01 | optimizer: 103.46 | batch-generator: 360.45
[2022-12-22 16:47:39,544] [INFO] [logging.py:68:log_dist] [Rank 0] step=138, skipped=0, lr=[9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05, 9.999998727022573e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:47:39,560] [INFO] [timer.py:197:stop] 0/138, RunningAvgSamplesPerSec=44.690823828515626, CurrSamplesPerSec=49.43729337471226, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:47:48,817] [INFO] [logging.py:68:log_dist] [Rank 0] step=139, skipped=0, lr=[9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05, 9.999998708371137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:47:48,818] [INFO] [timer.py:197:stop] 0/139, RunningAvgSamplesPerSec=44.72253450187909, CurrSamplesPerSec=49.49919224079098, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      139/     200 | consumed samples:        71168 | consumed tokens:     72876032 | elapsed time per iteration (ms): 9285.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.338439E+00 | moe loss: 8.248625E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4373.93 | backward-compute: 4755.11 | backward-embedding-all-reduce: 0.01 | optimizer: 122.11 | batch-generator: 544.96
 iteration      140/     200 | consumed samples:        71680 | consumed tokens:     73400320 | elapsed time per iteration (ms): 8826.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.338456E+00 | moe loss: 8.192165E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4398.58 | backward-compute: 4305.92 | backward-embedding-all-reduce: 0.01 | optimizer: 107.59 | batch-generator: 376.02
[2022-12-22 16:47:57,655] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05, 9.999998689584056e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:47:57,670] [INFO] [timer.py:197:stop] 0/140, RunningAvgSamplesPerSec=44.75301897552601, CurrSamplesPerSec=49.36271044190524, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:48:06,823] [INFO] [logging.py:68:log_dist] [Rank 0] step=141, skipped=0, lr=[9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05, 9.999998670661327e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:48:06,823] [INFO] [timer.py:197:stop] 0/141, RunningAvgSamplesPerSec=44.78332148314039, CurrSamplesPerSec=49.39921012189357, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      141/     200 | consumed samples:        72192 | consumed tokens:     73924608 | elapsed time per iteration (ms): 9170.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.322465E+00 | moe loss: 8.225968E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4409.81 | backward-compute: 4617.76 | backward-embedding-all-reduce: 0.01 | optimizer: 106.81 | batch-generator: 416.51
 iteration      142/     200 | consumed samples:        72704 | consumed tokens:     74448896 | elapsed time per iteration (ms): 8941.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.324036E+00 | moe loss: 8.228255E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4326.84 | backward-compute: 4484.47 | backward-embedding-all-reduce: 0.01 | optimizer: 113.80 | batch-generator: 378.88
[2022-12-22 16:48:15,767] [INFO] [logging.py:68:log_dist] [Rank 0] step=142, skipped=0, lr=[9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05, 9.999998651602953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:48:15,781] [INFO] [timer.py:197:stop] 0/142, RunningAvgSamplesPerSec=44.81393164579437, CurrSamplesPerSec=49.51864165653765, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:48:24,455] [INFO] [logging.py:68:log_dist] [Rank 0] step=143, skipped=0, lr=[9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05, 9.999998632408931e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:48:24,458] [INFO] [timer.py:197:stop] 0/143, RunningAvgSamplesPerSec=44.84400249706743, CurrSamplesPerSec=49.4935337790236, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      143/     200 | consumed samples:        73216 | consumed tokens:     74973184 | elapsed time per iteration (ms): 8695.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.312038E+00 | moe loss: 8.150917E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4031.19 | backward-compute: 4516.54 | backward-embedding-all-reduce: 0.01 | optimizer: 117.02 | batch-generator: 417.90
 iteration      144/     200 | consumed samples:        73728 | consumed tokens:     75497472 | elapsed time per iteration (ms): 9260.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.308957E+00 | moe loss: 8.218466E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4645.82 | backward-compute: 4493.21 | backward-embedding-all-reduce: 0.01 | optimizer: 107.35 | batch-generator: 386.39
[2022-12-22 16:48:33,723] [INFO] [logging.py:68:log_dist] [Rank 0] step=144, skipped=0, lr=[9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05, 9.999998613079263e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:48:33,736] [INFO] [timer.py:197:stop] 0/144, RunningAvgSamplesPerSec=44.82484127788629, CurrSamplesPerSec=42.277721008937, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:48:42,368] [INFO] [logging.py:68:log_dist] [Rank 0] step=145, skipped=0, lr=[9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05, 9.999998593613947e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:48:42,369] [INFO] [timer.py:197:stop] 0/145, RunningAvgSamplesPerSec=44.69211961869077, CurrSamplesPerSec=31.46341659527579, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      145/     200 | consumed samples:        74240 | consumed tokens:     76021760 | elapsed time per iteration (ms): 8678.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.308187E+00 | moe loss: 8.221176E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4380.09 | backward-compute: 4118.16 | backward-embedding-all-reduce: 0.01 | optimizer: 145.70 | batch-generator: 343.60
[2022-12-22 16:48:50,278] [INFO] [logging.py:68:log_dist] [Rank 0] step=146, skipped=0, lr=[9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05, 9.999998574012986e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:48:50,278] [INFO] [timer.py:197:stop] 0/146, RunningAvgSamplesPerSec=44.73137160198218, CurrSamplesPerSec=51.156258105261635, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      146/     200 | consumed samples:        74752 | consumed tokens:     76546048 | elapsed time per iteration (ms): 7879.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.306185E+00 | moe loss: 8.246098E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3783.11 | backward-compute: 3990.26 | backward-embedding-all-reduce: 0.01 | optimizer: 90.73 | batch-generator: 408.87
 iteration      147/     200 | consumed samples:        75264 | consumed tokens:     77070336 | elapsed time per iteration (ms): 9013.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.317818E+00 | moe loss: 8.211527E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4668.23 | backward-compute: 4235.58 | backward-embedding-all-reduce: 0.01 | optimizer: 97.64 | batch-generator: 354.70
[2022-12-22 16:48:59,302] [INFO] [logging.py:68:log_dist] [Rank 0] step=147, skipped=0, lr=[9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05, 9.999998554276376e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:48:59,306] [INFO] [timer.py:197:stop] 0/147, RunningAvgSamplesPerSec=44.76204759219853, CurrSamplesPerSec=49.66677441181602, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:49:07,660] [INFO] [logging.py:68:log_dist] [Rank 0] step=148, skipped=0, lr=[9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05, 9.999998534404121e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:49:07,661] [INFO] [timer.py:197:stop] 0/148, RunningAvgSamplesPerSec=44.79102815133048, CurrSamplesPerSec=49.43157623362358, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      148/     200 | consumed samples:        75776 | consumed tokens:     77594624 | elapsed time per iteration (ms): 8383.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.293228E+00 | moe loss: 8.191068E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4178.57 | backward-compute: 4075.36 | backward-embedding-all-reduce: 0.01 | optimizer: 108.58 | batch-generator: 376.74
 iteration      149/     200 | consumed samples:        76288 | consumed tokens:     78118912 | elapsed time per iteration (ms): 9525.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.302566E+00 | moe loss: 8.316280E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4769.08 | backward-compute: 4635.83 | backward-embedding-all-reduce: 0.01 | optimizer: 107.43 | batch-generator: 352.97
[2022-12-22 16:49:17,217] [INFO] [logging.py:68:log_dist] [Rank 0] step=149, skipped=0, lr=[9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05, 9.999998514396219e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:49:17,232] [INFO] [timer.py:197:stop] 0/149, RunningAvgSamplesPerSec=44.812615500123194, CurrSamplesPerSec=48.20456435491903, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:49:25,844] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05, 9.99999849425267e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:49:25,844] [INFO] [timer.py:197:stop] 0/150, RunningAvgSamplesPerSec=44.845175708419006, CurrSamplesPerSec=50.207772872380616, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      150/     200 | consumed samples:        76800 | consumed tokens:     78643200 | elapsed time per iteration (ms): 8694.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.289131E+00 | moe loss: 8.305867E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4437.03 | backward-compute: 4061.69 | backward-embedding-all-reduce: 0.01 | optimizer: 148.89 | batch-generator: 395.06
 iteration      151/     200 | consumed samples:        77312 | consumed tokens:     79167488 | elapsed time per iteration (ms): 9103.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.281848E+00 | moe loss: 8.275347E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4525.06 | backward-compute: 4465.34 | backward-embedding-all-reduce: 0.01 | optimizer: 98.31 | batch-generator: 389.08
[2022-12-22 16:49:35,015] [INFO] [logging.py:68:log_dist] [Rank 0] step=151, skipped=0, lr=[9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05, 9.999998473973474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:49:35,019] [INFO] [timer.py:197:stop] 0/151, RunningAvgSamplesPerSec=44.87416789530401, CurrSamplesPerSec=49.62207099307564, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:49:43,724] [INFO] [logging.py:68:log_dist] [Rank 0] step=152, skipped=0, lr=[9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05, 9.999998453558632e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:49:43,724] [INFO] [timer.py:197:stop] 0/152, RunningAvgSamplesPerSec=44.89707063610024, CurrSamplesPerSec=48.592331064844274, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      152/     200 | consumed samples:        77824 | consumed tokens:     79691776 | elapsed time per iteration (ms): 8740.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.282993E+00 | moe loss: 8.243316E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4027.76 | backward-compute: 4563.04 | backward-embedding-all-reduce: 0.01 | optimizer: 112.60 | batch-generator: 404.44
 iteration      153/     200 | consumed samples:        78336 | consumed tokens:     80216064 | elapsed time per iteration (ms): 8692.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.285560E+00 | moe loss: 8.328763E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4203.56 | backward-compute: 4342.69 | backward-embedding-all-reduce: 0.01 | optimizer: 133.14 | batch-generator: 293.59
[2022-12-22 16:49:52,450] [INFO] [logging.py:68:log_dist] [Rank 0] step=153, skipped=0, lr=[9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05, 9.999998433008142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:49:52,462] [INFO] [timer.py:197:stop] 0/153, RunningAvgSamplesPerSec=44.731606744406754, CurrSamplesPerSec=28.806862940262093, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:50:00,393] [INFO] [logging.py:68:log_dist] [Rank 0] step=154, skipped=0, lr=[9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05, 9.999998412322005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:50:00,394] [INFO] [timer.py:197:stop] 0/154, RunningAvgSamplesPerSec=44.76798277894444, CurrSamplesPerSec=51.03475380314594, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      154/     200 | consumed samples:        78848 | consumed tokens:     80740352 | elapsed time per iteration (ms): 7961.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.271526E+00 | moe loss: 8.303974E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3839.76 | backward-compute: 3994.20 | backward-embedding-all-reduce: 0.01 | optimizer: 92.13 | batch-generator: 323.51
[2022-12-22 16:50:09,247] [INFO] [logging.py:68:log_dist] [Rank 0] step=155, skipped=0, lr=[9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05, 9.999998391500224e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:50:09,248] [INFO] [timer.py:197:stop] 0/155, RunningAvgSamplesPerSec=44.79502765175061, CurrSamplesPerSec=49.324225359152, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      155/     200 | consumed samples:        79360 | consumed tokens:     81264640 | elapsed time per iteration (ms): 8856.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.266053E+00 | moe loss: 8.203712E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4373.20 | backward-compute: 4253.86 | backward-embedding-all-reduce: 0.01 | optimizer: 103.86 | batch-generator: 376.48
 iteration      156/     200 | consumed samples:        79872 | consumed tokens:     81788928 | elapsed time per iteration (ms): 9033.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.265741E+00 | moe loss: 8.203365E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4555.92 | backward-compute: 4357.53 | backward-embedding-all-reduce: 0.01 | optimizer: 106.82 | batch-generator: 330.74
[2022-12-22 16:50:18,293] [INFO] [logging.py:68:log_dist] [Rank 0] step=156, skipped=0, lr=[9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05, 9.999998370542794e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:50:18,301] [INFO] [timer.py:197:stop] 0/156, RunningAvgSamplesPerSec=44.81528499003052, CurrSamplesPerSec=48.146549932085385, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:50:26,767] [INFO] [logging.py:68:log_dist] [Rank 0] step=157, skipped=0, lr=[9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05, 9.999998349449718e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:50:26,767] [INFO] [timer.py:197:stop] 0/157, RunningAvgSamplesPerSec=44.836773351678275, CurrSamplesPerSec=48.411531676329055, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      157/     200 | consumed samples:        80384 | consumed tokens:     82313216 | elapsed time per iteration (ms): 8484.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.257594E+00 | moe loss: 8.177209E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4276.76 | backward-compute: 4081.31 | backward-embedding-all-reduce: 0.01 | optimizer: 100.26 | batch-generator: 379.61
 iteration      158/     200 | consumed samples:        80896 | consumed tokens:     82837504 | elapsed time per iteration (ms): 8806.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.256624E+00 | moe loss: 8.196013E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4414.66 | backward-compute: 4254.62 | backward-embedding-all-reduce: 0.01 | optimizer: 107.12 | batch-generator: 346.17
[2022-12-22 16:50:35,584] [INFO] [logging.py:68:log_dist] [Rank 0] step=158, skipped=0, lr=[9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05, 9.999998328220994e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:50:35,593] [INFO] [timer.py:197:stop] 0/158, RunningAvgSamplesPerSec=44.86207699860118, CurrSamplesPerSec=49.16253525764707, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:50:43,916] [INFO] [logging.py:68:log_dist] [Rank 0] step=159, skipped=0, lr=[9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05, 9.999998306856624e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:50:43,917] [INFO] [timer.py:197:stop] 0/159, RunningAvgSamplesPerSec=44.879395587823005, CurrSamplesPerSec=47.75533427253984, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      159/     200 | consumed samples:        81408 | consumed tokens:     83361792 | elapsed time per iteration (ms): 8345.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.265200E+00 | moe loss: 8.215969E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4163.36 | backward-compute: 4042.06 | backward-embedding-all-reduce: 0.01 | optimizer: 110.07 | batch-generator: 341.64
[2022-12-22 16:50:53,067] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05, 9.999998285356607e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:50:53,070] [INFO] [timer.py:197:stop] 0/160, RunningAvgSamplesPerSec=44.63102725550202, CurrSamplesPerSec=23.881441988794464, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      160/     200 | consumed samples:        81920 | consumed tokens:     83886080 | elapsed time per iteration (ms): 9146.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.260103E+00 | moe loss: 8.156686E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4939.34 | backward-compute: 4074.67 | backward-embedding-all-reduce: 0.01 | optimizer: 110.91 | batch-generator: 315.01
[2022-12-22 16:51:01,027] [INFO] [logging.py:68:log_dist] [Rank 0] step=161, skipped=0, lr=[9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05, 9.999998263720944e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:51:01,027] [INFO] [timer.py:197:stop] 0/161, RunningAvgSamplesPerSec=44.63496560848552, CurrSamplesPerSec=45.266079477006386, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      161/     200 | consumed samples:        82432 | consumed tokens:     84410368 | elapsed time per iteration (ms): 7973.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.255320E+00 | moe loss: 8.201417E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3693.89 | backward-compute: 4119.82 | backward-embedding-all-reduce: 0.01 | optimizer: 125.95 | batch-generator: 326.14
 iteration      162/     200 | consumed samples:        82944 | consumed tokens:     84934656 | elapsed time per iteration (ms): 8819.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.255963E+00 | moe loss: 8.158282E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4586.25 | backward-compute: 4106.96 | backward-embedding-all-reduce: 0.01 | optimizer: 107.79 | batch-generator: 359.44
[2022-12-22 16:51:09,877] [INFO] [logging.py:68:log_dist] [Rank 0] step=162, skipped=0, lr=[9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05, 9.999998241949633e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:51:09,879] [INFO] [timer.py:197:stop] 0/162, RunningAvgSamplesPerSec=44.48698505681876, CurrSamplesPerSec=29.130901036278484, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:51:17,671] [INFO] [logging.py:68:log_dist] [Rank 0] step=163, skipped=0, lr=[9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05, 9.999998220042677e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:51:17,671] [INFO] [timer.py:197:stop] 0/163, RunningAvgSamplesPerSec=44.52271373062078, CurrSamplesPerSec=51.087464689312576, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      163/     200 | consumed samples:        83456 | consumed tokens:     85458944 | elapsed time per iteration (ms): 7810.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.235006E+00 | moe loss: 8.191096E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3703.82 | backward-compute: 3986.24 | backward-embedding-all-reduce: 0.01 | optimizer: 90.94 | batch-generator: 328.37
[2022-12-22 16:51:27,191] [INFO] [logging.py:68:log_dist] [Rank 0] step=164, skipped=0, lr=[9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05, 9.999998198000073e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:51:27,195] [INFO] [timer.py:197:stop] 0/164, RunningAvgSamplesPerSec=44.5430209442627, CurrSamplesPerSec=48.073207758965125, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      164/     200 | consumed samples:        83968 | consumed tokens:     85983232 | elapsed time per iteration (ms): 9519.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.243438E+00 | moe loss: 8.225902E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5297.89 | backward-compute: 4084.37 | backward-embedding-all-reduce: 0.01 | optimizer: 112.00 | batch-generator: 351.09
[2022-12-22 16:51:35,196] [INFO] [logging.py:68:log_dist] [Rank 0] step=165, skipped=0, lr=[9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05, 9.999998175821824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:51:35,197] [INFO] [timer.py:197:stop] 0/165, RunningAvgSamplesPerSec=44.5778733177726, CurrSamplesPerSec=51.048574175723864, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      165/     200 | consumed samples:        84480 | consumed tokens:     86507520 | elapsed time per iteration (ms): 8005.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.232895E+00 | moe loss: 8.256528E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3761.42 | backward-compute: 4134.89 | backward-embedding-all-reduce: 0.01 | optimizer: 92.25 | batch-generator: 334.92
 iteration      166/     200 | consumed samples:        84992 | consumed tokens:     87031808 | elapsed time per iteration (ms): 9541.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.239403E+00 | moe loss: 8.216251E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4439.03 | backward-compute: 4961.08 | backward-embedding-all-reduce: 0.01 | optimizer: 115.71 | batch-generator: 411.95
[2022-12-22 16:51:44,748] [INFO] [logging.py:68:log_dist] [Rank 0] step=166, skipped=0, lr=[9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05, 9.999998153507925e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:51:44,762] [INFO] [timer.py:197:stop] 0/166, RunningAvgSamplesPerSec=44.5968139488597, CurrSamplesPerSec=47.91527470711105, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      167/     200 | consumed samples:        85504 | consumed tokens:     87556096 | elapsed time per iteration (ms): 8009.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.246531E+00 | moe loss: 8.343141E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3888.20 | backward-compute: 3993.53 | backward-embedding-all-reduce: 0.01 | optimizer: 92.35 | batch-generator: 319.58
[2022-12-22 16:51:52,752] [INFO] [logging.py:68:log_dist] [Rank 0] step=167, skipped=0, lr=[9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05, 9.99999813105838e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:51:52,754] [INFO] [timer.py:197:stop] 0/167, RunningAvgSamplesPerSec=44.60366976735458, CurrSamplesPerSec=45.75728116283145, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:52:00,520] [INFO] [logging.py:68:log_dist] [Rank 0] step=168, skipped=0, lr=[9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05, 9.999998108473192e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:52:00,521] [INFO] [timer.py:197:stop] 0/168, RunningAvgSamplesPerSec=44.63772040117557, CurrSamplesPerSec=51.07066931219901, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      168/     200 | consumed samples:        86016 | consumed tokens:     88080384 | elapsed time per iteration (ms): 7773.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.234637E+00 | moe loss: 8.161747E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3685.73 | backward-compute: 3980.10 | backward-embedding-all-reduce: 0.01 | optimizer: 92.40 | batch-generator: 315.03
[2022-12-22 16:52:09,845] [INFO] [logging.py:68:log_dist] [Rank 0] step=169, skipped=0, lr=[9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05, 9.999998085752354e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:52:09,846] [INFO] [timer.py:197:stop] 0/169, RunningAvgSamplesPerSec=44.662725211030796, CurrSamplesPerSec=49.24163516400949, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      169/     200 | consumed samples:        86528 | consumed tokens:     88604672 | elapsed time per iteration (ms): 9330.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.217963E+00 | moe loss: 8.117568E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5108.47 | backward-compute: 4075.47 | backward-embedding-all-reduce: 0.01 | optimizer: 119.58 | batch-generator: 379.61
[2022-12-22 16:52:17,837] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05, 9.99999806289587e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:52:17,840] [INFO] [timer.py:197:stop] 0/170, RunningAvgSamplesPerSec=44.69562368429059, CurrSamplesPerSec=50.96491252052377, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      170/     200 | consumed samples:        87040 | consumed tokens:     89128960 | elapsed time per iteration (ms): 7986.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.216280E+00 | moe loss: 8.122557E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3813.58 | backward-compute: 4059.31 | backward-embedding-all-reduce: 0.01 | optimizer: 92.01 | batch-generator: 313.60
 iteration      171/     200 | consumed samples:        87552 | consumed tokens:     89653248 | elapsed time per iteration (ms): 9111.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.217241E+00 | moe loss: 8.125880E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4760.79 | backward-compute: 4205.01 | backward-embedding-all-reduce: 0.01 | optimizer: 114.74 | batch-generator: 461.20
[2022-12-22 16:52:26,956] [INFO] [logging.py:68:log_dist] [Rank 0] step=171, skipped=0, lr=[9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05, 9.999998039903739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:52:26,966] [INFO] [timer.py:197:stop] 0/171, RunningAvgSamplesPerSec=44.71796547061084, CurrSamplesPerSec=48.81753256330392, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:52:35,632] [INFO] [logging.py:68:log_dist] [Rank 0] step=172, skipped=0, lr=[9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05, 9.999998016775961e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:52:35,635] [INFO] [timer.py:197:stop] 0/172, RunningAvgSamplesPerSec=44.74717353286825, CurrSamplesPerSec=50.29944378111914, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      172/     200 | consumed samples:        88064 | consumed tokens:     90177536 | elapsed time per iteration (ms): 8683.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.231765E+00 | moe loss: 8.158636E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4188.11 | backward-compute: 4370.36 | backward-embedding-all-reduce: 0.01 | optimizer: 96.13 | batch-generator: 340.87
[2022-12-22 16:52:44,583] [INFO] [logging.py:68:log_dist] [Rank 0] step=173, skipped=0, lr=[9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05, 9.999997993512536e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:52:44,584] [INFO] [timer.py:197:stop] 0/173, RunningAvgSamplesPerSec=44.764275538900414, CurrSamplesPerSec=47.874828496456324, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      173/     200 | consumed samples:        88576 | consumed tokens:     90701824 | elapsed time per iteration (ms): 8952.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.219771E+00 | moe loss: 8.291240E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4490.04 | backward-compute: 4322.40 | backward-embedding-all-reduce: 0.01 | optimizer: 124.58 | batch-generator: 360.19
[2022-12-22 16:52:53,150] [INFO] [logging.py:68:log_dist] [Rank 0] step=174, skipped=0, lr=[9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05, 9.999997970113465e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:52:53,151] [INFO] [timer.py:197:stop] 0/174, RunningAvgSamplesPerSec=44.72208295077865, CurrSamplesPerSec=38.51446985751242, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      174/     200 | consumed samples:        89088 | consumed tokens:     91226112 | elapsed time per iteration (ms): 8566.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.202196E+00 | moe loss: 8.304948E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4102.60 | backward-compute: 4346.36 | backward-embedding-all-reduce: 0.01 | optimizer: 94.76 | batch-generator: 348.49
[2022-12-22 16:53:01,302] [INFO] [logging.py:68:log_dist] [Rank 0] step=175, skipped=0, lr=[9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05, 9.999997946578748e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:53:01,302] [INFO] [timer.py:197:stop] 0/175, RunningAvgSamplesPerSec=44.69588150408261, CurrSamplesPerSec=40.60419287287847, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      175/     200 | consumed samples:        89600 | consumed tokens:     91750400 | elapsed time per iteration (ms): 8188.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.211196E+00 | moe loss: 8.278698E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3732.82 | backward-compute: 4252.28 | backward-embedding-all-reduce: 0.01 | optimizer: 162.39 | batch-generator: 315.30
 iteration      176/     200 | consumed samples:        90112 | consumed tokens:     92274688 | elapsed time per iteration (ms): 8539.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.208113E+00 | moe loss: 8.197989E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4375.94 | backward-compute: 4042.80 | backward-embedding-all-reduce: 0.01 | optimizer: 105.17 | batch-generator: 378.83
[2022-12-22 16:53:09,898] [INFO] [logging.py:68:log_dist] [Rank 0] step=176, skipped=0, lr=[9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05, 9.999997922908382e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:53:09,912] [INFO] [timer.py:197:stop] 0/176, RunningAvgSamplesPerSec=44.715851393622415, CurrSamplesPerSec=48.46172507813963, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:53:18,737] [INFO] [logging.py:68:log_dist] [Rank 0] step=177, skipped=0, lr=[9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05, 9.999997899102371e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:53:18,739] [INFO] [timer.py:197:stop] 0/177, RunningAvgSamplesPerSec=44.746872988680835, CurrSamplesPerSec=50.88991674796477, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      177/     200 | consumed samples:        90624 | consumed tokens:     92798976 | elapsed time per iteration (ms): 8859.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.203690E+00 | moe loss: 8.230150E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4189.71 | backward-compute: 4532.97 | backward-embedding-all-reduce: 0.01 | optimizer: 91.96 | batch-generator: 341.37
 iteration      178/     200 | consumed samples:        91136 | consumed tokens:     93323264 | elapsed time per iteration (ms): 8893.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.196485E+00 | moe loss: 8.282449E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4532.83 | backward-compute: 4231.45 | backward-embedding-all-reduce: 0.01 | optimizer: 114.67 | batch-generator: 400.81
[2022-12-22 16:53:27,641] [INFO] [logging.py:68:log_dist] [Rank 0] step=178, skipped=0, lr=[9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05, 9.999997875160712e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:53:27,653] [INFO] [timer.py:197:stop] 0/178, RunningAvgSamplesPerSec=44.767136026253844, CurrSamplesPerSec=48.62010777241188, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:53:36,231] [INFO] [logging.py:68:log_dist] [Rank 0] step=179, skipped=0, lr=[9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05, 9.999997851083408e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:53:36,234] [INFO] [timer.py:197:stop] 0/179, RunningAvgSamplesPerSec=44.795975715391, CurrSamplesPerSec=50.524547564818256, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      179/     200 | consumed samples:        91648 | consumed tokens:     93847552 | elapsed time per iteration (ms): 8600.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.199808E+00 | moe loss: 8.194942E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4421.19 | backward-compute: 4052.98 | backward-embedding-all-reduce: 0.01 | optimizer: 93.63 | batch-generator: 306.50
 iteration      180/     200 | consumed samples:        92160 | consumed tokens:     94371840 | elapsed time per iteration (ms): 9436.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.178241E+00 | moe loss: 8.159885E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4383.75 | backward-compute: 4915.71 | backward-embedding-all-reduce: 0.01 | optimizer: 118.71 | batch-generator: 445.16
[2022-12-22 16:53:45,678] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05, 9.999997826870456e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:53:45,692] [INFO] [timer.py:197:stop] 0/180, RunningAvgSamplesPerSec=44.808639249546964, CurrSamplesPerSec=47.16881393482145, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:53:53,754] [INFO] [logging.py:68:log_dist] [Rank 0] step=181, skipped=0, lr=[9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05, 9.999997802521857e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:53:53,756] [INFO] [timer.py:197:stop] 0/181, RunningAvgSamplesPerSec=44.83400852756479, CurrSamplesPerSec=49.858673046644206, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      181/     200 | consumed samples:        92672 | consumed tokens:     94896128 | elapsed time per iteration (ms): 8083.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.186800E+00 | moe loss: 8.194114E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3821.08 | backward-compute: 4142.49 | backward-embedding-all-reduce: 0.01 | optimizer: 92.05 | batch-generator: 328.06
 iteration      182/     200 | consumed samples:        93184 | consumed tokens:     95420416 | elapsed time per iteration (ms): 9195.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.188232E+00 | moe loss: 8.201621E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3741.57 | backward-compute: 5277.29 | backward-embedding-all-reduce: 0.01 | optimizer: 146.44 | batch-generator: 285.82
[2022-12-22 16:54:02,953] [INFO] [logging.py:68:log_dist] [Rank 0] step=182, skipped=0, lr=[9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05, 9.999997778037612e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:54:02,971] [INFO] [timer.py:197:stop] 0/182, RunningAvgSamplesPerSec=44.62333879152977, CurrSamplesPerSec=24.237325390608213, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:54:11,276] [INFO] [logging.py:68:log_dist] [Rank 0] step=183, skipped=0, lr=[9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05, 9.99999775341772e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:54:11,277] [INFO] [timer.py:197:stop] 0/183, RunningAvgSamplesPerSec=44.62015226092947, CurrSamplesPerSec=44.05389620342991, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      183/     200 | consumed samples:        93696 | consumed tokens:     95944704 | elapsed time per iteration (ms): 8327.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.188609E+00 | moe loss: 8.189820E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4010.32 | backward-compute: 4148.82 | backward-embedding-all-reduce: 0.01 | optimizer: 135.76 | batch-generator: 377.74
[2022-12-22 16:54:19,463] [INFO] [logging.py:68:log_dist] [Rank 0] step=184, skipped=0, lr=[9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05, 9.99999772866218e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      184/     200 | consumed samples:        94208 | consumed tokens:     96468992 | elapsed time per iteration (ms): 8185.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.173141E+00 | moe loss: 8.206584E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3951.92 | backward-compute: 4103.68 | backward-embedding-all-reduce: 0.01 | optimizer: 92.52 | batch-generator: 282.47
[2022-12-22 16:54:19,467] [INFO] [timer.py:197:stop] 0/184, RunningAvgSamplesPerSec=44.65032714189876, CurrSamplesPerSec=50.877956416769614, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:54:28,769] [INFO] [logging.py:68:log_dist] [Rank 0] step=185, skipped=0, lr=[9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05, 9.999997703770995e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:54:28,769] [INFO] [timer.py:197:stop] 0/185, RunningAvgSamplesPerSec=44.66565506432882, CurrSamplesPerSec=47.642268729022824, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      185/     200 | consumed samples:        94720 | consumed tokens:     96993280 | elapsed time per iteration (ms): 9307.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.177759E+00 | moe loss: 8.173013E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4700.60 | backward-compute: 4478.91 | backward-embedding-all-reduce: 0.01 | optimizer: 107.15 | batch-generator: 410.27
[2022-12-22 16:54:37,112] [INFO] [logging.py:68:log_dist] [Rank 0] step=186, skipped=0, lr=[9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05, 9.999997678744164e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:54:37,115] [INFO] [timer.py:197:stop] 0/186, RunningAvgSamplesPerSec=44.6954941178454, CurrSamplesPerSec=50.92075015858467, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      186/     200 | consumed samples:        95232 | consumed tokens:     97517568 | elapsed time per iteration (ms): 8345.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.169830E+00 | moe loss: 8.120585E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4178.36 | backward-compute: 4049.93 | backward-embedding-all-reduce: 0.01 | optimizer: 92.93 | batch-generator: 312.59
 iteration      187/     200 | consumed samples:        95744 | consumed tokens:     98041856 | elapsed time per iteration (ms): 9506.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.174050E+00 | moe loss: 8.167820E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4751.47 | backward-compute: 4621.08 | backward-embedding-all-reduce: 0.01 | optimizer: 122.39 | batch-generator: 336.00
[2022-12-22 16:54:46,635] [INFO] [logging.py:68:log_dist] [Rank 0] step=187, skipped=0, lr=[9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05, 9.999997653581685e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:54:46,647] [INFO] [timer.py:197:stop] 0/187, RunningAvgSamplesPerSec=44.71040457655372, CurrSamplesPerSec=47.63432178057852, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:54:54,796] [INFO] [logging.py:68:log_dist] [Rank 0] step=188, skipped=0, lr=[9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05, 9.99999762828356e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:54:54,798] [INFO] [timer.py:197:stop] 0/188, RunningAvgSamplesPerSec=44.74041377506021, CurrSamplesPerSec=51.08345923311476, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      188/     200 | consumed samples:        96256 | consumed tokens:     98566144 | elapsed time per iteration (ms): 8176.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.165527E+00 | moe loss: 8.178090E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3798.43 | backward-compute: 4248.70 | backward-embedding-all-reduce: 0.01 | optimizer: 93.18 | batch-generator: 314.89
 iteration      189/     200 | consumed samples:        96768 | consumed tokens:     99090432 | elapsed time per iteration (ms): 8981.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.172896E+00 | moe loss: 8.165125E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4143.48 | backward-compute: 4721.10 | backward-embedding-all-reduce: 0.01 | optimizer: 108.13 | batch-generator: 349.21
[2022-12-22 16:55:03,783] [INFO] [logging.py:68:log_dist] [Rank 0] step=189, skipped=0, lr=[9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05, 9.999997602849787e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:55:03,790] [INFO] [timer.py:197:stop] 0/189, RunningAvgSamplesPerSec=44.714376770600694, CurrSamplesPerSec=40.34704941800606, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:55:11,731] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05, 9.999997577280368e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:55:11,732] [INFO] [timer.py:197:stop] 0/190, RunningAvgSamplesPerSec=44.71221686662661, CurrSamplesPerSec=44.311949918923474, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      190/     200 | consumed samples:        97280 | consumed tokens:     99614720 | elapsed time per iteration (ms): 7952.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.167946E+00 | moe loss: 8.174754E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3668.29 | backward-compute: 4177.35 | backward-embedding-all-reduce: 0.01 | optimizer: 90.92 | batch-generator: 299.79
[2022-12-22 16:55:19,880] [INFO] [logging.py:68:log_dist] [Rank 0] step=191, skipped=0, lr=[9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05, 9.999997551575302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:55:19,883] [INFO] [timer.py:197:stop] 0/191, RunningAvgSamplesPerSec=44.74013210364916, CurrSamplesPerSec=50.689812712285125, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      191/     200 | consumed samples:        97792 | consumed tokens:    100139008 | elapsed time per iteration (ms): 8148.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.162812E+00 | moe loss: 8.203459E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4017.00 | backward-compute: 4025.13 | backward-embedding-all-reduce: 0.01 | optimizer: 94.97 | batch-generator: 289.28
 iteration      192/     200 | consumed samples:        98304 | consumed tokens:    100663296 | elapsed time per iteration (ms): 9358.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.155947E+00 | moe loss: 8.179876E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5178.59 | backward-compute: 4023.19 | backward-embedding-all-reduce: 0.01 | optimizer: 146.75 | batch-generator: 353.92
[2022-12-22 16:55:29,242] [INFO] [logging.py:68:log_dist] [Rank 0] step=192, skipped=0, lr=[9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05, 9.99999752573459e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:55:29,254] [INFO] [timer.py:197:stop] 0/192, RunningAvgSamplesPerSec=44.73492900303464, CurrSamplesPerSec=43.77280479597865, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:55:37,781] [INFO] [logging.py:68:log_dist] [Rank 0] step=193, skipped=0, lr=[9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05, 9.999997499758229e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:55:37,784] [INFO] [timer.py:197:stop] 0/193, RunningAvgSamplesPerSec=44.76225554375894, CurrSamplesPerSec=50.63960962093411, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      193/     200 | consumed samples:        98816 | consumed tokens:    101187584 | elapsed time per iteration (ms): 8541.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.147356E+00 | moe loss: 8.140081E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4076.53 | backward-compute: 4342.68 | backward-embedding-all-reduce: 0.01 | optimizer: 95.25 | batch-generator: 292.32
[2022-12-22 16:55:46,758] [INFO] [logging.py:68:log_dist] [Rank 0] step=194, skipped=0, lr=[9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05, 9.999997473646224e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      194/     200 | consumed samples:        99328 | consumed tokens:    101711872 | elapsed time per iteration (ms): 8975.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.141164E+00 | moe loss: 8.171094E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4522.43 | backward-compute: 4319.37 | backward-embedding-all-reduce: 0.01 | optimizer: 117.31 | batch-generator: 397.31
[2022-12-22 16:55:46,770] [INFO] [timer.py:197:stop] 0/194, RunningAvgSamplesPerSec=44.78128790339898, CurrSamplesPerSec=48.73946080287111, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:55:55,124] [INFO] [logging.py:68:log_dist] [Rank 0] step=195, skipped=0, lr=[9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05, 9.999997447398572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:55:55,127] [INFO] [timer.py:197:stop] 0/195, RunningAvgSamplesPerSec=44.80845191319087, CurrSamplesPerSec=50.71501867460467, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      195/     200 | consumed samples:        99840 | consumed tokens:    102236160 | elapsed time per iteration (ms): 8369.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.150433E+00 | moe loss: 8.129055E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3992.74 | backward-compute: 4255.99 | backward-embedding-all-reduce: 0.01 | optimizer: 94.74 | batch-generator: 306.39
 iteration      196/     200 | consumed samples:       100352 | consumed tokens:    102760448 | elapsed time per iteration (ms): 9227.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.137091E+00 | moe loss: 8.156095E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4771.17 | backward-compute: 4329.44 | backward-embedding-all-reduce: 0.01 | optimizer: 114.56 | batch-generator: 363.83
[2022-12-22 16:56:04,362] [INFO] [logging.py:68:log_dist] [Rank 0] step=196, skipped=0, lr=[9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05, 9.999997421015272e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:56:04,374] [INFO] [timer.py:197:stop] 0/196, RunningAvgSamplesPerSec=44.81045375360534, CurrSamplesPerSec=45.200186636622504, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      197/     200 | consumed samples:       100864 | consumed tokens:    103284736 | elapsed time per iteration (ms): 8280.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.160548E+00 | moe loss: 8.105880E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3922.72 | backward-compute: 4176.93 | backward-embedding-all-reduce: 0.01 | optimizer: 150.99 | batch-generator: 293.72
[2022-12-22 16:56:12,660] [INFO] [logging.py:68:log_dist] [Rank 0] step=197, skipped=0, lr=[9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05, 9.999997394496326e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:56:12,663] [INFO] [timer.py:197:stop] 0/197, RunningAvgSamplesPerSec=44.758231021010694, CurrSamplesPerSec=36.50483985617034, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:56:20,705] [INFO] [logging.py:68:log_dist] [Rank 0] step=198, skipped=0, lr=[9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05, 9.999997367841731e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:56:20,706] [INFO] [timer.py:197:stop] 0/198, RunningAvgSamplesPerSec=44.78509289073347, CurrSamplesPerSec=50.72097906524362, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      198/     200 | consumed samples:       101376 | consumed tokens:    103809024 | elapsed time per iteration (ms): 8070.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.140553E+00 | moe loss: 8.145016E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3892.22 | backward-compute: 4035.29 | backward-embedding-all-reduce: 0.01 | optimizer: 91.21 | batch-generator: 287.16
[2022-12-22 16:56:29,940] [INFO] [logging.py:68:log_dist] [Rank 0] step=199, skipped=0, lr=[9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05, 9.999997341051493e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      199/     200 | consumed samples:       101888 | consumed tokens:    104333312 | elapsed time per iteration (ms): 9234.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.148950E+00 | moe loss: 8.118196E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4778.80 | backward-compute: 4317.38 | backward-embedding-all-reduce: 0.01 | optimizer: 121.43 | batch-generator: 344.23
[2022-12-22 16:56:29,955] [INFO] [timer.py:197:stop] 0/199, RunningAvgSamplesPerSec=44.80259254702273, CurrSamplesPerSec=48.518449848689066, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
[2022-12-22 16:56:38,506] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05, 9.999997314125606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-22 16:56:38,508] [INFO] [timer.py:197:stop] 0/200, RunningAvgSamplesPerSec=44.82724571935441, CurrSamplesPerSec=50.27739868681692, MemAllocated=18.14GB, MaxMemAllocated=30.35GB
 iteration      200/     200 | consumed samples:       102400 | consumed tokens:    104857600 | elapsed time per iteration (ms): 8564.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 6.140750E+00 | moe loss: 8.196344E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4383.01 | backward-compute: 4062.44 | backward-embedding-all-reduce: 0.01 | optimizer: 94.96 | batch-generator: 268.80
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3191251754760742 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.31859254837036133 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.32209038734436035 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3189971446990967 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.31928300857543945 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.31933116912841797 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3188185691833496 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.31862592697143555 seconds
[after training is done] datetime: 2022-12-22 16:56:38 
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24039721488952637 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24436545372009277 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2403888702392578 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24053359031677246 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24045133590698242 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2405376434326172 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.24032926559448242 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3441760540008545 seconds
Loading extension module utils...
Time to load utils op: 0.37108540534973145 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.35430431365966797 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3472104072570801 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.34309887886047363 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.35449767112731934 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.36678004264831543 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.33721923828125 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3374180793762207 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.33765745162963867 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.34002184867858887 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3376145362854004 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3378574848175049 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.34203243255615234 seconds
No existing process group found, creating a new group named: ep_size_32
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3376586437225342 seconds
