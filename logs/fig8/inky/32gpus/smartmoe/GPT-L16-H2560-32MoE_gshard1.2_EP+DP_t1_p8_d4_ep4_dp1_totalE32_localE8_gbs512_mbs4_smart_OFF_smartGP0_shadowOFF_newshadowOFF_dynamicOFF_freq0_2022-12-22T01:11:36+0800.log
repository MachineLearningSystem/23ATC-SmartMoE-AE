+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 6 == 0 ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 7 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 4 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 2 == 0 ']'
+ '[' 5 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 3 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 0 == 0 ']'
+ '[' 1 == 0 ']'
+ sleep 10s
+ hostname
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 8 == 0 ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 16 == 0 ']'
+ sleep 10s
+ '[' 17 == 0 ']'
+ sleep 10s
+ '[' 20 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 23 == 0 ']'
+ '[' 22 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 18 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 21 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 19 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 29 == 0 ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 30 == 0 ']'
+ sleep 10s
+ '[' sh-lab == sh-lab ']'
+ '[' 31 == 0 ']'
+ sleep 10s
+ '[' sh-lab == sh-lab ']'
+ '[' 26 == 0 ']'
+ sleep 10s
+ '[' 28 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 25 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 24 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 27 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 11 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 9 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 10 == 0 ']'
+ sleep 10s
+ '[' 12 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 14 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 15 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 13 == 0 ']'
+ sleep 10s
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
++ cat /mnt/cache/zhaishuming/master_node
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ export NODE_RANK=0
+ NODE_RANK=0
+ export NODE_RANK=1
+ NODE_RANK=1
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == nico ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ python_args=
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=102400
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ true
+ EXEC=./pretrain_gpt.py
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=22
+ RANK=22
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=16
+ RANK=16
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export RANK=23
+ RANK=23
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=21
+ RANK=21
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=19
+ RANK=19
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export RANK=20
+ RANK=20
+ export RANK=18
+ RANK=18
+ export WORLD_SIZE=32
+ localrank=3
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ WORLD_SIZE=32
+ localrank=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export RANK=17
+ RANK=17
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export NNODES=4
+ NNODES=4
+ '[' sh-lab == nico ']'
+ export NNODES=4
+ NNODES=4
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=5
+ NODE_RANK=5
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=30
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=24
+ RANK=24
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=25
+ export RANK=26
+ RANK=26
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ RANK=30
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=31
+ RANK=31
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ RANK=25
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export RANK=27
+ RANK=27
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=29
+ RANK=29
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=28
+ RANK=28
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MAX_JOBS=64
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=8
+ RANK=8
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=32
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ localrank=2
+ export RANK=14
+ RANK=14
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=12
+ RANK=12
+ export RANK=9
+ RANK=9
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ localrank=7
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export NNODES=4
+ NNODES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ export NNODES=4
+ NNODES=4
+ '[' sh-lab == nico ']'
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export NODE_RANK=2
+ NODE_RANK=2
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=102400
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof/table
+ python_args=
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 8         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='  --expert-ep-size 4                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_gshard1.2_EP+DP_t1_p8_d4_ep4_dp1_totalE32_localE8_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-22T01:11:36+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 8 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
using world size: 32, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 4
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 4
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2560
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 8
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 8
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 102400
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 32
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 32
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 8
[INFO] 0 in EP group [0, 1, 2, 3]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.204 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 1 in EP group [0, 1, 2, 3]
[INFO] 1 in DP group [1]
[INFO] 2 in EP group [0, 1, 2, 3]
[INFO] 2 in DP group [2]
[INFO] 31 in EP group [28, 29, 30, 31]
[INFO] 31 in DP group [31]
[INFO] 17 in EP group [16, 17, 18, 19]
[INFO] 17 in DP group [17]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 8.077 seconds
time to initialize megatron (seconds): -19.297
[after megatron is initialized] datetime: 2022-12-22 01:12:12 
[INFO] 4 in EP group [4, 5, 6, 7]
[INFO] 4 in DP group [4]
hhs=5120
hhs=5120
[INFO] 6 in EP group [4, 5, 6, 7]
[INFO] 6 in DP group [6]
hhs=5120
hhs=5120
[INFO] 3 in EP group [0, 1, 2, 3]
[INFO] 3 in DP group [3]
hhs=5120
[INFO] 7 in EP group [4, 5, 6, 7]
[INFO] 7 in DP group [7]
hhs=5120
[INFO] 5 in EP group [4, 5, 6, 7]
[INFO] 5 in DP group [5]
hhs=5120
hhs=5120
building GPT model ...
hhs=5120
[INFO] 24 in EP group [24, 25, 26, 27]
[INFO] 24 in DP group [24]
hhs=5120
[INFO] 25 in EP group [24, 25, 26, 27]
[INFO] 25 in DP group [25]
hhs=5120
[INFO] 26 in EP group [24, 25, 26, 27]
[INFO] 26 in DP group [26]
hhs=5120
[INFO] 27 in EP group [24, 25, 26, 27]
[INFO] 27 in DP group [27]
hhs=5120
[INFO] 30 in EP group [28, 29, 30, 31]
[INFO] 30 in DP group [30]
hhs=5120
[INFO] 28 in EP group [28, 29, 30, 31]
[INFO] 28 in DP group [28]
hhs=5120
[INFO] 29 in EP group [28, 29, 30, 31]
[INFO] 29 in DP group [29]
hhs=5120
hhs=5120
[INFO] 23 in EP group [20, 21, 22, 23]
[INFO] 23 in DP group [23]
hhs=5120
[INFO] 18 in EP group [16, 17, 18, 19]
[INFO] 18 in DP group [18]
hhs=5120
[INFO] 19 in EP group [16, 17, 18, 19]
[INFO] 19 in DP group [19]
hhs=5120
[INFO] 20 in EP group [20, 21, 22, 23]
[INFO] 20 in DP group [20]
hhs=5120
[INFO] 21 in EP group [20, 21, 22, 23]
[INFO] 21 in DP group [21]
hhs=5120
[INFO] 22 in EP group [20, 21, 22, 23]
[INFO] 22 in DP group [22]
hhs=5120
[INFO] 16 in EP group [16, 17, 18, 19]
[INFO] 16 in DP group [16]
hhs=5120
[INFO] 11 in EP group [8, 9, 10, 11]
[INFO] 11 in DP group [11]
hhs=5120
[INFO] 8 in EP group [8, 9, 10, 11]
[INFO] 8 in DP group [8]
hhs=5120
[INFO] 9 in EP group [8, 9, 10, 11]
[INFO] 9 in DP group [9]
hhs=5120
[INFO] 10 in EP group [8, 9, 10, 11]
[INFO] 10 in DP group [10]
hhs=5120
[INFO] 12 in EP group [12, 13, 14, 15]
[INFO] 12 in DP group [12]
hhs=5120
[INFO] 13 in EP group [12, 13, 14, 15]
[INFO] 13 in DP group [13]
hhs=5120
[INFO] 14 in EP group [12, 13, 14, 15]
[INFO] 14 in DP group [14]
hhs=5120
[INFO] 15 in EP group [12, 13, 14, 15]
[INFO] 15 in DP group [15]
hhs=5120
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 472186944
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 600970304
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 472186944
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 472186944
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 472186944
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 603586624
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 472186944
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 472186944
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-22 01:12:23 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: -512
    test:       -512
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.137112 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.036 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-22 01:12:39 
done with setup ...
training ...
[before the start of training step] datetime: 2022-12-22 01:12:39 
[Rank 8] (after 1 iterations) memory (MB) | allocated: 9053.7421875 | max allocated: 12658.5224609375 | reserved: 13998.0 | max reserved: 13998.0
[Rank 12] (after 1 iterations) memory (MB) | allocated: 9052.94921875 | max allocated: 11287.08056640625 | reserved: 12760.0 | max reserved: 12760.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 11517.611328125 | max allocated: 16671.728515625 | reserved: 19592.0 | max reserved: 19592.0
[Rank 4] (after 1 iterations) memory (MB) | allocated: 9055.685546875 | max allocated: 14197.07958984375 | reserved: 15544.0 | max reserved: 15544.0
 iteration        1/     200 | consumed samples:          512 | elapsed time per iteration (ms): 32699.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.082743E+01 | gshard_loss: 1.282275E-04 | loss scale: 131072.0 | grad norm: 16.661 | num zeros: 57219.0 | params norm: 305.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 28] (after 1 iterations) memory (MB) | allocated: 12751.8955078125 | max allocated: 12751.92626953125 | reserved: 13050.0 | max reserved: 13050.0
[Rank 24] (after 1 iterations) memory (MB) | allocated: 9054.79296875 | max allocated: 9054.79541015625 | reserved: 10494.0 | max reserved: 10494.0
[Rank 20] (after 1 iterations) memory (MB) | allocated: 9056.29296875 | max allocated: 9056.29541015625 | reserved: 11392.0 | max reserved: 11392.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 9052.8271484375 | max allocated: 9690.32421875 | reserved: 12016.0 | max reserved: 12016.0
 iteration        2/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 6752.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.033675E+01 | gshard_loss: 3.710303E-04 | loss scale: 131072.0 | grad norm: 6.991 | num zeros: 1842828160.0 | params norm: 305.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 7304.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.007581E+01 | gshard_loss: 3.618853E-04 | loss scale: 131072.0 | grad norm: 5.202 | num zeros: 944431232.0 | params norm: 305.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 8840.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.177924E+01 | gshard_loss: 2.936614E-04 | loss scale: 131072.0 | grad norm: 40.427 | num zeros: 1053273088.0 | params norm: 305.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 6753.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.621920E+00 | gshard_loss: 2.907900E-04 | loss scale: 131072.0 | grad norm: 5.035 | num zeros: 1446263808.0 | params norm: 305.996 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 5653.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.239183E+01 | gshard_loss: 4.336839E-04 | loss scale: 131072.0 | grad norm: 5.099 | num zeros: 1244864896.0 | params norm: 306.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 6511.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.241422E+01 | gshard_loss: 3.900871E-04 | loss scale: 131072.0 | grad norm: 5.053 | num zeros: 652641792.0 | params norm: 306.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 5434.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.207941E+01 | gshard_loss: 3.230843E-04 | loss scale: 131072.0 | grad norm: 6.470 | num zeros: 211484096.0 | params norm: 306.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 5973.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.190303E+01 | gshard_loss: 3.332041E-04 | loss scale: 131072.0 | grad norm: 5.218 | num zeros: 485754272.0 | params norm: 306.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 5900.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.087682E+01 | gshard_loss: 3.397089E-04 | loss scale: 131072.0 | grad norm: 6.991 | num zeros: 1508349440.0 | params norm: 306.484 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 6082.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.706923E+00 | gshard_loss: 3.443265E-04 | loss scale: 131072.0 | grad norm: 6.488 | num zeros: 1079004416.0 | params norm: 306.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 5306.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.620672E+00 | gshard_loss: 3.242552E-04 | loss scale: 131072.0 | grad norm: 5.012 | num zeros: 1544074368.0 | params norm: 306.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 4823.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.550393E+00 | gshard_loss: 3.050025E-04 | loss scale: 131072.0 | grad norm: 4.999 | num zeros: 1885913344.0 | params norm: 306.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 5504.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.432192E+00 | gshard_loss: 2.818445E-04 | loss scale: 131072.0 | grad norm: 4.988 | num zeros: 1926673536.0 | params norm: 306.854 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 4649.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.288497E+00 | gshard_loss: 2.960937E-04 | loss scale: 131072.0 | grad norm: 4.980 | num zeros: 2249063424.0 | params norm: 306.943 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 4501.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.137767E+00 | gshard_loss: 3.047338E-04 | loss scale: 131072.0 | grad norm: 4.957 | num zeros: 1799582592.0 | params norm: 307.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 3755.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.985605E+00 | gshard_loss: 3.149676E-04 | loss scale: 131072.0 | grad norm: 4.934 | num zeros: 1840162048.0 | params norm: 307.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 4213.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.833600E+00 | gshard_loss: 3.091054E-04 | loss scale: 131072.0 | grad norm: 4.927 | num zeros: 1507248000.0 | params norm: 307.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 4909.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.692764E+00 | gshard_loss: 2.969238E-04 | loss scale: 131072.0 | grad norm: 4.914 | num zeros: 1509043328.0 | params norm: 307.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 3803.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.553771E+00 | gshard_loss: 2.802136E-04 | loss scale: 131072.0 | grad norm: 4.864 | num zeros: 1568652416.0 | params norm: 307.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 4827.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.420854E+00 | gshard_loss: 2.992433E-04 | loss scale: 131072.0 | grad norm: 4.800 | num zeros: 2140136960.0 | params norm: 307.440 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 4086.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.290356E+00 | gshard_loss: 3.301669E-04 | loss scale: 131072.0 | grad norm: 4.736 | num zeros: 2087158656.0 | params norm: 307.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 5135.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.164384E+00 | gshard_loss: 3.413349E-04 | loss scale: 131072.0 | grad norm: 4.668 | num zeros: 1870912128.0 | params norm: 307.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 4008.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.042694E+00 | gshard_loss: 3.205816E-04 | loss scale: 131072.0 | grad norm: 4.547 | num zeros: 2548522496.0 | params norm: 307.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 3997.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.930923E+00 | gshard_loss: 3.012384E-04 | loss scale: 131072.0 | grad norm: 4.404 | num zeros: 2576088832.0 | params norm: 307.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 3888.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.822947E+00 | gshard_loss: 2.854552E-04 | loss scale: 131072.0 | grad norm: 4.227 | num zeros: 2590330368.0 | params norm: 307.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 3972.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.732155E+00 | gshard_loss: 2.784708E-04 | loss scale: 131072.0 | grad norm: 3.969 | num zeros: 2331609600.0 | params norm: 307.863 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 4422.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.654181E+00 | gshard_loss: 2.751417E-04 | loss scale: 131072.0 | grad norm: 3.602 | num zeros: 1733724416.0 | params norm: 307.926 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 4018.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.567957E+00 | gshard_loss: 2.503625E-04 | loss scale: 131072.0 | grad norm: 3.201 | num zeros: 2273066496.0 | params norm: 307.987 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 3002.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.500170E+00 | gshard_loss: 2.462248E-04 | loss scale: 131072.0 | grad norm: 2.679 | num zeros: 2227871232.0 | params norm: 308.046 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 3543.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.454667E+00 | gshard_loss: 2.537862E-04 | loss scale: 131072.0 | grad norm: 2.073 | num zeros: 2377662976.0 | params norm: 308.104 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 3363.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.396553E+00 | gshard_loss: 2.421808E-04 | loss scale: 131072.0 | grad norm: 1.427 | num zeros: 2418286080.0 | params norm: 308.159 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 3054.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.371766E+00 | gshard_loss: 2.501616E-04 | loss scale: 131072.0 | grad norm: 0.758 | num zeros: 2623555840.0 | params norm: 308.213 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 3632.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.356568E+00 | gshard_loss: 2.484433E-04 | loss scale: 131072.0 | grad norm: 0.563 | num zeros: 2614593280.0 | params norm: 308.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 3397.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.350205E+00 | gshard_loss: 2.307941E-04 | loss scale: 131072.0 | grad norm: 0.919 | num zeros: 2708609792.0 | params norm: 308.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 2948.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.356874E+00 | gshard_loss: 2.455504E-04 | loss scale: 131072.0 | grad norm: 1.240 | num zeros: 2421892608.0 | params norm: 308.365 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 3552.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.357248E+00 | gshard_loss: 2.331674E-04 | loss scale: 131072.0 | grad norm: 1.311 | num zeros: 2637137664.0 | params norm: 308.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 2945.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.349866E+00 | gshard_loss: 2.246347E-04 | loss scale: 131072.0 | grad norm: 1.183 | num zeros: 2698900992.0 | params norm: 308.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 2787.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.355970E+00 | gshard_loss: 2.225101E-04 | loss scale: 131072.0 | grad norm: 0.890 | num zeros: 2550486016.0 | params norm: 308.499 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 3757.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.344056E+00 | gshard_loss: 2.207136E-04 | loss scale: 131072.0 | grad norm: 0.618 | num zeros: 2748164096.0 | params norm: 308.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 3802.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.338428E+00 | gshard_loss: 2.291942E-04 | loss scale: 131072.0 | grad norm: 0.564 | num zeros: 2733739008.0 | params norm: 308.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 3251.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.340781E+00 | gshard_loss: 2.465053E-04 | loss scale: 131072.0 | grad norm: 0.464 | num zeros: 2750168064.0 | params norm: 308.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 3319.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.330953E+00 | gshard_loss: 2.426018E-04 | loss scale: 131072.0 | grad norm: 0.490 | num zeros: 2620792064.0 | params norm: 308.647 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 2235.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.325831E+00 | gshard_loss: 2.295533E-04 | loss scale: 131072.0 | grad norm: 0.669 | num zeros: 2557881856.0 | params norm: 308.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 2623.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.328793E+00 | gshard_loss: 2.324915E-04 | loss scale: 131072.0 | grad norm: 0.741 | num zeros: 2763465216.0 | params norm: 308.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 2438.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.321913E+00 | gshard_loss: 2.244623E-04 | loss scale: 131072.0 | grad norm: 0.629 | num zeros: 2762492672.0 | params norm: 308.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 3288.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.316385E+00 | gshard_loss: 2.385664E-04 | loss scale: 131072.0 | grad norm: 0.460 | num zeros: 2612684800.0 | params norm: 308.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 2375.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.322384E+00 | gshard_loss: 2.464069E-04 | loss scale: 131072.0 | grad norm: 0.443 | num zeros: 2835090944.0 | params norm: 308.799 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 2865.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.322992E+00 | gshard_loss: 2.371981E-04 | loss scale: 131072.0 | grad norm: 0.536 | num zeros: 2820967424.0 | params norm: 308.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 3449.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.320762E+00 | gshard_loss: 2.571878E-04 | loss scale: 131072.0 | grad norm: 0.584 | num zeros: 2838316544.0 | params norm: 308.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 2374.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.322243E+00 | gshard_loss: 2.394939E-04 | loss scale: 131072.0 | grad norm: 0.562 | num zeros: 2819019776.0 | params norm: 308.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 2463.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.320174E+00 | gshard_loss: 2.295436E-04 | loss scale: 131072.0 | grad norm: 0.481 | num zeros: 2819863296.0 | params norm: 308.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 2444.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.319101E+00 | gshard_loss: 2.282906E-04 | loss scale: 131072.0 | grad norm: 0.353 | num zeros: 2855135744.0 | params norm: 308.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 3149.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.324030E+00 | gshard_loss: 2.211078E-04 | loss scale: 131072.0 | grad norm: 0.342 | num zeros: 2872156160.0 | params norm: 308.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 2321.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.319037E+00 | gshard_loss: 2.209086E-04 | loss scale: 131072.0 | grad norm: 0.378 | num zeros: 2660075008.0 | params norm: 308.957 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 2420.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.312842E+00 | gshard_loss: 2.180724E-04 | loss scale: 131072.0 | grad norm: 0.363 | num zeros: 2713757184.0 | params norm: 308.977 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 3084.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.311973E+00 | gshard_loss: 2.138721E-04 | loss scale: 131072.0 | grad norm: 0.326 | num zeros: 2754497280.0 | params norm: 308.996 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 2768.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.307188E+00 | gshard_loss: 2.150116E-04 | loss scale: 131072.0 | grad norm: 0.308 | num zeros: 2554844416.0 | params norm: 309.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 2371.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.311648E+00 | gshard_loss: 2.092092E-04 | loss scale: 131072.0 | grad norm: 0.294 | num zeros: 2402086912.0 | params norm: 309.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 2565.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.305866E+00 | gshard_loss: 2.037356E-04 | loss scale: 131072.0 | grad norm: 0.285 | num zeros: 2481246464.0 | params norm: 309.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 2998.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.301115E+00 | gshard_loss: 1.988573E-04 | loss scale: 131072.0 | grad norm: 0.198 | num zeros: 2242324992.0 | params norm: 309.069 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 3036.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.300447E+00 | gshard_loss: 1.917560E-04 | loss scale: 131072.0 | grad norm: 0.373 | num zeros: 1407511040.0 | params norm: 309.084 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 2467.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.308418E+00 | gshard_loss: 1.870078E-04 | loss scale: 131072.0 | grad norm: 0.278 | num zeros: 1914857856.0 | params norm: 309.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 2866.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.304797E+00 | gshard_loss: 1.894407E-04 | loss scale: 131072.0 | grad norm: 0.212 | num zeros: 1934847360.0 | params norm: 309.114 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 3185.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.310105E+00 | gshard_loss: 1.909100E-04 | loss scale: 131072.0 | grad norm: 1.999 | num zeros: 1312238336.0 | params norm: 309.129 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 2479.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.298672E+00 | gshard_loss: 2.004826E-04 | loss scale: 131072.0 | grad norm: 0.413 | num zeros: 1215747840.0 | params norm: 309.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 2341.4 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.307298E+00 | gshard_loss: 2.050063E-04 | loss scale: 131072.0 | grad norm: 0.357 | num zeros: 1410570240.0 | params norm: 309.158 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 2355.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.296650E+00 | gshard_loss: 1.943297E-04 | loss scale: 131072.0 | grad norm: 0.432 | num zeros: 1518888192.0 | params norm: 309.173 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 3338.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.308295E+00 | gshard_loss: 1.939909E-04 | loss scale: 131072.0 | grad norm: 0.936 | num zeros: 759040704.0 | params norm: 309.189 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 2493.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.301730E+00 | gshard_loss: 1.939575E-04 | loss scale: 131072.0 | grad norm: 0.184 | num zeros: 1418926080.0 | params norm: 309.207 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 3009.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.280936E+00 | gshard_loss: 2.052683E-04 | loss scale: 131072.0 | grad norm: 0.212 | num zeros: 1328182528.0 | params norm: 309.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 3507.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.289001E+00 | gshard_loss: 1.951591E-04 | loss scale: 131072.0 | grad norm: 0.221 | num zeros: 1449465856.0 | params norm: 309.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 2396.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.284288E+00 | gshard_loss: 1.983175E-04 | loss scale: 131072.0 | grad norm: 0.387 | num zeros: 1417033472.0 | params norm: 309.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 2546.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.276218E+00 | gshard_loss: 2.001147E-04 | loss scale: 131072.0 | grad norm: 1.495 | num zeros: 712215936.0 | params norm: 309.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 2475.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.272598E+00 | gshard_loss: 2.019802E-04 | loss scale: 131072.0 | grad norm: 0.380 | num zeros: 805797184.0 | params norm: 309.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 2589.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.262891E+00 | gshard_loss: 2.037292E-04 | loss scale: 131072.0 | grad norm: 0.420 | num zeros: 749171136.0 | params norm: 309.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 2489.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.248899E+00 | gshard_loss: 2.185801E-04 | loss scale: 131072.0 | grad norm: 0.551 | num zeros: 1347973376.0 | params norm: 309.361 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 3004.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.246237E+00 | gshard_loss: 2.112867E-04 | loss scale: 131072.0 | grad norm: 0.788 | num zeros: 1128044800.0 | params norm: 309.386 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 2667.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.237506E+00 | gshard_loss: 2.394940E-04 | loss scale: 131072.0 | grad norm: 0.836 | num zeros: 1241635968.0 | params norm: 309.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 3630.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.222565E+00 | gshard_loss: 2.555683E-04 | loss scale: 131072.0 | grad norm: 0.666 | num zeros: 1351699072.0 | params norm: 309.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 2378.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.225702E+00 | gshard_loss: 2.832508E-04 | loss scale: 131072.0 | grad norm: 2.640 | num zeros: 1260420992.0 | params norm: 309.460 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 2504.8 | learning rate: 9.998E-05 | global batch size:   512 | loss scale: 131072.0 | params norm: 309.460 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 2464.1 | learning rate: 9.998E-05 | global batch size:   512 | loss scale: 65536.0 | params norm: 309.460 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 2379.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.204360E+00 | gshard_loss: 3.250710E-04 | loss scale: 65536.0 | grad norm: 2.075 | num zeros: 1320935552.0 | params norm: 309.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 2324.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.196453E+00 | gshard_loss: 3.759140E-04 | loss scale: 65536.0 | grad norm: 0.590 | num zeros: 1339036032.0 | params norm: 309.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 2609.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.196170E+00 | gshard_loss: 3.965034E-04 | loss scale: 65536.0 | grad norm: 1.851 | num zeros: 1372726016.0 | params norm: 309.541 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 2917.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.178344E+00 | gshard_loss: 4.115474E-04 | loss scale: 65536.0 | grad norm: 0.776 | num zeros: 1347334528.0 | params norm: 309.564 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 2576.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.175301E+00 | gshard_loss: 4.205223E-04 | loss scale: 65536.0 | grad norm: 0.470 | num zeros: 1239155584.0 | params norm: 309.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 2563.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.167809E+00 | gshard_loss: 4.237688E-04 | loss scale: 65536.0 | grad norm: 1.657 | num zeros: 1255623040.0 | params norm: 309.613 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 2445.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.149377E+00 | gshard_loss: 4.124397E-04 | loss scale: 65536.0 | grad norm: 0.675 | num zeros: 1317566720.0 | params norm: 309.636 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 3511.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.161772E+00 | gshard_loss: 4.239396E-04 | loss scale: 65536.0 | grad norm: 2.513 | num zeros: 1315376256.0 | params norm: 309.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 2277.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.146135E+00 | gshard_loss: 4.384654E-04 | loss scale: 65536.0 | grad norm: 1.385 | num zeros: 1262009472.0 | params norm: 309.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 2492.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.133568E+00 | gshard_loss: 4.244852E-04 | loss scale: 65536.0 | grad norm: 0.622 | num zeros: 1304567552.0 | params norm: 309.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 2509.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.143317E+00 | gshard_loss: 4.044850E-04 | loss scale: 65536.0 | grad norm: 0.759 | num zeros: 1232142464.0 | params norm: 309.720 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 2635.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.122773E+00 | gshard_loss: 3.696530E-04 | loss scale: 65536.0 | grad norm: 2.432 | num zeros: 1297536640.0 | params norm: 309.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 2357.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.128450E+00 | gshard_loss: 3.476730E-04 | loss scale: 65536.0 | grad norm: 1.647 | num zeros: 1306936192.0 | params norm: 309.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 2443.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.110289E+00 | gshard_loss: 3.206181E-04 | loss scale: 65536.0 | grad norm: 1.583 | num zeros: 1311178880.0 | params norm: 309.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 2361.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.105271E+00 | gshard_loss: 2.901123E-04 | loss scale: 65536.0 | grad norm: 1.554 | num zeros: 1313350528.0 | params norm: 309.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 3556.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.105600E+00 | gshard_loss: 2.767474E-04 | loss scale: 65536.0 | grad norm: 4.228 | num zeros: 1356014848.0 | params norm: 309.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 2385.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.105571E+00 | gshard_loss: 2.674911E-04 | loss scale: 65536.0 | grad norm: 1.595 | num zeros: 1309093120.0 | params norm: 309.833 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        51712 | elapsed time per iteration (ms): 2785.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.102228E+00 | gshard_loss: 3.200778E-04 | loss scale: 65536.0 | grad norm: 1.136 | num zeros: 1391200512.0 | params norm: 309.850 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        52224 | elapsed time per iteration (ms): 3010.8 | learning rate: 9.998E-05 | global batch size:   512 | loss scale: 32768.0 | params norm: 309.850 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        52736 | elapsed time per iteration (ms): 2351.1 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.089611E+00 | gshard_loss: 3.771684E-04 | loss scale: 32768.0 | grad norm: 1.044 | num zeros: 1590539520.0 | params norm: 309.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        53248 | elapsed time per iteration (ms): 2453.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 7.090927E+00 | gshard_loss: 3.441351E-04 | loss scale: 32768.0 | grad norm: 1.147 | num zeros: 1412795776.0 | params norm: 309.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        53760 | elapsed time per iteration (ms): 2503.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.091498E+00 | gshard_loss: 3.225160E-04 | loss scale: 32768.0 | grad norm: 1.273 | num zeros: 1364135680.0 | params norm: 309.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        54272 | elapsed time per iteration (ms): 2447.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.083588E+00 | gshard_loss: 3.708682E-04 | loss scale: 32768.0 | grad norm: 0.786 | num zeros: 1571583104.0 | params norm: 309.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        54784 | elapsed time per iteration (ms): 2338.2 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.089015E+00 | gshard_loss: 4.343288E-04 | loss scale: 32768.0 | grad norm: 0.618 | num zeros: 1354647296.0 | params norm: 309.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        55296 | elapsed time per iteration (ms): 2350.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.069779E+00 | gshard_loss: 3.990104E-04 | loss scale: 32768.0 | grad norm: 3.146 | num zeros: 1441053696.0 | params norm: 309.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        55808 | elapsed time per iteration (ms): 2573.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.074974E+00 | gshard_loss: 3.650285E-04 | loss scale: 32768.0 | grad norm: 1.255 | num zeros: 1402392192.0 | params norm: 309.943 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        56320 | elapsed time per iteration (ms): 4085.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.065209E+00 | gshard_loss: 4.215048E-04 | loss scale: 32768.0 | grad norm: 1.374 | num zeros: 1524246912.0 | params norm: 309.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        56832 | elapsed time per iteration (ms): 2404.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.070800E+00 | gshard_loss: 4.410407E-04 | loss scale: 32768.0 | grad norm: 0.899 | num zeros: 1466103296.0 | params norm: 309.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        57344 | elapsed time per iteration (ms): 2547.2 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.058534E+00 | gshard_loss: 3.489164E-04 | loss scale: 32768.0 | grad norm: 0.926 | num zeros: 1348558336.0 | params norm: 309.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        57856 | elapsed time per iteration (ms): 2441.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.057911E+00 | gshard_loss: 3.413183E-04 | loss scale: 32768.0 | grad norm: 1.044 | num zeros: 1295205120.0 | params norm: 309.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        58368 | elapsed time per iteration (ms): 2516.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.061078E+00 | gshard_loss: 3.643240E-04 | loss scale: 32768.0 | grad norm: 0.546 | num zeros: 1408725248.0 | params norm: 310.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        58880 | elapsed time per iteration (ms): 2343.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.059272E+00 | gshard_loss: 3.280166E-04 | loss scale: 32768.0 | grad norm: 0.629 | num zeros: 1251239680.0 | params norm: 310.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        59392 | elapsed time per iteration (ms): 2660.1 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.044955E+00 | gshard_loss: 2.761901E-04 | loss scale: 32768.0 | grad norm: 1.681 | num zeros: 1120138496.0 | params norm: 310.030 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        59904 | elapsed time per iteration (ms): 2594.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.041886E+00 | gshard_loss: 2.713667E-04 | loss scale: 32768.0 | grad norm: 3.245 | num zeros: 1080500864.0 | params norm: 310.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        60416 | elapsed time per iteration (ms): 3783.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.042145E+00 | gshard_loss: 2.796664E-04 | loss scale: 32768.0 | grad norm: 0.662 | num zeros: 1004512768.0 | params norm: 310.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        60928 | elapsed time per iteration (ms): 2371.7 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.029607E+00 | gshard_loss: 2.628053E-04 | loss scale: 32768.0 | grad norm: 2.337 | num zeros: 861060608.0 | params norm: 310.069 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        61440 | elapsed time per iteration (ms): 2378.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.030383E+00 | gshard_loss: 2.804612E-04 | loss scale: 32768.0 | grad norm: 1.281 | num zeros: 765333696.0 | params norm: 310.083 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        61952 | elapsed time per iteration (ms): 2409.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.016089E+00 | gshard_loss: 3.262002E-04 | loss scale: 32768.0 | grad norm: 1.313 | num zeros: 912841792.0 | params norm: 310.094 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        62464 | elapsed time per iteration (ms): 5715.1 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 7.024518E+00 | gshard_loss: 2.602204E-04 | loss scale: 32768.0 | grad norm: 1.338 | num zeros: 706768064.0 | params norm: 310.108 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        62976 | elapsed time per iteration (ms): 5846.7 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.005814E+00 | gshard_loss: 2.431768E-04 | loss scale: 32768.0 | grad norm: 0.861 | num zeros: 730152000.0 | params norm: 310.124 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        63488 | elapsed time per iteration (ms): 4181.6 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.014062E+00 | gshard_loss: 2.511790E-04 | loss scale: 32768.0 | grad norm: 1.069 | num zeros: 704983296.0 | params norm: 310.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        64000 | elapsed time per iteration (ms): 3502.9 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 7.001620E+00 | gshard_loss: 2.347006E-04 | loss scale: 32768.0 | grad norm: 0.981 | num zeros: 765736320.0 | params norm: 310.153 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        64512 | elapsed time per iteration (ms): 3538.6 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.985573E+00 | gshard_loss: 2.473178E-04 | loss scale: 32768.0 | grad norm: 2.031 | num zeros: 703302400.0 | params norm: 310.170 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        65024 | elapsed time per iteration (ms): 3344.4 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.982565E+00 | gshard_loss: 2.570757E-04 | loss scale: 32768.0 | grad norm: 1.188 | num zeros: 843661312.0 | params norm: 310.183 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        65536 | elapsed time per iteration (ms): 2905.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.975354E+00 | gshard_loss: 2.499204E-04 | loss scale: 32768.0 | grad norm: 0.989 | num zeros: 809111936.0 | params norm: 310.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        66048 | elapsed time per iteration (ms): 3473.4 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.962094E+00 | gshard_loss: 2.461881E-04 | loss scale: 32768.0 | grad norm: 1.222 | num zeros: 828944128.0 | params norm: 310.219 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        66560 | elapsed time per iteration (ms): 2612.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.959205E+00 | gshard_loss: 2.505120E-04 | loss scale: 32768.0 | grad norm: 0.792 | num zeros: 882237376.0 | params norm: 310.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        67072 | elapsed time per iteration (ms): 2830.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.953534E+00 | gshard_loss: 2.477023E-04 | loss scale: 32768.0 | grad norm: 0.944 | num zeros: 754665600.0 | params norm: 310.257 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        67584 | elapsed time per iteration (ms): 2953.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.925642E+00 | gshard_loss: 2.606545E-04 | loss scale: 32768.0 | grad norm: 0.911 | num zeros: 893545024.0 | params norm: 310.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        68096 | elapsed time per iteration (ms): 2598.1 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.918247E+00 | gshard_loss: 2.719462E-04 | loss scale: 32768.0 | grad norm: 0.774 | num zeros: 828728448.0 | params norm: 310.300 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        68608 | elapsed time per iteration (ms): 3094.6 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.923261E+00 | gshard_loss: 2.584624E-04 | loss scale: 32768.0 | grad norm: 0.904 | num zeros: 711696512.0 | params norm: 310.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        69120 | elapsed time per iteration (ms): 3263.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.914285E+00 | gshard_loss: 2.639403E-04 | loss scale: 32768.0 | grad norm: 0.779 | num zeros: 901822656.0 | params norm: 310.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        69632 | elapsed time per iteration (ms): 3075.4 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.912643E+00 | gshard_loss: 2.549192E-04 | loss scale: 32768.0 | grad norm: 1.237 | num zeros: 778365824.0 | params norm: 310.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        70144 | elapsed time per iteration (ms): 2459.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.894547E+00 | gshard_loss: 2.651241E-04 | loss scale: 32768.0 | grad norm: 1.281 | num zeros: 746318976.0 | params norm: 310.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        70656 | elapsed time per iteration (ms): 2433.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.883383E+00 | gshard_loss: 2.688463E-04 | loss scale: 32768.0 | grad norm: 0.685 | num zeros: 775830976.0 | params norm: 310.422 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        71168 | elapsed time per iteration (ms): 3101.8 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.877640E+00 | gshard_loss: 2.685275E-04 | loss scale: 32768.0 | grad norm: 0.700 | num zeros: 861244672.0 | params norm: 310.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        71680 | elapsed time per iteration (ms): 2445.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.876542E+00 | gshard_loss: 2.828006E-04 | loss scale: 32768.0 | grad norm: 0.599 | num zeros: 838410624.0 | params norm: 310.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        72192 | elapsed time per iteration (ms): 2749.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.863390E+00 | gshard_loss: 2.738475E-04 | loss scale: 32768.0 | grad norm: 0.738 | num zeros: 729672320.0 | params norm: 310.504 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        72704 | elapsed time per iteration (ms): 2706.8 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.864773E+00 | gshard_loss: 2.982850E-04 | loss scale: 32768.0 | grad norm: 0.656 | num zeros: 938367616.0 | params norm: 310.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        73216 | elapsed time per iteration (ms): 3534.1 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.838591E+00 | gshard_loss: 2.820511E-04 | loss scale: 32768.0 | grad norm: 0.412 | num zeros: 961159232.0 | params norm: 310.556 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        73728 | elapsed time per iteration (ms): 2476.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.846659E+00 | gshard_loss: 2.623140E-04 | loss scale: 32768.0 | grad norm: 0.765 | num zeros: 792798528.0 | params norm: 310.587 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        74240 | elapsed time per iteration (ms): 2512.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.838046E+00 | gshard_loss: 2.775826E-04 | loss scale: 32768.0 | grad norm: 0.657 | num zeros: 920953344.0 | params norm: 310.614 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        74752 | elapsed time per iteration (ms): 2681.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.831992E+00 | gshard_loss: 2.668443E-04 | loss scale: 32768.0 | grad norm: 0.399 | num zeros: 857484352.0 | params norm: 310.639 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        75264 | elapsed time per iteration (ms): 2566.6 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.840872E+00 | gshard_loss: 2.521097E-04 | loss scale: 32768.0 | grad norm: 0.557 | num zeros: 669290496.0 | params norm: 310.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        75776 | elapsed time per iteration (ms): 2502.3 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.807807E+00 | gshard_loss: 2.572448E-04 | loss scale: 32768.0 | grad norm: 0.438 | num zeros: 645170688.0 | params norm: 310.699 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        76288 | elapsed time per iteration (ms): 3032.8 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.820356E+00 | gshard_loss: 2.748163E-04 | loss scale: 32768.0 | grad norm: 0.610 | num zeros: 777670080.0 | params norm: 310.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        76800 | elapsed time per iteration (ms): 4145.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.801336E+00 | gshard_loss: 2.585646E-04 | loss scale: 32768.0 | grad norm: 0.439 | num zeros: 737225088.0 | params norm: 310.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        77312 | elapsed time per iteration (ms): 2833.6 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.796011E+00 | gshard_loss: 2.413885E-04 | loss scale: 32768.0 | grad norm: 0.606 | num zeros: 674291520.0 | params norm: 310.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        77824 | elapsed time per iteration (ms): 2508.2 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.790021E+00 | gshard_loss: 2.380108E-04 | loss scale: 32768.0 | grad norm: 0.454 | num zeros: 697599232.0 | params norm: 310.819 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        78336 | elapsed time per iteration (ms): 2558.1 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.789104E+00 | gshard_loss: 2.275357E-04 | loss scale: 32768.0 | grad norm: 0.382 | num zeros: 645662400.0 | params norm: 310.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        78848 | elapsed time per iteration (ms): 2588.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.776185E+00 | gshard_loss: 2.184519E-04 | loss scale: 32768.0 | grad norm: 0.391 | num zeros: 730495040.0 | params norm: 310.878 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        79360 | elapsed time per iteration (ms): 2734.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.768844E+00 | gshard_loss: 2.157859E-04 | loss scale: 32768.0 | grad norm: 0.391 | num zeros: 713035392.0 | params norm: 310.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        79872 | elapsed time per iteration (ms): 2923.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.767118E+00 | gshard_loss: 2.137300E-04 | loss scale: 32768.0 | grad norm: 0.634 | num zeros: 769352704.0 | params norm: 310.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        80384 | elapsed time per iteration (ms): 3515.7 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.756958E+00 | gshard_loss: 2.184239E-04 | loss scale: 32768.0 | grad norm: 0.372 | num zeros: 822672384.0 | params norm: 310.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        80896 | elapsed time per iteration (ms): 2502.6 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.746489E+00 | gshard_loss: 2.139075E-04 | loss scale: 32768.0 | grad norm: 0.911 | num zeros: 654186176.0 | params norm: 311.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        81408 | elapsed time per iteration (ms): 2527.1 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.748782E+00 | gshard_loss: 2.123774E-04 | loss scale: 32768.0 | grad norm: 0.319 | num zeros: 511895552.0 | params norm: 311.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        81920 | elapsed time per iteration (ms): 2671.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.743925E+00 | gshard_loss: 2.101432E-04 | loss scale: 32768.0 | grad norm: 0.353 | num zeros: 556932160.0 | params norm: 311.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        82432 | elapsed time per iteration (ms): 2554.5 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.733093E+00 | gshard_loss: 2.108404E-04 | loss scale: 32768.0 | grad norm: 0.293 | num zeros: 510215328.0 | params norm: 311.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        82944 | elapsed time per iteration (ms): 2529.3 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.730622E+00 | gshard_loss: 2.106528E-04 | loss scale: 32768.0 | grad norm: 0.600 | num zeros: 517680704.0 | params norm: 311.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        83456 | elapsed time per iteration (ms): 2809.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.713597E+00 | gshard_loss: 2.141747E-04 | loss scale: 32768.0 | grad norm: 0.324 | num zeros: 556139200.0 | params norm: 311.159 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        83968 | elapsed time per iteration (ms): 2978.6 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.716272E+00 | gshard_loss: 2.120885E-04 | loss scale: 32768.0 | grad norm: 0.399 | num zeros: 461068544.0 | params norm: 311.192 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        84480 | elapsed time per iteration (ms): 3210.1 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.714939E+00 | gshard_loss: 2.186063E-04 | loss scale: 32768.0 | grad norm: 0.562 | num zeros: 551160832.0 | params norm: 311.222 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        84992 | elapsed time per iteration (ms): 2552.5 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.726301E+00 | gshard_loss: 2.135156E-04 | loss scale: 32768.0 | grad norm: 0.927 | num zeros: 463125856.0 | params norm: 311.255 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        85504 | elapsed time per iteration (ms): 2613.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.708216E+00 | gshard_loss: 2.246623E-04 | loss scale: 32768.0 | grad norm: 0.687 | num zeros: 507924672.0 | params norm: 311.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        86016 | elapsed time per iteration (ms): 2670.7 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.692113E+00 | gshard_loss: 2.170417E-04 | loss scale: 32768.0 | grad norm: 0.448 | num zeros: 534790112.0 | params norm: 311.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        86528 | elapsed time per iteration (ms): 2564.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.688519E+00 | gshard_loss: 2.174452E-04 | loss scale: 32768.0 | grad norm: 0.661 | num zeros: 546125312.0 | params norm: 311.349 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        87040 | elapsed time per iteration (ms): 2732.3 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.672044E+00 | gshard_loss: 2.290853E-04 | loss scale: 32768.0 | grad norm: 0.448 | num zeros: 610987904.0 | params norm: 311.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        87552 | elapsed time per iteration (ms): 2612.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.672396E+00 | gshard_loss: 2.306463E-04 | loss scale: 32768.0 | grad norm: 0.448 | num zeros: 559913024.0 | params norm: 311.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        88064 | elapsed time per iteration (ms): 4134.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.676980E+00 | gshard_loss: 2.232635E-04 | loss scale: 32768.0 | grad norm: 0.493 | num zeros: 410563168.0 | params norm: 311.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        88576 | elapsed time per iteration (ms): 2601.1 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.662062E+00 | gshard_loss: 2.294045E-04 | loss scale: 32768.0 | grad norm: 0.476 | num zeros: 459704576.0 | params norm: 311.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        89088 | elapsed time per iteration (ms): 2585.9 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.643615E+00 | gshard_loss: 2.342219E-04 | loss scale: 32768.0 | grad norm: 0.453 | num zeros: 630699392.0 | params norm: 311.510 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        89600 | elapsed time per iteration (ms): 2720.9 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.649344E+00 | gshard_loss: 2.303843E-04 | loss scale: 32768.0 | grad norm: 0.451 | num zeros: 550735232.0 | params norm: 311.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        90112 | elapsed time per iteration (ms): 2524.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.643958E+00 | gshard_loss: 2.250460E-04 | loss scale: 32768.0 | grad norm: 0.360 | num zeros: 677776512.0 | params norm: 311.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        90624 | elapsed time per iteration (ms): 2786.9 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.637819E+00 | gshard_loss: 2.197417E-04 | loss scale: 32768.0 | grad norm: 0.421 | num zeros: 695582720.0 | params norm: 311.611 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        91136 | elapsed time per iteration (ms): 2533.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.623189E+00 | gshard_loss: 2.161147E-04 | loss scale: 32768.0 | grad norm: 0.302 | num zeros: 696169728.0 | params norm: 311.646 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        91648 | elapsed time per iteration (ms): 4903.3 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.629574E+00 | gshard_loss: 2.174245E-04 | loss scale: 32768.0 | grad norm: 0.498 | num zeros: 685817344.0 | params norm: 311.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        92160 | elapsed time per iteration (ms): 3467.9 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.605699E+00 | gshard_loss: 2.192215E-04 | loss scale: 32768.0 | grad norm: 0.257 | num zeros: 663824896.0 | params norm: 311.717 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        92672 | elapsed time per iteration (ms): 3242.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.609797E+00 | gshard_loss: 2.194639E-04 | loss scale: 32768.0 | grad norm: 0.563 | num zeros: 617513152.0 | params norm: 311.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        93184 | elapsed time per iteration (ms): 2927.0 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.604389E+00 | gshard_loss: 2.214770E-04 | loss scale: 32768.0 | grad norm: 0.334 | num zeros: 564848896.0 | params norm: 311.791 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        93696 | elapsed time per iteration (ms): 2938.2 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.604323E+00 | gshard_loss: 2.265144E-04 | loss scale: 32768.0 | grad norm: 0.414 | num zeros: 508050816.0 | params norm: 311.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        94208 | elapsed time per iteration (ms): 2564.4 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.584192E+00 | gshard_loss: 2.301278E-04 | loss scale: 32768.0 | grad norm: 0.463 | num zeros: 610170688.0 | params norm: 311.869 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        94720 | elapsed time per iteration (ms): 3241.0 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.581408E+00 | gshard_loss: 2.326706E-04 | loss scale: 32768.0 | grad norm: 0.305 | num zeros: 534815200.0 | params norm: 311.910 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        95232 | elapsed time per iteration (ms): 2972.3 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.577129E+00 | gshard_loss: 2.383539E-04 | loss scale: 32768.0 | grad norm: 0.445 | num zeros: 615114240.0 | params norm: 311.952 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        95744 | elapsed time per iteration (ms): 3481.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.575091E+00 | gshard_loss: 2.424500E-04 | loss scale: 32768.0 | grad norm: 0.379 | num zeros: 432292160.0 | params norm: 311.993 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        96256 | elapsed time per iteration (ms): 2697.6 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.565377E+00 | gshard_loss: 2.475792E-04 | loss scale: 32768.0 | grad norm: 0.375 | num zeros: 423800640.0 | params norm: 312.035 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        96768 | elapsed time per iteration (ms): 2543.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.568782E+00 | gshard_loss: 2.456628E-04 | loss scale: 32768.0 | grad norm: 0.673 | num zeros: 536742496.0 | params norm: 312.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        97280 | elapsed time per iteration (ms): 2550.3 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.578824E+00 | gshard_loss: 2.475908E-04 | loss scale: 32768.0 | grad norm: 0.883 | num zeros: 529591360.0 | params norm: 312.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        97792 | elapsed time per iteration (ms): 2744.9 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.563533E+00 | gshard_loss: 2.428774E-04 | loss scale: 32768.0 | grad norm: 0.631 | num zeros: 458714240.0 | params norm: 312.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        98304 | elapsed time per iteration (ms): 3138.4 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.555957E+00 | gshard_loss: 2.342300E-04 | loss scale: 32768.0 | grad norm: 0.731 | num zeros: 639000064.0 | params norm: 312.203 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        98816 | elapsed time per iteration (ms): 2637.8 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.535754E+00 | gshard_loss: 2.306829E-04 | loss scale: 32768.0 | grad norm: 0.423 | num zeros: 542897664.0 | params norm: 312.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        99328 | elapsed time per iteration (ms): 3649.1 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.533356E+00 | gshard_loss: 2.311343E-04 | loss scale: 32768.0 | grad norm: 0.781 | num zeros: 486075968.0 | params norm: 312.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        99840 | elapsed time per iteration (ms): 2962.4 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.527893E+00 | gshard_loss: 2.309923E-04 | loss scale: 32768.0 | grad norm: 0.351 | num zeros: 520356672.0 | params norm: 312.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:       100352 | elapsed time per iteration (ms): 2548.3 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.518882E+00 | gshard_loss: 2.284687E-04 | loss scale: 32768.0 | grad norm: 0.507 | num zeros: 598482176.0 | params norm: 312.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:       100864 | elapsed time per iteration (ms): 2550.9 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.530501E+00 | gshard_loss: 2.302917E-04 | loss scale: 32768.0 | grad norm: 0.422 | num zeros: 534849536.0 | params norm: 312.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:       101376 | elapsed time per iteration (ms): 2617.6 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.504630E+00 | gshard_loss: 2.355064E-04 | loss scale: 32768.0 | grad norm: 0.473 | num zeros: 600812160.0 | params norm: 312.458 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:       101888 | elapsed time per iteration (ms): 3338.8 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.504880E+00 | gshard_loss: 2.404359E-04 | loss scale: 32768.0 | grad norm: 0.418 | num zeros: 528358848.0 | params norm: 312.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:       102400 | elapsed time per iteration (ms): 2668.3 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.496274E+00 | gshard_loss: 2.412621E-04 | loss scale: 32768.0 | grad norm: 0.391 | num zeros: 531590976.0 | params norm: 312.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2022-12-22 01:23:47 
