+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 2 == 0 ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 4 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 5 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 6 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 1 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 3 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 7 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 0 == 0 ']'
+ hostname
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 9 == 0 ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 14 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 13 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 15 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 8 == 0 ']'
+ '[' 10 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 11 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 12 == 0 ']'
+ sleep 10s
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ export RANK=3
+ RANK=3
+ localrank=4
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ localrank=5
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=1
+ NODE_RANK=1
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=0
+ NODE_RANK=0
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ '[' sh-lab == nico ']'
+ export NODE_RANK=1
+ NODE_RANK=1
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ WORLD_SIZE=16
+ localrank=6
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ export NNODES=2
+ NNODES=2
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ python_args+='
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=8
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ localrank=3
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export RANK=9
+ RANK=9
+ WORLD_SIZE=16
+ localrank=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=2
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=4
+ NODE_RANK=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=6
+ NODE_RANK=6
+ export NODE_RANK=6
+ NODE_RANK=6
+ export NNODES=2
+ NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=4
+ NODE_RANK=4
+ '[' sh-lab == nico ']'
+ export NODE_RANK=5
+ NODE_RANK=5
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ false
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:37:27+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 16
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2560
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 1
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 51200
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 4
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.497 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1]
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 11.763 seconds
time to initialize megatron (seconds): -7.438
[after megatron is initialized] datetime: 2022-12-23 01:38:00 
hhs=5120
hhs=5120
hhs=5120
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4]
hhs=5120
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5]
hhs=5120
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7]
hhs=5120
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3]
hhs=5120
hhs=5120
building GPT model ...
hhs=5120
hhs=5120
hhs=5120
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
hhs=5120
hhs=5120
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
hhs=5120
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
hhs=5120
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
hhs=5120
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 971371776
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-23 01:38:11 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.126629 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.039 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-23 01:38:26 
done with setup ...
training ...
[before the start of training step] datetime: 2022-12-23 01:38:26 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 22797.2666015625 | max allocated: 24622.85546875 | reserved: 27346.0 | max reserved: 27346.0
 iteration        1/     200 | consumed samples:          256 | elapsed time per iteration (ms): 15802.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082777E+01 | gshard_loss: 3.984180E-03 | loss scale: 131072.0 | grad norm: 22.029 | num zeros: 30518.0 | params norm: 295.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/     200 | consumed samples:          512 | elapsed time per iteration (ms): 2353.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.037297E+01 | gshard_loss: 8.967387E-03 | loss scale: 131072.0 | grad norm: 7.182 | num zeros: 104913336.0 | params norm: 295.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:          768 | elapsed time per iteration (ms): 2523.2 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 2536.2 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         1280 | elapsed time per iteration (ms): 3147.5 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 32768.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 2681.6 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 16384.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         1792 | elapsed time per iteration (ms): 2528.9 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 8192.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 3566.7 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 4096.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         2304 | elapsed time per iteration (ms): 2600.8 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 2048.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 2582.3 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 1024.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         2816 | elapsed time per iteration (ms): 2580.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.071136E+01 | gshard_loss: 6.167764E-03 | loss scale: 1024.0 | grad norm: 1142.315 | num zeros: 21002.0 | params norm: 295.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 2439.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.040689E+01 | gshard_loss: 8.361991E-03 | loss scale: 1024.0 | grad norm: 51.414 | num zeros: 26250.0 | params norm: 295.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         3328 | elapsed time per iteration (ms): 2143.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.860344E+00 | gshard_loss: 1.240647E-02 | loss scale: 1024.0 | grad norm: 4.691 | num zeros: 182214320.0 | params norm: 295.411 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 2182.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.587220E+00 | gshard_loss: 1.045526E-02 | loss scale: 1024.0 | grad norm: 4.686 | num zeros: 169462944.0 | params norm: 295.437 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         3840 | elapsed time per iteration (ms): 2310.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.347878E+00 | gshard_loss: 9.795763E-03 | loss scale: 1024.0 | grad norm: 4.667 | num zeros: 169746832.0 | params norm: 295.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 2694.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.154756E+00 | gshard_loss: 9.457223E-03 | loss scale: 1024.0 | grad norm: 4.623 | num zeros: 90042408.0 | params norm: 295.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         4352 | elapsed time per iteration (ms): 2772.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.955097E+00 | gshard_loss: 8.975747E-03 | loss scale: 1024.0 | grad norm: 4.618 | num zeros: 93580712.0 | params norm: 295.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 2368.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.762267E+00 | gshard_loss: 8.198289E-03 | loss scale: 1024.0 | grad norm: 4.609 | num zeros: 122625824.0 | params norm: 295.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         4864 | elapsed time per iteration (ms): 2493.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.575715E+00 | gshard_loss: 7.185950E-03 | loss scale: 1024.0 | grad norm: 4.566 | num zeros: 125003304.0 | params norm: 295.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 3106.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.405179E+00 | gshard_loss: 5.932099E-03 | loss scale: 1024.0 | grad norm: 4.490 | num zeros: 129435712.0 | params norm: 295.664 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:         5376 | elapsed time per iteration (ms): 2555.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.254910E+00 | gshard_loss: 5.810041E-03 | loss scale: 1024.0 | grad norm: 4.411 | num zeros: 104973416.0 | params norm: 295.706 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 2537.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.114864E+00 | gshard_loss: 5.653528E-03 | loss scale: 1024.0 | grad norm: 4.253 | num zeros: 107798968.0 | params norm: 295.749 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:         5888 | elapsed time per iteration (ms): 2591.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.974255E+00 | gshard_loss: 5.515214E-03 | loss scale: 1024.0 | grad norm: 4.176 | num zeros: 32762506.0 | params norm: 295.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 2999.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.857416E+00 | gshard_loss: 5.472558E-03 | loss scale: 1024.0 | grad norm: 3.980 | num zeros: 92893760.0 | params norm: 295.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:         6400 | elapsed time per iteration (ms): 2627.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.751616E+00 | gshard_loss: 5.341977E-03 | loss scale: 1024.0 | grad norm: 3.736 | num zeros: 71754128.0 | params norm: 295.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 2660.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.660965E+00 | gshard_loss: 5.254386E-03 | loss scale: 1024.0 | grad norm: 3.397 | num zeros: 69446136.0 | params norm: 295.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:         6912 | elapsed time per iteration (ms): 3176.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.574411E+00 | gshard_loss: 5.073464E-03 | loss scale: 1024.0 | grad norm: 3.013 | num zeros: 72992488.0 | params norm: 295.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 2826.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.504465E+00 | gshard_loss: 4.899692E-03 | loss scale: 1024.0 | grad norm: 2.500 | num zeros: 78997304.0 | params norm: 296.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:         7424 | elapsed time per iteration (ms): 2912.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.443344E+00 | gshard_loss: 4.844199E-03 | loss scale: 1024.0 | grad norm: 1.915 | num zeros: 80253920.0 | params norm: 296.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 2862.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.392289E+00 | gshard_loss: 4.710856E-03 | loss scale: 1024.0 | grad norm: 1.255 | num zeros: 78400264.0 | params norm: 296.105 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:         7936 | elapsed time per iteration (ms): 4077.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.375079E+00 | gshard_loss: 4.634541E-03 | loss scale: 1024.0 | grad norm: 0.651 | num zeros: 72542928.0 | params norm: 296.147 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 2880.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.352907E+00 | gshard_loss: 4.602239E-03 | loss scale: 1024.0 | grad norm: 0.633 | num zeros: 43680868.0 | params norm: 296.189 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:         8448 | elapsed time per iteration (ms): 3101.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.368979E+00 | gshard_loss: 4.609315E-03 | loss scale: 1024.0 | grad norm: 1.074 | num zeros: 35994468.0 | params norm: 296.230 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 3033.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.367360E+00 | gshard_loss: 4.607274E-03 | loss scale: 1024.0 | grad norm: 1.324 | num zeros: 30378840.0 | params norm: 296.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:         8960 | elapsed time per iteration (ms): 2864.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.368970E+00 | gshard_loss: 4.610469E-03 | loss scale: 1024.0 | grad norm: 1.432 | num zeros: 26405140.0 | params norm: 296.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 2958.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.362425E+00 | gshard_loss: 4.557603E-03 | loss scale: 1024.0 | grad norm: 1.309 | num zeros: 24319116.0 | params norm: 296.351 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:         9472 | elapsed time per iteration (ms): 3594.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.371821E+00 | gshard_loss: 4.542815E-03 | loss scale: 1024.0 | grad norm: 1.201 | num zeros: 20587412.0 | params norm: 296.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 2892.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.365893E+00 | gshard_loss: 4.510287E-03 | loss scale: 1024.0 | grad norm: 0.994 | num zeros: 21889560.0 | params norm: 296.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:         9984 | elapsed time per iteration (ms): 2861.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.371314E+00 | gshard_loss: 4.513928E-03 | loss scale: 1024.0 | grad norm: 0.675 | num zeros: 24716810.0 | params norm: 296.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 3323.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.340424E+00 | gshard_loss: 4.555784E-03 | loss scale: 1024.0 | grad norm: 0.410 | num zeros: 31370096.0 | params norm: 296.510 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        10496 | elapsed time per iteration (ms): 2946.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.333618E+00 | gshard_loss: 4.571497E-03 | loss scale: 1024.0 | grad norm: 0.299 | num zeros: 69246280.0 | params norm: 296.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 2870.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.355355E+00 | gshard_loss: 4.526726E-03 | loss scale: 1024.0 | grad norm: 0.277 | num zeros: 96075504.0 | params norm: 296.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        11008 | elapsed time per iteration (ms): 3294.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.332641E+00 | gshard_loss: 4.478246E-03 | loss scale: 1024.0 | grad norm: 0.411 | num zeros: 89690440.0 | params norm: 296.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 3415.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.789193E+00 | gshard_loss: 4.499729E-03 | loss scale: 1024.0 | grad norm: 0.400 | num zeros: 5854008.0 | params norm: 296.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        11520 | elapsed time per iteration (ms): 2625.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.218470E+00 | gshard_loss: 4.539953E-03 | loss scale: 1024.0 | grad norm: 0.442 | num zeros: 15738951.0 | params norm: 296.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 2626.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.830318E+00 | gshard_loss: 4.871374E-03 | loss scale: 1024.0 | grad norm: 0.688 | num zeros: 10628146.0 | params norm: 296.689 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        12032 | elapsed time per iteration (ms): 2624.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.468859E+00 | gshard_loss: 5.081483E-03 | loss scale: 1024.0 | grad norm: 2.237 | num zeros: 16581745.0 | params norm: 296.706 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 2540.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.341526E+00 | gshard_loss: 4.983679E-03 | loss scale: 1024.0 | grad norm: 0.921 | num zeros: 15342446.0 | params norm: 296.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        12544 | elapsed time per iteration (ms): 2474.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.340510E+00 | gshard_loss: 5.396964E-03 | loss scale: 1024.0 | grad norm: 0.655 | num zeros: 17383838.0 | params norm: 296.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 2561.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.333260E+00 | gshard_loss: 5.702595E-03 | loss scale: 1024.0 | grad norm: 0.534 | num zeros: 14854113.0 | params norm: 296.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        13056 | elapsed time per iteration (ms): 3825.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.329025E+00 | gshard_loss: 6.774724E-03 | loss scale: 1024.0 | grad norm: 0.328 | num zeros: 11678501.0 | params norm: 296.785 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 2525.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.312396E+00 | gshard_loss: 7.939407E-03 | loss scale: 1024.0 | grad norm: 0.536 | num zeros: 2753596.0 | params norm: 296.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        13568 | elapsed time per iteration (ms): 2465.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.337502E+00 | gshard_loss: 7.353072E-03 | loss scale: 1024.0 | grad norm: 4.055 | num zeros: 248594.0 | params norm: 296.823 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 2832.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.315628E+00 | gshard_loss: 7.646920E-03 | loss scale: 1024.0 | grad norm: 0.365 | num zeros: 6789203.0 | params norm: 296.842 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        14080 | elapsed time per iteration (ms): 2557.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.314015E+00 | gshard_loss: 7.921064E-03 | loss scale: 1024.0 | grad norm: 0.478 | num zeros: 57695072.0 | params norm: 296.861 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 2489.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.332572E+00 | gshard_loss: 7.815002E-03 | loss scale: 1024.0 | grad norm: 0.519 | num zeros: 57196316.0 | params norm: 296.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        14592 | elapsed time per iteration (ms): 2528.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.308772E+00 | gshard_loss: 7.762686E-03 | loss scale: 1024.0 | grad norm: 0.706 | num zeros: 53785392.0 | params norm: 296.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 3318.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.303014E+00 | gshard_loss: 7.706651E-03 | loss scale: 1024.0 | grad norm: 0.574 | num zeros: 53349440.0 | params norm: 296.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        15104 | elapsed time per iteration (ms): 2593.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.296142E+00 | gshard_loss: 7.735523E-03 | loss scale: 1024.0 | grad norm: 0.556 | num zeros: 53601192.0 | params norm: 296.937 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 2531.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.289356E+00 | gshard_loss: 7.819412E-03 | loss scale: 1024.0 | grad norm: 0.344 | num zeros: 53572800.0 | params norm: 296.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        15616 | elapsed time per iteration (ms): 2515.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.320878E+00 | gshard_loss: 7.883214E-03 | loss scale: 1024.0 | grad norm: 18.493 | num zeros: 26268704.0 | params norm: 296.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 2625.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.289189E+00 | gshard_loss: 7.901821E-03 | loss scale: 1024.0 | grad norm: 0.698 | num zeros: 53787800.0 | params norm: 296.994 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        16128 | elapsed time per iteration (ms): 2734.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.264459E+00 | gshard_loss: 7.655004E-03 | loss scale: 1024.0 | grad norm: 1.222 | num zeros: 53994456.0 | params norm: 297.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 2506.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.267387E+00 | gshard_loss: 7.684233E-03 | loss scale: 1024.0 | grad norm: 0.506 | num zeros: 53428500.0 | params norm: 297.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        16640 | elapsed time per iteration (ms): 3178.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.276063E+00 | gshard_loss: 7.971535E-03 | loss scale: 1024.0 | grad norm: 1.224 | num zeros: 53013948.0 | params norm: 297.052 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 3463.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.257453E+00 | gshard_loss: 7.807843E-03 | loss scale: 1024.0 | grad norm: 0.556 | num zeros: 53158744.0 | params norm: 297.072 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        17152 | elapsed time per iteration (ms): 2483.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.258118E+00 | gshard_loss: 8.160766E-03 | loss scale: 1024.0 | grad norm: 0.586 | num zeros: 53406536.0 | params norm: 297.091 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 2519.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.254036E+00 | gshard_loss: 8.581548E-03 | loss scale: 1024.0 | grad norm: 0.551 | num zeros: 52848252.0 | params norm: 297.110 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        17664 | elapsed time per iteration (ms): 2575.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.246538E+00 | gshard_loss: 9.126917E-03 | loss scale: 1024.0 | grad norm: 0.615 | num zeros: 55783560.0 | params norm: 297.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 2492.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.237313E+00 | gshard_loss: 9.443847E-03 | loss scale: 1024.0 | grad norm: 1.403 | num zeros: 54607504.0 | params norm: 297.145 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        18176 | elapsed time per iteration (ms): 2546.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.226257E+00 | gshard_loss: 9.538806E-03 | loss scale: 1024.0 | grad norm: 0.690 | num zeros: 32392822.0 | params norm: 297.162 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 3123.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.239780E+00 | gshard_loss: 9.340221E-03 | loss scale: 1024.0 | grad norm: 0.316 | num zeros: 31265244.0 | params norm: 297.178 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        18688 | elapsed time per iteration (ms): 2953.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.232525E+00 | gshard_loss: 8.928990E-03 | loss scale: 1024.0 | grad norm: 0.611 | num zeros: 31097796.0 | params norm: 297.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 2563.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.215224E+00 | gshard_loss: 8.694606E-03 | loss scale: 1024.0 | grad norm: 0.701 | num zeros: 27575788.0 | params norm: 297.211 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        19200 | elapsed time per iteration (ms): 2622.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.212106E+00 | gshard_loss: 8.477431E-03 | loss scale: 1024.0 | grad norm: 0.707 | num zeros: 27663984.0 | params norm: 297.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 2742.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.196224E+00 | gshard_loss: 8.454429E-03 | loss scale: 1024.0 | grad norm: 0.604 | num zeros: 27779692.0 | params norm: 297.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        19712 | elapsed time per iteration (ms): 2922.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.212643E+00 | gshard_loss: 8.350661E-03 | loss scale: 1024.0 | grad norm: 1.469 | num zeros: 27494176.0 | params norm: 297.262 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 2598.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.196330E+00 | gshard_loss: 8.292254E-03 | loss scale: 1024.0 | grad norm: 2.534 | num zeros: 27156368.0 | params norm: 297.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        20224 | elapsed time per iteration (ms): 2641.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.192522E+00 | gshard_loss: 8.268223E-03 | loss scale: 1024.0 | grad norm: 1.185 | num zeros: 27372078.0 | params norm: 297.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 3679.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.208049E+00 | gshard_loss: 8.167314E-03 | loss scale: 1024.0 | grad norm: 2.036 | num zeros: 26901756.0 | params norm: 297.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        20736 | elapsed time per iteration (ms): 2636.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.199265E+00 | gshard_loss: 7.756998E-03 | loss scale: 1024.0 | grad norm: 2.223 | num zeros: 28092596.0 | params norm: 297.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 2554.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.175935E+00 | gshard_loss: 8.044792E-03 | loss scale: 1024.0 | grad norm: 0.736 | num zeros: 27944504.0 | params norm: 297.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        21248 | elapsed time per iteration (ms): 2692.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.211777E+00 | gshard_loss: 8.167040E-03 | loss scale: 1024.0 | grad norm: 0.822 | num zeros: 27590096.0 | params norm: 297.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 2961.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.179532E+00 | gshard_loss: 7.726653E-03 | loss scale: 1024.0 | grad norm: 0.750 | num zeros: 37251904.0 | params norm: 297.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        21760 | elapsed time per iteration (ms): 2600.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.170927E+00 | gshard_loss: 7.936219E-03 | loss scale: 1024.0 | grad norm: 0.522 | num zeros: 28884696.0 | params norm: 297.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 2578.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.184611E+00 | gshard_loss: 8.023418E-03 | loss scale: 1024.0 | grad norm: 0.764 | num zeros: 27200804.0 | params norm: 297.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        22272 | elapsed time per iteration (ms): 2671.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.169607E+00 | gshard_loss: 8.138152E-03 | loss scale: 1024.0 | grad norm: 0.847 | num zeros: 26879580.0 | params norm: 297.422 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 2572.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.164489E+00 | gshard_loss: 7.922834E-03 | loss scale: 1024.0 | grad norm: 1.145 | num zeros: 27093268.0 | params norm: 297.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        22784 | elapsed time per iteration (ms): 2573.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.159055E+00 | gshard_loss: 7.762980E-03 | loss scale: 1024.0 | grad norm: 0.685 | num zeros: 27389478.0 | params norm: 297.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 2625.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.165902E+00 | gshard_loss: 7.615169E-03 | loss scale: 1024.0 | grad norm: 0.985 | num zeros: 26857468.0 | params norm: 297.470 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        23296 | elapsed time per iteration (ms): 3520.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.148857E+00 | gshard_loss: 7.416342E-03 | loss scale: 1024.0 | grad norm: 1.173 | num zeros: 27266036.0 | params norm: 297.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 2597.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.144471E+00 | gshard_loss: 7.336848E-03 | loss scale: 1024.0 | grad norm: 1.109 | num zeros: 29319048.0 | params norm: 297.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        23808 | elapsed time per iteration (ms): 2685.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.134316E+00 | gshard_loss: 7.353602E-03 | loss scale: 1024.0 | grad norm: 1.027 | num zeros: 31089800.0 | params norm: 297.520 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 2696.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.128804E+00 | gshard_loss: 7.349489E-03 | loss scale: 1024.0 | grad norm: 0.736 | num zeros: 31325036.0 | params norm: 297.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        24320 | elapsed time per iteration (ms): 2821.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.131347E+00 | gshard_loss: 7.326471E-03 | loss scale: 1024.0 | grad norm: 1.374 | num zeros: 30297516.0 | params norm: 297.555 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 2640.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.125188E+00 | gshard_loss: 7.257818E-03 | loss scale: 1024.0 | grad norm: 0.876 | num zeros: 30432540.0 | params norm: 297.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        24832 | elapsed time per iteration (ms): 2641.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.130064E+00 | gshard_loss: 7.141889E-03 | loss scale: 1024.0 | grad norm: 1.193 | num zeros: 53636152.0 | params norm: 297.591 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 3292.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.115033E+00 | gshard_loss: 7.075555E-03 | loss scale: 1024.0 | grad norm: 0.599 | num zeros: 53517848.0 | params norm: 297.609 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        25344 | elapsed time per iteration (ms): 2672.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.109960E+00 | gshard_loss: 6.964054E-03 | loss scale: 1024.0 | grad norm: 0.583 | num zeros: 54547872.0 | params norm: 297.627 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 2680.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.121435E+00 | gshard_loss: 6.965002E-03 | loss scale: 1024.0 | grad norm: 0.458 | num zeros: 54856996.0 | params norm: 297.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        25856 | elapsed time per iteration (ms): 3196.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.119182E+00 | gshard_loss: 6.898237E-03 | loss scale: 1024.0 | grad norm: 0.720 | num zeros: 54622016.0 | params norm: 297.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 2765.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.112062E+00 | gshard_loss: 7.001561E-03 | loss scale: 1024.0 | grad norm: 0.539 | num zeros: 54231952.0 | params norm: 297.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        26368 | elapsed time per iteration (ms): 2758.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.099344E+00 | gshard_loss: 6.963507E-03 | loss scale: 1024.0 | grad norm: 0.705 | num zeros: 56673912.0 | params norm: 297.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 2692.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.116545E+00 | gshard_loss: 6.964696E-03 | loss scale: 1024.0 | grad norm: 0.361 | num zeros: 58188964.0 | params norm: 297.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        26880 | elapsed time per iteration (ms): 3659.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.106428E+00 | gshard_loss: 7.070244E-03 | loss scale: 1024.0 | grad norm: 0.395 | num zeros: 59032804.0 | params norm: 297.735 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 3028.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.108587E+00 | gshard_loss: 7.062648E-03 | loss scale: 1024.0 | grad norm: 0.468 | num zeros: 59767796.0 | params norm: 297.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        27392 | elapsed time per iteration (ms): 2750.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.119360E+00 | gshard_loss: 7.140278E-03 | loss scale: 1024.0 | grad norm: 0.681 | num zeros: 54530976.0 | params norm: 297.771 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 2955.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.101237E+00 | gshard_loss: 7.015845E-03 | loss scale: 1024.0 | grad norm: 0.559 | num zeros: 63341360.0 | params norm: 297.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        27904 | elapsed time per iteration (ms): 2815.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.096724E+00 | gshard_loss: 6.883021E-03 | loss scale: 1024.0 | grad norm: 0.412 | num zeros: 69133288.0 | params norm: 297.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 2691.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.095243E+00 | gshard_loss: 7.003928E-03 | loss scale: 1024.0 | grad norm: 0.487 | num zeros: 69289016.0 | params norm: 297.822 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        28416 | elapsed time per iteration (ms): 2778.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.085717E+00 | gshard_loss: 7.080389E-03 | loss scale: 1024.0 | grad norm: 0.478 | num zeros: 66349720.0 | params norm: 297.839 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 3423.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.082677E+00 | gshard_loss: 7.321270E-03 | loss scale: 1024.0 | grad norm: 0.451 | num zeros: 67531728.0 | params norm: 297.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        28928 | elapsed time per iteration (ms): 2867.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.072809E+00 | gshard_loss: 7.135616E-03 | loss scale: 1024.0 | grad norm: 0.404 | num zeros: 59907364.0 | params norm: 297.871 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 2764.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.073475E+00 | gshard_loss: 7.074873E-03 | loss scale: 1024.0 | grad norm: 0.417 | num zeros: 57859168.0 | params norm: 297.887 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        29440 | elapsed time per iteration (ms): 3389.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.063804E+00 | gshard_loss: 7.170650E-03 | loss scale: 1024.0 | grad norm: 0.448 | num zeros: 58664788.0 | params norm: 297.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 2736.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.059340E+00 | gshard_loss: 7.107513E-03 | loss scale: 1024.0 | grad norm: 0.377 | num zeros: 58240300.0 | params norm: 297.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        29952 | elapsed time per iteration (ms): 2719.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.059233E+00 | gshard_loss: 7.077355E-03 | loss scale: 1024.0 | grad norm: 0.278 | num zeros: 59905508.0 | params norm: 297.938 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 3010.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.061419E+00 | gshard_loss: 7.074644E-03 | loss scale: 1024.0 | grad norm: 0.273 | num zeros: 37639600.0 | params norm: 297.955 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        30464 | elapsed time per iteration (ms): 3211.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.045165E+00 | gshard_loss: 6.873587E-03 | loss scale: 1024.0 | grad norm: 0.279 | num zeros: 31511484.0 | params norm: 297.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 2770.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.043735E+00 | gshard_loss: 6.785881E-03 | loss scale: 1024.0 | grad norm: 0.246 | num zeros: 31128632.0 | params norm: 297.991 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        30976 | elapsed time per iteration (ms): 2773.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.033165E+00 | gshard_loss: 6.762221E-03 | loss scale: 1024.0 | grad norm: 0.291 | num zeros: 23921884.0 | params norm: 298.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 3340.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.028186E+00 | gshard_loss: 6.720461E-03 | loss scale: 1024.0 | grad norm: 0.311 | num zeros: 21896388.0 | params norm: 298.029 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        31488 | elapsed time per iteration (ms): 2744.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.023306E+00 | gshard_loss: 6.725662E-03 | loss scale: 1024.0 | grad norm: 0.261 | num zeros: 26649566.0 | params norm: 298.048 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 2872.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.011865E+00 | gshard_loss: 6.927203E-03 | loss scale: 1024.0 | grad norm: 0.322 | num zeros: 26443460.0 | params norm: 298.068 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        32000 | elapsed time per iteration (ms): 2740.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.008957E+00 | gshard_loss: 7.054474E-03 | loss scale: 1024.0 | grad norm: 0.220 | num zeros: 35455416.0 | params norm: 298.089 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 3389.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.014106E+00 | gshard_loss: 7.027113E-03 | loss scale: 1024.0 | grad norm: 0.942 | num zeros: 42437708.0 | params norm: 298.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        32512 | elapsed time per iteration (ms): 2673.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.008990E+00 | gshard_loss: 7.051422E-03 | loss scale: 1024.0 | grad norm: 0.392 | num zeros: 24513000.0 | params norm: 298.129 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 2691.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.988797E+00 | gshard_loss: 6.904799E-03 | loss scale: 1024.0 | grad norm: 0.302 | num zeros: 39496044.0 | params norm: 298.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        33024 | elapsed time per iteration (ms): 3034.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.997922E+00 | gshard_loss: 7.053599E-03 | loss scale: 1024.0 | grad norm: 0.361 | num zeros: 31011916.0 | params norm: 298.170 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 2676.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.030378E+00 | gshard_loss: 7.287859E-03 | loss scale: 1024.0 | grad norm: 1.277 | num zeros: 31997746.0 | params norm: 298.189 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        33536 | elapsed time per iteration (ms): 2727.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.993234E+00 | gshard_loss: 7.150534E-03 | loss scale: 1024.0 | grad norm: 1.004 | num zeros: 63542872.0 | params norm: 298.209 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 2801.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.987946E+00 | gshard_loss: 7.066739E-03 | loss scale: 1024.0 | grad norm: 0.644 | num zeros: 90822736.0 | params norm: 298.229 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        34048 | elapsed time per iteration (ms): 3370.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.985796E+00 | gshard_loss: 7.167575E-03 | loss scale: 1024.0 | grad norm: 0.320 | num zeros: 34895728.0 | params norm: 298.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 2673.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.981222E+00 | gshard_loss: 7.135670E-03 | loss scale: 1024.0 | grad norm: 0.662 | num zeros: 46418764.0 | params norm: 298.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        34560 | elapsed time per iteration (ms): 2723.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.965164E+00 | gshard_loss: 7.178952E-03 | loss scale: 1024.0 | grad norm: 0.549 | num zeros: 42852520.0 | params norm: 298.288 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 2943.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.962694E+00 | gshard_loss: 7.321340E-03 | loss scale: 1024.0 | grad norm: 0.267 | num zeros: 45472792.0 | params norm: 298.308 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        35072 | elapsed time per iteration (ms): 2595.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.955518E+00 | gshard_loss: 7.375800E-03 | loss scale: 1024.0 | grad norm: 0.342 | num zeros: 44437212.0 | params norm: 298.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 2688.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.941917E+00 | gshard_loss: 7.337534E-03 | loss scale: 1024.0 | grad norm: 0.234 | num zeros: 40235336.0 | params norm: 298.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        35584 | elapsed time per iteration (ms): 3010.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.952759E+00 | gshard_loss: 7.228606E-03 | loss scale: 1024.0 | grad norm: 0.419 | num zeros: 39821264.0 | params norm: 298.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 3039.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.947072E+00 | gshard_loss: 7.191199E-03 | loss scale: 1024.0 | grad norm: 0.291 | num zeros: 36994988.0 | params norm: 298.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        36096 | elapsed time per iteration (ms): 2653.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.917119E+00 | gshard_loss: 7.321985E-03 | loss scale: 1024.0 | grad norm: 0.330 | num zeros: 36489368.0 | params norm: 298.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 2722.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.916400E+00 | gshard_loss: 7.383832E-03 | loss scale: 1024.0 | grad norm: 0.291 | num zeros: 37603448.0 | params norm: 298.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        36608 | elapsed time per iteration (ms): 3345.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.927173E+00 | gshard_loss: 7.298650E-03 | loss scale: 1024.0 | grad norm: 0.242 | num zeros: 36971768.0 | params norm: 298.434 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 2690.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.905730E+00 | gshard_loss: 7.202442E-03 | loss scale: 1024.0 | grad norm: 0.408 | num zeros: 34742940.0 | params norm: 298.451 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        37120 | elapsed time per iteration (ms): 2663.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.892941E+00 | gshard_loss: 7.364177E-03 | loss scale: 1024.0 | grad norm: 0.245 | num zeros: 26945516.0 | params norm: 298.467 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 2791.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.900390E+00 | gshard_loss: 7.533262E-03 | loss scale: 1024.0 | grad norm: 0.321 | num zeros: 26648852.0 | params norm: 298.483 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        37632 | elapsed time per iteration (ms): 3718.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.873227E+00 | gshard_loss: 7.397766E-03 | loss scale: 1024.0 | grad norm: 0.278 | num zeros: 29726496.0 | params norm: 298.498 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 2687.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.877376E+00 | gshard_loss: 7.580012E-03 | loss scale: 1024.0 | grad norm: 0.324 | num zeros: 28464434.0 | params norm: 298.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        38144 | elapsed time per iteration (ms): 2659.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.864758E+00 | gshard_loss: 7.796299E-03 | loss scale: 1024.0 | grad norm: 0.273 | num zeros: 27862736.0 | params norm: 298.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 2848.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.869068E+00 | gshard_loss: 8.023115E-03 | loss scale: 1024.0 | grad norm: 0.432 | num zeros: 27595308.0 | params norm: 298.545 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        38656 | elapsed time per iteration (ms): 2932.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.841323E+00 | gshard_loss: 7.955305E-03 | loss scale: 1024.0 | grad norm: 0.295 | num zeros: 28803106.0 | params norm: 298.563 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 2790.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.841475E+00 | gshard_loss: 7.979671E-03 | loss scale: 1024.0 | grad norm: 0.323 | num zeros: 26983208.0 | params norm: 298.582 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        39168 | elapsed time per iteration (ms): 2751.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.814659E+00 | gshard_loss: 7.871043E-03 | loss scale: 1024.0 | grad norm: 0.387 | num zeros: 26984060.0 | params norm: 298.604 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 3128.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.832242E+00 | gshard_loss: 8.307692E-03 | loss scale: 1024.0 | grad norm: 0.493 | num zeros: 26758824.0 | params norm: 298.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        39680 | elapsed time per iteration (ms): 2659.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.855797E+00 | gshard_loss: 7.533278E-03 | loss scale: 1024.0 | grad norm: 0.995 | num zeros: 26497194.0 | params norm: 298.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 2575.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.828113E+00 | gshard_loss: 7.894984E-03 | loss scale: 1024.0 | grad norm: 0.588 | num zeros: 29289768.0 | params norm: 298.671 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        40192 | elapsed time per iteration (ms): 2823.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.818671E+00 | gshard_loss: 7.598527E-03 | loss scale: 1024.0 | grad norm: 0.436 | num zeros: 33887352.0 | params norm: 298.692 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 2665.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.830125E+00 | gshard_loss: 7.078363E-03 | loss scale: 1024.0 | grad norm: 0.504 | num zeros: 29400552.0 | params norm: 298.714 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        40704 | elapsed time per iteration (ms): 2689.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.812733E+00 | gshard_loss: 7.178212E-03 | loss scale: 1024.0 | grad norm: 0.381 | num zeros: 27272382.0 | params norm: 298.736 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 2970.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.797562E+00 | gshard_loss: 7.665092E-03 | loss scale: 1024.0 | grad norm: 0.367 | num zeros: 30349372.0 | params norm: 298.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        41216 | elapsed time per iteration (ms): 3420.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.799894E+00 | gshard_loss: 7.992877E-03 | loss scale: 1024.0 | grad norm: 0.400 | num zeros: 30537012.0 | params norm: 298.777 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 2549.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.794906E+00 | gshard_loss: 7.936314E-03 | loss scale: 1024.0 | grad norm: 0.349 | num zeros: 23504036.0 | params norm: 298.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        41728 | elapsed time per iteration (ms): 2611.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.795172E+00 | gshard_loss: 7.975809E-03 | loss scale: 1024.0 | grad norm: 0.359 | num zeros: 30710098.0 | params norm: 298.817 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 2798.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.771766E+00 | gshard_loss: 8.167721E-03 | loss scale: 1024.0 | grad norm: 0.306 | num zeros: 27221146.0 | params norm: 298.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        42240 | elapsed time per iteration (ms): 2646.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.776896E+00 | gshard_loss: 8.240082E-03 | loss scale: 1024.0 | grad norm: 0.378 | num zeros: 26754232.0 | params norm: 298.858 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 2583.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.753755E+00 | gshard_loss: 8.522627E-03 | loss scale: 1024.0 | grad norm: 0.547 | num zeros: 28151832.0 | params norm: 298.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        42752 | elapsed time per iteration (ms): 2617.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.770627E+00 | gshard_loss: 7.892495E-03 | loss scale: 1024.0 | grad norm: 0.762 | num zeros: 29688322.0 | params norm: 298.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 3750.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.728178E+00 | gshard_loss: 8.143703E-03 | loss scale: 1024.0 | grad norm: 0.599 | num zeros: 32574352.0 | params norm: 298.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        43264 | elapsed time per iteration (ms): 2560.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.734079E+00 | gshard_loss: 8.290796E-03 | loss scale: 1024.0 | grad norm: 0.437 | num zeros: 28206852.0 | params norm: 298.937 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 2627.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.735902E+00 | gshard_loss: 8.012213E-03 | loss scale: 1024.0 | grad norm: 0.374 | num zeros: 27268836.0 | params norm: 298.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        43776 | elapsed time per iteration (ms): 2962.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.731997E+00 | gshard_loss: 8.159228E-03 | loss scale: 1024.0 | grad norm: 0.546 | num zeros: 30264300.0 | params norm: 298.976 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 2600.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.716680E+00 | gshard_loss: 8.064885E-03 | loss scale: 1024.0 | grad norm: 0.337 | num zeros: 30455072.0 | params norm: 298.996 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        44288 | elapsed time per iteration (ms): 2531.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.698209E+00 | gshard_loss: 8.063715E-03 | loss scale: 1024.0 | grad norm: 0.373 | num zeros: 35491240.0 | params norm: 299.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 2620.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.696285E+00 | gshard_loss: 7.787203E-03 | loss scale: 1024.0 | grad norm: 0.337 | num zeros: 28855186.0 | params norm: 299.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        44800 | elapsed time per iteration (ms): 3526.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.697124E+00 | gshard_loss: 7.723584E-03 | loss scale: 1024.0 | grad norm: 0.361 | num zeros: 31304140.0 | params norm: 299.050 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 2661.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.682804E+00 | gshard_loss: 8.175696E-03 | loss scale: 1024.0 | grad norm: 0.382 | num zeros: 27834436.0 | params norm: 299.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        45312 | elapsed time per iteration (ms): 2510.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.677729E+00 | gshard_loss: 8.117935E-03 | loss scale: 1024.0 | grad norm: 0.442 | num zeros: 27653448.0 | params norm: 299.085 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 2734.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.668394E+00 | gshard_loss: 7.843144E-03 | loss scale: 1024.0 | grad norm: 0.366 | num zeros: 27546416.0 | params norm: 299.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        45824 | elapsed time per iteration (ms): 2819.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.651832E+00 | gshard_loss: 7.890720E-03 | loss scale: 1024.0 | grad norm: 0.318 | num zeros: 11562484.0 | params norm: 299.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 2531.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.639975E+00 | gshard_loss: 7.868348E-03 | loss scale: 1024.0 | grad norm: 0.352 | num zeros: 15248871.0 | params norm: 299.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        46336 | elapsed time per iteration (ms): 2519.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.645609E+00 | gshard_loss: 8.001987E-03 | loss scale: 1024.0 | grad norm: 0.304 | num zeros: 6014162.0 | params norm: 299.157 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 3463.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.651763E+00 | gshard_loss: 7.726731E-03 | loss scale: 1024.0 | grad norm: 0.361 | num zeros: 12804221.0 | params norm: 299.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        46848 | elapsed time per iteration (ms): 2734.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.639040E+00 | gshard_loss: 7.795391E-03 | loss scale: 1024.0 | grad norm: 0.420 | num zeros: 3252482.0 | params norm: 299.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 2777.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.624501E+00 | gshard_loss: 7.581498E-03 | loss scale: 1024.0 | grad norm: 0.418 | num zeros: 4999855.0 | params norm: 299.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        47360 | elapsed time per iteration (ms): 2556.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.610146E+00 | gshard_loss: 7.595269E-03 | loss scale: 1024.0 | grad norm: 0.405 | num zeros: 10064598.0 | params norm: 299.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 2858.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.611560E+00 | gshard_loss: 7.424466E-03 | loss scale: 1024.0 | grad norm: 0.418 | num zeros: 11927617.0 | params norm: 299.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        47872 | elapsed time per iteration (ms): 2594.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.620173E+00 | gshard_loss: 7.619685E-03 | loss scale: 1024.0 | grad norm: 0.525 | num zeros: 16119747.0 | params norm: 299.266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 2580.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.611973E+00 | gshard_loss: 7.402275E-03 | loss scale: 1024.0 | grad norm: 0.603 | num zeros: 12049520.0 | params norm: 299.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        48384 | elapsed time per iteration (ms): 2929.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.595726E+00 | gshard_loss: 7.385002E-03 | loss scale: 1024.0 | grad norm: 0.295 | num zeros: 4999225.0 | params norm: 299.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 3463.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.592250E+00 | gshard_loss: 7.316899E-03 | loss scale: 1024.0 | grad norm: 0.475 | num zeros: 2128389.0 | params norm: 299.324 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        48896 | elapsed time per iteration (ms): 2637.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.598701E+00 | gshard_loss: 7.172624E-03 | loss scale: 1024.0 | grad norm: 0.508 | num zeros: 1316256.0 | params norm: 299.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 2815.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.585173E+00 | gshard_loss: 7.164261E-03 | loss scale: 1024.0 | grad norm: 0.290 | num zeros: 2052877.0 | params norm: 299.365 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        49408 | elapsed time per iteration (ms): 2661.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.565915E+00 | gshard_loss: 7.196791E-03 | loss scale: 1024.0 | grad norm: 0.414 | num zeros: 6031514.0 | params norm: 299.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 2639.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.569232E+00 | gshard_loss: 7.094083E-03 | loss scale: 1024.0 | grad norm: 0.358 | num zeros: 1626907.0 | params norm: 299.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        49920 | elapsed time per iteration (ms): 2664.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.561291E+00 | gshard_loss: 7.025335E-03 | loss scale: 1024.0 | grad norm: 0.314 | num zeros: 1036941.0 | params norm: 299.426 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 2886.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.558379E+00 | gshard_loss: 7.055586E-03 | loss scale: 1024.0 | grad norm: 0.409 | num zeros: 841676.0 | params norm: 299.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:        50432 | elapsed time per iteration (ms): 3451.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.558357E+00 | gshard_loss: 6.867491E-03 | loss scale: 1024.0 | grad norm: 0.398 | num zeros: 600303.0 | params norm: 299.469 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 2630.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.547992E+00 | gshard_loss: 6.857601E-03 | loss scale: 1024.0 | grad norm: 0.274 | num zeros: 1588909.0 | params norm: 299.490 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:        50944 | elapsed time per iteration (ms): 2686.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.546109E+00 | gshard_loss: 6.923639E-03 | loss scale: 1024.0 | grad norm: 0.290 | num zeros: 753301.0 | params norm: 299.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 2991.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.544464E+00 | gshard_loss: 6.923300E-03 | loss scale: 1024.0 | grad norm: 0.268 | num zeros: 662931.0 | params norm: 299.533 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2022-12-23 01:48:02 
