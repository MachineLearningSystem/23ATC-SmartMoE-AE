+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 5 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 7 == 0 ']'
+ sleep 10s
+ '[' 0 == 0 ']'
+ hostname
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 1 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 2 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 6 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 3 == 0 ']'
+ '[' 4 == 0 ']'
+ sleep 10s
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 9 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 12 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 15 == 0 ']'
+ sleep 10s
+ '[' 8 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 10 == 0 ']'
+ sleep 10s
+ '[' 11 == 0 ']'
+ sleep 10s
+ '[' 13 == 0 ']'
+ sleep 10s
+ '[' 14 == 0 ']'
+ sleep 10s
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=2
+ RANK=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export RANK=5
+ RANK=5
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ localrank=5
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export RANK=7
+ RANK=7
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ python_args+='
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=15
+ RANK=15
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=10
+ RANK=10
+ WORLD_SIZE=16
+ localrank=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=6
+ NODE_RANK=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export NNODES=2
+ NNODES=2
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=4
+ NODE_RANK=4
+ export RANK=9
+ RANK=9
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == nico ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NNODES=2
+ NNODES=2
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_ON_smartGP16_shadowON_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:00:28+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 16
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2560
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 1
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 51200
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 4
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.186 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1]
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.442 seconds
time to initialize megatron (seconds): 73.731
[after megatron is initialized] datetime: 2022-12-23 01:00:57 
hhs=5120
hhs=5120
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2]
hhs=5120
hhs=5120
hhs=5120
building GPT model ...
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
hhs=5120
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6]
hhs=5120
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
hhs=5120
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
hhs=5120
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
hhs=5120
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
hhs=5120
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
hhs=5120
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
hhs=5120
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4]
hhs=5120
hhs=5120
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5]
hhs=5120
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 971371776
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-23 01:01:08 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.149343 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.035 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-23 01:01:25 
done with setup ...
training ...
[before the start of training step] datetime: 2022-12-23 01:01:25 
 iteration        1/     200 | consumed samples:          256 | elapsed time per iteration (ms): 14490.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082814E+01 | loss scale: 131072.0 | grad norm: 22.381 | num zeros: 31101.0 | params norm: 295.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18559.666015625 | max allocated: 22806.3564453125 | reserved: 26048.0 | max reserved: 26048.0
 iteration        2/     200 | consumed samples:          512 | elapsed time per iteration (ms): 5975.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.042279E+01 | loss scale: 131072.0 | grad norm: 12.340 | num zeros: 157589968.0 | params norm: 295.372 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:          768 | elapsed time per iteration (ms): 5554.0 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 295.372 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 5912.7 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 295.372 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         1280 | elapsed time per iteration (ms): 5510.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.029178E+01 | loss scale: 65536.0 | grad norm: 10.085 | num zeros: 33546.0 | params norm: 295.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 7761.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.852599E+00 | loss scale: 65536.0 | grad norm: 4.473 | num zeros: 131702064.0 | params norm: 295.393 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         1792 | elapsed time per iteration (ms): 5830.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.145815E+01 | loss scale: 65536.0 | grad norm: 15.875 | num zeros: 26554248.0 | params norm: 295.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 6140.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.465239E+00 | loss scale: 65536.0 | grad norm: 4.472 | num zeros: 210549856.0 | params norm: 295.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         2304 | elapsed time per iteration (ms): 6029.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.284475E+00 | loss scale: 65536.0 | grad norm: 4.462 | num zeros: 315519424.0 | params norm: 295.476 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 4978.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.069091E+00 | loss scale: 65536.0 | grad norm: 4.451 | num zeros: 238952896.0 | params norm: 295.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         2816 | elapsed time per iteration (ms): 6208.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.870899E+00 | loss scale: 65536.0 | grad norm: 4.409 | num zeros: 187642336.0 | params norm: 295.550 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 5575.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.686681E+00 | loss scale: 65536.0 | grad norm: 4.388 | num zeros: 132943016.0 | params norm: 295.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         3328 | elapsed time per iteration (ms): 6434.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.511883E+00 | loss scale: 65536.0 | grad norm: 4.292 | num zeros: 159694336.0 | params norm: 295.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 5763.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.348848E+00 | loss scale: 65536.0 | grad norm: 4.237 | num zeros: 239010480.0 | params norm: 295.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         3840 | elapsed time per iteration (ms): 5573.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.190759E+00 | loss scale: 65536.0 | grad norm: 4.154 | num zeros: 265778496.0 | params norm: 295.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 6133.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.061842E+00 | loss scale: 65536.0 | grad norm: 4.012 | num zeros: 291387680.0 | params norm: 295.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         4352 | elapsed time per iteration (ms): 6038.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.926725E+00 | loss scale: 65536.0 | grad norm: 3.873 | num zeros: 250863424.0 | params norm: 295.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 6877.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.810672E+00 | loss scale: 65536.0 | grad norm: 3.699 | num zeros: 219421552.0 | params norm: 295.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         4864 | elapsed time per iteration (ms): 6853.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.702102E+00 | loss scale: 65536.0 | grad norm: 3.432 | num zeros: 267065600.0 | params norm: 295.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 6680.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.610331E+00 | loss scale: 65536.0 | grad norm: 3.078 | num zeros: 195130208.0 | params norm: 295.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:         5376 | elapsed time per iteration (ms): 7322.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.539084E+00 | loss scale: 65536.0 | grad norm: 2.666 | num zeros: 197209440.0 | params norm: 295.985 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 6937.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.474435E+00 | loss scale: 65536.0 | grad norm: 2.098 | num zeros: 193685968.0 | params norm: 296.032 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:         5888 | elapsed time per iteration (ms): 6627.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.405926E+00 | loss scale: 65536.0 | grad norm: 1.574 | num zeros: 198237344.0 | params norm: 296.077 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 6429.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.375462E+00 | loss scale: 65536.0 | grad norm: 0.915 | num zeros: 171496096.0 | params norm: 296.121 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:         6400 | elapsed time per iteration (ms): 7191.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.356358E+00 | loss scale: 65536.0 | grad norm: 0.448 | num zeros: 170882880.0 | params norm: 296.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 6167.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.361854E+00 | loss scale: 65536.0 | grad norm: 0.542 | num zeros: 97222088.0 | params norm: 296.208 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:         6912 | elapsed time per iteration (ms): 5748.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.359934E+00 | loss scale: 65536.0 | grad norm: 0.748 | num zeros: 225511152.0 | params norm: 296.250 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 5435.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.363923E+00 | loss scale: 65536.0 | grad norm: 0.698 | num zeros: 125185928.0 | params norm: 296.293 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:         7424 | elapsed time per iteration (ms): 5446.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.358720E+00 | loss scale: 65536.0 | grad norm: 0.670 | num zeros: 270860992.0 | params norm: 296.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 6110.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.350903E+00 | loss scale: 65536.0 | grad norm: 0.663 | num zeros: 269198400.0 | params norm: 296.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:         7936 | elapsed time per iteration (ms): 5141.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.353192E+00 | loss scale: 65536.0 | grad norm: 0.534 | num zeros: 278906112.0 | params norm: 296.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 4986.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.335386E+00 | loss scale: 65536.0 | grad norm: 0.324 | num zeros: 256760544.0 | params norm: 296.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:         8448 | elapsed time per iteration (ms): 5294.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.342669E+00 | loss scale: 65536.0 | grad norm: 0.291 | num zeros: 246936768.0 | params norm: 296.502 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 5711.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.334901E+00 | loss scale: 65536.0 | grad norm: 0.240 | num zeros: 250224480.0 | params norm: 296.541 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:         8960 | elapsed time per iteration (ms): 5017.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.328765E+00 | loss scale: 65536.0 | grad norm: 0.264 | num zeros: 276361440.0 | params norm: 296.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 5703.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.321971E+00 | loss scale: 65536.0 | grad norm: 0.259 | num zeros: 267442080.0 | params norm: 296.615 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:         9472 | elapsed time per iteration (ms): 5331.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.333055E+00 | loss scale: 65536.0 | grad norm: 0.462 | num zeros: 206681584.0 | params norm: 296.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 5538.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.329973E+00 | loss scale: 65536.0 | grad norm: 0.301 | num zeros: 209491264.0 | params norm: 296.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:         9984 | elapsed time per iteration (ms): 6051.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.338878E+00 | loss scale: 65536.0 | grad norm: 0.473 | num zeros: 186181184.0 | params norm: 296.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 5598.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.307625E+00 | loss scale: 65536.0 | grad norm: 0.355 | num zeros: 188584784.0 | params norm: 296.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        10496 | elapsed time per iteration (ms): 5307.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.297330E+00 | loss scale: 65536.0 | grad norm: 0.392 | num zeros: 199574032.0 | params norm: 296.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 4873.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.315968E+00 | loss scale: 65536.0 | grad norm: 0.380 | num zeros: 183667584.0 | params norm: 296.790 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        11008 | elapsed time per iteration (ms): 5269.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.278702E+00 | loss scale: 65536.0 | grad norm: 0.599 | num zeros: 195186000.0 | params norm: 296.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 4705.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.270537E+00 | loss scale: 65536.0 | grad norm: 0.490 | num zeros: 236505456.0 | params norm: 296.840 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        11520 | elapsed time per iteration (ms): 5375.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.257318E+00 | loss scale: 65536.0 | grad norm: 0.562 | num zeros: 223290784.0 | params norm: 296.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 4666.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.241051E+00 | loss scale: 65536.0 | grad norm: 0.881 | num zeros: 173029392.0 | params norm: 296.890 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        12032 | elapsed time per iteration (ms): 5208.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.226567E+00 | loss scale: 65536.0 | grad norm: 0.502 | num zeros: 160058960.0 | params norm: 296.914 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 4346.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.216592E+00 | loss scale: 65536.0 | grad norm: 0.529 | num zeros: 197309712.0 | params norm: 296.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        12544 | elapsed time per iteration (ms): 5092.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.209707E+00 | loss scale: 65536.0 | grad norm: 0.656 | num zeros: 171544480.0 | params norm: 296.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 4674.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.187531E+00 | loss scale: 65536.0 | grad norm: 0.757 | num zeros: 161960992.0 | params norm: 296.987 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        13056 | elapsed time per iteration (ms): 5312.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.178154E+00 | loss scale: 65536.0 | grad norm: 0.377 | num zeros: 164480864.0 | params norm: 297.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 4641.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.158338E+00 | loss scale: 65536.0 | grad norm: 0.357 | num zeros: 179622992.0 | params norm: 297.032 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        13568 | elapsed time per iteration (ms): 4798.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.167432E+00 | loss scale: 65536.0 | grad norm: 0.337 | num zeros: 205220208.0 | params norm: 297.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 4646.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.146682E+00 | loss scale: 65536.0 | grad norm: 0.356 | num zeros: 238247568.0 | params norm: 297.077 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        14080 | elapsed time per iteration (ms): 5248.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.143509E+00 | loss scale: 65536.0 | grad norm: 0.493 | num zeros: 233930384.0 | params norm: 297.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 4754.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.162066E+00 | loss scale: 65536.0 | grad norm: 0.418 | num zeros: 187998432.0 | params norm: 297.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        14592 | elapsed time per iteration (ms): 4982.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.134676E+00 | loss scale: 65536.0 | grad norm: 0.473 | num zeros: 192093616.0 | params norm: 297.141 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 5076.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.129549E+00 | loss scale: 65536.0 | grad norm: 0.361 | num zeros: 193333712.0 | params norm: 297.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        15104 | elapsed time per iteration (ms): 5511.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.124782E+00 | loss scale: 65536.0 | grad norm: 0.376 | num zeros: 201938992.0 | params norm: 297.180 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 5277.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.120842E+00 | loss scale: 65536.0 | grad norm: 0.485 | num zeros: 198963504.0 | params norm: 297.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        15616 | elapsed time per iteration (ms): 5807.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.131758E+00 | loss scale: 65536.0 | grad norm: 0.371 | num zeros: 196946768.0 | params norm: 297.217 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 5071.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.117337E+00 | loss scale: 65536.0 | grad norm: 0.308 | num zeros: 191192432.0 | params norm: 297.235 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        16128 | elapsed time per iteration (ms): 5611.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.096604E+00 | loss scale: 65536.0 | grad norm: 0.383 | num zeros: 191718400.0 | params norm: 297.252 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 4841.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.095715E+00 | loss scale: 65536.0 | grad norm: 0.347 | num zeros: 191004528.0 | params norm: 297.269 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        16640 | elapsed time per iteration (ms): 4805.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.111963E+00 | loss scale: 65536.0 | grad norm: 1.090 | num zeros: 172568848.0 | params norm: 297.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 4985.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.124236E+00 | loss scale: 65536.0 | grad norm: 1.206 | num zeros: 173900816.0 | params norm: 297.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        17152 | elapsed time per iteration (ms): 6240.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.102543E+00 | loss scale: 65536.0 | grad norm: 0.613 | num zeros: 197972208.0 | params norm: 297.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 4453.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.085645E+00 | loss scale: 65536.0 | grad norm: 0.567 | num zeros: 163639152.0 | params norm: 297.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        17664 | elapsed time per iteration (ms): 4815.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.093832E+00 | loss scale: 65536.0 | grad norm: 0.764 | num zeros: 187215920.0 | params norm: 297.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 4774.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.079732E+00 | loss scale: 65536.0 | grad norm: 0.500 | num zeros: 209979440.0 | params norm: 297.357 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        18176 | elapsed time per iteration (ms): 5196.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.061488E+00 | loss scale: 65536.0 | grad norm: 0.324 | num zeros: 192558912.0 | params norm: 297.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 4710.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.095229E+00 | loss scale: 65536.0 | grad norm: 1.135 | num zeros: 135453328.0 | params norm: 297.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        18688 | elapsed time per iteration (ms): 4838.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.110253E+00 | loss scale: 65536.0 | grad norm: 0.902 | num zeros: 189008496.0 | params norm: 297.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 4787.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.066763E+00 | loss scale: 65536.0 | grad norm: 0.697 | num zeros: 173483328.0 | params norm: 297.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        19200 | elapsed time per iteration (ms): 5749.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.078135E+00 | loss scale: 65536.0 | grad norm: 0.666 | num zeros: 200071024.0 | params norm: 297.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 4676.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.060067E+00 | loss scale: 65536.0 | grad norm: 0.578 | num zeros: 200440432.0 | params norm: 297.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        19712 | elapsed time per iteration (ms): 4485.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.079378E+00 | loss scale: 65536.0 | grad norm: 0.643 | num zeros: 185135136.0 | params norm: 297.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 4592.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.058932E+00 | loss scale: 65536.0 | grad norm: 0.546 | num zeros: 158520928.0 | params norm: 297.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        20224 | elapsed time per iteration (ms): 5591.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.040177E+00 | loss scale: 65536.0 | grad norm: 0.483 | num zeros: 158526384.0 | params norm: 297.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 4665.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.053597E+00 | loss scale: 65536.0 | grad norm: 0.511 | num zeros: 121104688.0 | params norm: 297.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        20736 | elapsed time per iteration (ms): 5167.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.046931E+00 | loss scale: 65536.0 | grad norm: 0.463 | num zeros: 82661784.0 | params norm: 297.491 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 4514.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.020146E+00 | loss scale: 65536.0 | grad norm: 0.386 | num zeros: 99446752.0 | params norm: 297.504 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        21248 | elapsed time per iteration (ms): 4947.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.044932E+00 | loss scale: 65536.0 | grad norm: 0.446 | num zeros: 171043408.0 | params norm: 297.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 4947.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.016176E+00 | loss scale: 65536.0 | grad norm: 0.651 | num zeros: 164855728.0 | params norm: 297.529 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        21760 | elapsed time per iteration (ms): 5045.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.008053E+00 | loss scale: 65536.0 | grad norm: 0.487 | num zeros: 140075552.0 | params norm: 297.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 4435.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.034601E+00 | loss scale: 65536.0 | grad norm: 0.845 | num zeros: 135070976.0 | params norm: 297.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        22272 | elapsed time per iteration (ms): 5225.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.011102E+00 | loss scale: 65536.0 | grad norm: 0.336 | num zeros: 117608240.0 | params norm: 297.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 4580.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.007765E+00 | loss scale: 65536.0 | grad norm: 0.338 | num zeros: 150433504.0 | params norm: 297.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        22784 | elapsed time per iteration (ms): 4570.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.006194E+00 | loss scale: 65536.0 | grad norm: 0.368 | num zeros: 132010616.0 | params norm: 297.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 4629.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.008979E+00 | loss scale: 65536.0 | grad norm: 0.323 | num zeros: 141201584.0 | params norm: 297.605 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        23296 | elapsed time per iteration (ms): 4929.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.999006E+00 | loss scale: 65536.0 | grad norm: 0.678 | num zeros: 131651984.0 | params norm: 297.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 5107.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.992620E+00 | loss scale: 65536.0 | grad norm: 0.339 | num zeros: 142843952.0 | params norm: 297.631 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        23808 | elapsed time per iteration (ms): 4962.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.987365E+00 | loss scale: 65536.0 | grad norm: 0.437 | num zeros: 157054736.0 | params norm: 297.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 4955.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.978747E+00 | loss scale: 65536.0 | grad norm: 0.464 | num zeros: 129357968.0 | params norm: 297.658 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        24320 | elapsed time per iteration (ms): 4921.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.976352E+00 | loss scale: 65536.0 | grad norm: 0.389 | num zeros: 133061920.0 | params norm: 297.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 5300.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.971290E+00 | loss scale: 65536.0 | grad norm: 0.431 | num zeros: 118672784.0 | params norm: 297.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        24832 | elapsed time per iteration (ms): 4578.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.977263E+00 | loss scale: 65536.0 | grad norm: 0.473 | num zeros: 138932832.0 | params norm: 297.703 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 4768.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.953115E+00 | loss scale: 65536.0 | grad norm: 0.391 | num zeros: 162946896.0 | params norm: 297.720 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        25344 | elapsed time per iteration (ms): 5237.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.947515E+00 | loss scale: 65536.0 | grad norm: 0.419 | num zeros: 161593312.0 | params norm: 297.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 5746.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.955101E+00 | loss scale: 65536.0 | grad norm: 0.425 | num zeros: 134782960.0 | params norm: 297.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        25856 | elapsed time per iteration (ms): 4638.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.953188E+00 | loss scale: 65536.0 | grad norm: 0.482 | num zeros: 104159208.0 | params norm: 297.773 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 4526.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.935795E+00 | loss scale: 65536.0 | grad norm: 0.276 | num zeros: 125874760.0 | params norm: 297.792 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        26368 | elapsed time per iteration (ms): 5058.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.922280E+00 | loss scale: 65536.0 | grad norm: 0.337 | num zeros: 157654640.0 | params norm: 297.810 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 5671.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.933361E+00 | loss scale: 65536.0 | grad norm: 0.307 | num zeros: 107409312.0 | params norm: 297.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        26880 | elapsed time per iteration (ms): 4869.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.921655E+00 | loss scale: 65536.0 | grad norm: 0.422 | num zeros: 106130040.0 | params norm: 297.851 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 4958.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.915322E+00 | loss scale: 65536.0 | grad norm: 0.399 | num zeros: 80116264.0 | params norm: 297.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        27392 | elapsed time per iteration (ms): 5800.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.916369E+00 | loss scale: 65536.0 | grad norm: 0.657 | num zeros: 78205000.0 | params norm: 297.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 5031.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.898414E+00 | loss scale: 65536.0 | grad norm: 0.540 | num zeros: 97674824.0 | params norm: 297.914 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        27904 | elapsed time per iteration (ms): 5257.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.892644E+00 | loss scale: 65536.0 | grad norm: 0.614 | num zeros: 103535264.0 | params norm: 297.935 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 5039.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.885653E+00 | loss scale: 65536.0 | grad norm: 0.541 | num zeros: 108958800.0 | params norm: 297.957 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        28416 | elapsed time per iteration (ms): 5423.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.869175E+00 | loss scale: 65536.0 | grad norm: 0.384 | num zeros: 107928376.0 | params norm: 297.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 4770.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.870545E+00 | loss scale: 65536.0 | grad norm: 0.510 | num zeros: 107503896.0 | params norm: 298.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        28928 | elapsed time per iteration (ms): 5010.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.859437E+00 | loss scale: 65536.0 | grad norm: 0.562 | num zeros: 96053320.0 | params norm: 298.029 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 4627.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.855371E+00 | loss scale: 65536.0 | grad norm: 0.391 | num zeros: 67211688.0 | params norm: 298.053 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        29440 | elapsed time per iteration (ms): 5603.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.840950E+00 | loss scale: 65536.0 | grad norm: 0.372 | num zeros: 68310928.0 | params norm: 298.077 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 4737.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.834740E+00 | loss scale: 65536.0 | grad norm: 0.411 | num zeros: 67037616.0 | params norm: 298.101 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        29952 | elapsed time per iteration (ms): 4873.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.839786E+00 | loss scale: 65536.0 | grad norm: 0.461 | num zeros: 68376592.0 | params norm: 298.125 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 4776.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.830196E+00 | loss scale: 65536.0 | grad norm: 0.388 | num zeros: 67593488.0 | params norm: 298.149 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        30464 | elapsed time per iteration (ms): 6089.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.816522E+00 | loss scale: 65536.0 | grad norm: 0.365 | num zeros: 67103448.0 | params norm: 298.173 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 4826.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.817218E+00 | loss scale: 65536.0 | grad norm: 0.494 | num zeros: 67471864.0 | params norm: 298.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        30976 | elapsed time per iteration (ms): 4833.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.804667E+00 | loss scale: 65536.0 | grad norm: 0.429 | num zeros: 72955528.0 | params norm: 298.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 5056.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.797407E+00 | loss scale: 65536.0 | grad norm: 0.293 | num zeros: 68771376.0 | params norm: 298.250 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        31488 | elapsed time per iteration (ms): 6247.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.791611E+00 | loss scale: 65536.0 | grad norm: 0.364 | num zeros: 67554456.0 | params norm: 298.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 4890.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.780208E+00 | loss scale: 65536.0 | grad norm: 0.377 | num zeros: 71405856.0 | params norm: 298.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        32000 | elapsed time per iteration (ms): 5060.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.784616E+00 | loss scale: 65536.0 | grad norm: 0.462 | num zeros: 70747352.0 | params norm: 298.332 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 4923.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.787041E+00 | loss scale: 65536.0 | grad norm: 0.411 | num zeros: 69188864.0 | params norm: 298.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        32512 | elapsed time per iteration (ms): 5572.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.778513E+00 | loss scale: 65536.0 | grad norm: 0.351 | num zeros: 72334216.0 | params norm: 298.389 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 4970.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.761077E+00 | loss scale: 65536.0 | grad norm: 0.379 | num zeros: 80712496.0 | params norm: 298.418 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        33024 | elapsed time per iteration (ms): 5421.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.763755E+00 | loss scale: 65536.0 | grad norm: 0.244 | num zeros: 81022072.0 | params norm: 298.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 4721.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.764195E+00 | loss scale: 65536.0 | grad norm: 0.344 | num zeros: 72147752.0 | params norm: 298.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        33536 | elapsed time per iteration (ms): 5534.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.748604E+00 | loss scale: 65536.0 | grad norm: 0.477 | num zeros: 74531976.0 | params norm: 298.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 4790.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.755122E+00 | loss scale: 65536.0 | grad norm: 0.619 | num zeros: 79623088.0 | params norm: 298.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        34048 | elapsed time per iteration (ms): 5429.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.751200E+00 | loss scale: 65536.0 | grad norm: 0.343 | num zeros: 83886464.0 | params norm: 298.562 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 4805.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.745463E+00 | loss scale: 65536.0 | grad norm: 0.437 | num zeros: 80275752.0 | params norm: 298.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        34560 | elapsed time per iteration (ms): 5165.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.742130E+00 | loss scale: 65536.0 | grad norm: 0.505 | num zeros: 79866360.0 | params norm: 298.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 4935.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.739460E+00 | loss scale: 65536.0 | grad norm: 0.415 | num zeros: 83353760.0 | params norm: 298.648 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        35072 | elapsed time per iteration (ms): 5368.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.733802E+00 | loss scale: 65536.0 | grad norm: 0.490 | num zeros: 80950776.0 | params norm: 298.679 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 4786.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.732386E+00 | loss scale: 65536.0 | grad norm: 0.433 | num zeros: 70560584.0 | params norm: 298.708 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        35584 | elapsed time per iteration (ms): 5362.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.728974E+00 | loss scale: 65536.0 | grad norm: 0.432 | num zeros: 92017544.0 | params norm: 298.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 5087.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.737155E+00 | loss scale: 65536.0 | grad norm: 0.515 | num zeros: 82683376.0 | params norm: 298.767 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        36096 | elapsed time per iteration (ms): 5256.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.704402E+00 | loss scale: 65536.0 | grad norm: 0.281 | num zeros: 80796432.0 | params norm: 298.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 4762.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.709297E+00 | loss scale: 65536.0 | grad norm: 0.418 | num zeros: 72136232.0 | params norm: 298.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        36608 | elapsed time per iteration (ms): 5033.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.725831E+00 | loss scale: 65536.0 | grad norm: 0.446 | num zeros: 67907168.0 | params norm: 298.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 5053.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.709803E+00 | loss scale: 65536.0 | grad norm: 0.338 | num zeros: 96461648.0 | params norm: 298.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        37120 | elapsed time per iteration (ms): 5109.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.712582E+00 | loss scale: 65536.0 | grad norm: 0.614 | num zeros: 96419040.0 | params norm: 298.907 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 4695.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.719685E+00 | loss scale: 65536.0 | grad norm: 0.666 | num zeros: 93877936.0 | params norm: 298.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        37632 | elapsed time per iteration (ms): 4978.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.693889E+00 | loss scale: 65536.0 | grad norm: 0.330 | num zeros: 93561416.0 | params norm: 298.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 4914.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.702293E+00 | loss scale: 65536.0 | grad norm: 0.348 | num zeros: 95674192.0 | params norm: 298.986 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        38144 | elapsed time per iteration (ms): 5097.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.689215E+00 | loss scale: 65536.0 | grad norm: 0.313 | num zeros: 93669744.0 | params norm: 299.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 4698.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.703051E+00 | loss scale: 65536.0 | grad norm: 0.374 | num zeros: 94894344.0 | params norm: 299.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        38656 | elapsed time per iteration (ms): 5108.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.689581E+00 | loss scale: 65536.0 | grad norm: 0.460 | num zeros: 94740840.0 | params norm: 299.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 4940.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.693032E+00 | loss scale: 65536.0 | grad norm: 0.432 | num zeros: 95660096.0 | params norm: 299.095 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        39168 | elapsed time per iteration (ms): 5321.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.672136E+00 | loss scale: 65536.0 | grad norm: 0.344 | num zeros: 88363560.0 | params norm: 299.123 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 4788.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.687755E+00 | loss scale: 65536.0 | grad norm: 0.417 | num zeros: 93370264.0 | params norm: 299.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        39680 | elapsed time per iteration (ms): 4897.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.699605E+00 | loss scale: 65536.0 | grad norm: 0.638 | num zeros: 93794920.0 | params norm: 299.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 4639.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.680576E+00 | loss scale: 65536.0 | grad norm: 0.477 | num zeros: 94768832.0 | params norm: 299.202 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        40192 | elapsed time per iteration (ms): 4967.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.677129E+00 | loss scale: 65536.0 | grad norm: 0.349 | num zeros: 84380768.0 | params norm: 299.229 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 4628.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.685217E+00 | loss scale: 65536.0 | grad norm: 0.577 | num zeros: 82277352.0 | params norm: 299.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        40704 | elapsed time per iteration (ms): 5049.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.682827E+00 | loss scale: 65536.0 | grad norm: 0.597 | num zeros: 61008016.0 | params norm: 299.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 4916.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.691309E+00 | loss scale: 65536.0 | grad norm: 0.726 | num zeros: 71252912.0 | params norm: 299.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        41216 | elapsed time per iteration (ms): 5634.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.686880E+00 | loss scale: 65536.0 | grad norm: 0.448 | num zeros: 71386904.0 | params norm: 299.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 4617.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.690089E+00 | loss scale: 65536.0 | grad norm: 0.643 | num zeros: 68835592.0 | params norm: 299.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        41728 | elapsed time per iteration (ms): 5154.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.702227E+00 | loss scale: 65536.0 | grad norm: 0.746 | num zeros: 69125800.0 | params norm: 299.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 4899.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.678528E+00 | loss scale: 65536.0 | grad norm: 0.401 | num zeros: 79105136.0 | params norm: 299.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        42240 | elapsed time per iteration (ms): 5272.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.686845E+00 | loss scale: 65536.0 | grad norm: 0.684 | num zeros: 65792620.0 | params norm: 299.458 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 4776.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.666726E+00 | loss scale: 65536.0 | grad norm: 0.607 | num zeros: 68715784.0 | params norm: 299.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        42752 | elapsed time per iteration (ms): 5449.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.659750E+00 | loss scale: 65536.0 | grad norm: 0.456 | num zeros: 52718804.0 | params norm: 299.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 4795.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.659521E+00 | loss scale: 65536.0 | grad norm: 0.680 | num zeros: 66891036.0 | params norm: 299.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        43264 | elapsed time per iteration (ms): 5591.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.653514E+00 | loss scale: 65536.0 | grad norm: 0.469 | num zeros: 69190112.0 | params norm: 299.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 5103.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.663540E+00 | loss scale: 65536.0 | grad norm: 0.467 | num zeros: 78812928.0 | params norm: 299.597 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        43776 | elapsed time per iteration (ms): 5073.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.660716E+00 | loss scale: 65536.0 | grad norm: 0.568 | num zeros: 59818004.0 | params norm: 299.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 4987.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.647573E+00 | loss scale: 65536.0 | grad norm: 0.359 | num zeros: 53358872.0 | params norm: 299.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        44288 | elapsed time per iteration (ms): 5517.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.637916E+00 | loss scale: 65536.0 | grad norm: 0.598 | num zeros: 67325256.0 | params norm: 299.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 4849.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.632007E+00 | loss scale: 65536.0 | grad norm: 0.384 | num zeros: 53104936.0 | params norm: 299.711 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        44800 | elapsed time per iteration (ms): 4822.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.635487E+00 | loss scale: 65536.0 | grad norm: 0.443 | num zeros: 70069712.0 | params norm: 299.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 5075.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.623287E+00 | loss scale: 65536.0 | grad norm: 0.409 | num zeros: 62050936.0 | params norm: 299.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        45312 | elapsed time per iteration (ms): 5476.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.620579E+00 | loss scale: 65536.0 | grad norm: 0.425 | num zeros: 78777944.0 | params norm: 299.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 5132.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.619799E+00 | loss scale: 65536.0 | grad norm: 0.509 | num zeros: 58853288.0 | params norm: 299.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        45824 | elapsed time per iteration (ms): 4733.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.609960E+00 | loss scale: 65536.0 | grad norm: 0.508 | num zeros: 64002080.0 | params norm: 299.856 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 5050.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.604824E+00 | loss scale: 65536.0 | grad norm: 0.688 | num zeros: 58423376.0 | params norm: 299.887 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        46336 | elapsed time per iteration (ms): 4888.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.611688E+00 | loss scale: 65536.0 | grad norm: 0.628 | num zeros: 58980928.0 | params norm: 299.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 5407.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.615725E+00 | loss scale: 65536.0 | grad norm: 0.461 | num zeros: 62181184.0 | params norm: 299.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        46848 | elapsed time per iteration (ms): 4818.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.596801E+00 | loss scale: 65536.0 | grad norm: 0.375 | num zeros: 54107680.0 | params norm: 299.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 5280.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.592828E+00 | loss scale: 65536.0 | grad norm: 0.369 | num zeros: 53780048.0 | params norm: 300.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        47360 | elapsed time per iteration (ms): 4859.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.581408E+00 | loss scale: 65536.0 | grad norm: 0.456 | num zeros: 45386704.0 | params norm: 300.041 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 4840.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.592141E+00 | loss scale: 65536.0 | grad norm: 0.774 | num zeros: 51994384.0 | params norm: 300.072 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        47872 | elapsed time per iteration (ms): 4842.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.596774E+00 | loss scale: 65536.0 | grad norm: 0.725 | num zeros: 54391416.0 | params norm: 300.104 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 5114.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.581312E+00 | loss scale: 65536.0 | grad norm: 0.418 | num zeros: 51176524.0 | params norm: 300.137 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        48384 | elapsed time per iteration (ms): 5058.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.585010E+00 | loss scale: 65536.0 | grad norm: 0.833 | num zeros: 54247464.0 | params norm: 300.170 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 5165.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.584138E+00 | loss scale: 65536.0 | grad norm: 0.917 | num zeros: 54587504.0 | params norm: 300.202 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        48896 | elapsed time per iteration (ms): 4877.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.578164E+00 | loss scale: 65536.0 | grad norm: 0.499 | num zeros: 61718892.0 | params norm: 300.235 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 4837.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.581287E+00 | loss scale: 65536.0 | grad norm: 1.034 | num zeros: 58878332.0 | params norm: 300.267 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        49408 | elapsed time per iteration (ms): 4878.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.565832E+00 | loss scale: 65536.0 | grad norm: 0.790 | num zeros: 53736156.0 | params norm: 300.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 4811.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.569803E+00 | loss scale: 65536.0 | grad norm: 0.547 | num zeros: 55642916.0 | params norm: 300.331 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        49920 | elapsed time per iteration (ms): 4914.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.574172E+00 | loss scale: 65536.0 | grad norm: 0.990 | num zeros: 70804048.0 | params norm: 300.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 4994.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.568964E+00 | loss scale: 65536.0 | grad norm: 0.610 | num zeros: 64148208.0 | params norm: 300.398 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:        50432 | elapsed time per iteration (ms): 5268.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.573814E+00 | loss scale: 65536.0 | grad norm: 0.802 | num zeros: 68214000.0 | params norm: 300.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 4750.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.569510E+00 | loss scale: 65536.0 | grad norm: 0.715 | num zeros: 53285168.0 | params norm: 300.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:        50944 | elapsed time per iteration (ms): 5096.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.565318E+00 | loss scale: 65536.0 | grad norm: 0.572 | num zeros: 69421136.0 | params norm: 300.494 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 4953.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.564569E+00 | loss scale: 65536.0 | grad norm: 0.613 | num zeros: 67462704.0 | params norm: 300.528 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2022-12-23 01:18:58 
