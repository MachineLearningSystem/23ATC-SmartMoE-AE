+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 0 == 0 ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 1 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 6 == 0 ']'
+ sleep 10s
+ hostname
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 7 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 2 == 0 ']'
+ '[' 3 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 5 == 0 ']'
+ '[' 4 == 0 ']'
+ sleep 10s
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 12 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 15 == 0 ']'
+ sleep 10s
+ '[' 11 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 8 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 13 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 14 == 0 ']'
+ sleep 10s
+ '[' sh-lab == sh-lab ']'
+ '[' 9 == 0 ']'
+ sleep 10s
+ '[' 10 == 0 ']'
+ sleep 10s
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=0
+ RANK=0
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
++ cat /mnt/cache/zhaishuming/master_node
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=0
+ NODE_RANK=0
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=1
+ NODE_RANK=1
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=2
+ NODE_RANK=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ python_args+='
+ python_args+='
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=10
+ RANK=10
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=15
+ RANK=15
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export RANK=8
+ RANK=8
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ localrank=5
+ localrank=0
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=5
+ NODE_RANK=5
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ export NODE_RANK=5
+ NODE_RANK=5
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ MAX_JOBS=64
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 2.4         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard2.4_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T02:06:10+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 2.4 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 16
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 2.4
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2560
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 1
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 51200
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 4
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.179 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1]
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.153 seconds
time to initialize megatron (seconds): 47.212
[after megatron is initialized] datetime: 2022-12-23 02:06:39 
hhs=5120
hhs=5120
hhs=5120
hhs=5120
building GPT model ...
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6]
hhs=5120
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3]
hhs=5120
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5]
hhs=5120
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7]
hhs=5120
hhs=5120
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
hhs=5120
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
hhs=5120
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
hhs=5120
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
hhs=5120
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
hhs=5120
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
hhs=5120
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
hhs=5120
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 971371776
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-23 02:06:50 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.132022 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.057 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-23 02:07:05 
done with setup ...
training ...
[before the start of training step] datetime: 2022-12-23 02:07:05 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18559.4072265625 | max allocated: 20214.9970703125 | reserved: 23322.0 | max reserved: 23322.0
 iteration        1/     200 | consumed samples:          256 | elapsed time per iteration (ms): 12767.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082783E+01 | gshard_loss: 3.983261E-03 | loss scale: 131072.0 | grad norm: 22.055 | num zeros: 31015.0 | params norm: 295.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/     200 | consumed samples:          512 | elapsed time per iteration (ms): 3188.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.039294E+01 | gshard_loss: 9.057187E-03 | loss scale: 131072.0 | grad norm: 8.039 | num zeros: 131131104.0 | params norm: 295.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:          768 | elapsed time per iteration (ms): 3523.4 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 3119.7 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         1280 | elapsed time per iteration (ms): 3111.7 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 32768.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 3332.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.029305E+01 | gshard_loss: 7.176047E-03 | loss scale: 32768.0 | grad norm: 13.965 | num zeros: 22839.0 | params norm: 295.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         1792 | elapsed time per iteration (ms): 3083.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.145581E+01 | gshard_loss: 1.244753E-02 | loss scale: 32768.0 | grad norm: 10.714 | num zeros: 24243.0 | params norm: 295.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 3010.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.927772E+00 | gshard_loss: 1.024264E-02 | loss scale: 32768.0 | grad norm: 4.564 | num zeros: 166218.0 | params norm: 295.409 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         2304 | elapsed time per iteration (ms): 3588.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.649362E+00 | gshard_loss: 1.098499E-02 | loss scale: 32768.0 | grad norm: 4.457 | num zeros: 78886864.0 | params norm: 295.432 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 3112.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.399673E+00 | gshard_loss: 1.067881E-02 | loss scale: 32768.0 | grad norm: 4.499 | num zeros: 104952072.0 | params norm: 295.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         2816 | elapsed time per iteration (ms): 3426.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.194206E+00 | gshard_loss: 9.145608E-03 | loss scale: 32768.0 | grad norm: 4.496 | num zeros: 52904440.0 | params norm: 295.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 3274.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.994813E+00 | gshard_loss: 7.910981E-03 | loss scale: 32768.0 | grad norm: 4.492 | num zeros: 54037616.0 | params norm: 295.533 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         3328 | elapsed time per iteration (ms): 3278.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.800428E+00 | gshard_loss: 7.525678E-03 | loss scale: 32768.0 | grad norm: 4.426 | num zeros: 78732224.0 | params norm: 295.572 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 3418.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.616709E+00 | gshard_loss: 6.972688E-03 | loss scale: 32768.0 | grad norm: 4.397 | num zeros: 28119668.0 | params norm: 295.612 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         3840 | elapsed time per iteration (ms): 4207.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.440652E+00 | gshard_loss: 6.680275E-03 | loss scale: 32768.0 | grad norm: 4.349 | num zeros: 52663936.0 | params norm: 295.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 3447.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.291677E+00 | gshard_loss: 6.346714E-03 | loss scale: 32768.0 | grad norm: 4.244 | num zeros: 29053316.0 | params norm: 295.694 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         4352 | elapsed time per iteration (ms): 3696.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.140698E+00 | gshard_loss: 6.169365E-03 | loss scale: 32768.0 | grad norm: 4.155 | num zeros: 1557753.0 | params norm: 295.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 3602.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.007883E+00 | gshard_loss: 5.994239E-03 | loss scale: 32768.0 | grad norm: 4.055 | num zeros: 202363.0 | params norm: 295.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         4864 | elapsed time per iteration (ms): 3492.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.881856E+00 | gshard_loss: 5.769954E-03 | loss scale: 32768.0 | grad norm: 3.883 | num zeros: 26271492.0 | params norm: 295.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 3625.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.772490E+00 | gshard_loss: 5.709790E-03 | loss scale: 32768.0 | grad norm: 3.647 | num zeros: 1523909.0 | params norm: 295.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:         5376 | elapsed time per iteration (ms): 4524.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.681690E+00 | gshard_loss: 5.665435E-03 | loss scale: 32768.0 | grad norm: 3.366 | num zeros: 148561.0 | params norm: 295.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 3764.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.595979E+00 | gshard_loss: 5.468365E-03 | loss scale: 32768.0 | grad norm: 2.945 | num zeros: 26453172.0 | params norm: 295.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:         5888 | elapsed time per iteration (ms): 3699.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.504400E+00 | gshard_loss: 5.451042E-03 | loss scale: 32768.0 | grad norm: 2.558 | num zeros: 26444500.0 | params norm: 296.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 3595.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.445157E+00 | gshard_loss: 5.420524E-03 | loss scale: 32768.0 | grad norm: 2.005 | num zeros: 461084.0 | params norm: 296.047 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:         6400 | elapsed time per iteration (ms): 3611.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.398631E+00 | gshard_loss: 5.269227E-03 | loss scale: 32768.0 | grad norm: 1.381 | num zeros: 300202.0 | params norm: 296.091 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 5136.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.378826E+00 | gshard_loss: 5.261549E-03 | loss scale: 32768.0 | grad norm: 0.724 | num zeros: 59320.0 | params norm: 296.134 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:         6912 | elapsed time per iteration (ms): 3666.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.363532E+00 | gshard_loss: 5.147174E-03 | loss scale: 32768.0 | grad norm: 0.434 | num zeros: 26432216.0 | params norm: 296.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 3962.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.365675E+00 | gshard_loss: 5.063489E-03 | loss scale: 32768.0 | grad norm: 0.826 | num zeros: 26352552.0 | params norm: 296.218 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:         7424 | elapsed time per iteration (ms): 3633.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.365439E+00 | gshard_loss: 5.101468E-03 | loss scale: 32768.0 | grad norm: 1.110 | num zeros: 26285394.0 | params norm: 296.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 3604.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.360697E+00 | gshard_loss: 5.275976E-03 | loss scale: 32768.0 | grad norm: 1.139 | num zeros: 101033.0 | params norm: 296.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:         7936 | elapsed time per iteration (ms): 4715.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.363417E+00 | gshard_loss: 5.326131E-03 | loss scale: 32768.0 | grad norm: 1.037 | num zeros: 66216.0 | params norm: 296.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 3725.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.341046E+00 | gshard_loss: 5.277208E-03 | loss scale: 32768.0 | grad norm: 0.814 | num zeros: 73826.0 | params norm: 296.387 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:         8448 | elapsed time per iteration (ms): 3693.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.340218E+00 | gshard_loss: 5.459241E-03 | loss scale: 32768.0 | grad norm: 0.694 | num zeros: 65705.0 | params norm: 296.429 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 3582.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.326267E+00 | gshard_loss: 5.660249E-03 | loss scale: 32768.0 | grad norm: 0.449 | num zeros: 54953.0 | params norm: 296.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:         8960 | elapsed time per iteration (ms): 3524.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.315750E+00 | gshard_loss: 5.621484E-03 | loss scale: 32768.0 | grad norm: 0.725 | num zeros: 53355.0 | params norm: 296.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 4604.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.293001E+00 | gshard_loss: 5.968939E-03 | loss scale: 32768.0 | grad norm: 0.473 | num zeros: 61449.0 | params norm: 296.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:         9472 | elapsed time per iteration (ms): 3945.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.294384E+00 | gshard_loss: 6.346537E-03 | loss scale: 32768.0 | grad norm: 0.450 | num zeros: 68600.0 | params norm: 296.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 3705.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.281306E+00 | gshard_loss: 6.319940E-03 | loss scale: 32768.0 | grad norm: 0.442 | num zeros: 69271.0 | params norm: 296.630 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:         9984 | elapsed time per iteration (ms): 3661.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.284277E+00 | gshard_loss: 6.195891E-03 | loss scale: 32768.0 | grad norm: 0.405 | num zeros: 46660.0 | params norm: 296.666 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 3526.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.268917E+00 | gshard_loss: 7.165026E-03 | loss scale: 32768.0 | grad norm: 0.935 | num zeros: 50182.0 | params norm: 296.699 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        10496 | elapsed time per iteration (ms): 4835.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.247684E+00 | gshard_loss: 6.111120E-03 | loss scale: 32768.0 | grad norm: 0.533 | num zeros: 65156.0 | params norm: 296.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 3526.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.261718E+00 | gshard_loss: 6.503984E-03 | loss scale: 32768.0 | grad norm: 0.697 | num zeros: 61305.0 | params norm: 296.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        11008 | elapsed time per iteration (ms): 3520.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.233354E+00 | gshard_loss: 6.838489E-03 | loss scale: 32768.0 | grad norm: 0.710 | num zeros: 2009557.0 | params norm: 296.795 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 3958.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.222958E+00 | gshard_loss: 6.243913E-03 | loss scale: 32768.0 | grad norm: 0.430 | num zeros: 72597.0 | params norm: 296.825 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        11520 | elapsed time per iteration (ms): 3446.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.214574E+00 | gshard_loss: 6.105043E-03 | loss scale: 32768.0 | grad norm: 0.493 | num zeros: 293497.0 | params norm: 296.855 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 4358.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.188933E+00 | gshard_loss: 6.356717E-03 | loss scale: 32768.0 | grad norm: 0.356 | num zeros: 3004719.0 | params norm: 296.884 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        12032 | elapsed time per iteration (ms): 3759.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.171356E+00 | gshard_loss: 6.119955E-03 | loss scale: 32768.0 | grad norm: 0.258 | num zeros: 1552362.0 | params norm: 296.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 3547.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.169540E+00 | gshard_loss: 5.824399E-03 | loss scale: 32768.0 | grad norm: 0.420 | num zeros: 26276628.0 | params norm: 296.941 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        12544 | elapsed time per iteration (ms): 3879.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.158509E+00 | gshard_loss: 5.902660E-03 | loss scale: 32768.0 | grad norm: 0.318 | num zeros: 26284804.0 | params norm: 296.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 3649.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.139355E+00 | gshard_loss: 5.996041E-03 | loss scale: 32768.0 | grad norm: 0.329 | num zeros: 26265028.0 | params norm: 296.995 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        13056 | elapsed time per iteration (ms): 4043.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.144916E+00 | gshard_loss: 6.140268E-03 | loss scale: 32768.0 | grad norm: 0.577 | num zeros: 26268832.0 | params norm: 297.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 4324.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.124109E+00 | gshard_loss: 6.269236E-03 | loss scale: 32768.0 | grad norm: 0.452 | num zeros: 26250736.0 | params norm: 297.046 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        13568 | elapsed time per iteration (ms): 3627.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.124033E+00 | gshard_loss: 6.165870E-03 | loss scale: 32768.0 | grad norm: 0.333 | num zeros: 26291564.0 | params norm: 297.071 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 3668.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.104223E+00 | gshard_loss: 6.255501E-03 | loss scale: 32768.0 | grad norm: 0.296 | num zeros: 1693808.0 | params norm: 297.095 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        14080 | elapsed time per iteration (ms): 3583.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.095821E+00 | gshard_loss: 6.382107E-03 | loss scale: 32768.0 | grad norm: 0.316 | num zeros: 237062.0 | params norm: 297.119 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 3586.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.114765E+00 | gshard_loss: 6.261674E-03 | loss scale: 32768.0 | grad norm: 0.355 | num zeros: 26299080.0 | params norm: 297.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        14592 | elapsed time per iteration (ms): 4952.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.090168E+00 | gshard_loss: 6.583803E-03 | loss scale: 32768.0 | grad norm: 0.372 | num zeros: 26265544.0 | params norm: 297.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 3558.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.083008E+00 | gshard_loss: 6.527522E-03 | loss scale: 32768.0 | grad norm: 0.447 | num zeros: 26258010.0 | params norm: 297.187 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        15104 | elapsed time per iteration (ms): 3633.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.069648E+00 | gshard_loss: 6.780696E-03 | loss scale: 32768.0 | grad norm: 0.398 | num zeros: 26275676.0 | params norm: 297.209 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 3512.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.063569E+00 | gshard_loss: 6.469646E-03 | loss scale: 32768.0 | grad norm: 0.357 | num zeros: 26276312.0 | params norm: 297.229 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        15616 | elapsed time per iteration (ms): 3520.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.068810E+00 | gshard_loss: 6.509473E-03 | loss scale: 32768.0 | grad norm: 0.494 | num zeros: 26313702.0 | params norm: 297.250 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 5075.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.050890E+00 | gshard_loss: 6.281399E-03 | loss scale: 32768.0 | grad norm: 0.370 | num zeros: 26266310.0 | params norm: 297.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        16128 | elapsed time per iteration (ms): 3417.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.043799E+00 | gshard_loss: 7.291865E-03 | loss scale: 32768.0 | grad norm: 0.546 | num zeros: 26250716.0 | params norm: 297.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 3513.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.030116E+00 | gshard_loss: 6.459861E-03 | loss scale: 32768.0 | grad norm: 0.619 | num zeros: 26270632.0 | params norm: 297.308 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        16640 | elapsed time per iteration (ms): 3555.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.031410E+00 | gshard_loss: 6.607627E-03 | loss scale: 32768.0 | grad norm: 0.341 | num zeros: 15892480.0 | params norm: 297.327 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 3511.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.007588E+00 | gshard_loss: 6.603208E-03 | loss scale: 32768.0 | grad norm: 0.360 | num zeros: 26252540.0 | params norm: 297.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        17152 | elapsed time per iteration (ms): 4938.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.001398E+00 | gshard_loss: 6.664163E-03 | loss scale: 32768.0 | grad norm: 0.334 | num zeros: 3257082.0 | params norm: 297.364 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 3669.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.995799E+00 | gshard_loss: 6.870684E-03 | loss scale: 32768.0 | grad norm: 0.417 | num zeros: 26246100.0 | params norm: 297.382 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        17664 | elapsed time per iteration (ms): 3483.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.981864E+00 | gshard_loss: 7.376155E-03 | loss scale: 32768.0 | grad norm: 0.454 | num zeros: 26249052.0 | params norm: 297.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 3460.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.983182E+00 | gshard_loss: 6.542380E-03 | loss scale: 32768.0 | grad norm: 0.642 | num zeros: 26240746.0 | params norm: 297.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        18176 | elapsed time per iteration (ms): 3432.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.979060E+00 | gshard_loss: 7.762948E-03 | loss scale: 32768.0 | grad norm: 0.934 | num zeros: 3726705.0 | params norm: 297.436 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 3555.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.976930E+00 | gshard_loss: 6.976972E-03 | loss scale: 32768.0 | grad norm: 0.468 | num zeros: 26265270.0 | params norm: 297.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        18688 | elapsed time per iteration (ms): 4622.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.964931E+00 | gshard_loss: 7.125694E-03 | loss scale: 32768.0 | grad norm: 0.411 | num zeros: 26306664.0 | params norm: 297.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 3879.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.967252E+00 | gshard_loss: 7.695544E-03 | loss scale: 32768.0 | grad norm: 0.857 | num zeros: 26425392.0 | params norm: 297.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        19200 | elapsed time per iteration (ms): 3439.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.976656E+00 | gshard_loss: 6.686205E-03 | loss scale: 32768.0 | grad norm: 0.766 | num zeros: 26253988.0 | params norm: 297.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 3330.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.929221E+00 | gshard_loss: 6.858115E-03 | loss scale: 32768.0 | grad norm: 0.349 | num zeros: 16657653.0 | params norm: 297.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        19712 | elapsed time per iteration (ms): 3378.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.948568E+00 | gshard_loss: 7.470418E-03 | loss scale: 32768.0 | grad norm: 0.522 | num zeros: 28588876.0 | params norm: 297.543 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 4421.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.929062E+00 | gshard_loss: 7.324362E-03 | loss scale: 32768.0 | grad norm: 0.482 | num zeros: 26523888.0 | params norm: 297.562 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        20224 | elapsed time per iteration (ms): 3263.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.918439E+00 | gshard_loss: 7.708780E-03 | loss scale: 32768.0 | grad norm: 0.511 | num zeros: 2684192.0 | params norm: 297.580 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 3336.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.907946E+00 | gshard_loss: 7.020297E-03 | loss scale: 32768.0 | grad norm: 0.368 | num zeros: 2329673.0 | params norm: 297.600 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        20736 | elapsed time per iteration (ms): 3748.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.898077E+00 | gshard_loss: 6.748936E-03 | loss scale: 32768.0 | grad norm: 0.424 | num zeros: 146083.0 | params norm: 297.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 3292.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.876218E+00 | gshard_loss: 7.032114E-03 | loss scale: 32768.0 | grad norm: 0.402 | num zeros: 86674.0 | params norm: 297.639 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        21248 | elapsed time per iteration (ms): 3340.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.896408E+00 | gshard_loss: 6.652671E-03 | loss scale: 32768.0 | grad norm: 0.501 | num zeros: 41867.0 | params norm: 297.660 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 3796.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.877620E+00 | gshard_loss: 7.169749E-03 | loss scale: 32768.0 | grad norm: 0.863 | num zeros: 26644.0 | params norm: 297.681 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        21760 | elapsed time per iteration (ms): 3336.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.881464E+00 | gshard_loss: 6.924380E-03 | loss scale: 32768.0 | grad norm: 0.937 | num zeros: 36514.0 | params norm: 297.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 3503.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.871840E+00 | gshard_loss: 7.290024E-03 | loss scale: 32768.0 | grad norm: 0.453 | num zeros: 49090.0 | params norm: 297.720 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        22272 | elapsed time per iteration (ms): 3370.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.865972E+00 | gshard_loss: 7.013542E-03 | loss scale: 32768.0 | grad norm: 1.150 | num zeros: 35445.0 | params norm: 297.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 3266.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.847406E+00 | gshard_loss: 7.437866E-03 | loss scale: 32768.0 | grad norm: 0.480 | num zeros: 40157.0 | params norm: 297.763 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        22784 | elapsed time per iteration (ms): 3328.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.842199E+00 | gshard_loss: 7.624709E-03 | loss scale: 32768.0 | grad norm: 0.444 | num zeros: 33040.0 | params norm: 297.783 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 4220.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.830637E+00 | gshard_loss: 7.284206E-03 | loss scale: 32768.0 | grad norm: 0.470 | num zeros: 29811.0 | params norm: 297.804 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        23296 | elapsed time per iteration (ms): 3461.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.832097E+00 | gshard_loss: 7.314134E-03 | loss scale: 32768.0 | grad norm: 0.647 | num zeros: 26229.0 | params norm: 297.827 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 3369.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.801393E+00 | gshard_loss: 7.233674E-03 | loss scale: 32768.0 | grad norm: 0.449 | num zeros: 24803.0 | params norm: 297.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        23808 | elapsed time per iteration (ms): 3247.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.816902E+00 | gshard_loss: 7.115500E-03 | loss scale: 32768.0 | grad norm: 0.996 | num zeros: 30732.0 | params norm: 297.870 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 3337.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.788718E+00 | gshard_loss: 7.467771E-03 | loss scale: 32768.0 | grad norm: 0.617 | num zeros: 25139.0 | params norm: 297.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        24320 | elapsed time per iteration (ms): 4477.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.801521E+00 | gshard_loss: 7.658247E-03 | loss scale: 32768.0 | grad norm: 0.927 | num zeros: 18631.0 | params norm: 297.914 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 3218.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.779908E+00 | gshard_loss: 6.954945E-03 | loss scale: 32768.0 | grad norm: 0.532 | num zeros: 22224.0 | params norm: 297.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        24832 | elapsed time per iteration (ms): 3251.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.777023E+00 | gshard_loss: 6.915424E-03 | loss scale: 32768.0 | grad norm: 0.408 | num zeros: 25840.0 | params norm: 297.960 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 3357.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.765178E+00 | gshard_loss: 7.224645E-03 | loss scale: 32768.0 | grad norm: 0.619 | num zeros: 19724.0 | params norm: 297.982 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        25344 | elapsed time per iteration (ms): 3212.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.746177E+00 | gshard_loss: 6.946880E-03 | loss scale: 32768.0 | grad norm: 0.348 | num zeros: 29462.0 | params norm: 298.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 3320.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.762365E+00 | gshard_loss: 6.926960E-03 | loss scale: 32768.0 | grad norm: 0.533 | num zeros: 21203.0 | params norm: 298.030 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        25856 | elapsed time per iteration (ms): 4511.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.756476E+00 | gshard_loss: 7.099110E-03 | loss scale: 32768.0 | grad norm: 0.506 | num zeros: 23711.0 | params norm: 298.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 3190.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.739995E+00 | gshard_loss: 6.987311E-03 | loss scale: 32768.0 | grad norm: 0.321 | num zeros: 32483.0 | params norm: 298.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        26368 | elapsed time per iteration (ms): 3206.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.731362E+00 | gshard_loss: 6.804363E-03 | loss scale: 32768.0 | grad norm: 0.588 | num zeros: 21601.0 | params norm: 298.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 3379.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.734293E+00 | gshard_loss: 6.662460E-03 | loss scale: 32768.0 | grad norm: 0.315 | num zeros: 33080.0 | params norm: 298.127 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        26880 | elapsed time per iteration (ms): 3192.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.725728E+00 | gshard_loss: 6.649667E-03 | loss scale: 32768.0 | grad norm: 0.403 | num zeros: 28166.0 | params norm: 298.151 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 3209.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.720420E+00 | gshard_loss: 6.579966E-03 | loss scale: 32768.0 | grad norm: 0.380 | num zeros: 35575.0 | params norm: 298.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        27392 | elapsed time per iteration (ms): 3830.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.720513E+00 | gshard_loss: 6.461435E-03 | loss scale: 32768.0 | grad norm: 0.353 | num zeros: 37791.0 | params norm: 298.201 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 3197.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.702674E+00 | gshard_loss: 6.404261E-03 | loss scale: 32768.0 | grad norm: 0.491 | num zeros: 28332.0 | params norm: 298.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        27904 | elapsed time per iteration (ms): 3274.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.704296E+00 | gshard_loss: 6.298320E-03 | loss scale: 32768.0 | grad norm: 0.667 | num zeros: 21429.0 | params norm: 298.249 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 3440.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.705948E+00 | gshard_loss: 6.377797E-03 | loss scale: 32768.0 | grad norm: 0.767 | num zeros: 15298.0 | params norm: 298.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        28416 | elapsed time per iteration (ms): 3238.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.702741E+00 | gshard_loss: 6.485262E-03 | loss scale: 32768.0 | grad norm: 0.682 | num zeros: 18086.0 | params norm: 298.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 3227.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.678523E+00 | gshard_loss: 6.369764E-03 | loss scale: 32768.0 | grad norm: 0.421 | num zeros: 27178.0 | params norm: 298.325 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        28928 | elapsed time per iteration (ms): 4043.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.672433E+00 | gshard_loss: 6.319141E-03 | loss scale: 32768.0 | grad norm: 0.703 | num zeros: 18706.0 | params norm: 298.352 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 3166.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.665260E+00 | gshard_loss: 6.341756E-03 | loss scale: 32768.0 | grad norm: 0.419 | num zeros: 26374.0 | params norm: 298.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        29440 | elapsed time per iteration (ms): 3302.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.655650E+00 | gshard_loss: 6.346875E-03 | loss scale: 32768.0 | grad norm: 0.585 | num zeros: 21295.0 | params norm: 298.403 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 3515.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.647858E+00 | gshard_loss: 6.356524E-03 | loss scale: 32768.0 | grad norm: 0.410 | num zeros: 31821.0 | params norm: 298.428 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        29952 | elapsed time per iteration (ms): 3199.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.650156E+00 | gshard_loss: 6.355767E-03 | loss scale: 32768.0 | grad norm: 0.652 | num zeros: 25748.0 | params norm: 298.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 3340.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.647926E+00 | gshard_loss: 6.256003E-03 | loss scale: 32768.0 | grad norm: 0.595 | num zeros: 26655.0 | params norm: 298.480 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        30464 | elapsed time per iteration (ms): 3801.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.629410E+00 | gshard_loss: 6.233758E-03 | loss scale: 32768.0 | grad norm: 0.468 | num zeros: 25679.0 | params norm: 298.505 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 3190.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.627176E+00 | gshard_loss: 6.073035E-03 | loss scale: 32768.0 | grad norm: 0.397 | num zeros: 30463.0 | params norm: 298.532 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        30976 | elapsed time per iteration (ms): 3314.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.614919E+00 | gshard_loss: 6.037013E-03 | loss scale: 32768.0 | grad norm: 0.367 | num zeros: 35026.0 | params norm: 298.558 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 3308.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.611527E+00 | gshard_loss: 6.066613E-03 | loss scale: 32768.0 | grad norm: 0.492 | num zeros: 25164.0 | params norm: 298.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        31488 | elapsed time per iteration (ms): 3212.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.604761E+00 | gshard_loss: 5.943745E-03 | loss scale: 32768.0 | grad norm: 0.584 | num zeros: 23083.0 | params norm: 298.611 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 3944.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.592352E+00 | gshard_loss: 5.999115E-03 | loss scale: 32768.0 | grad norm: 0.467 | num zeros: 25815.0 | params norm: 298.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        32000 | elapsed time per iteration (ms): 3284.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.589780E+00 | gshard_loss: 5.901108E-03 | loss scale: 32768.0 | grad norm: 0.373 | num zeros: 33259.0 | params norm: 298.665 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 3538.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.596599E+00 | gshard_loss: 5.787456E-03 | loss scale: 32768.0 | grad norm: 0.447 | num zeros: 30950.0 | params norm: 298.693 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        32512 | elapsed time per iteration (ms): 3513.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.591082E+00 | gshard_loss: 5.836167E-03 | loss scale: 32768.0 | grad norm: 0.572 | num zeros: 29859.0 | params norm: 298.720 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 3427.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.577291E+00 | gshard_loss: 5.797697E-03 | loss scale: 32768.0 | grad norm: 0.547 | num zeros: 29067.0 | params norm: 298.748 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        33024 | elapsed time per iteration (ms): 3224.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.581089E+00 | gshard_loss: 5.726182E-03 | loss scale: 32768.0 | grad norm: 0.455 | num zeros: 28117.0 | params norm: 298.775 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 3582.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.575070E+00 | gshard_loss: 5.750848E-03 | loss scale: 32768.0 | grad norm: 0.336 | num zeros: 34731.0 | params norm: 298.803 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        33536 | elapsed time per iteration (ms): 3297.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.551894E+00 | gshard_loss: 5.685131E-03 | loss scale: 32768.0 | grad norm: 0.346 | num zeros: 41381.0 | params norm: 298.831 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 3263.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.549965E+00 | gshard_loss: 5.593114E-03 | loss scale: 32768.0 | grad norm: 0.339 | num zeros: 37091.0 | params norm: 298.859 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        34048 | elapsed time per iteration (ms): 3823.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.558628E+00 | gshard_loss: 5.622171E-03 | loss scale: 32768.0 | grad norm: 0.440 | num zeros: 33241.0 | params norm: 298.887 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 3385.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.556744E+00 | gshard_loss: 5.672052E-03 | loss scale: 32768.0 | grad norm: 0.639 | num zeros: 27028.0 | params norm: 298.914 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        34560 | elapsed time per iteration (ms): 3261.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.537056E+00 | gshard_loss: 5.597493E-03 | loss scale: 32768.0 | grad norm: 0.618 | num zeros: 29308.0 | params norm: 298.942 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 3707.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.536307E+00 | gshard_loss: 5.531727E-03 | loss scale: 32768.0 | grad norm: 0.437 | num zeros: 31300.0 | params norm: 298.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        35072 | elapsed time per iteration (ms): 3256.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.527567E+00 | gshard_loss: 5.517760E-03 | loss scale: 32768.0 | grad norm: 0.383 | num zeros: 32384.0 | params norm: 298.998 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 3261.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.519189E+00 | gshard_loss: 5.496427E-03 | loss scale: 32768.0 | grad norm: 0.360 | num zeros: 39791.0 | params norm: 299.026 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        35584 | elapsed time per iteration (ms): 3954.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.530697E+00 | gshard_loss: 5.500013E-03 | loss scale: 32768.0 | grad norm: 0.405 | num zeros: 36225.0 | params norm: 299.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 3415.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.537654E+00 | gshard_loss: 5.579803E-03 | loss scale: 32768.0 | grad norm: 0.711 | num zeros: 28416.0 | params norm: 299.084 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        36096 | elapsed time per iteration (ms): 3282.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.501335E+00 | gshard_loss: 5.513050E-03 | loss scale: 32768.0 | grad norm: 0.426 | num zeros: 26911.0 | params norm: 299.113 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 3793.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.507433E+00 | gshard_loss: 5.484220E-03 | loss scale: 32768.0 | grad norm: 0.840 | num zeros: 18775.0 | params norm: 299.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        36608 | elapsed time per iteration (ms): 3284.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.527235E+00 | gshard_loss: 5.604260E-03 | loss scale: 32768.0 | grad norm: 0.838 | num zeros: 17170.0 | params norm: 299.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 3387.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.506256E+00 | gshard_loss: 5.471732E-03 | loss scale: 32768.0 | grad norm: 0.522 | num zeros: 21114.0 | params norm: 299.198 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        37120 | elapsed time per iteration (ms): 3501.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.491525E+00 | gshard_loss: 5.471649E-03 | loss scale: 32768.0 | grad norm: 0.391 | num zeros: 32091.0 | params norm: 299.228 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 3389.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.504124E+00 | gshard_loss: 5.523469E-03 | loss scale: 32768.0 | grad norm: 0.431 | num zeros: 20703.0 | params norm: 299.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        37632 | elapsed time per iteration (ms): 3387.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.482327E+00 | gshard_loss: 5.497406E-03 | loss scale: 32768.0 | grad norm: 0.476 | num zeros: 24292.0 | params norm: 299.285 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 3991.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.480572E+00 | gshard_loss: 5.499708E-03 | loss scale: 32768.0 | grad norm: 0.353 | num zeros: 28016.0 | params norm: 299.315 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        38144 | elapsed time per iteration (ms): 3528.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.468126E+00 | gshard_loss: 5.527459E-03 | loss scale: 32768.0 | grad norm: 0.404 | num zeros: 24498.0 | params norm: 299.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 3523.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.476234E+00 | gshard_loss: 5.494725E-03 | loss scale: 32768.0 | grad norm: 0.417 | num zeros: 29070.0 | params norm: 299.374 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        38656 | elapsed time per iteration (ms): 3178.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.456073E+00 | gshard_loss: 5.462623E-03 | loss scale: 32768.0 | grad norm: 0.415 | num zeros: 31501.0 | params norm: 299.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 3369.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.466166E+00 | gshard_loss: 5.421861E-03 | loss scale: 32768.0 | grad norm: 0.388 | num zeros: 29109.0 | params norm: 299.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        39168 | elapsed time per iteration (ms): 4106.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.439898E+00 | gshard_loss: 5.421281E-03 | loss scale: 32768.0 | grad norm: 0.395 | num zeros: 28909.0 | params norm: 299.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 3204.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.449447E+00 | gshard_loss: 5.432906E-03 | loss scale: 32768.0 | grad norm: 0.376 | num zeros: 28001.0 | params norm: 299.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        39680 | elapsed time per iteration (ms): 3205.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.451900E+00 | gshard_loss: 5.493506E-03 | loss scale: 32768.0 | grad norm: 0.445 | num zeros: 29949.0 | params norm: 299.527 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 3515.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.435760E+00 | gshard_loss: 5.445979E-03 | loss scale: 32768.0 | grad norm: 0.332 | num zeros: 38832.0 | params norm: 299.558 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        40192 | elapsed time per iteration (ms): 3190.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.432150E+00 | gshard_loss: 5.469391E-03 | loss scale: 32768.0 | grad norm: 0.390 | num zeros: 33060.0 | params norm: 299.589 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 3195.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.433411E+00 | gshard_loss: 5.506140E-03 | loss scale: 32768.0 | grad norm: 0.297 | num zeros: 35390.0 | params norm: 299.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        40704 | elapsed time per iteration (ms): 4285.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.423548E+00 | gshard_loss: 5.454557E-03 | loss scale: 32768.0 | grad norm: 0.302 | num zeros: 44397.0 | params norm: 299.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 3167.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.419645E+00 | gshard_loss: 5.442659E-03 | loss scale: 32768.0 | grad norm: 0.300 | num zeros: 37648.0 | params norm: 299.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        41216 | elapsed time per iteration (ms): 3179.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.425968E+00 | gshard_loss: 5.489920E-03 | loss scale: 32768.0 | grad norm: 0.331 | num zeros: 39843.0 | params norm: 299.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 3395.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.421328E+00 | gshard_loss: 5.468526E-03 | loss scale: 32768.0 | grad norm: 0.446 | num zeros: 33789.0 | params norm: 299.748 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        41728 | elapsed time per iteration (ms): 3209.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.424904E+00 | gshard_loss: 5.439955E-03 | loss scale: 32768.0 | grad norm: 0.463 | num zeros: 33516.0 | params norm: 299.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 3161.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.415967E+00 | gshard_loss: 5.472838E-03 | loss scale: 32768.0 | grad norm: 0.554 | num zeros: 29836.0 | params norm: 299.811 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        42240 | elapsed time per iteration (ms): 4637.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.417404E+00 | gshard_loss: 5.407784E-03 | loss scale: 32768.0 | grad norm: 0.433 | num zeros: 27135.0 | params norm: 299.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 3211.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.386199E+00 | gshard_loss: 5.401395E-03 | loss scale: 32768.0 | grad norm: 0.358 | num zeros: 30475.0 | params norm: 299.876 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        42752 | elapsed time per iteration (ms): 3161.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.387505E+00 | gshard_loss: 5.406654E-03 | loss scale: 32768.0 | grad norm: 0.423 | num zeros: 34929.0 | params norm: 299.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 3435.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.374458E+00 | gshard_loss: 5.346012E-03 | loss scale: 32768.0 | grad norm: 0.386 | num zeros: 27561.0 | params norm: 299.941 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        43264 | elapsed time per iteration (ms): 3123.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.378470E+00 | gshard_loss: 5.419364E-03 | loss scale: 32768.0 | grad norm: 0.394 | num zeros: 27980.0 | params norm: 299.974 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 3145.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.389106E+00 | gshard_loss: 5.434084E-03 | loss scale: 32768.0 | grad norm: 0.447 | num zeros: 30371.0 | params norm: 300.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        43776 | elapsed time per iteration (ms): 4196.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.378616E+00 | gshard_loss: 5.411763E-03 | loss scale: 32768.0 | grad norm: 0.334 | num zeros: 33395.0 | params norm: 300.041 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 3135.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.372510E+00 | gshard_loss: 5.400976E-03 | loss scale: 32768.0 | grad norm: 0.312 | num zeros: 34710.0 | params norm: 300.074 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        44288 | elapsed time per iteration (ms): 3125.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.353271E+00 | gshard_loss: 5.396302E-03 | loss scale: 32768.0 | grad norm: 0.420 | num zeros: 26950.0 | params norm: 300.108 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 3529.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.354630E+00 | gshard_loss: 5.367881E-03 | loss scale: 32768.0 | grad norm: 0.323 | num zeros: 34034.0 | params norm: 300.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        44800 | elapsed time per iteration (ms): 3129.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.356024E+00 | gshard_loss: 5.360823E-03 | loss scale: 32768.0 | grad norm: 0.322 | num zeros: 40087.0 | params norm: 300.176 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 3117.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.343784E+00 | gshard_loss: 5.371719E-03 | loss scale: 32768.0 | grad norm: 0.393 | num zeros: 34830.0 | params norm: 300.211 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        45312 | elapsed time per iteration (ms): 3846.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.343453E+00 | gshard_loss: 5.370269E-03 | loss scale: 32768.0 | grad norm: 0.328 | num zeros: 38329.0 | params norm: 300.245 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 3121.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.338373E+00 | gshard_loss: 5.365243E-03 | loss scale: 32768.0 | grad norm: 0.494 | num zeros: 38864.0 | params norm: 300.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        45824 | elapsed time per iteration (ms): 3117.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.331189E+00 | gshard_loss: 5.313001E-03 | loss scale: 32768.0 | grad norm: 0.467 | num zeros: 31063.0 | params norm: 300.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 3624.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.323116E+00 | gshard_loss: 5.303401E-03 | loss scale: 32768.0 | grad norm: 0.456 | num zeros: 28413.0 | params norm: 300.348 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        46336 | elapsed time per iteration (ms): 3106.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.323719E+00 | gshard_loss: 5.432816E-03 | loss scale: 32768.0 | grad norm: 0.312 | num zeros: 39265.0 | params norm: 300.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 3125.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.341279E+00 | gshard_loss: 5.324475E-03 | loss scale: 32768.0 | grad norm: 0.381 | num zeros: 31613.0 | params norm: 300.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        46848 | elapsed time per iteration (ms): 4052.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.330615E+00 | gshard_loss: 5.238599E-03 | loss scale: 32768.0 | grad norm: 0.436 | num zeros: 24633.0 | params norm: 300.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 3269.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.317847E+00 | gshard_loss: 5.290001E-03 | loss scale: 32768.0 | grad norm: 0.368 | num zeros: 33432.0 | params norm: 300.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        47360 | elapsed time per iteration (ms): 3270.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.307509E+00 | gshard_loss: 5.266138E-03 | loss scale: 32768.0 | grad norm: 0.396 | num zeros: 32658.0 | params norm: 300.521 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 3554.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.308640E+00 | gshard_loss: 5.214428E-03 | loss scale: 32768.0 | grad norm: 0.331 | num zeros: 42629.0 | params norm: 300.555 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        47872 | elapsed time per iteration (ms): 3147.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.313787E+00 | gshard_loss: 5.231501E-03 | loss scale: 32768.0 | grad norm: 0.356 | num zeros: 34080.0 | params norm: 300.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 3095.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.307830E+00 | gshard_loss: 5.236266E-03 | loss scale: 32768.0 | grad norm: 0.383 | num zeros: 32867.0 | params norm: 300.625 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        48384 | elapsed time per iteration (ms): 3864.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.302170E+00 | gshard_loss: 5.242398E-03 | loss scale: 32768.0 | grad norm: 0.263 | num zeros: 38101.0 | params norm: 300.660 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 3287.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.291274E+00 | gshard_loss: 5.241479E-03 | loss scale: 32768.0 | grad norm: 0.316 | num zeros: 42893.0 | params norm: 300.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        48896 | elapsed time per iteration (ms): 3248.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.294310E+00 | gshard_loss: 5.217001E-03 | loss scale: 32768.0 | grad norm: 0.290 | num zeros: 36093.0 | params norm: 300.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 3328.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.293484E+00 | gshard_loss: 5.262224E-03 | loss scale: 32768.0 | grad norm: 0.285 | num zeros: 34748.0 | params norm: 300.767 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        49408 | elapsed time per iteration (ms): 3081.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.268000E+00 | gshard_loss: 5.233658E-03 | loss scale: 32768.0 | grad norm: 0.259 | num zeros: 41810.0 | params norm: 300.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 3143.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.277754E+00 | gshard_loss: 5.238379E-03 | loss scale: 32768.0 | grad norm: 0.302 | num zeros: 34971.0 | params norm: 300.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        49920 | elapsed time per iteration (ms): 3594.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.272955E+00 | gshard_loss: 5.236768E-03 | loss scale: 32768.0 | grad norm: 0.303 | num zeros: 40232.0 | params norm: 300.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 3112.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.268169E+00 | gshard_loss: 5.177926E-03 | loss scale: 32768.0 | grad norm: 0.284 | num zeros: 41022.0 | params norm: 300.909 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:        50432 | elapsed time per iteration (ms): 3080.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.268456E+00 | gshard_loss: 5.221446E-03 | loss scale: 32768.0 | grad norm: 0.291 | num zeros: 36358.0 | params norm: 300.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 3798.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.266605E+00 | gshard_loss: 5.193601E-03 | loss scale: 32768.0 | grad norm: 0.283 | num zeros: 37372.0 | params norm: 300.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:        50944 | elapsed time per iteration (ms): 3086.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.264716E+00 | gshard_loss: 5.216671E-03 | loss scale: 32768.0 | grad norm: 0.235 | num zeros: 48074.0 | params norm: 301.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 3057.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.264459E+00 | gshard_loss: 5.179026E-03 | loss scale: 32768.0 | grad norm: 0.276 | num zeros: 40402.0 | params norm: 301.051 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2022-12-23 02:19:02 
