+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 13 == 0 ']'
+ sleep 10s
+ '[' 6 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 0 == 0 ']'
+ hostname
+ '[' 3 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 8 == 0 ']'
+ sleep 10s
+ '[' 5 == 0 ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 7 == 0 ']'
+ '[' sh-lab == sh-lab ']'
+ sleep 10s
+ '[' 9 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 1 == 0 ']'
+ '[' 15 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 11 == 0 ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 10 == 0 ']'
+ sleep 10s
+ '[' sh-lab == sh-lab ']'
+ sleep 10s
+ '[' 2 == 0 ']'
+ '[' srun == srun ']'
+ '[' 4 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ sleep 10s
+ '[' sh-lab == sh-lab ']'
+ sleep 10s
+ '[' sh-lab == sh-lab ']'
+ '[' 12 == 0 ']'
+ '[' 14 == 0 ']'
+ sleep 10s
+ sleep 10s
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=3
+ RANK=3
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=2
+ NNODES=2
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ export NODE_RANK=1
+ NODE_RANK=1
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export RANK=2
+ RANK=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=6
+ RANK=6
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export RANK=4
+ RANK=4
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=6
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ CUDA_VISIBLE_DEVICES=6
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=7
+ RANK=7
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ NODE_RANK=3
+ export MAX_JOBS=64
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ MAX_JOBS=64
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ MAX_JOBS=64
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ localrank=5
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=9
+ RANK=9
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export RANK=11
+ RANK=11
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=6
+ NODE_RANK=6
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=4
+ NODE_RANK=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-46
+ MASTER_ADDR=SH-IDC1-10-140-1-46
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ python_args=
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof/table
+ python_args=
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_gshard1.2_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T01:26:49+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 1 --balance-strategy gshard --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 16
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2560
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 1
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 51200
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 4
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.168 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4]
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.636 seconds
time to initialize megatron (seconds): -10.203
[after megatron is initialized] datetime: 2022-12-23 01:27:17 
hhs=5120
hhs=5120
hhs=5120
building GPT model ...
hhs=5120
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7]
hhs=5120
hhs=5120
hhs=5120
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6]
hhs=5120
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
hhs=5120
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
hhs=5120
hhs=5120
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
hhs=5120
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
hhs=5120
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
hhs=5120
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
hhs=5120
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
hhs=5120
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 971371776
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-23 01:27:29 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.082843 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.034 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-23 01:27:44 
done with setup ...
training ...
[before the start of training step] datetime: 2022-12-23 01:27:44 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18557.666015625 | max allocated: 20196.2626953125 | reserved: 23258.0 | max reserved: 23258.0
 iteration        1/     200 | consumed samples:          256 | elapsed time per iteration (ms): 11973.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082778E+01 | gshard_loss: 3.984370E-03 | loss scale: 131072.0 | grad norm: 22.023 | num zeros: 31047.0 | params norm: 295.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/     200 | consumed samples:          512 | elapsed time per iteration (ms): 2262.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.037292E+01 | gshard_loss: 8.976590E-03 | loss scale: 131072.0 | grad norm: 7.179 | num zeros: 104913216.0 | params norm: 295.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:          768 | elapsed time per iteration (ms): 2632.5 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 2534.8 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         1280 | elapsed time per iteration (ms): 2511.4 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 32768.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 2779.8 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 16384.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         1792 | elapsed time per iteration (ms): 2981.6 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 8192.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 2520.8 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 4096.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         2304 | elapsed time per iteration (ms): 2496.7 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 2048.0 | params norm: 295.371 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 2910.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.056099E+01 | gshard_loss: 6.532163E-03 | loss scale: 2048.0 | grad norm: 966.888 | num zeros: 14790.0 | params norm: 295.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         2816 | elapsed time per iteration (ms): 2515.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.037931E+01 | gshard_loss: 8.724393E-03 | loss scale: 2048.0 | grad norm: 19.723 | num zeros: 30197.0 | params norm: 295.391 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 2055.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.822973E+00 | gshard_loss: 1.353962E-02 | loss scale: 2048.0 | grad norm: 4.730 | num zeros: 165400816.0 | params norm: 295.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         3328 | elapsed time per iteration (ms): 2132.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.564919E+00 | gshard_loss: 1.166915E-02 | loss scale: 2048.0 | grad norm: 4.659 | num zeros: 88705520.0 | params norm: 295.438 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 2270.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.336832E+00 | gshard_loss: 1.039281E-02 | loss scale: 2048.0 | grad norm: 6.629 | num zeros: 52614308.0 | params norm: 295.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         3840 | elapsed time per iteration (ms): 2339.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.130110E+00 | gshard_loss: 9.910817E-03 | loss scale: 2048.0 | grad norm: 4.648 | num zeros: 108275608.0 | params norm: 295.508 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 2359.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.939777E+00 | gshard_loss: 9.921524E-03 | loss scale: 2048.0 | grad norm: 4.601 | num zeros: 83411016.0 | params norm: 295.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         4352 | elapsed time per iteration (ms): 2339.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.741957E+00 | gshard_loss: 9.708903E-03 | loss scale: 2048.0 | grad norm: 4.584 | num zeros: 111686880.0 | params norm: 295.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 2359.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.559454E+00 | gshard_loss: 9.498239E-03 | loss scale: 2048.0 | grad norm: 4.562 | num zeros: 111906696.0 | params norm: 295.627 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         4864 | elapsed time per iteration (ms): 3287.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.387963E+00 | gshard_loss: 9.232135E-03 | loss scale: 2048.0 | grad norm: 4.500 | num zeros: 113920488.0 | params norm: 295.669 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 2480.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.235196E+00 | gshard_loss: 9.163136E-03 | loss scale: 2048.0 | grad norm: 4.402 | num zeros: 92599544.0 | params norm: 295.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:         5376 | elapsed time per iteration (ms): 2474.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.100247E+00 | gshard_loss: 8.984114E-03 | loss scale: 2048.0 | grad norm: 4.292 | num zeros: 91728688.0 | params norm: 295.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 2505.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.973392E+00 | gshard_loss: 8.899799E-03 | loss scale: 2048.0 | grad norm: 4.094 | num zeros: 105440224.0 | params norm: 295.802 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:         5888 | elapsed time per iteration (ms): 2597.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.843870E+00 | gshard_loss: 8.834675E-03 | loss scale: 2048.0 | grad norm: 3.965 | num zeros: 105153624.0 | params norm: 295.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 2548.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.738314E+00 | gshard_loss: 8.755250E-03 | loss scale: 2048.0 | grad norm: 3.703 | num zeros: 106075728.0 | params norm: 295.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:         6400 | elapsed time per iteration (ms): 2595.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.642854E+00 | gshard_loss: 8.690418E-03 | loss scale: 2048.0 | grad norm: 3.375 | num zeros: 79153336.0 | params norm: 295.941 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 3194.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.565264E+00 | gshard_loss: 8.596659E-03 | loss scale: 2048.0 | grad norm: 2.936 | num zeros: 83131992.0 | params norm: 295.987 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:         6912 | elapsed time per iteration (ms): 2730.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.492603E+00 | gshard_loss: 8.525960E-03 | loss scale: 2048.0 | grad norm: 2.446 | num zeros: 59705884.0 | params norm: 296.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 2810.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.438571E+00 | gshard_loss: 8.409098E-03 | loss scale: 2048.0 | grad norm: 1.826 | num zeros: 59434452.0 | params norm: 296.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:         7424 | elapsed time per iteration (ms): 2803.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.395223E+00 | gshard_loss: 8.320011E-03 | loss scale: 2048.0 | grad norm: 1.173 | num zeros: 60726356.0 | params norm: 296.124 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 2920.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.363573E+00 | gshard_loss: 8.293652E-03 | loss scale: 2048.0 | grad norm: 0.590 | num zeros: 31700548.0 | params norm: 296.168 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:         7936 | elapsed time per iteration (ms): 2770.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.364650E+00 | gshard_loss: 8.285359E-03 | loss scale: 2048.0 | grad norm: 0.690 | num zeros: 34256592.0 | params norm: 296.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 2861.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.354661E+00 | gshard_loss: 8.279444E-03 | loss scale: 2048.0 | grad norm: 1.077 | num zeros: 31524090.0 | params norm: 296.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:         8448 | elapsed time per iteration (ms): 3342.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.374005E+00 | gshard_loss: 8.274423E-03 | loss scale: 2048.0 | grad norm: 1.345 | num zeros: 29180432.0 | params norm: 296.296 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 2854.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.370601E+00 | gshard_loss: 8.141214E-03 | loss scale: 2048.0 | grad norm: 1.370 | num zeros: 28449662.0 | params norm: 296.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:         8960 | elapsed time per iteration (ms): 2792.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.365950E+00 | gshard_loss: 7.238358E-03 | loss scale: 2048.0 | grad norm: 1.259 | num zeros: 27972094.0 | params norm: 296.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 2793.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.349946E+00 | gshard_loss: 6.761643E-03 | loss scale: 2048.0 | grad norm: 0.906 | num zeros: 26623122.0 | params norm: 296.422 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:         9472 | elapsed time per iteration (ms): 2902.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.344469E+00 | gshard_loss: 7.380365E-03 | loss scale: 2048.0 | grad norm: 0.743 | num zeros: 26366300.0 | params norm: 296.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 2715.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.323974E+00 | gshard_loss: 7.932863E-03 | loss scale: 2048.0 | grad norm: 1.118 | num zeros: 1550243.0 | params norm: 296.507 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:         9984 | elapsed time per iteration (ms): 2470.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.320141E+00 | gshard_loss: 7.948588E-03 | loss scale: 2048.0 | grad norm: 1.797 | num zeros: 646030.0 | params norm: 296.549 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 2687.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.284837E+00 | gshard_loss: 7.460291E-03 | loss scale: 2048.0 | grad norm: 0.788 | num zeros: 726959.0 | params norm: 296.590 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        10496 | elapsed time per iteration (ms): 2587.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.270939E+00 | gshard_loss: 6.785962E-03 | loss scale: 2048.0 | grad norm: 0.732 | num zeros: 935859.0 | params norm: 296.632 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 2615.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.286887E+00 | gshard_loss: 6.615586E-03 | loss scale: 2048.0 | grad norm: 0.465 | num zeros: 899739.0 | params norm: 296.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        11008 | elapsed time per iteration (ms): 2650.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.253270E+00 | gshard_loss: 6.483416E-03 | loss scale: 2048.0 | grad norm: 0.554 | num zeros: 1126591.0 | params norm: 296.714 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 3287.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.239935E+00 | gshard_loss: 6.316877E-03 | loss scale: 2048.0 | grad norm: 0.525 | num zeros: 708543.0 | params norm: 296.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        11520 | elapsed time per iteration (ms): 3086.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.226703E+00 | gshard_loss: 6.432976E-03 | loss scale: 2048.0 | grad norm: 1.059 | num zeros: 328095.0 | params norm: 296.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 2711.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.214040E+00 | gshard_loss: 6.569661E-03 | loss scale: 2048.0 | grad norm: 0.330 | num zeros: 527606.0 | params norm: 296.831 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        12032 | elapsed time per iteration (ms): 2817.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.213664E+00 | gshard_loss: 6.649753E-03 | loss scale: 2048.0 | grad norm: 0.916 | num zeros: 575170.0 | params norm: 296.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 2863.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.207449E+00 | gshard_loss: 6.618267E-03 | loss scale: 2048.0 | grad norm: 0.385 | num zeros: 1491503.0 | params norm: 296.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        12544 | elapsed time per iteration (ms): 2714.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.201619E+00 | gshard_loss: 6.524244E-03 | loss scale: 2048.0 | grad norm: 0.515 | num zeros: 534290.0 | params norm: 296.939 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 2775.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.186432E+00 | gshard_loss: 6.393114E-03 | loss scale: 2048.0 | grad norm: 0.380 | num zeros: 445223.0 | params norm: 296.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        13056 | elapsed time per iteration (ms): 3985.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.185652E+00 | gshard_loss: 6.384848E-03 | loss scale: 2048.0 | grad norm: 1.115 | num zeros: 2146347.0 | params norm: 297.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 2696.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.177333E+00 | gshard_loss: 6.471845E-03 | loss scale: 2048.0 | grad norm: 1.014 | num zeros: 26846836.0 | params norm: 297.035 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        13568 | elapsed time per iteration (ms): 2712.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.196361E+00 | gshard_loss: 6.446266E-03 | loss scale: 2048.0 | grad norm: 5.249 | num zeros: 56731800.0 | params norm: 297.065 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 3057.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.186180E+00 | gshard_loss: 6.569571E-03 | loss scale: 2048.0 | grad norm: 2.481 | num zeros: 79682560.0 | params norm: 297.093 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        14080 | elapsed time per iteration (ms): 2694.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.191868E+00 | gshard_loss: 6.613913E-03 | loss scale: 2048.0 | grad norm: 0.528 | num zeros: 81937608.0 | params norm: 297.119 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 2827.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.218550E+00 | gshard_loss: 6.590229E-03 | loss scale: 2048.0 | grad norm: 1.228 | num zeros: 63721056.0 | params norm: 297.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        14592 | elapsed time per iteration (ms): 2691.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.196342E+00 | gshard_loss: 6.713497E-03 | loss scale: 2048.0 | grad norm: 0.578 | num zeros: 85253392.0 | params norm: 297.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 3147.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.187354E+00 | gshard_loss: 6.518815E-03 | loss scale: 2048.0 | grad norm: 1.036 | num zeros: 70630912.0 | params norm: 297.186 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        15104 | elapsed time per iteration (ms): 2712.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.178499E+00 | gshard_loss: 6.647575E-03 | loss scale: 2048.0 | grad norm: 2.445 | num zeros: 125352792.0 | params norm: 297.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 2759.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.177561E+00 | gshard_loss: 6.741831E-03 | loss scale: 2048.0 | grad norm: 1.548 | num zeros: 111938768.0 | params norm: 297.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        15616 | elapsed time per iteration (ms): 3179.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.189982E+00 | gshard_loss: 6.757211E-03 | loss scale: 2048.0 | grad norm: 3.388 | num zeros: 101475280.0 | params norm: 297.244 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 2965.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.185227E+00 | gshard_loss: 6.812319E-03 | loss scale: 2048.0 | grad norm: 1.477 | num zeros: 61258756.0 | params norm: 297.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        16128 | elapsed time per iteration (ms): 2747.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.164016E+00 | gshard_loss: 6.830084E-03 | loss scale: 2048.0 | grad norm: 0.876 | num zeros: 109111920.0 | params norm: 297.278 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 2803.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.163903E+00 | gshard_loss: 6.749772E-03 | loss scale: 2048.0 | grad norm: 1.647 | num zeros: 109617184.0 | params norm: 297.294 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        16640 | elapsed time per iteration (ms): 3119.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.168982E+00 | gshard_loss: 6.691959E-03 | loss scale: 2048.0 | grad norm: 0.834 | num zeros: 104471328.0 | params norm: 297.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 2751.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.152407E+00 | gshard_loss: 6.837778E-03 | loss scale: 2048.0 | grad norm: 0.651 | num zeros: 76662400.0 | params norm: 297.326 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        17152 | elapsed time per iteration (ms): 2644.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.150889E+00 | gshard_loss: 7.004047E-03 | loss scale: 2048.0 | grad norm: 1.652 | num zeros: 84691896.0 | params norm: 297.342 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 3161.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.147614E+00 | gshard_loss: 6.937654E-03 | loss scale: 2048.0 | grad norm: 0.526 | num zeros: 83825320.0 | params norm: 297.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        17664 | elapsed time per iteration (ms): 3075.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.142776E+00 | gshard_loss: 6.802523E-03 | loss scale: 2048.0 | grad norm: 0.714 | num zeros: 42883472.0 | params norm: 297.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 2735.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.133704E+00 | gshard_loss: 6.743456E-03 | loss scale: 2048.0 | grad norm: 0.364 | num zeros: 81002904.0 | params norm: 297.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        18176 | elapsed time per iteration (ms): 2729.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.124271E+00 | gshard_loss: 6.665764E-03 | loss scale: 2048.0 | grad norm: 0.457 | num zeros: 94766456.0 | params norm: 297.399 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 3358.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.144192E+00 | gshard_loss: 6.628191E-03 | loss scale: 2048.0 | grad norm: 0.670 | num zeros: 93241016.0 | params norm: 297.413 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        18688 | elapsed time per iteration (ms): 2799.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.138270E+00 | gshard_loss: 6.646430E-03 | loss scale: 2048.0 | grad norm: 5.702 | num zeros: 81182192.0 | params norm: 297.426 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 2720.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.121491E+00 | gshard_loss: 6.752349E-03 | loss scale: 2048.0 | grad norm: 0.833 | num zeros: 83582320.0 | params norm: 297.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        19200 | elapsed time per iteration (ms): 3005.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.126248E+00 | gshard_loss: 6.867610E-03 | loss scale: 2048.0 | grad norm: 1.428 | num zeros: 84290176.0 | params norm: 297.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 2754.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.108809E+00 | gshard_loss: 6.844155E-03 | loss scale: 2048.0 | grad norm: 0.923 | num zeros: 86439864.0 | params norm: 297.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        19712 | elapsed time per iteration (ms): 2751.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.134917E+00 | gshard_loss: 6.697146E-03 | loss scale: 2048.0 | grad norm: 1.549 | num zeros: 32909108.0 | params norm: 297.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 2720.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.118427E+00 | gshard_loss: 6.710519E-03 | loss scale: 2048.0 | grad norm: 1.029 | num zeros: 39523028.0 | params norm: 297.491 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        20224 | elapsed time per iteration (ms): 3839.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.111422E+00 | gshard_loss: 6.829796E-03 | loss scale: 2048.0 | grad norm: 15.079 | num zeros: 81060784.0 | params norm: 297.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 2873.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.134336E+00 | gshard_loss: 6.958871E-03 | loss scale: 2048.0 | grad norm: 0.763 | num zeros: 82754064.0 | params norm: 297.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        20736 | elapsed time per iteration (ms): 2794.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.130895E+00 | gshard_loss: 6.657392E-03 | loss scale: 2048.0 | grad norm: 1.957 | num zeros: 26726010.0 | params norm: 297.523 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 2871.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.124401E+00 | gshard_loss: 6.728986E-03 | loss scale: 2048.0 | grad norm: 1.498 | num zeros: 26805528.0 | params norm: 297.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        21248 | elapsed time per iteration (ms): 2628.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.131135E+00 | gshard_loss: 6.959536E-03 | loss scale: 2048.0 | grad norm: 0.788 | num zeros: 81723632.0 | params norm: 297.545 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 2693.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.103004E+00 | gshard_loss: 6.999412E-03 | loss scale: 2048.0 | grad norm: 0.769 | num zeros: 84165944.0 | params norm: 297.557 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        21760 | elapsed time per iteration (ms): 3035.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.098030E+00 | gshard_loss: 6.940982E-03 | loss scale: 2048.0 | grad norm: 0.547 | num zeros: 84420928.0 | params norm: 297.569 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 3248.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.115959E+00 | gshard_loss: 6.959056E-03 | loss scale: 2048.0 | grad norm: 0.492 | num zeros: 82692272.0 | params norm: 297.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        22272 | elapsed time per iteration (ms): 2627.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.098289E+00 | gshard_loss: 7.060903E-03 | loss scale: 2048.0 | grad norm: 0.668 | num zeros: 35340132.0 | params norm: 297.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 2695.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.093996E+00 | gshard_loss: 6.956615E-03 | loss scale: 2048.0 | grad norm: 0.882 | num zeros: 43697892.0 | params norm: 297.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        22784 | elapsed time per iteration (ms): 3043.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.088023E+00 | gshard_loss: 6.877627E-03 | loss scale: 2048.0 | grad norm: 0.364 | num zeros: 93215120.0 | params norm: 297.619 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 2640.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.093375E+00 | gshard_loss: 6.863496E-03 | loss scale: 2048.0 | grad norm: 0.443 | num zeros: 106515856.0 | params norm: 297.632 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        23296 | elapsed time per iteration (ms): 2604.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.078245E+00 | gshard_loss: 7.005787E-03 | loss scale: 2048.0 | grad norm: 0.466 | num zeros: 85016536.0 | params norm: 297.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 3027.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.069431E+00 | gshard_loss: 6.976683E-03 | loss scale: 2048.0 | grad norm: 0.338 | num zeros: 44526048.0 | params norm: 297.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        23808 | elapsed time per iteration (ms): 3196.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.064393E+00 | gshard_loss: 6.952591E-03 | loss scale: 2048.0 | grad norm: 0.295 | num zeros: 47704168.0 | params norm: 297.670 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 2664.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.058010E+00 | gshard_loss: 6.921065E-03 | loss scale: 2048.0 | grad norm: 0.317 | num zeros: 30577820.0 | params norm: 297.683 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        24320 | elapsed time per iteration (ms): 2608.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.060622E+00 | gshard_loss: 6.956249E-03 | loss scale: 2048.0 | grad norm: 0.365 | num zeros: 28102228.0 | params norm: 297.696 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 2981.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.054027E+00 | gshard_loss: 7.095205E-03 | loss scale: 2048.0 | grad norm: 0.290 | num zeros: 32982692.0 | params norm: 297.709 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        24832 | elapsed time per iteration (ms): 2637.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.053992E+00 | gshard_loss: 6.873392E-03 | loss scale: 2048.0 | grad norm: 0.335 | num zeros: 44344916.0 | params norm: 297.722 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 2629.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.036465E+00 | gshard_loss: 6.869548E-03 | loss scale: 2048.0 | grad norm: 0.289 | num zeros: 44978048.0 | params norm: 297.735 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        25344 | elapsed time per iteration (ms): 2657.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.032636E+00 | gshard_loss: 6.777196E-03 | loss scale: 2048.0 | grad norm: 0.430 | num zeros: 48091816.0 | params norm: 297.748 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 3580.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.037469E+00 | gshard_loss: 6.860226E-03 | loss scale: 2048.0 | grad norm: 0.325 | num zeros: 40972952.0 | params norm: 297.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        25856 | elapsed time per iteration (ms): 2938.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.032151E+00 | gshard_loss: 6.871265E-03 | loss scale: 2048.0 | grad norm: 0.355 | num zeros: 44233088.0 | params norm: 297.773 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 2648.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.028166E+00 | gshard_loss: 6.713144E-03 | loss scale: 2048.0 | grad norm: 0.653 | num zeros: 45541096.0 | params norm: 297.786 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        26368 | elapsed time per iteration (ms): 2850.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.026274E+00 | gshard_loss: 7.100001E-03 | loss scale: 2048.0 | grad norm: 0.766 | num zeros: 49263724.0 | params norm: 297.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 2690.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.023601E+00 | gshard_loss: 6.508413E-03 | loss scale: 2048.0 | grad norm: 0.461 | num zeros: 42075832.0 | params norm: 297.810 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        26880 | elapsed time per iteration (ms): 2681.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.015681E+00 | gshard_loss: 6.388125E-03 | loss scale: 2048.0 | grad norm: 0.632 | num zeros: 41486976.0 | params norm: 297.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 2691.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.006094E+00 | gshard_loss: 6.496705E-03 | loss scale: 2048.0 | grad norm: 0.454 | num zeros: 42670720.0 | params norm: 297.836 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        27392 | elapsed time per iteration (ms): 3624.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.007205E+00 | gshard_loss: 6.547755E-03 | loss scale: 2048.0 | grad norm: 0.462 | num zeros: 41095164.0 | params norm: 297.849 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 2720.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.990316E+00 | gshard_loss: 6.417174E-03 | loss scale: 2048.0 | grad norm: 0.398 | num zeros: 40943224.0 | params norm: 297.863 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        27904 | elapsed time per iteration (ms): 2680.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.982257E+00 | gshard_loss: 6.412196E-03 | loss scale: 2048.0 | grad norm: 0.373 | num zeros: 45339972.0 | params norm: 297.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 2876.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.975687E+00 | gshard_loss: 6.517353E-03 | loss scale: 2048.0 | grad norm: 0.290 | num zeros: 34722216.0 | params norm: 297.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        28416 | elapsed time per iteration (ms): 2686.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.964433E+00 | gshard_loss: 6.485028E-03 | loss scale: 2048.0 | grad norm: 0.297 | num zeros: 40008944.0 | params norm: 297.905 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 2731.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.956033E+00 | gshard_loss: 6.554536E-03 | loss scale: 2048.0 | grad norm: 0.344 | num zeros: 41671688.0 | params norm: 297.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        28928 | elapsed time per iteration (ms): 2695.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.941306E+00 | gshard_loss: 6.386071E-03 | loss scale: 2048.0 | grad norm: 0.373 | num zeros: 33270040.0 | params norm: 297.937 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 3566.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.939440E+00 | gshard_loss: 6.383198E-03 | loss scale: 2048.0 | grad norm: 0.328 | num zeros: 41278140.0 | params norm: 297.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        29440 | elapsed time per iteration (ms): 2923.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.932466E+00 | gshard_loss: 6.427164E-03 | loss scale: 2048.0 | grad norm: 0.389 | num zeros: 52815564.0 | params norm: 297.972 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 2726.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.924762E+00 | gshard_loss: 6.422620E-03 | loss scale: 2048.0 | grad norm: 0.644 | num zeros: 52700624.0 | params norm: 297.989 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        29952 | elapsed time per iteration (ms): 2774.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.930489E+00 | gshard_loss: 6.614166E-03 | loss scale: 2048.0 | grad norm: 0.734 | num zeros: 28625888.0 | params norm: 298.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 2697.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.918988E+00 | gshard_loss: 6.349935E-03 | loss scale: 2048.0 | grad norm: 0.598 | num zeros: 53297380.0 | params norm: 298.024 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        30464 | elapsed time per iteration (ms): 2652.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.901497E+00 | gshard_loss: 6.651076E-03 | loss scale: 2048.0 | grad norm: 0.479 | num zeros: 45497836.0 | params norm: 298.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 2715.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.893649E+00 | gshard_loss: 6.786312E-03 | loss scale: 2048.0 | grad norm: 0.500 | num zeros: 44686800.0 | params norm: 298.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        30976 | elapsed time per iteration (ms): 3554.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.883024E+00 | gshard_loss: 6.526907E-03 | loss scale: 2048.0 | grad norm: 0.603 | num zeros: 31769596.0 | params norm: 298.080 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 2829.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.871083E+00 | gshard_loss: 6.734162E-03 | loss scale: 2048.0 | grad norm: 0.529 | num zeros: 29151316.0 | params norm: 298.100 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        31488 | elapsed time per iteration (ms): 2675.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.863364E+00 | gshard_loss: 6.641417E-03 | loss scale: 2048.0 | grad norm: 0.410 | num zeros: 27332552.0 | params norm: 298.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 2791.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.849574E+00 | gshard_loss: 6.655579E-03 | loss scale: 2048.0 | grad norm: 0.415 | num zeros: 27177048.0 | params norm: 298.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        32000 | elapsed time per iteration (ms): 2815.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.857169E+00 | gshard_loss: 6.817954E-03 | loss scale: 2048.0 | grad norm: 0.516 | num zeros: 26985084.0 | params norm: 298.163 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 2680.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.855318E+00 | gshard_loss: 6.285581E-03 | loss scale: 2048.0 | grad norm: 0.443 | num zeros: 26989744.0 | params norm: 298.186 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        32512 | elapsed time per iteration (ms): 2656.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.844427E+00 | gshard_loss: 6.530467E-03 | loss scale: 2048.0 | grad norm: 0.354 | num zeros: 27289116.0 | params norm: 298.209 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 3040.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.832829E+00 | gshard_loss: 6.776345E-03 | loss scale: 2048.0 | grad norm: 0.490 | num zeros: 27357316.0 | params norm: 298.231 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        33024 | elapsed time per iteration (ms): 2697.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.830672E+00 | gshard_loss: 6.297773E-03 | loss scale: 2048.0 | grad norm: 0.425 | num zeros: 27226954.0 | params norm: 298.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 2700.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.829550E+00 | gshard_loss: 6.245648E-03 | loss scale: 2048.0 | grad norm: 0.505 | num zeros: 27698564.0 | params norm: 298.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        33536 | elapsed time per iteration (ms): 3734.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.808982E+00 | gshard_loss: 6.233585E-03 | loss scale: 2048.0 | grad norm: 0.435 | num zeros: 27239768.0 | params norm: 298.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 2763.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.802186E+00 | gshard_loss: 6.214433E-03 | loss scale: 2048.0 | grad norm: 0.549 | num zeros: 27326940.0 | params norm: 298.316 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        34048 | elapsed time per iteration (ms): 2736.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.817268E+00 | gshard_loss: 6.140563E-03 | loss scale: 2048.0 | grad norm: 0.664 | num zeros: 28107760.0 | params norm: 298.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 2765.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.800747E+00 | gshard_loss: 6.156199E-03 | loss scale: 2048.0 | grad norm: 0.582 | num zeros: 27622256.0 | params norm: 298.356 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        34560 | elapsed time per iteration (ms): 2889.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.785500E+00 | gshard_loss: 6.264811E-03 | loss scale: 2048.0 | grad norm: 0.420 | num zeros: 28383540.0 | params norm: 298.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 2693.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.786038E+00 | gshard_loss: 6.103785E-03 | loss scale: 2048.0 | grad norm: 0.516 | num zeros: 28011740.0 | params norm: 298.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        35072 | elapsed time per iteration (ms): 2821.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.776440E+00 | gshard_loss: 5.947094E-03 | loss scale: 2048.0 | grad norm: 0.441 | num zeros: 27253084.0 | params norm: 298.414 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 3636.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.766620E+00 | gshard_loss: 6.103397E-03 | loss scale: 2048.0 | grad norm: 0.338 | num zeros: 18787196.0 | params norm: 298.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        35584 | elapsed time per iteration (ms): 2764.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.773898E+00 | gshard_loss: 6.121776E-03 | loss scale: 2048.0 | grad norm: 0.373 | num zeros: 3220319.0 | params norm: 298.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 2755.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.768816E+00 | gshard_loss: 5.982991E-03 | loss scale: 2048.0 | grad norm: 0.350 | num zeros: 1395254.0 | params norm: 298.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        36096 | elapsed time per iteration (ms): 2798.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.739543E+00 | gshard_loss: 6.023564E-03 | loss scale: 2048.0 | grad norm: 0.350 | num zeros: 2819825.0 | params norm: 298.492 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 2840.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.740462E+00 | gshard_loss: 5.961516E-03 | loss scale: 2048.0 | grad norm: 0.425 | num zeros: 581294.0 | params norm: 298.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        36608 | elapsed time per iteration (ms): 2722.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.765208E+00 | gshard_loss: 6.156941E-03 | loss scale: 2048.0 | grad norm: 0.771 | num zeros: 3630432.0 | params norm: 298.530 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 2825.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.789708E+00 | gshard_loss: 5.734432E-03 | loss scale: 2048.0 | grad norm: 1.161 | num zeros: 199384.0 | params norm: 298.551 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        37120 | elapsed time per iteration (ms): 3733.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.728822E+00 | gshard_loss: 5.977115E-03 | loss scale: 2048.0 | grad norm: 0.462 | num zeros: 1325385.0 | params norm: 298.571 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 2693.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.759252E+00 | gshard_loss: 6.265744E-03 | loss scale: 2048.0 | grad norm: 0.761 | num zeros: 546506.0 | params norm: 298.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        37632 | elapsed time per iteration (ms): 2739.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.724671E+00 | gshard_loss: 6.167946E-03 | loss scale: 2048.0 | grad norm: 0.583 | num zeros: 527036.0 | params norm: 298.606 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 3318.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.736979E+00 | gshard_loss: 6.002542E-03 | loss scale: 2048.0 | grad norm: 0.627 | num zeros: 230942.0 | params norm: 298.626 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        38144 | elapsed time per iteration (ms): 2780.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.715393E+00 | gshard_loss: 6.177318E-03 | loss scale: 2048.0 | grad norm: 0.489 | num zeros: 1218919.0 | params norm: 298.643 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 2686.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.723054E+00 | gshard_loss: 6.192932E-03 | loss scale: 2048.0 | grad norm: 0.398 | num zeros: 718198.0 | params norm: 298.660 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        38656 | elapsed time per iteration (ms): 2713.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.705622E+00 | gshard_loss: 6.115491E-03 | loss scale: 2048.0 | grad norm: 0.459 | num zeros: 206127.0 | params norm: 298.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 3500.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.713280E+00 | gshard_loss: 6.115665E-03 | loss scale: 2048.0 | grad norm: 0.393 | num zeros: 250480.0 | params norm: 298.697 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        39168 | elapsed time per iteration (ms): 2685.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.689443E+00 | gshard_loss: 6.088225E-03 | loss scale: 2048.0 | grad norm: 0.387 | num zeros: 424418.0 | params norm: 298.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 2706.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.702138E+00 | gshard_loss: 6.063407E-03 | loss scale: 2048.0 | grad norm: 0.528 | num zeros: 282283.0 | params norm: 298.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        39680 | elapsed time per iteration (ms): 2960.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.714391E+00 | gshard_loss: 6.162974E-03 | loss scale: 2048.0 | grad norm: 0.841 | num zeros: 221168.0 | params norm: 298.747 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 2723.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.724857E+00 | gshard_loss: 6.195765E-03 | loss scale: 2048.0 | grad norm: 1.289 | num zeros: 842780.0 | params norm: 298.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        40192 | elapsed time per iteration (ms): 2692.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.688083E+00 | gshard_loss: 6.150628E-03 | loss scale: 2048.0 | grad norm: 0.543 | num zeros: 1473919.0 | params norm: 298.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 2698.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.701964E+00 | gshard_loss: 6.200208E-03 | loss scale: 2048.0 | grad norm: 0.697 | num zeros: 407577.0 | params norm: 298.796 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        40704 | elapsed time per iteration (ms): 3980.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.687804E+00 | gshard_loss: 6.063940E-03 | loss scale: 2048.0 | grad norm: 0.615 | num zeros: 299506.0 | params norm: 298.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 2739.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.678437E+00 | gshard_loss: 6.044088E-03 | loss scale: 2048.0 | grad norm: 0.377 | num zeros: 624327.0 | params norm: 298.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        41216 | elapsed time per iteration (ms): 2689.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.688940E+00 | gshard_loss: 6.033069E-03 | loss scale: 2048.0 | grad norm: 0.587 | num zeros: 591201.0 | params norm: 298.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 2903.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.680573E+00 | gshard_loss: 6.055928E-03 | loss scale: 2048.0 | grad norm: 0.325 | num zeros: 829823.0 | params norm: 298.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        41728 | elapsed time per iteration (ms): 2880.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.689056E+00 | gshard_loss: 6.001610E-03 | loss scale: 2048.0 | grad norm: 0.522 | num zeros: 656550.0 | params norm: 298.878 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 2724.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.667651E+00 | gshard_loss: 5.973316E-03 | loss scale: 2048.0 | grad norm: 0.533 | num zeros: 857501.0 | params norm: 298.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        42240 | elapsed time per iteration (ms): 3069.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.667458E+00 | gshard_loss: 5.921442E-03 | loss scale: 2048.0 | grad norm: 0.376 | num zeros: 785426.0 | params norm: 298.908 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 3310.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.645274E+00 | gshard_loss: 5.922975E-03 | loss scale: 2048.0 | grad norm: 0.506 | num zeros: 402149.0 | params norm: 298.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        42752 | elapsed time per iteration (ms): 2727.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.647031E+00 | gshard_loss: 5.926798E-03 | loss scale: 2048.0 | grad norm: 0.477 | num zeros: 800598.0 | params norm: 298.938 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 2731.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.631698E+00 | gshard_loss: 5.840378E-03 | loss scale: 2048.0 | grad norm: 0.542 | num zeros: 257746.0 | params norm: 298.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        43264 | elapsed time per iteration (ms): 3163.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.630530E+00 | gshard_loss: 5.945980E-03 | loss scale: 2048.0 | grad norm: 0.514 | num zeros: 408254.0 | params norm: 298.969 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 2756.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.639943E+00 | gshard_loss: 5.831289E-03 | loss scale: 2048.0 | grad norm: 0.378 | num zeros: 945919.0 | params norm: 298.984 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        43776 | elapsed time per iteration (ms): 2773.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.632827E+00 | gshard_loss: 5.711093E-03 | loss scale: 2048.0 | grad norm: 0.350 | num zeros: 625329.0 | params norm: 298.999 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 3606.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.628141E+00 | gshard_loss: 5.561946E-03 | loss scale: 2048.0 | grad norm: 0.341 | num zeros: 1275164.0 | params norm: 299.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        44288 | elapsed time per iteration (ms): 2811.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.607283E+00 | gshard_loss: 5.582513E-03 | loss scale: 2048.0 | grad norm: 0.368 | num zeros: 1587938.0 | params norm: 299.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 2783.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.611323E+00 | gshard_loss: 5.461588E-03 | loss scale: 2048.0 | grad norm: 0.325 | num zeros: 401852.0 | params norm: 299.046 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        44800 | elapsed time per iteration (ms): 2962.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.612362E+00 | gshard_loss: 5.383580E-03 | loss scale: 2048.0 | grad norm: 0.438 | num zeros: 344742.0 | params norm: 299.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 3104.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.598083E+00 | gshard_loss: 5.496412E-03 | loss scale: 2048.0 | grad norm: 0.386 | num zeros: 427817.0 | params norm: 299.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        45312 | elapsed time per iteration (ms): 2835.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.598720E+00 | gshard_loss: 5.539029E-03 | loss scale: 2048.0 | grad norm: 0.395 | num zeros: 465530.0 | params norm: 299.094 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 2799.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.592095E+00 | gshard_loss: 5.517450E-03 | loss scale: 2048.0 | grad norm: 0.398 | num zeros: 323812.0 | params norm: 299.110 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        45824 | elapsed time per iteration (ms): 3779.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.581678E+00 | gshard_loss: 5.511350E-03 | loss scale: 2048.0 | grad norm: 0.320 | num zeros: 413749.0 | params norm: 299.126 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 2832.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.572167E+00 | gshard_loss: 5.320063E-03 | loss scale: 2048.0 | grad norm: 0.383 | num zeros: 261564.0 | params norm: 299.144 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        46336 | elapsed time per iteration (ms): 2866.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.579642E+00 | gshard_loss: 5.405165E-03 | loss scale: 2048.0 | grad norm: 0.351 | num zeros: 1162471.0 | params norm: 299.161 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 2987.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.594060E+00 | gshard_loss: 5.294180E-03 | loss scale: 2048.0 | grad norm: 0.544 | num zeros: 154635.0 | params norm: 299.178 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        46848 | elapsed time per iteration (ms): 2850.2 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.573391E+00 | gshard_loss: 5.340784E-03 | loss scale: 2048.0 | grad norm: 0.519 | num zeros: 2266834.0 | params norm: 299.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 2847.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.565467E+00 | gshard_loss: 5.418584E-03 | loss scale: 2048.0 | grad norm: 0.371 | num zeros: 397552.0 | params norm: 299.211 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        47360 | elapsed time per iteration (ms): 2852.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.560513E+00 | gshard_loss: 5.402399E-03 | loss scale: 2048.0 | grad norm: 0.707 | num zeros: 82983.0 | params norm: 299.230 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 3678.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.563313E+00 | gshard_loss: 5.350239E-03 | loss scale: 2048.0 | grad norm: 0.441 | num zeros: 6338637.0 | params norm: 299.247 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        47872 | elapsed time per iteration (ms): 2803.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.569759E+00 | gshard_loss: 5.335376E-03 | loss scale: 2048.0 | grad norm: 0.423 | num zeros: 3133487.0 | params norm: 299.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 2792.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.558362E+00 | gshard_loss: 5.387482E-03 | loss scale: 2048.0 | grad norm: 0.419 | num zeros: 310047.0 | params norm: 299.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        48384 | elapsed time per iteration (ms): 3042.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.553066E+00 | gshard_loss: 5.508550E-03 | loss scale: 2048.0 | grad norm: 0.428 | num zeros: 227734.0 | params norm: 299.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 2807.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.541836E+00 | gshard_loss: 5.504196E-03 | loss scale: 2048.0 | grad norm: 0.370 | num zeros: 295235.0 | params norm: 299.320 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        48896 | elapsed time per iteration (ms): 2806.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.542798E+00 | gshard_loss: 5.422164E-03 | loss scale: 2048.0 | grad norm: 0.341 | num zeros: 186411.0 | params norm: 299.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 3116.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.541656E+00 | gshard_loss: 5.335271E-03 | loss scale: 2048.0 | grad norm: 0.433 | num zeros: 173793.0 | params norm: 299.358 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        49408 | elapsed time per iteration (ms): 3298.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.517293E+00 | gshard_loss: 5.382395E-03 | loss scale: 2048.0 | grad norm: 0.401 | num zeros: 412603.0 | params norm: 299.376 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 2799.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.525759E+00 | gshard_loss: 5.389665E-03 | loss scale: 2048.0 | grad norm: 0.404 | num zeros: 318560.0 | params norm: 299.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        49920 | elapsed time per iteration (ms): 2807.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.515207E+00 | gshard_loss: 5.366612E-03 | loss scale: 2048.0 | grad norm: 0.355 | num zeros: 236187.0 | params norm: 299.412 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 3123.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.512550E+00 | gshard_loss: 5.355483E-03 | loss scale: 2048.0 | grad norm: 0.352 | num zeros: 264645.0 | params norm: 299.430 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:        50432 | elapsed time per iteration (ms): 3048.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.510713E+00 | gshard_loss: 5.381728E-03 | loss scale: 2048.0 | grad norm: 0.335 | num zeros: 166855.0 | params norm: 299.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 2828.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.503659E+00 | gshard_loss: 5.367619E-03 | loss scale: 2048.0 | grad norm: 0.400 | num zeros: 213753.0 | params norm: 299.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:        50944 | elapsed time per iteration (ms): 3651.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.502129E+00 | gshard_loss: 5.364659E-03 | loss scale: 2048.0 | grad norm: 0.304 | num zeros: 121902.0 | params norm: 299.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 2841.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.497585E+00 | gshard_loss: 5.338216E-03 | loss scale: 2048.0 | grad norm: 0.327 | num zeros: 175807.0 | params norm: 299.508 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2022-12-23 01:37:24 
