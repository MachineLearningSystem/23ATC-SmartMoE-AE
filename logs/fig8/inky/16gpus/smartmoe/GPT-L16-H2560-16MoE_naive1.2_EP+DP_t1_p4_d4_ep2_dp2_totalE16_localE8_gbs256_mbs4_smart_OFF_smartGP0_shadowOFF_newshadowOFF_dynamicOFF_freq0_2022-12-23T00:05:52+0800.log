+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 4 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 5 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 7 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 2 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 3 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 0 == 0 ']'
+ hostname
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 1 == 0 ']'
+ '[' 6 == 0 ']'
+ sleep 10s
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 9 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 11 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 13 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 15 == 0 ']'
+ sleep 10s
+ '[' 8 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 12 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 10 == 0 ']'
+ '[' 14 == 0 ']'
+ sleep 10s
+ sleep 10s
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=15
+ RANK=15
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ export RANK=10
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ localrank=0
+ RANK=10
+ export WORLD_SIZE=16
+ export RANK=9
+ RANK=9
+ WORLD_SIZE=16
+ localrank=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export MASTER_ADDR=SH-IDC1-10-140-1-68
+ MASTER_ADDR=SH-IDC1-10-140-1-68
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ export NNODES=2
+ NNODES=2
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' EP+DP == EP+DP ']'
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export NNODES=2
+ NNODES=2
+ '[' EP+DP == EP+DP ']'
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export NNODES=2
+ NNODES=2
+ '[' EP+DP == EP+DP ']'
+ MAX_JOBS=64
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ MAX_JOBS=64
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ export NODE_RANK=4
+ NODE_RANK=4
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ export NODE_RANK=6
+ NODE_RANK=6
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export MAX_JOBS=64
+ MAX_JOBS=64
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' sh-lab == nico ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' sh-lab == nico ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' EP+DP == EP+DP ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ false
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=51200
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof/table
+ python_args=
+ python_args+='
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 256         --train-samples 51200 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 2                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1223_GPT-L16-H2560-16MoE/GPT-L16-H2560-16MoE_naive1.2_EP+DP_t1_p4_d4_ep2_dp2_totalE16_localE8_gbs256_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-23T00:05:52+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 256 --train-samples 51200 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 2 --expert-dp-size 2 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 4 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 4
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 2
  expert_ep_size .................................. 2
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2560
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 8
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 4
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 51200
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 16
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 4
[INFO] 0 in EP group [0, 1]
[INFO] 0 in DP group [0, 2]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.174 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 4 in EP group [4, 5]
[INFO] 4 in DP group [4, 6]
[INFO] 5 in EP group [4, 5]
[INFO] 5 in DP group [5, 7]
[INFO] 1 in EP group [0, 1]
[INFO] 1 in DP group [1, 3]
[INFO] 9 in EP group [8, 9]
[INFO] 9 in DP group [9, 11]
[INFO] 15 in EP group [14, 15]
[INFO] 15 in DP group [13, 15]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 8.362 seconds
time to initialize megatron (seconds): 0.649
[after megatron is initialized] datetime: 2022-12-23 00:06:24 
hhs=5120
hhs=5120
[INFO] 3 in EP group [2, 3]
[INFO] 3 in DP group [1, 3]
hhs=5120
[INFO] 6 in EP group [6, 7]
[INFO] 6 in DP group [4, 6]
hhs=5120
hhs=5120
building GPT model ...
[INFO] 2 in EP group [2, 3]
[INFO] 2 in DP group [0, 2]
hhs=5120
[INFO] 7 in EP group [6, 7]
[INFO] 7 in DP group [5, 7]
hhs=5120
hhs=5120
hhs=5120
hhs=5120
[INFO] 11 in EP group [10, 11]
[INFO] 11 in DP group [9, 11]
hhs=5120
[INFO] 13 in EP group [12, 13]
[INFO] 13 in DP group [13, 15]
hhs=5120
[INFO] 12 in EP group [12, 13]
[INFO] 12 in DP group [12, 14]
hhs=5120
[INFO] 14 in EP group [14, 15]
[INFO] 14 in DP group [12, 14]
hhs=5120
[INFO] 8 in EP group [8, 9]
[INFO] 8 in DP group [8, 10]
hhs=5120
[INFO] 10 in EP group [10, 11]
[INFO] 10 in DP group [8, 10]
hhs=5120
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 944209984
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1072993344
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 944209984
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1075609664
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-23 00:06:47 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.143575 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.052 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-23 00:07:01 
done with setup ...
training ...
[before the start of training step] datetime: 2022-12-23 00:07:02 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 20525.6328125 | max allocated: 20525.63525390625 | reserved: 26518.0 | max reserved: 26518.0
[Rank 4] (after 1 iterations) memory (MB) | allocated: 18062.42578125 | max allocated: 18062.42822265625 | reserved: 22400.0 | max reserved: 22400.0
[Rank 12] (after 1 iterations) memory (MB) | allocated: 20522.048828125 | max allocated: 20522.06396484375 | reserved: 21802.0 | max reserved: 21802.0
[Rank 8] (after 1 iterations) memory (MB) | allocated: 18065.7392578125 | max allocated: 18065.74169921875 | reserved: 21086.0 | max reserved: 21086.0
 iteration        1/     200 | consumed samples:          256 | elapsed time per iteration (ms): 14576.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082688E+01 | loss scale: 131072.0 | grad norm: 15.815 | num zeros: 379143.0 | params norm: 305.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/     200 | consumed samples:          512 | elapsed time per iteration (ms): 2861.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.036742E+01 | loss scale: 131072.0 | grad norm: 6.081 | num zeros: 1945214976.0 | params norm: 305.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:          768 | elapsed time per iteration (ms): 2815.2 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 305.566 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 2686.6 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 305.566 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         1280 | elapsed time per iteration (ms): 2687.5 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 32768.0 | params norm: 305.566 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 2872.6 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 16384.0 | params norm: 305.566 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         1792 | elapsed time per iteration (ms): 2789.9 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 8192.0 | params norm: 305.566 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 2804.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.010754E+01 | loss scale: 8192.0 | grad norm: 102.944 | num zeros: 357772.0 | params norm: 305.652 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         2304 | elapsed time per iteration (ms): 2714.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.103714E+01 | loss scale: 8192.0 | grad norm: 59.487 | num zeros: 27749564.0 | params norm: 305.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 3042.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.803322E+00 | loss scale: 8192.0 | grad norm: 5.154 | num zeros: 1813464832.0 | params norm: 305.836 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         2816 | elapsed time per iteration (ms): 2736.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.549023E+00 | loss scale: 8192.0 | grad norm: 5.042 | num zeros: 2035074816.0 | params norm: 305.941 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 2551.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.339378E+00 | loss scale: 8192.0 | grad norm: 5.033 | num zeros: 2006448768.0 | params norm: 306.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         3328 | elapsed time per iteration (ms): 2804.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.144827E+00 | loss scale: 8192.0 | grad norm: 4.972 | num zeros: 2185228288.0 | params norm: 306.167 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 2662.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.947414E+00 | loss scale: 8192.0 | grad norm: 4.965 | num zeros: 2329536000.0 | params norm: 306.279 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         3840 | elapsed time per iteration (ms): 2631.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.749832E+00 | loss scale: 8192.0 | grad norm: 4.939 | num zeros: 2225597184.0 | params norm: 306.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 2616.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.578631E+00 | loss scale: 8192.0 | grad norm: 4.857 | num zeros: 2281334272.0 | params norm: 306.501 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         4352 | elapsed time per iteration (ms): 2851.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.407089E+00 | loss scale: 8192.0 | grad norm: 4.808 | num zeros: 1111931392.0 | params norm: 306.608 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 2832.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.251616E+00 | loss scale: 8192.0 | grad norm: 4.743 | num zeros: 2439614976.0 | params norm: 306.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         4864 | elapsed time per iteration (ms): 2677.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.107459E+00 | loss scale: 8192.0 | grad norm: 4.632 | num zeros: 2287133440.0 | params norm: 306.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 2720.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.982956E+00 | loss scale: 8192.0 | grad norm: 4.844 | num zeros: 405798720.0 | params norm: 306.911 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:         5376 | elapsed time per iteration (ms): 3116.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.963235E+00 | loss scale: 8192.0 | grad norm: 16.027 | num zeros: 251623008.0 | params norm: 307.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 2824.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.782951E+00 | loss scale: 8192.0 | grad norm: 4.020 | num zeros: 1669832576.0 | params norm: 307.093 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:         5888 | elapsed time per iteration (ms): 2818.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.677716E+00 | loss scale: 8192.0 | grad norm: 3.784 | num zeros: 1635734912.0 | params norm: 307.186 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 2720.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.595913E+00 | loss scale: 8192.0 | grad norm: 3.377 | num zeros: 1876290944.0 | params norm: 307.276 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:         6400 | elapsed time per iteration (ms): 2682.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.520989E+00 | loss scale: 8192.0 | grad norm: 2.907 | num zeros: 1920861056.0 | params norm: 307.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 2627.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.465041E+00 | loss scale: 8192.0 | grad norm: 2.315 | num zeros: 1768363008.0 | params norm: 307.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:         6912 | elapsed time per iteration (ms): 2823.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.413322E+00 | loss scale: 8192.0 | grad norm: 1.685 | num zeros: 1481863680.0 | params norm: 307.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 2760.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.382259E+00 | loss scale: 8192.0 | grad norm: 0.981 | num zeros: 1370925056.0 | params norm: 307.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:         7424 | elapsed time per iteration (ms): 2776.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.360663E+00 | loss scale: 8192.0 | grad norm: 0.373 | num zeros: 1725667712.0 | params norm: 307.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 2614.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.350894E+00 | loss scale: 8192.0 | grad norm: 0.594 | num zeros: 1614032128.0 | params norm: 307.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:         7936 | elapsed time per iteration (ms): 2918.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.363112E+00 | loss scale: 8192.0 | grad norm: 0.983 | num zeros: 1613953152.0 | params norm: 307.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 2684.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.353868E+00 | loss scale: 8192.0 | grad norm: 0.977 | num zeros: 1423434752.0 | params norm: 307.932 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:         8448 | elapsed time per iteration (ms): 2586.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.366218E+00 | loss scale: 8192.0 | grad norm: 0.809 | num zeros: 1566819712.0 | params norm: 308.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 2544.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.353229E+00 | loss scale: 8192.0 | grad norm: 0.808 | num zeros: 1574964736.0 | params norm: 308.076 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:         8960 | elapsed time per iteration (ms): 2616.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.342994E+00 | loss scale: 8192.0 | grad norm: 0.876 | num zeros: 1705629184.0 | params norm: 308.146 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 2612.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.327929E+00 | loss scale: 8192.0 | grad norm: 0.442 | num zeros: 1636590592.0 | params norm: 308.214 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:         9472 | elapsed time per iteration (ms): 2580.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.334221E+00 | loss scale: 8192.0 | grad norm: 0.303 | num zeros: 1634856576.0 | params norm: 308.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 2577.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.330248E+00 | loss scale: 8192.0 | grad norm: 0.259 | num zeros: 1659795200.0 | params norm: 308.344 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:         9984 | elapsed time per iteration (ms): 3137.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.341404E+00 | loss scale: 8192.0 | grad norm: 0.294 | num zeros: 1492537600.0 | params norm: 308.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 2671.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.317243E+00 | loss scale: 8192.0 | grad norm: 0.492 | num zeros: 1410515072.0 | params norm: 308.465 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        10496 | elapsed time per iteration (ms): 2492.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.317350E+00 | loss scale: 8192.0 | grad norm: 0.366 | num zeros: 1498283008.0 | params norm: 308.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 2538.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.346689E+00 | loss scale: 8192.0 | grad norm: 0.314 | num zeros: 1404804992.0 | params norm: 308.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        11008 | elapsed time per iteration (ms): 2501.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.323726E+00 | loss scale: 8192.0 | grad norm: 0.332 | num zeros: 1325454208.0 | params norm: 308.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 2612.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.320555E+00 | loss scale: 8192.0 | grad norm: 0.310 | num zeros: 1104673792.0 | params norm: 308.676 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        11520 | elapsed time per iteration (ms): 2532.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.315895E+00 | loss scale: 8192.0 | grad norm: 0.431 | num zeros: 596149248.0 | params norm: 308.720 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 3087.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.302687E+00 | loss scale: 8192.0 | grad norm: 0.421 | num zeros: 721177856.0 | params norm: 308.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        12032 | elapsed time per iteration (ms): 2999.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.304082E+00 | loss scale: 8192.0 | grad norm: 0.974 | num zeros: 660136192.0 | params norm: 308.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 2557.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.393001E+00 | loss scale: 8192.0 | grad norm: 4.215 | num zeros: 309132992.0 | params norm: 308.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        12544 | elapsed time per iteration (ms): 2523.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.312274E+00 | loss scale: 8192.0 | grad norm: 1.225 | num zeros: 892982784.0 | params norm: 308.878 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 2600.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.299138E+00 | loss scale: 8192.0 | grad norm: 0.535 | num zeros: 992995520.0 | params norm: 308.916 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        13056 | elapsed time per iteration (ms): 2683.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.298994E+00 | loss scale: 8192.0 | grad norm: 1.021 | num zeros: 955281792.0 | params norm: 308.954 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 2647.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.280881E+00 | loss scale: 8192.0 | grad norm: 0.892 | num zeros: 899636672.0 | params norm: 308.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        13568 | elapsed time per iteration (ms): 2568.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.284671E+00 | loss scale: 8192.0 | grad norm: 2.733 | num zeros: 700074240.0 | params norm: 309.030 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 2852.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.266021E+00 | loss scale: 8192.0 | grad norm: 1.641 | num zeros: 695566080.0 | params norm: 309.069 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        14080 | elapsed time per iteration (ms): 2801.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.260964E+00 | loss scale: 8192.0 | grad norm: 4.027 | num zeros: 790336128.0 | params norm: 309.106 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 2719.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.271550E+00 | loss scale: 8192.0 | grad norm: 1.736 | num zeros: 1011157760.0 | params norm: 309.143 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        14592 | elapsed time per iteration (ms): 2567.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.254742E+00 | loss scale: 8192.0 | grad norm: 5.349 | num zeros: 1118126848.0 | params norm: 309.179 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 2583.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.242931E+00 | loss scale: 8192.0 | grad norm: 1.542 | num zeros: 1376934400.0 | params norm: 309.214 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        15104 | elapsed time per iteration (ms): 2718.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.221902E+00 | loss scale: 8192.0 | grad norm: 0.845 | num zeros: 1619315584.0 | params norm: 309.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 2671.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.210340E+00 | loss scale: 8192.0 | grad norm: 0.780 | num zeros: 1826320000.0 | params norm: 309.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        15616 | elapsed time per iteration (ms): 2810.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.207325E+00 | loss scale: 8192.0 | grad norm: 0.405 | num zeros: 1903041664.0 | params norm: 309.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 2813.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.196879E+00 | loss scale: 8192.0 | grad norm: 0.736 | num zeros: 1916084352.0 | params norm: 309.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        16128 | elapsed time per iteration (ms): 2604.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.165066E+00 | loss scale: 8192.0 | grad norm: 0.473 | num zeros: 1992821248.0 | params norm: 309.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 2665.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.156741E+00 | loss scale: 8192.0 | grad norm: 0.389 | num zeros: 2004801920.0 | params norm: 309.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        16640 | elapsed time per iteration (ms): 2642.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.159918E+00 | loss scale: 8192.0 | grad norm: 0.426 | num zeros: 2054301952.0 | params norm: 309.435 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 3051.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.131912E+00 | loss scale: 8192.0 | grad norm: 0.599 | num zeros: 2033692032.0 | params norm: 309.463 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        17152 | elapsed time per iteration (ms): 2562.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.130182E+00 | loss scale: 8192.0 | grad norm: 0.542 | num zeros: 2181637120.0 | params norm: 309.491 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 2935.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.127316E+00 | loss scale: 8192.0 | grad norm: 0.599 | num zeros: 2173766656.0 | params norm: 309.517 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        17664 | elapsed time per iteration (ms): 2679.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.105164E+00 | loss scale: 8192.0 | grad norm: 0.404 | num zeros: 2235900672.0 | params norm: 309.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 2635.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.097097E+00 | loss scale: 8192.0 | grad norm: 0.605 | num zeros: 2159215104.0 | params norm: 309.568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        18176 | elapsed time per iteration (ms): 2559.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.086447E+00 | loss scale: 8192.0 | grad norm: 0.650 | num zeros: 2202660864.0 | params norm: 309.592 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 2698.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.094643E+00 | loss scale: 8192.0 | grad norm: 1.375 | num zeros: 1955569664.0 | params norm: 309.615 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        18688 | elapsed time per iteration (ms): 2668.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.090337E+00 | loss scale: 8192.0 | grad norm: 0.579 | num zeros: 2151110656.0 | params norm: 309.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 2870.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.096357E+00 | loss scale: 8192.0 | grad norm: 2.011 | num zeros: 2093771264.0 | params norm: 309.659 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        19200 | elapsed time per iteration (ms): 2547.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.079713E+00 | loss scale: 8192.0 | grad norm: 0.724 | num zeros: 2029482112.0 | params norm: 309.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 3096.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.051020E+00 | loss scale: 8192.0 | grad norm: 1.484 | num zeros: 1968840832.0 | params norm: 309.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        19712 | elapsed time per iteration (ms): 2716.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.076707E+00 | loss scale: 8192.0 | grad norm: 1.021 | num zeros: 2052606592.0 | params norm: 309.718 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 2586.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.052631E+00 | loss scale: 8192.0 | grad norm: 0.705 | num zeros: 2107648384.0 | params norm: 309.735 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        20224 | elapsed time per iteration (ms): 2621.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.044641E+00 | loss scale: 8192.0 | grad norm: 0.968 | num zeros: 1947549696.0 | params norm: 309.752 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 2589.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.048491E+00 | loss scale: 8192.0 | grad norm: 0.700 | num zeros: 2012492544.0 | params norm: 309.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        20736 | elapsed time per iteration (ms): 2654.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.038555E+00 | loss scale: 8192.0 | grad norm: 0.910 | num zeros: 2020480896.0 | params norm: 309.790 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 2632.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.065867E+00 | loss scale: 8192.0 | grad norm: 0.988 | num zeros: 2030078592.0 | params norm: 309.805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        21248 | elapsed time per iteration (ms): 3022.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.066535E+00 | loss scale: 8192.0 | grad norm: 1.003 | num zeros: 2024137856.0 | params norm: 309.820 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 2838.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.019696E+00 | loss scale: 8192.0 | grad norm: 0.845 | num zeros: 2106200960.0 | params norm: 309.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        21760 | elapsed time per iteration (ms): 2601.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.023416E+00 | loss scale: 8192.0 | grad norm: 1.100 | num zeros: 2043685376.0 | params norm: 309.854 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 2628.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.041718E+00 | loss scale: 8192.0 | grad norm: 1.052 | num zeros: 2130794240.0 | params norm: 309.870 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        22272 | elapsed time per iteration (ms): 2592.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.994658E+00 | loss scale: 8192.0 | grad norm: 0.420 | num zeros: 2166461952.0 | params norm: 309.886 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 2845.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.010245E+00 | loss scale: 8192.0 | grad norm: 0.954 | num zeros: 2151508992.0 | params norm: 309.902 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        22784 | elapsed time per iteration (ms): 2594.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.989505E+00 | loss scale: 8192.0 | grad norm: 0.935 | num zeros: 2174989824.0 | params norm: 309.916 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 2573.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.980453E+00 | loss scale: 8192.0 | grad norm: 0.769 | num zeros: 1994994048.0 | params norm: 309.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        23296 | elapsed time per iteration (ms): 2977.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.980972E+00 | loss scale: 8192.0 | grad norm: 1.103 | num zeros: 2193846784.0 | params norm: 309.949 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 2785.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.965617E+00 | loss scale: 8192.0 | grad norm: 0.928 | num zeros: 2092204800.0 | params norm: 309.967 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        23808 | elapsed time per iteration (ms): 2583.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.951333E+00 | loss scale: 8192.0 | grad norm: 0.587 | num zeros: 2142963328.0 | params norm: 309.984 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 2578.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.954990E+00 | loss scale: 8192.0 | grad norm: 0.804 | num zeros: 2120641024.0 | params norm: 310.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        24320 | elapsed time per iteration (ms): 2882.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.942494E+00 | loss scale: 8192.0 | grad norm: 0.384 | num zeros: 2106264064.0 | params norm: 310.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 3256.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.941377E+00 | loss scale: 8192.0 | grad norm: 0.892 | num zeros: 2097825792.0 | params norm: 310.039 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        24832 | elapsed time per iteration (ms): 2557.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.948088E+00 | loss scale: 8192.0 | grad norm: 0.916 | num zeros: 1984578816.0 | params norm: 310.062 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 2959.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.919321E+00 | loss scale: 8192.0 | grad norm: 0.547 | num zeros: 2011526400.0 | params norm: 310.084 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        25344 | elapsed time per iteration (ms): 2684.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.916138E+00 | loss scale: 8192.0 | grad norm: 0.590 | num zeros: 1982977024.0 | params norm: 310.105 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 2691.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.917145E+00 | loss scale: 8192.0 | grad norm: 0.486 | num zeros: 1949714816.0 | params norm: 310.127 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        25856 | elapsed time per iteration (ms): 2636.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.911690E+00 | loss scale: 8192.0 | grad norm: 0.447 | num zeros: 1909509120.0 | params norm: 310.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 2705.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.899382E+00 | loss scale: 8192.0 | grad norm: 0.806 | num zeros: 1733878400.0 | params norm: 310.173 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        26368 | elapsed time per iteration (ms): 2782.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.892675E+00 | loss scale: 8192.0 | grad norm: 0.883 | num zeros: 1834118016.0 | params norm: 310.200 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 2674.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.907578E+00 | loss scale: 8192.0 | grad norm: 1.321 | num zeros: 1792351744.0 | params norm: 310.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        26880 | elapsed time per iteration (ms): 2729.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.880667E+00 | loss scale: 8192.0 | grad norm: 0.728 | num zeros: 1697820288.0 | params norm: 310.249 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 2609.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.886306E+00 | loss scale: 8192.0 | grad norm: 0.944 | num zeros: 1762191616.0 | params norm: 310.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        27392 | elapsed time per iteration (ms): 2866.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.892931E+00 | loss scale: 8192.0 | grad norm: 1.228 | num zeros: 1784275456.0 | params norm: 310.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 3027.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.861299E+00 | loss scale: 8192.0 | grad norm: 0.779 | num zeros: 1703925632.0 | params norm: 310.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        27904 | elapsed time per iteration (ms): 2611.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.881139E+00 | loss scale: 8192.0 | grad norm: 1.542 | num zeros: 1722670080.0 | params norm: 310.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 2806.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.844934E+00 | loss scale: 8192.0 | grad norm: 0.489 | num zeros: 1640132096.0 | params norm: 310.370 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        28416 | elapsed time per iteration (ms): 2643.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.853706E+00 | loss scale: 8192.0 | grad norm: 1.301 | num zeros: 1648071936.0 | params norm: 310.392 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 3055.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.847701E+00 | loss scale: 8192.0 | grad norm: 1.060 | num zeros: 1635438592.0 | params norm: 310.416 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        28928 | elapsed time per iteration (ms): 2711.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.821646E+00 | loss scale: 8192.0 | grad norm: 0.786 | num zeros: 1649332736.0 | params norm: 310.442 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 2793.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.844419E+00 | loss scale: 8192.0 | grad norm: 1.933 | num zeros: 1625818880.0 | params norm: 310.466 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        29440 | elapsed time per iteration (ms): 2616.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.814650E+00 | loss scale: 8192.0 | grad norm: 0.739 | num zeros: 1610499712.0 | params norm: 310.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 2719.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.830363E+00 | loss scale: 8192.0 | grad norm: 1.266 | num zeros: 1645952512.0 | params norm: 310.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        29952 | elapsed time per iteration (ms): 2828.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.807537E+00 | loss scale: 8192.0 | grad norm: 0.595 | num zeros: 1674385024.0 | params norm: 310.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 3061.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.816683E+00 | loss scale: 8192.0 | grad norm: 0.853 | num zeros: 1604722688.0 | params norm: 310.559 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        30464 | elapsed time per iteration (ms): 2630.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.791878E+00 | loss scale: 8192.0 | grad norm: 0.562 | num zeros: 1630648448.0 | params norm: 310.584 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 2713.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.802587E+00 | loss scale: 8192.0 | grad norm: 0.861 | num zeros: 1577235328.0 | params norm: 310.612 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        30976 | elapsed time per iteration (ms): 2695.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.783546E+00 | loss scale: 8192.0 | grad norm: 0.653 | num zeros: 1641747840.0 | params norm: 310.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 2629.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.777867E+00 | loss scale: 8192.0 | grad norm: 0.572 | num zeros: 1583090816.0 | params norm: 310.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        31488 | elapsed time per iteration (ms): 2821.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.778476E+00 | loss scale: 8192.0 | grad norm: 1.227 | num zeros: 1519085312.0 | params norm: 310.690 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 2714.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.763552E+00 | loss scale: 8192.0 | grad norm: 0.798 | num zeros: 1474348672.0 | params norm: 310.716 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        32000 | elapsed time per iteration (ms): 2753.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.761572E+00 | loss scale: 8192.0 | grad norm: 0.597 | num zeros: 1457382144.0 | params norm: 310.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 2599.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.775581E+00 | loss scale: 8192.0 | grad norm: 0.826 | num zeros: 1574401664.0 | params norm: 310.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        32512 | elapsed time per iteration (ms): 2833.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.759755E+00 | loss scale: 8192.0 | grad norm: 0.536 | num zeros: 1577323392.0 | params norm: 310.794 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 2730.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.750102E+00 | loss scale: 8192.0 | grad norm: 0.616 | num zeros: 1572221440.0 | params norm: 310.818 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        33024 | elapsed time per iteration (ms): 2645.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.748717E+00 | loss scale: 8192.0 | grad norm: 0.492 | num zeros: 1593132288.0 | params norm: 310.843 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 2623.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.750986E+00 | loss scale: 8192.0 | grad norm: 0.736 | num zeros: 1542704256.0 | params norm: 310.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        33536 | elapsed time per iteration (ms): 3038.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.735358E+00 | loss scale: 8192.0 | grad norm: 0.669 | num zeros: 1533046272.0 | params norm: 310.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 2686.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.738618E+00 | loss scale: 8192.0 | grad norm: 0.881 | num zeros: 1488100096.0 | params norm: 310.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        34048 | elapsed time per iteration (ms): 2622.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.749801E+00 | loss scale: 8192.0 | grad norm: 1.003 | num zeros: 1519413888.0 | params norm: 310.943 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 2880.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.750482E+00 | loss scale: 8192.0 | grad norm: 1.960 | num zeros: 1482777600.0 | params norm: 310.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        34560 | elapsed time per iteration (ms): 2692.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.721211E+00 | loss scale: 8192.0 | grad norm: 0.763 | num zeros: 1520507008.0 | params norm: 310.991 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 2747.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.742284E+00 | loss scale: 8192.0 | grad norm: 1.038 | num zeros: 1508333056.0 | params norm: 311.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        35072 | elapsed time per iteration (ms): 2713.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.729247E+00 | loss scale: 8192.0 | grad norm: 0.868 | num zeros: 1501532288.0 | params norm: 311.035 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 2815.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.720448E+00 | loss scale: 8192.0 | grad norm: 0.929 | num zeros: 1546312704.0 | params norm: 311.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        35584 | elapsed time per iteration (ms): 2771.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.725226E+00 | loss scale: 8192.0 | grad norm: 0.659 | num zeros: 1562949888.0 | params norm: 311.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 2633.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.726072E+00 | loss scale: 8192.0 | grad norm: 0.915 | num zeros: 1487381632.0 | params norm: 311.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        36096 | elapsed time per iteration (ms): 2711.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.704217E+00 | loss scale: 8192.0 | grad norm: 1.152 | num zeros: 1529123072.0 | params norm: 311.118 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 2697.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.718016E+00 | loss scale: 8192.0 | grad norm: 1.356 | num zeros: 1545041152.0 | params norm: 311.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        36608 | elapsed time per iteration (ms): 2709.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.714571E+00 | loss scale: 8192.0 | grad norm: 0.714 | num zeros: 1563100672.0 | params norm: 311.155 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 2617.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.712602E+00 | loss scale: 8192.0 | grad norm: 1.390 | num zeros: 1533719168.0 | params norm: 311.175 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        37120 | elapsed time per iteration (ms): 2930.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.725967E+00 | loss scale: 8192.0 | grad norm: 2.554 | num zeros: 1519588608.0 | params norm: 311.193 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 2841.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.725006E+00 | loss scale: 8192.0 | grad norm: 1.489 | num zeros: 1519663104.0 | params norm: 311.209 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        37632 | elapsed time per iteration (ms): 2918.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.716696E+00 | loss scale: 8192.0 | grad norm: 1.225 | num zeros: 1515672832.0 | params norm: 311.225 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 2640.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.711922E+00 | loss scale: 8192.0 | grad norm: 1.096 | num zeros: 1622687488.0 | params norm: 311.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        38144 | elapsed time per iteration (ms): 2738.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.691028E+00 | loss scale: 8192.0 | grad norm: 0.728 | num zeros: 1448732416.0 | params norm: 311.258 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 2738.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.722323E+00 | loss scale: 8192.0 | grad norm: 1.180 | num zeros: 1413109632.0 | params norm: 311.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        38656 | elapsed time per iteration (ms): 2659.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.688735E+00 | loss scale: 8192.0 | grad norm: 0.761 | num zeros: 1419818112.0 | params norm: 311.289 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 2700.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.701115E+00 | loss scale: 8192.0 | grad norm: 0.904 | num zeros: 1411398144.0 | params norm: 311.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        39168 | elapsed time per iteration (ms): 2908.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.677907E+00 | loss scale: 8192.0 | grad norm: 0.802 | num zeros: 1527035648.0 | params norm: 311.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 2928.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.681390E+00 | loss scale: 8192.0 | grad norm: 0.726 | num zeros: 1490865024.0 | params norm: 311.336 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        39680 | elapsed time per iteration (ms): 2663.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.690849E+00 | loss scale: 8192.0 | grad norm: 0.941 | num zeros: 1468307072.0 | params norm: 311.350 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 2766.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.681081E+00 | loss scale: 8192.0 | grad norm: 1.370 | num zeros: 1481581312.0 | params norm: 311.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        40192 | elapsed time per iteration (ms): 2727.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.698020E+00 | loss scale: 8192.0 | grad norm: 1.532 | num zeros: 1524542464.0 | params norm: 311.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 2792.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.678505E+00 | loss scale: 8192.0 | grad norm: 0.757 | num zeros: 1493133568.0 | params norm: 311.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        40704 | elapsed time per iteration (ms): 2671.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.705128E+00 | loss scale: 8192.0 | grad norm: 2.778 | num zeros: 1398942208.0 | params norm: 311.409 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 2957.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.687552E+00 | loss scale: 8192.0 | grad norm: 1.219 | num zeros: 1358880512.0 | params norm: 311.425 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        41216 | elapsed time per iteration (ms): 2948.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.690576E+00 | loss scale: 8192.0 | grad norm: 1.334 | num zeros: 1436074880.0 | params norm: 311.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 2662.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.693518E+00 | loss scale: 8192.0 | grad norm: 1.544 | num zeros: 1357644800.0 | params norm: 311.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        41728 | elapsed time per iteration (ms): 2775.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.692690E+00 | loss scale: 8192.0 | grad norm: 1.394 | num zeros: 1404522624.0 | params norm: 311.468 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 2780.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.682374E+00 | loss scale: 8192.0 | grad norm: 1.240 | num zeros: 1415610496.0 | params norm: 311.482 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        42240 | elapsed time per iteration (ms): 2733.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.694118E+00 | loss scale: 8192.0 | grad norm: 1.907 | num zeros: 1331598720.0 | params norm: 311.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 2646.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.658629E+00 | loss scale: 8192.0 | grad norm: 1.096 | num zeros: 1333540096.0 | params norm: 311.513 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        42752 | elapsed time per iteration (ms): 2853.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.675023E+00 | loss scale: 8192.0 | grad norm: 1.913 | num zeros: 1279694336.0 | params norm: 311.527 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 2877.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.649776E+00 | loss scale: 8192.0 | grad norm: 1.094 | num zeros: 1411215616.0 | params norm: 311.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        43264 | elapsed time per iteration (ms): 2634.6 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.690455E+00 | loss scale: 8192.0 | grad norm: 3.006 | num zeros: 1410345984.0 | params norm: 311.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 2666.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.673053E+00 | loss scale: 8192.0 | grad norm: 1.345 | num zeros: 1472817152.0 | params norm: 311.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        43776 | elapsed time per iteration (ms): 2710.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.690258E+00 | loss scale: 8192.0 | grad norm: 2.022 | num zeros: 1381041536.0 | params norm: 311.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 2767.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.678742E+00 | loss scale: 8192.0 | grad norm: 1.707 | num zeros: 1395046528.0 | params norm: 311.588 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        44288 | elapsed time per iteration (ms): 2601.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.646505E+00 | loss scale: 8192.0 | grad norm: 1.541 | num zeros: 1316458240.0 | params norm: 311.601 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 2935.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.641848E+00 | loss scale: 8192.0 | grad norm: 0.874 | num zeros: 1382499840.0 | params norm: 311.614 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        44800 | elapsed time per iteration (ms): 2722.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.651706E+00 | loss scale: 8192.0 | grad norm: 1.044 | num zeros: 1311035904.0 | params norm: 311.626 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 2808.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.627540E+00 | loss scale: 8192.0 | grad norm: 0.596 | num zeros: 1331709440.0 | params norm: 311.637 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        45312 | elapsed time per iteration (ms): 2626.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.648020E+00 | loss scale: 8192.0 | grad norm: 1.899 | num zeros: 1285568384.0 | params norm: 311.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 2962.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.626340E+00 | loss scale: 8192.0 | grad norm: 0.987 | num zeros: 1269068032.0 | params norm: 311.663 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        45824 | elapsed time per iteration (ms): 2810.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.631146E+00 | loss scale: 8192.0 | grad norm: 1.223 | num zeros: 1320010240.0 | params norm: 311.676 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 2613.8 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.611443E+00 | loss scale: 8192.0 | grad norm: 1.409 | num zeros: 1279548928.0 | params norm: 311.688 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        46336 | elapsed time per iteration (ms): 2781.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.622624E+00 | loss scale: 8192.0 | grad norm: 1.115 | num zeros: 1246516224.0 | params norm: 311.702 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 2916.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.630198E+00 | loss scale: 8192.0 | grad norm: 0.767 | num zeros: 1276243456.0 | params norm: 311.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        46848 | elapsed time per iteration (ms): 2773.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.619570E+00 | loss scale: 8192.0 | grad norm: 1.418 | num zeros: 1269180928.0 | params norm: 311.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 2650.0 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.611258E+00 | loss scale: 8192.0 | grad norm: 1.460 | num zeros: 1272273280.0 | params norm: 311.741 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        47360 | elapsed time per iteration (ms): 2636.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.602319E+00 | loss scale: 8192.0 | grad norm: 0.786 | num zeros: 1287276800.0 | params norm: 311.755 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 2884.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.607348E+00 | loss scale: 8192.0 | grad norm: 0.746 | num zeros: 1282588032.0 | params norm: 311.769 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        47872 | elapsed time per iteration (ms): 2671.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.606652E+00 | loss scale: 8192.0 | grad norm: 0.577 | num zeros: 1302165504.0 | params norm: 311.784 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 2623.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.603196E+00 | loss scale: 8192.0 | grad norm: 0.702 | num zeros: 1295056128.0 | params norm: 311.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        48384 | elapsed time per iteration (ms): 2857.7 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.594551E+00 | loss scale: 8192.0 | grad norm: 1.000 | num zeros: 1203088512.0 | params norm: 311.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 2693.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.584411E+00 | loss scale: 8192.0 | grad norm: 0.534 | num zeros: 1193829120.0 | params norm: 311.829 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        48896 | elapsed time per iteration (ms): 2723.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.588467E+00 | loss scale: 8192.0 | grad norm: 0.823 | num zeros: 1255121792.0 | params norm: 311.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 2637.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.592744E+00 | loss scale: 8192.0 | grad norm: 0.857 | num zeros: 1207860864.0 | params norm: 311.862 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        49408 | elapsed time per iteration (ms): 2637.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.566130E+00 | loss scale: 8192.0 | grad norm: 0.571 | num zeros: 1236023168.0 | params norm: 311.879 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 2791.9 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.572151E+00 | loss scale: 8192.0 | grad norm: 0.590 | num zeros: 1160970240.0 | params norm: 311.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        49920 | elapsed time per iteration (ms): 2745.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.570444E+00 | loss scale: 8192.0 | grad norm: 0.806 | num zeros: 1210606080.0 | params norm: 311.913 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 2946.1 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.564779E+00 | loss scale: 8192.0 | grad norm: 0.754 | num zeros: 1259480320.0 | params norm: 311.931 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:        50432 | elapsed time per iteration (ms): 3010.3 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.563715E+00 | loss scale: 8192.0 | grad norm: 0.510 | num zeros: 1263387136.0 | params norm: 311.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 2669.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.560007E+00 | loss scale: 8192.0 | grad norm: 0.648 | num zeros: 1200024576.0 | params norm: 311.970 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:        50944 | elapsed time per iteration (ms): 2722.5 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.558920E+00 | loss scale: 8192.0 | grad norm: 0.924 | num zeros: 1213068032.0 | params norm: 311.989 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 2691.4 | learning rate: 9.998E-05 | global batch size:   256 | lm loss: 6.558771E+00 | loss scale: 8192.0 | grad norm: 0.534 | num zeros: 1228847360.0 | params norm: 312.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2022-12-23 00:16:21 
