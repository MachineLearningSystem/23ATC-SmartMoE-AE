--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,155] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,155] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,156] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,156] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,156] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,156] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,156] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[2022-12-23 16:12:46,162] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /mnt/cache/zhaishuming/Megatron-DeepSpeed/examples/MoE/ds_config_gpt_gpt-L16_H2560_MoE16B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-2-gpus-32-mp-1-pp-1-ep-16-mlc-0.01-cap-1.2-drop-false.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2560
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 16
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. False
  moe_train_capacity_factor ....................... 1.2
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [16]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /mnt/cache/zhaishuming/Auto-Megatron/deepspeed/output/tensorboard/gpt-L16_H2560_MoE16B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-2-gpus-32-mp-1-pp-1-ep-16-mlc-0.01-cap-1.2-drop-false_SH-IDC1-10-140-1-46_2022.12.23-16.12.18
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... 200
  train_samples ................................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 16
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,271] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,271] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,271] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,271] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,271] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,271] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 16:12:46,271] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-23 16:12:46,271] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-23 16:12:47,826] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,826] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,826] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,824] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,826] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,826] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,826] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,826] [INFO] [comm.py:654:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2022-12-23 16:12:47,826] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,826] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,824] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=4, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,824] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=7, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,824] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=5, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,824] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,824] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,824] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 16:12:47,825] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=6, world_size=16, master_addr=10.140.1.46, master_port=29500
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.232 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.506 seconds
time to initialize megatron (seconds): 9.268
[after megatron is initialized] datetime: 2022-12-23 16:12:57 
building GPT model ...
[2022-12-23 16:12:57,399] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-12-23 16:12:57,402] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-12-23 16:12:57,402] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 185.26 GB, percent = 18.4%
[2022-12-23 16:12:57,452] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 16:12:57,467] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 16:12:57,477] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 16:12:57,483] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 16:12:57,496] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 16:12:57,505] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 16:12:57,512] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 16:12:57,520] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 16:12:57,561] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-12-23 16:12:57,562] [INFO] [utils.py:828:see_memory_usage] MA 2.59 GB         Max_MA 2.69 GB         CA 2.7 GB         Max_CA 3 GB 
[2022-12-23 16:12:57,563] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 185.3 GB, percent = 18.4%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1390556160
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-12-23 16:12:57,567] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_16
[2022-12-23 16:12:57,629] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 16
[2022-12-23 16:12:57,884] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [0]
[2022-12-23 16:12:57,894] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [1]
[2022-12-23 16:12:57,905] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [2]
[2022-12-23 16:12:57,915] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [3]
[2022-12-23 16:12:57,925] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [4]
[2022-12-23 16:12:57,935] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [5]
[2022-12-23 16:12:57,946] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [6]
[2022-12-23 16:12:57,956] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [7]
[2022-12-23 16:12:57,966] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [8]
[2022-12-23 16:12:57,976] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [9]
[2022-12-23 16:12:57,986] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [10]
[2022-12-23 16:12:57,986] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [11]
[2022-12-23 16:12:57,997] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [12]
[2022-12-23 16:12:58,007] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [13]
[2022-12-23 16:12:58,017] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [14]
[2022-12-23 16:12:58,027] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [15]
[2022-12-23 16:12:58,037] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_16 with ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[2022-12-23 16:13:00,535] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-12-23 16:13:00,535] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-12-23 16:13:00,535] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-12-23 16:13:00,541] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2022-12-23 16:13:00,541] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2022-12-23 16:13:00,901] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2022-12-23 16:13:00,902] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-12-23 16:13:00,902] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f5edef901f0>
[2022-12-23 16:13:00,902] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:13:00,902] [INFO] [config.py:1020:print] DeepSpeedEngine configuration:
[2022-12-23 16:13:00,902] [INFO] [config.py:1024:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   amp_enabled .................. False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   amp_params ................... False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   bfloat16_enabled ............. False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   checkpoint_parallel_write_pipeline  False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   checkpoint_tag_validation_enabled  True
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   checkpoint_tag_validation_fail  False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5edef939a0>
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   communication_data_type ...... None
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   curriculum_enabled ........... False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 424592, 'difficulty_step': 8}}
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   dataloader_drop_last ......... False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   disable_allgather ............ False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   dump_state ................... False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   eigenvalue_enabled ........... False
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   eigenvalue_gas_boundary_resolution  1
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   eigenvalue_layer_num ......... 0
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   eigenvalue_max_iter .......... 100
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   eigenvalue_stability ......... 1e-06
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   eigenvalue_tol ............... 0.01
[2022-12-23 16:13:00,903] [INFO] [config.py:1024:print]   eigenvalue_verbose ........... False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   elasticity_enabled ........... False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   fp16_auto_cast ............... False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   fp16_enabled ................. True
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   fp16_master_weights_and_gradients  False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   global_rank .................. 0
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   grad_accum_dtype ............. None
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   gradient_accumulation_steps .. 8
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   gradient_clipping ............ 1.0
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   gradient_predivide_factor .... 1.0
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   initial_dynamic_scale ........ 2048
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   load_universal_checkpoint .... False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   loss_scale ................... 0
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   memory_breakdown ............. False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f5ed3c066b0>
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   optimizer_legacy_fusion ...... False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   optimizer_name ............... None
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   optimizer_params ............. None
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   pld_enabled .................. False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   pld_params ................... False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   prescale_gradients ........... True
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   scheduler_name ............... None
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   scheduler_params ............. None
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   sparse_attention ............. None
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   sparse_gradients_enabled ..... False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   steps_per_print .............. 1
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   train_batch_size ............. 256
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   train_micro_batch_size_per_gpu  2
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   use_node_local_storage ....... False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   wall_clock_breakdown ......... False
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   world_size ................... 16
[2022-12-23 16:13:00,904] [INFO] [config.py:1024:print]   zero_allow_untested_optimizer  False
[2022-12-23 16:13:00,909] [INFO] [config.py:1024:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-12-23 16:13:00,909] [INFO] [config.py:1024:print]   zero_enabled ................. False
[2022-12-23 16:13:00,910] [INFO] [config.py:1024:print]   zero_optimization_stage ...... 0
[2022-12-23 16:13:00,910] [INFO] [config.py:1009:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 4.245920e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3619194030761719 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-23 16:13:01 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: 25600000
    test:       25600000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.035789 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.031 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-23 16:13:03 
done with setup ...
training ...
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3481869697570801 seconds
time (ms) | model-and-optimizer-setup: 4040.58 | train/valid/test-data-iterators-setup: 2044.18
[before the start of training step] datetime: 2022-12-23 16:13:03 
[2022-12-23 16:13:18,315] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18567.935546875 | max allocated: 33792.20654296875 | reserved: 34582.0 | max reserved: 34582.0
 iteration        1/     200 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 14934.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.115570E+01 | moe loss: 1.254645E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 9790.57 | backward-compute: 4746.38 | backward-embedding-all-reduce: 0.01 | optimizer: 387.04 | batch-generator: 1196.21
[2022-12-23 16:13:30,626] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        2/     200 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 12306.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.062182E+01 | moe loss: 1.665016E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6356.01 | backward-compute: 5837.04 | backward-embedding-all-reduce: 0.01 | optimizer: 90.28 | batch-generator: 916.40
[2022-12-23 16:13:46,560] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:13:46,571] [INFO] [timer.py:197:stop] 0/3, RunningAvgSamplesPerSec=14.158982899009573, CurrSamplesPerSec=14.158982899009573, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration        3/     200 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 15934.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.247356E+01 | moe loss: 1.894682E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 8180.94 | backward-compute: 7551.92 | backward-embedding-all-reduce: 0.01 | optimizer: 166.54 | batch-generator: 1409.76
[2022-12-23 16:14:00,373] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:14:00,373] [INFO] [timer.py:197:stop] 0/4, RunningAvgSamplesPerSec=15.628568020150505, CurrSamplesPerSec=17.438544461050594, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration        4/     200 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 13828.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.056359E+01 | moe loss: 1.681646E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 7000.68 | backward-compute: 6697.76 | backward-embedding-all-reduce: 0.01 | optimizer: 112.66 | batch-generator: 1030.76
[2022-12-23 16:14:13,425] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:14:13,425] [INFO] [timer.py:197:stop] 0/5, RunningAvgSamplesPerSec=16.518720056082387, CurrSamplesPerSec=18.642332422776143, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration        5/     200 | consumed samples:         1280 | consumed tokens:      1310720 | elapsed time per iteration (ms): 13045.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.008811E+01 | moe loss: 1.526474E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6638.04 | backward-compute: 6273.21 | backward-embedding-all-reduce: 0.01 | optimizer: 121.54 | batch-generator: 1029.32
[2022-12-23 16:14:28,268] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:14:28,280] [INFO] [timer.py:197:stop] 0/6, RunningAvgSamplesPerSec=15.342391693249393, CurrSamplesPerSec=12.641680009946237, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration        6/     200 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 14845.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.641647E+00 | moe loss: 1.461166E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 7769.06 | backward-compute: 6930.78 | backward-embedding-all-reduce: 0.01 | optimizer: 129.83 | batch-generator: 977.04
[2022-12-23 16:14:39,967] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:14:39,968] [INFO] [timer.py:197:stop] 0/7, RunningAvgSamplesPerSec=16.12104977748443, CurrSamplesPerSec=20.2273738781846, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration        7/     200 | consumed samples:         1792 | consumed tokens:      1835008 | elapsed time per iteration (ms): 11682.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.182346E+00 | moe loss: 1.353672E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5881.37 | backward-compute: 5695.86 | backward-embedding-all-reduce: 0.01 | optimizer: 90.18 | batch-generator: 1024.87
[2022-12-23 16:14:51,834] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:14:51,837] [INFO] [timer.py:197:stop] 0/8, RunningAvgSamplesPerSec=16.26374254172032, CurrSamplesPerSec=17.01685155785054, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration        8/     200 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 11870.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.777250E+00 | moe loss: 1.269058E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6197.43 | backward-compute: 5532.59 | backward-embedding-all-reduce: 0.01 | optimizer: 122.11 | batch-generator: 930.75
 iteration        9/     200 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 13463.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.456534E+00 | moe loss: 1.340747E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6939.43 | backward-compute: 6416.25 | backward-embedding-all-reduce: 0.01 | optimizer: 95.78 | batch-generator: 878.83
[2022-12-23 16:15:05,317] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:15:05,320] [INFO] [timer.py:197:stop] 0/9, RunningAvgSamplesPerSec=15.854758409885637, CurrSamplesPerSec=13.776181603159195, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:15:17,979] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:15:17,979] [INFO] [timer.py:197:stop] 0/10, RunningAvgSamplesPerSec=15.957619450137338, CurrSamplesPerSec=16.716795233228897, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       10/     200 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 12704.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.109480E+00 | moe loss: 1.351020E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6411.63 | backward-compute: 6139.92 | backward-embedding-all-reduce: 0.01 | optimizer: 121.50 | batch-generator: 989.13
 iteration       11/     200 | consumed samples:         2816 | consumed tokens:      2883584 | elapsed time per iteration (ms): 12279.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.157989E+00 | moe loss: 1.343968E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6387.85 | backward-compute: 5769.47 | backward-embedding-all-reduce: 0.01 | optimizer: 93.40 | batch-generator: 945.28
[2022-12-23 16:15:30,314] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:15:30,317] [INFO] [timer.py:197:stop] 0/11, RunningAvgSamplesPerSec=16.31716398626211, CurrSamplesPerSec=19.90504205830695, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:15:42,114] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:15:42,115] [INFO] [timer.py:197:stop] 0/12, RunningAvgSamplesPerSec=16.61996207384777, CurrSamplesPerSec=19.95225292428392, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       12/     200 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 11844.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.283897E+00 | moe loss: 1.389074E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6063.67 | backward-compute: 5622.22 | backward-embedding-all-reduce: 0.01 | optimizer: 113.01 | batch-generator: 929.73
 iteration       13/     200 | consumed samples:         3328 | consumed tokens:      3407872 | elapsed time per iteration (ms): 12061.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.937675E+00 | moe loss: 1.483392E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6245.87 | backward-compute: 5691.39 | backward-embedding-all-reduce: 0.01 | optimizer: 103.21 | batch-generator: 888.77
[2022-12-23 16:15:54,210] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:15:54,223] [INFO] [timer.py:197:stop] 0/13, RunningAvgSamplesPerSec=16.85257054563646, CurrSamplesPerSec=19.595038934521067, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:16:04,970] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:16:04,971] [INFO] [timer.py:197:stop] 0/14, RunningAvgSamplesPerSec=17.145827903003585, CurrSamplesPerSec=21.20472559622119, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       14/     200 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 10788.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.732584E+00 | moe loss: 1.551558E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5328.00 | backward-compute: 5317.03 | backward-embedding-all-reduce: 0.01 | optimizer: 103.04 | batch-generator: 889.11
 iteration       15/     200 | consumed samples:         3840 | consumed tokens:      3932160 | elapsed time per iteration (ms): 11985.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.638124E+00 | moe loss: 1.480878E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6125.13 | backward-compute: 5727.75 | backward-embedding-all-reduce: 0.01 | optimizer: 119.52 | batch-generator: 834.78
[2022-12-23 16:16:16,969] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:16:16,983] [INFO] [timer.py:197:stop] 0/15, RunningAvgSamplesPerSec=17.256816276438293, CurrSamplesPerSec=18.710194231330053, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:16:28,056] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:16:28,057] [INFO] [timer.py:197:stop] 0/16, RunningAvgSamplesPerSec=17.209171899859328, CurrSamplesPerSec=16.61290605268591, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       16/     200 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 11094.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.577732E+00 | moe loss: 1.381359E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5697.64 | backward-compute: 5266.36 | backward-embedding-all-reduce: 0.01 | optimizer: 97.64 | batch-generator: 858.73
 iteration       17/     200 | consumed samples:         4352 | consumed tokens:      4456448 | elapsed time per iteration (ms): 11737.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.528873E+00 | moe loss: 1.372131E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6041.07 | backward-compute: 5574.46 | backward-embedding-all-reduce: 0.01 | optimizer: 106.54 | batch-generator: 876.73
[2022-12-23 16:16:39,813] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:16:39,814] [INFO] [timer.py:197:stop] 0/17, RunningAvgSamplesPerSec=17.37169659297582, CurrSamplesPerSec=20.018480792030534, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       18/     200 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 12814.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.484308E+00 | moe loss: 1.381273E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6362.91 | backward-compute: 6335.57 | backward-embedding-all-reduce: 0.01 | optimizer: 90.31 | batch-generator: 815.03
[2022-12-23 16:16:52,616] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:16:52,617] [INFO] [timer.py:197:stop] 0/18, RunningAvgSamplesPerSec=17.427062869311996, CurrSamplesPerSec=18.302033943743268, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       19/     200 | consumed samples:         4864 | consumed tokens:      4980736 | elapsed time per iteration (ms): 13206.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.433766E+00 | moe loss: 1.399899E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 6637.07 | backward-compute: 6440.34 | backward-embedding-all-reduce: 0.01 | optimizer: 114.80 | batch-generator: 861.72
[2022-12-23 16:17:05,831] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:17:05,843] [INFO] [timer.py:197:stop] 0/19, RunningAvgSamplesPerSec=17.44403168339437, CurrSamplesPerSec=17.720097983687516, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:17:16,893] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:17:16,893] [INFO] [timer.py:197:stop] 0/20, RunningAvgSamplesPerSec=17.57015067730508, CurrSamplesPerSec=20.032293403589335, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       20/     200 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 11089.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.402786E+00 | moe loss: 1.312804E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5604.53 | backward-compute: 5315.12 | backward-embedding-all-reduce: 0.01 | optimizer: 133.23 | batch-generator: 792.80
 iteration       21/     200 | consumed samples:         5376 | consumed tokens:      5505024 | elapsed time per iteration (ms): 11219.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.407339E+00 | moe loss: 1.252280E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5778.81 | backward-compute: 5324.96 | backward-embedding-all-reduce: 0.01 | optimizer: 99.80 | batch-generator: 840.94
[2022-12-23 16:17:28,167] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:17:28,182] [INFO] [timer.py:197:stop] 0/21, RunningAvgSamplesPerSec=17.67844534914869, CurrSamplesPerSec=19.884514379701393, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:17:38,843] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:17:38,844] [INFO] [timer.py:197:stop] 0/22, RunningAvgSamplesPerSec=17.7444193851214, CurrSamplesPerSec=19.098625251365615, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       22/     200 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 10729.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.387362E+00 | moe loss: 1.263447E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5291.07 | backward-compute: 5263.34 | backward-embedding-all-reduce: 0.01 | optimizer: 106.80 | batch-generator: 839.65
 iteration       23/     200 | consumed samples:         5888 | consumed tokens:      6029312 | elapsed time per iteration (ms): 11744.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.368657E+00 | moe loss: 1.279435E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5832.17 | backward-compute: 5800.70 | backward-embedding-all-reduce: 0.01 | optimizer: 103.40 | batch-generator: 791.28
[2022-12-23 16:17:50,616] [INFO] [logging.py:68:log_dist] [Rank 0] step=23, skipped=0, lr=[9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:17:50,629] [INFO] [timer.py:197:stop] 0/23, RunningAvgSamplesPerSec=17.84670870112535, CurrSamplesPerSec=20.172424090989363, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:18:00,298] [INFO] [logging.py:68:log_dist] [Rank 0] step=24, skipped=0, lr=[9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:18:00,298] [INFO] [timer.py:197:stop] 0/24, RunningAvgSamplesPerSec=18.069007429802955, CurrSamplesPerSec=24.469696315025963, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       24/     200 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 9713.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.371990E+00 | moe loss: 1.185154E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5124.42 | backward-compute: 4425.31 | backward-embedding-all-reduce: 0.01 | optimizer: 118.08 | batch-generator: 849.54
 iteration       25/     200 | consumed samples:         6400 | consumed tokens:      6553600 | elapsed time per iteration (ms): 10374.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.369956E+00 | moe loss: 1.182972E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5097.97 | backward-compute: 5176.78 | backward-embedding-all-reduce: 0.01 | optimizer: 93.95 | batch-generator: 744.49
[2022-12-23 16:18:10,705] [INFO] [logging.py:68:log_dist] [Rank 0] step=25, skipped=0, lr=[9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:18:10,708] [INFO] [timer.py:197:stop] 0/25, RunningAvgSamplesPerSec=18.21765477524578, CurrSamplesPerSec=22.243404839804004, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:18:21,244] [INFO] [logging.py:68:log_dist] [Rank 0] step=26, skipped=0, lr=[9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:18:21,244] [INFO] [timer.py:197:stop] 0/26, RunningAvgSamplesPerSec=18.3492689766466, CurrSamplesPerSec=22.005860786882696, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       26/     200 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 10569.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.365618E+00 | moe loss: 1.181024E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5237.57 | backward-compute: 5193.23 | backward-embedding-all-reduce: 0.01 | optimizer: 115.71 | batch-generator: 799.05
[2022-12-23 16:18:32,330] [INFO] [logging.py:68:log_dist] [Rank 0] step=27, skipped=0, lr=[9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:18:32,333] [INFO] [timer.py:197:stop] 0/27, RunningAvgSamplesPerSec=18.45215775475601, CurrSamplesPerSec=21.321468597799793, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       27/     200 | consumed samples:         6912 | consumed tokens:      7077888 | elapsed time per iteration (ms): 11063.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.352714E+00 | moe loss: 1.157434E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5662.61 | backward-compute: 5284.36 | backward-embedding-all-reduce: 0.01 | optimizer: 100.61 | batch-generator: 757.68
[2022-12-23 16:18:40,967] [INFO] [logging.py:68:log_dist] [Rank 0] step=28, skipped=0, lr=[9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:18:40,968] [INFO] [timer.py:197:stop] 0/28, RunningAvgSamplesPerSec=18.668532973801298, CurrSamplesPerSec=26.411147574145602, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       28/     200 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 8655.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.351749E+00 | moe loss: 1.120081E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4474.13 | backward-compute: 4054.41 | backward-embedding-all-reduce: 0.01 | optimizer: 103.34 | batch-generator: 875.49
[2022-12-23 16:18:50,646] [INFO] [logging.py:68:log_dist] [Rank 0] step=29, skipped=0, lr=[9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:18:50,648] [INFO] [timer.py:197:stop] 0/29, RunningAvgSamplesPerSec=18.83079021681918, CurrSamplesPerSec=24.328512622147624, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       29/     200 | consumed samples:         7424 | consumed tokens:      7602176 | elapsed time per iteration (ms): 9655.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.339660E+00 | moe loss: 1.086789E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4712.21 | backward-compute: 4838.45 | backward-embedding-all-reduce: 0.01 | optimizer: 97.65 | batch-generator: 701.74
[2022-12-23 16:19:00,285] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:19:00,286] [INFO] [timer.py:197:stop] 0/30, RunningAvgSamplesPerSec=18.9719764792498, CurrSamplesPerSec=23.787402033926423, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       30/     200 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 9673.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.322412E+00 | moe loss: 1.081714E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4941.10 | backward-compute: 4597.82 | backward-embedding-all-reduce: 0.01 | optimizer: 115.54 | batch-generator: 826.67
 iteration       31/     200 | consumed samples:         7936 | consumed tokens:      8126464 | elapsed time per iteration (ms): 9847.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.316631E+00 | moe loss: 1.081861E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4973.06 | backward-compute: 4757.18 | backward-embedding-all-reduce: 0.01 | optimizer: 102.62 | batch-generator: 768.81
[2022-12-23 16:19:10,179] [INFO] [logging.py:68:log_dist] [Rank 0] step=31, skipped=0, lr=[9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:19:10,180] [INFO] [timer.py:197:stop] 0/31, RunningAvgSamplesPerSec=19.1133744762495, CurrSamplesPerSec=24.15390230155205, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:19:19,219] [INFO] [logging.py:68:log_dist] [Rank 0] step=32, skipped=0, lr=[9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:19:19,228] [INFO] [timer.py:197:stop] 0/32, RunningAvgSamplesPerSec=19.258884938463844, CurrSamplesPerSec=24.71551503957735, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       32/     200 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 9072.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.290015E+00 | moe loss: 1.087616E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4623.41 | backward-compute: 4305.45 | backward-embedding-all-reduce: 0.01 | optimizer: 113.19 | batch-generator: 835.65
 iteration       33/     200 | consumed samples:         8448 | consumed tokens:      8650752 | elapsed time per iteration (ms): 10611.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.285771E+00 | moe loss: 1.093237E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 5313.53 | backward-compute: 5184.28 | backward-embedding-all-reduce: 0.01 | optimizer: 104.79 | batch-generator: 702.41
[2022-12-23 16:19:29,854] [INFO] [logging.py:68:log_dist] [Rank 0] step=33, skipped=0, lr=[9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:19:29,857] [INFO] [timer.py:197:stop] 0/33, RunningAvgSamplesPerSec=19.344078136294314, CurrSamplesPerSec=22.303980718080023, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       34/     200 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 8508.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.272662E+00 | moe loss: 1.088328E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4344.80 | backward-compute: 4030.43 | backward-embedding-all-reduce: 0.01 | optimizer: 110.20 | batch-generator: 751.13
[2022-12-23 16:19:38,354] [INFO] [logging.py:68:log_dist] [Rank 0] step=34, skipped=0, lr=[9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:19:38,358] [INFO] [timer.py:197:stop] 0/34, RunningAvgSamplesPerSec=19.481268853232205, CurrSamplesPerSec=24.971376572761017, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       35/     200 | consumed samples:         8960 | consumed tokens:      9175040 | elapsed time per iteration (ms): 8244.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.258925E+00 | moe loss: 1.071178E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4327.98 | backward-compute: 3806.82 | backward-embedding-all-reduce: 0.01 | optimizer: 95.19 | batch-generator: 698.54
[2022-12-23 16:19:46,603] [INFO] [logging.py:68:log_dist] [Rank 0] step=35, skipped=0, lr=[9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:19:46,603] [INFO] [timer.py:197:stop] 0/35, RunningAvgSamplesPerSec=19.651088377692464, CurrSamplesPerSec=27.253287881990577, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:19:55,138] [INFO] [logging.py:68:log_dist] [Rank 0] step=36, skipped=0, lr=[9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:19:55,139] [INFO] [timer.py:197:stop] 0/36, RunningAvgSamplesPerSec=19.76089775028546, CurrSamplesPerSec=24.228736721310113, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       36/     200 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 8546.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.237556E+00 | moe loss: 1.068490E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4360.37 | backward-compute: 4069.67 | backward-embedding-all-reduce: 0.01 | optimizer: 96.80 | batch-generator: 689.03
[2022-12-23 16:20:03,507] [INFO] [logging.py:68:log_dist] [Rank 0] step=37, skipped=0, lr=[9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:20:03,507] [INFO] [timer.py:197:stop] 0/37, RunningAvgSamplesPerSec=19.902558218330213, CurrSamplesPerSec=26.316969360074275, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       37/     200 | consumed samples:         9472 | consumed tokens:      9699328 | elapsed time per iteration (ms): 8377.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.247122E+00 | moe loss: 1.068823E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4098.50 | backward-compute: 4163.42 | backward-embedding-all-reduce: 0.01 | optimizer: 103.81 | batch-generator: 701.57
[2022-12-23 16:20:12,465] [INFO] [logging.py:68:log_dist] [Rank 0] step=38, skipped=0, lr=[9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:20:12,469] [INFO] [timer.py:197:stop] 0/38, RunningAvgSamplesPerSec=20.041455296789945, CurrSamplesPerSec=26.5189798061525, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       38/     200 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 8941.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.235936E+00 | moe loss: 1.079967E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4560.49 | backward-compute: 4266.13 | backward-embedding-all-reduce: 0.01 | optimizer: 99.27 | batch-generator: 728.50
[2022-12-23 16:20:20,342] [INFO] [logging.py:68:log_dist] [Rank 0] step=39, skipped=0, lr=[9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:20:20,342] [INFO] [timer.py:197:stop] 0/39, RunningAvgSamplesPerSec=20.1952862016395, CurrSamplesPerSec=27.906486943797063, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       39/     200 | consumed samples:         9984 | consumed tokens:     10223616 | elapsed time per iteration (ms): 7894.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.237901E+00 | moe loss: 1.045309E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3986.73 | backward-compute: 3789.03 | backward-embedding-all-reduce: 0.01 | optimizer: 104.04 | batch-generator: 698.55
 iteration       40/     200 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 7380.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.205171E+00 | moe loss: 1.015066E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3693.89 | backward-compute: 3580.62 | backward-embedding-all-reduce: 0.01 | optimizer: 98.87 | batch-generator: 667.73
[2022-12-23 16:20:27,752] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:20:27,757] [INFO] [timer.py:197:stop] 0/40, RunningAvgSamplesPerSec=20.3284815637959, CurrSamplesPerSec=26.890544894366375, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       41/     200 | consumed samples:        10496 | consumed tokens:     10747904 | elapsed time per iteration (ms): 7632.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.193950E+00 | moe loss: 1.018830E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3833.90 | backward-compute: 3674.46 | backward-embedding-all-reduce: 0.01 | optimizer: 96.90 | batch-generator: 670.00
[2022-12-23 16:20:35,374] [INFO] [logging.py:68:log_dist] [Rank 0] step=41, skipped=0, lr=[9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:20:35,382] [INFO] [timer.py:197:stop] 0/41, RunningAvgSamplesPerSec=20.46176240903071, CurrSamplesPerSec=27.251163046618714, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:20:43,378] [INFO] [logging.py:68:log_dist] [Rank 0] step=42, skipped=0, lr=[9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:20:43,379] [INFO] [timer.py:197:stop] 0/42, RunningAvgSamplesPerSec=20.590378106231046, CurrSamplesPerSec=27.27710980492736, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       42/     200 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 8022.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.218443E+00 | moe loss: 9.877952E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3923.43 | backward-compute: 3974.60 | backward-embedding-all-reduce: 0.01 | optimizer: 103.52 | batch-generator: 625.26
 iteration       43/     200 | consumed samples:        11008 | consumed tokens:     11272192 | elapsed time per iteration (ms): 7190.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.182798E+00 | moe loss: 9.717938E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3813.35 | backward-compute: 3279.46 | backward-embedding-all-reduce: 0.01 | optimizer: 90.91 | batch-generator: 637.30
[2022-12-23 16:20:50,597] [INFO] [logging.py:68:log_dist] [Rank 0] step=43, skipped=0, lr=[9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:20:50,600] [INFO] [timer.py:197:stop] 0/43, RunningAvgSamplesPerSec=20.758358946432022, CurrSamplesPerSec=30.813784661215966, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:20:57,097] [INFO] [logging.py:68:log_dist] [Rank 0] step=44, skipped=0, lr=[9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:20:57,097] [INFO] [timer.py:197:stop] 0/44, RunningAvgSamplesPerSec=20.89020371628638, CurrSamplesPerSec=28.245565063967124, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       44/     200 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 6536.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.173610E+00 | moe loss: 9.570594E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3149.49 | backward-compute: 3213.41 | backward-embedding-all-reduce: 0.01 | optimizer: 144.58 | batch-generator: 649.79
 iteration       45/     200 | consumed samples:        11520 | consumed tokens:     11796480 | elapsed time per iteration (ms): 8258.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.164909E+00 | moe loss: 9.699444E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4204.08 | backward-compute: 3945.91 | backward-embedding-all-reduce: 0.01 | optimizer: 93.00 | batch-generator: 685.72
[2022-12-23 16:21:05,406] [INFO] [logging.py:68:log_dist] [Rank 0] step=45, skipped=0, lr=[9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:21:05,409] [INFO] [timer.py:197:stop] 0/45, RunningAvgSamplesPerSec=20.93533481045804, CurrSamplesPerSec=23.024499497713638, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:21:11,785] [INFO] [logging.py:68:log_dist] [Rank 0] step=46, skipped=0, lr=[9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:21:11,785] [INFO] [timer.py:197:stop] 0/46, RunningAvgSamplesPerSec=21.08638636748229, CurrSamplesPerSec=30.571118800713567, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       46/     200 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 6408.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.143636E+00 | moe loss: 9.312894E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3152.94 | backward-compute: 3127.48 | backward-embedding-all-reduce: 0.01 | optimizer: 89.82 | batch-generator: 627.69
[2022-12-23 16:21:18,647] [INFO] [logging.py:68:log_dist] [Rank 0] step=47, skipped=0, lr=[9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:21:18,647] [INFO] [timer.py:197:stop] 0/47, RunningAvgSamplesPerSec=21.201960129277595, CurrSamplesPerSec=27.940052979167426, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       47/     200 | consumed samples:        12032 | consumed tokens:     12320768 | elapsed time per iteration (ms): 6874.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.128263E+00 | moe loss: 9.278660E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3381.75 | backward-compute: 3371.17 | backward-embedding-all-reduce: 0.01 | optimizer: 108.17 | batch-generator: 634.55
 iteration       48/     200 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 6896.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.120488E+00 | moe loss: 9.337163E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3681.46 | backward-compute: 3099.30 | backward-embedding-all-reduce: 0.01 | optimizer: 103.98 | batch-generator: 601.10
[2022-12-23 16:21:25,579] [INFO] [logging.py:68:log_dist] [Rank 0] step=48, skipped=0, lr=[9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:21:25,594] [INFO] [timer.py:197:stop] 0/48, RunningAvgSamplesPerSec=21.201225775290894, CurrSamplesPerSec=21.168232414787038, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:21:31,498] [INFO] [logging.py:68:log_dist] [Rank 0] step=49, skipped=0, lr=[9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:21:31,498] [INFO] [timer.py:197:stop] 0/49, RunningAvgSamplesPerSec=21.382353125909376, CurrSamplesPerSec=35.22566987513861, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       49/     200 | consumed samples:        12544 | consumed tokens:     12845056 | elapsed time per iteration (ms): 5943.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.103700E+00 | moe loss: 9.226432E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2940.37 | backward-compute: 2863.31 | backward-embedding-all-reduce: 0.01 | optimizer: 91.87 | batch-generator: 585.55
[2022-12-23 16:21:37,947] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:21:37,948] [INFO] [timer.py:197:stop] 0/50, RunningAvgSamplesPerSec=21.447447446187795, CurrSamplesPerSec=25.028592228809437, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       50/     200 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 6469.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.079974E+00 | moe loss: 9.145926E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3313.45 | backward-compute: 3035.81 | backward-embedding-all-reduce: 0.01 | optimizer: 112.83 | batch-generator: 596.28
 iteration       51/     200 | consumed samples:        13056 | consumed tokens:     13369344 | elapsed time per iteration (ms): 5888.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.071423E+00 | moe loss: 8.990064E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2923.10 | backward-compute: 2774.26 | backward-embedding-all-reduce: 0.01 | optimizer: 95.31 | batch-generator: 622.26
[2022-12-23 16:21:43,858] [INFO] [logging.py:68:log_dist] [Rank 0] step=51, skipped=0, lr=[9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:21:43,869] [INFO] [timer.py:197:stop] 0/51, RunningAvgSamplesPerSec=21.614129363048814, CurrSamplesPerSec=34.47443398247327, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       52/     200 | consumed samples:        13312 | consumed tokens:     13631488 | elapsed time per iteration (ms): 6623.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.051998E+00 | moe loss: 8.925711E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3426.25 | backward-compute: 3078.52 | backward-embedding-all-reduce: 0.01 | optimizer: 93.00 | batch-generator: 603.59
[2022-12-23 16:21:50,508] [INFO] [logging.py:68:log_dist] [Rank 0] step=52, skipped=0, lr=[9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:21:50,510] [INFO] [timer.py:197:stop] 0/52, RunningAvgSamplesPerSec=21.75530764660464, CurrSamplesPerSec=31.99574336414404, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       53/     200 | consumed samples:        13568 | consumed tokens:     13893632 | elapsed time per iteration (ms): 5591.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.059234E+00 | moe loss: 8.752708E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2796.65 | backward-compute: 2661.48 | backward-embedding-all-reduce: 0.01 | optimizer: 96.59 | batch-generator: 570.66
[2022-12-23 16:21:56,077] [INFO] [logging.py:68:log_dist] [Rank 0] step=53, skipped=0, lr=[9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:21:56,087] [INFO] [timer.py:197:stop] 0/53, RunningAvgSamplesPerSec=21.927117905858005, CurrSamplesPerSec=36.23536920233053, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:22:02,374] [INFO] [logging.py:68:log_dist] [Rank 0] step=54, skipped=0, lr=[9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:02,375] [INFO] [timer.py:197:stop] 0/54, RunningAvgSamplesPerSec=22.06445311552814, CurrSamplesPerSec=32.42036754333417, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       54/     200 | consumed samples:        13824 | consumed tokens:     14155776 | elapsed time per iteration (ms): 6316.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.036843E+00 | moe loss: 8.705590E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3238.24 | backward-compute: 2949.43 | backward-embedding-all-reduce: 0.01 | optimizer: 106.29 | batch-generator: 611.11
 iteration       55/     200 | consumed samples:        14080 | consumed tokens:     14417920 | elapsed time per iteration (ms): 5772.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.027024E+00 | moe loss: 8.657911E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3080.10 | backward-compute: 2592.64 | backward-embedding-all-reduce: 0.01 | optimizer: 93.22 | batch-generator: 584.27
[2022-12-23 16:22:08,166] [INFO] [logging.py:68:log_dist] [Rank 0] step=55, skipped=0, lr=[9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:08,168] [INFO] [timer.py:197:stop] 0/55, RunningAvgSamplesPerSec=22.228704002146593, CurrSamplesPerSec=36.26779825986201, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:22:13,504] [INFO] [logging.py:68:log_dist] [Rank 0] step=56, skipped=0, lr=[9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:13,505] [INFO] [timer.py:197:stop] 0/56, RunningAvgSamplesPerSec=22.3930401682614, CurrSamplesPerSec=36.82020970482158, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       56/     200 | consumed samples:        14336 | consumed tokens:     14680064 | elapsed time per iteration (ms): 5344.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.039434E+00 | moe loss: 8.596079E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2666.65 | backward-compute: 2572.62 | backward-embedding-all-reduce: 0.01 | optimizer: 90.56 | batch-generator: 547.77
[2022-12-23 16:22:19,262] [INFO] [logging.py:68:log_dist] [Rank 0] step=57, skipped=0, lr=[9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:19,262] [INFO] [timer.py:197:stop] 0/57, RunningAvgSamplesPerSec=22.566472502434067, CurrSamplesPerSec=38.78905356806377, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       57/     200 | consumed samples:        14592 | consumed tokens:     14942208 | elapsed time per iteration (ms): 5767.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.006043E+00 | moe loss: 8.466219E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2746.29 | backward-compute: 2905.92 | backward-embedding-all-reduce: 0.01 | optimizer: 107.33 | batch-generator: 636.69
 iteration       58/     200 | consumed samples:        14848 | consumed tokens:     15204352 | elapsed time per iteration (ms): 5237.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.998928E+00 | moe loss: 8.522484E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2525.83 | backward-compute: 2554.23 | backward-embedding-all-reduce: 0.01 | optimizer: 144.92 | batch-generator: 535.84
[2022-12-23 16:22:24,534] [INFO] [logging.py:68:log_dist] [Rank 0] step=58, skipped=0, lr=[9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:24,547] [INFO] [timer.py:197:stop] 0/58, RunningAvgSamplesPerSec=22.672266331509423, CurrSamplesPerSec=30.54924892232525, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:22:29,905] [INFO] [logging.py:68:log_dist] [Rank 0] step=59, skipped=0, lr=[9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:29,905] [INFO] [timer.py:197:stop] 0/59, RunningAvgSamplesPerSec=22.819974735795558, CurrSamplesPerSec=35.92771826752764, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       59/     200 | consumed samples:        15104 | consumed tokens:     15466496 | elapsed time per iteration (ms): 5395.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.984929E+00 | moe loss: 8.479229E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2732.21 | backward-compute: 2527.63 | backward-embedding-all-reduce: 0.01 | optimizer: 91.92 | batch-generator: 527.94
 iteration       60/     200 | consumed samples:        15360 | consumed tokens:     15728640 | elapsed time per iteration (ms): 4672.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.970572E+00 | moe loss: 8.389139E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2279.60 | backward-compute: 2254.66 | backward-embedding-all-reduce: 0.01 | optimizer: 127.09 | batch-generator: 542.87
[2022-12-23 16:22:34,593] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:34,593] [INFO] [timer.py:197:stop] 0/60, RunningAvgSamplesPerSec=22.964508546504717, CurrSamplesPerSec=35.93922414197064, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:22:39,858] [INFO] [logging.py:68:log_dist] [Rank 0] step=61, skipped=0, lr=[9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:39,859] [INFO] [timer.py:197:stop] 0/61, RunningAvgSamplesPerSec=23.13993912111025, CurrSamplesPerSec=41.54938554037095, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       61/     200 | consumed samples:        15616 | consumed tokens:     15990784 | elapsed time per iteration (ms): 5293.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.974546E+00 | moe loss: 8.313458E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2712.75 | backward-compute: 2436.40 | backward-embedding-all-reduce: 0.01 | optimizer: 112.02 | batch-generator: 669.71
 iteration       62/     200 | consumed samples:        15872 | consumed tokens:     16252928 | elapsed time per iteration (ms): 5067.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.955052E+00 | moe loss: 8.323567E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2404.63 | backward-compute: 2524.69 | backward-embedding-all-reduce: 0.01 | optimizer: 125.18 | batch-generator: 533.60
[2022-12-23 16:22:44,965] [INFO] [logging.py:68:log_dist] [Rank 0] step=62, skipped=0, lr=[9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:44,980] [INFO] [timer.py:197:stop] 0/62, RunningAvgSamplesPerSec=23.228094944671394, CurrSamplesPerSec=29.96289244362887, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:22:49,658] [INFO] [logging.py:68:log_dist] [Rank 0] step=63, skipped=0, lr=[9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:49,659] [INFO] [timer.py:197:stop] 0/63, RunningAvgSamplesPerSec=23.383173819130047, CurrSamplesPerSec=39.00972789222414, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       63/     200 | consumed samples:        16128 | consumed tokens:     16515072 | elapsed time per iteration (ms): 4719.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.929150E+00 | moe loss: 8.250117E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2344.77 | backward-compute: 2237.45 | backward-embedding-all-reduce: 0.01 | optimizer: 90.20 | batch-generator: 529.84
[2022-12-23 16:22:54,137] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:54,137] [INFO] [timer.py:197:stop] 0/64, RunningAvgSamplesPerSec=23.5524629333683, CurrSamplesPerSec=42.18050305076862, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       64/     200 | consumed samples:        16384 | consumed tokens:     16777216 | elapsed time per iteration (ms): 4477.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.922142E+00 | moe loss: 8.215787E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2235.08 | backward-compute: 2145.36 | backward-embedding-all-reduce: 0.01 | optimizer: 90.05 | batch-generator: 514.39
[2022-12-23 16:22:59,452] [INFO] [logging.py:68:log_dist] [Rank 0] step=65, skipped=0, lr=[9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:22:59,453] [INFO] [timer.py:197:stop] 0/65, RunningAvgSamplesPerSec=23.710118948362915, CurrSamplesPerSec=40.5312969298427, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       65/     200 | consumed samples:        16640 | consumed tokens:     17039360 | elapsed time per iteration (ms): 5328.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.923894E+00 | moe loss: 8.166829E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2734.75 | backward-compute: 2473.25 | backward-embedding-all-reduce: 0.01 | optimizer: 109.41 | batch-generator: 605.00
 iteration       66/     200 | consumed samples:        16896 | consumed tokens:     17301504 | elapsed time per iteration (ms): 4542.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.896719E+00 | moe loss: 8.159474E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2374.88 | backward-compute: 2064.69 | backward-embedding-all-reduce: 0.01 | optimizer: 90.69 | batch-generator: 494.91
[2022-12-23 16:23:04,026] [INFO] [logging.py:68:log_dist] [Rank 0] step=66, skipped=0, lr=[9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:04,026] [INFO] [timer.py:197:stop] 0/66, RunningAvgSamplesPerSec=23.86968216945507, CurrSamplesPerSec=41.43857712176393, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:23:08,566] [INFO] [logging.py:68:log_dist] [Rank 0] step=67, skipped=0, lr=[9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:08,568] [INFO] [timer.py:197:stop] 0/67, RunningAvgSamplesPerSec=24.0304425464503, CurrSamplesPerSec=42.23534820521906, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       67/     200 | consumed samples:        17152 | consumed tokens:     17563648 | elapsed time per iteration (ms): 4557.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.897197E+00 | moe loss: 8.229998E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2278.76 | backward-compute: 2162.01 | backward-embedding-all-reduce: 0.01 | optimizer: 92.39 | batch-generator: 509.88
[2022-12-23 16:23:12,690] [INFO] [logging.py:68:log_dist] [Rank 0] step=68, skipped=0, lr=[9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:12,691] [INFO] [timer.py:197:stop] 0/68, RunningAvgSamplesPerSec=24.200509471840263, CurrSamplesPerSec=44.816955267099814, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       68/     200 | consumed samples:        17408 | consumed tokens:     17825792 | elapsed time per iteration (ms): 4125.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.885501E+00 | moe loss: 8.203968E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2081.60 | backward-compute: 1943.43 | backward-embedding-all-reduce: 0.01 | optimizer: 91.21 | batch-generator: 544.24
 iteration       69/     200 | consumed samples:        17664 | consumed tokens:     18087936 | elapsed time per iteration (ms): 4283.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.868533E+00 | moe loss: 8.150406E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2192.35 | backward-compute: 1984.95 | backward-embedding-all-reduce: 0.01 | optimizer: 90.18 | batch-generator: 500.55
[2022-12-23 16:23:16,987] [INFO] [logging.py:68:log_dist] [Rank 0] step=69, skipped=0, lr=[9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:16,988] [INFO] [timer.py:197:stop] 0/69, RunningAvgSamplesPerSec=24.358359867050403, CurrSamplesPerSec=42.770883822281455, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:23:21,515] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:21,516] [INFO] [timer.py:197:stop] 0/70, RunningAvgSamplesPerSec=24.51545983294562, CurrSamplesPerSec=43.17002659012451, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       70/     200 | consumed samples:        17920 | consumed tokens:     18350080 | elapsed time per iteration (ms): 4555.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.857702E+00 | moe loss: 8.165781E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2458.42 | backward-compute: 1966.45 | backward-embedding-all-reduce: 0.01 | optimizer: 109.98 | batch-generator: 566.69
[2022-12-23 16:23:25,781] [INFO] [logging.py:68:log_dist] [Rank 0] step=71, skipped=0, lr=[9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       71/     200 | consumed samples:        18176 | consumed tokens:     18612224 | elapsed time per iteration (ms): 4250.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.844540E+00 | moe loss: 8.197931E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2243.21 | backward-compute: 1908.65 | backward-embedding-all-reduce: 0.01 | optimizer: 91.33 | batch-generator: 495.60
[2022-12-23 16:23:25,810] [INFO] [timer.py:197:stop] 0/71, RunningAvgSamplesPerSec=24.677887247407728, CurrSamplesPerSec=44.91254520777507, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:23:30,331] [INFO] [logging.py:68:log_dist] [Rank 0] step=72, skipped=0, lr=[9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:30,331] [INFO] [timer.py:197:stop] 0/72, RunningAvgSamplesPerSec=24.830319931722205, CurrSamplesPerSec=43.273901803786835, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       72/     200 | consumed samples:        18432 | consumed tokens:     18874368 | elapsed time per iteration (ms): 4551.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.851592E+00 | moe loss: 8.176044E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2285.91 | backward-compute: 2132.84 | backward-embedding-all-reduce: 0.01 | optimizer: 89.09 | batch-generator: 507.24
 iteration       73/     200 | consumed samples:        18688 | consumed tokens:     19136512 | elapsed time per iteration (ms): 4095.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.852493E+00 | moe loss: 8.126997E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2025.52 | backward-compute: 1959.29 | backward-embedding-all-reduce: 0.01 | optimizer: 99.64 | batch-generator: 494.64
[2022-12-23 16:23:34,439] [INFO] [logging.py:68:log_dist] [Rank 0] step=73, skipped=0, lr=[9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:34,439] [INFO] [timer.py:197:stop] 0/73, RunningAvgSamplesPerSec=24.982402890099166, CurrSamplesPerSec=43.73227781512161, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:23:38,988] [INFO] [logging.py:68:log_dist] [Rank 0] step=74, skipped=0, lr=[9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:38,989] [INFO] [timer.py:197:stop] 0/74, RunningAvgSamplesPerSec=25.127457055955084, CurrSamplesPerSec=42.75151115549206, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       74/     200 | consumed samples:        18944 | consumed tokens:     19398656 | elapsed time per iteration (ms): 4572.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.824956E+00 | moe loss: 8.191701E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2258.39 | backward-compute: 2183.33 | backward-embedding-all-reduce: 0.01 | optimizer: 107.91 | batch-generator: 560.02
[2022-12-23 16:23:42,978] [INFO] [logging.py:68:log_dist] [Rank 0] step=75, skipped=0, lr=[9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:42,979] [INFO] [timer.py:197:stop] 0/75, RunningAvgSamplesPerSec=25.283018540352888, CurrSamplesPerSec=45.61618338138487, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       75/     200 | consumed samples:        19200 | consumed tokens:     19660800 | elapsed time per iteration (ms): 3979.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.818965E+00 | moe loss: 8.148962E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1972.76 | backward-compute: 1901.58 | backward-embedding-all-reduce: 0.01 | optimizer: 92.83 | batch-generator: 455.18
[2022-12-23 16:23:47,455] [INFO] [logging.py:68:log_dist] [Rank 0] step=76, skipped=0, lr=[9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:47,461] [INFO] [timer.py:197:stop] 0/76, RunningAvgSamplesPerSec=25.391093719140674, CurrSamplesPerSec=36.908176501176534, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       76/     200 | consumed samples:        19456 | consumed tokens:     19922944 | elapsed time per iteration (ms): 4488.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.799479E+00 | moe loss: 8.123771E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2278.60 | backward-compute: 2044.37 | backward-embedding-all-reduce: 0.01 | optimizer: 137.10 | batch-generator: 526.44
[2022-12-23 16:23:51,851] [INFO] [logging.py:68:log_dist] [Rank 0] step=77, skipped=0, lr=[9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:51,852] [INFO] [timer.py:197:stop] 0/77, RunningAvgSamplesPerSec=25.53587269421011, CurrSamplesPerSec=44.17550433073406, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       77/     200 | consumed samples:        19712 | consumed tokens:     20185088 | elapsed time per iteration (ms): 4383.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.819546E+00 | moe loss: 8.217070E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2269.98 | backward-compute: 2017.10 | backward-embedding-all-reduce: 0.01 | optimizer: 89.85 | batch-generator: 531.96
 iteration       78/     200 | consumed samples:        19968 | consumed tokens:     20447232 | elapsed time per iteration (ms): 4641.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.805027E+00 | moe loss: 8.151578E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2538.78 | backward-compute: 2000.26 | backward-embedding-all-reduce: 0.01 | optimizer: 94.26 | batch-generator: 479.37
[2022-12-23 16:23:56,505] [INFO] [logging.py:68:log_dist] [Rank 0] step=78, skipped=0, lr=[9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:23:56,506] [INFO] [timer.py:197:stop] 0/78, RunningAvgSamplesPerSec=25.672379420126546, CurrSamplesPerSec=42.853461795852894, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:24:01,011] [INFO] [logging.py:68:log_dist] [Rank 0] step=79, skipped=0, lr=[9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:01,012] [INFO] [timer.py:197:stop] 0/79, RunningAvgSamplesPerSec=25.800558363238277, CurrSamplesPerSec=41.57748297391528, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       79/     200 | consumed samples:        20224 | consumed tokens:     20709376 | elapsed time per iteration (ms): 4552.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.776046E+00 | moe loss: 8.155119E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2453.44 | backward-compute: 1944.73 | backward-embedding-all-reduce: 0.01 | optimizer: 128.21 | batch-generator: 518.30
 iteration       80/     200 | consumed samples:        20480 | consumed tokens:     20971520 | elapsed time per iteration (ms): 3978.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.792711E+00 | moe loss: 8.057787E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1885.15 | backward-compute: 1969.53 | backward-embedding-all-reduce: 0.01 | optimizer: 109.44 | batch-generator: 448.23
[2022-12-23 16:24:05,029] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:05,041] [INFO] [timer.py:197:stop] 0/80, RunningAvgSamplesPerSec=25.902321070750915, CurrSamplesPerSec=37.200148891806485, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:24:09,007] [INFO] [logging.py:68:log_dist] [Rank 0] step=81, skipped=0, lr=[9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:09,007] [INFO] [timer.py:197:stop] 0/81, RunningAvgSamplesPerSec=26.045901152763943, CurrSamplesPerSec=45.884912455834765, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       81/     200 | consumed samples:        20736 | consumed tokens:     21233664 | elapsed time per iteration (ms): 3983.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.776233E+00 | moe loss: 8.072595E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1974.32 | backward-compute: 1893.00 | backward-embedding-all-reduce: 0.01 | optimizer: 89.44 | batch-generator: 455.60
[2022-12-23 16:24:12,805] [INFO] [logging.py:68:log_dist] [Rank 0] step=82, skipped=0, lr=[9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:12,806] [INFO] [timer.py:197:stop] 0/82, RunningAvgSamplesPerSec=26.1919133854942, CurrSamplesPerSec=47.012270263132365, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       82/     200 | consumed samples:        20992 | consumed tokens:     21495808 | elapsed time per iteration (ms): 3798.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.756196E+00 | moe loss: 8.125152E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1892.67 | backward-compute: 1808.54 | backward-embedding-all-reduce: 0.01 | optimizer: 88.95 | batch-generator: 457.34
 iteration       83/     200 | consumed samples:        21248 | consumed tokens:     21757952 | elapsed time per iteration (ms): 4358.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.775056E+00 | moe loss: 8.189920E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2141.65 | backward-compute: 2119.68 | backward-embedding-all-reduce: 0.01 | optimizer: 89.84 | batch-generator: 458.66
[2022-12-23 16:24:17,246] [INFO] [logging.py:68:log_dist] [Rank 0] step=83, skipped=0, lr=[9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:17,249] [INFO] [timer.py:197:stop] 0/83, RunningAvgSamplesPerSec=26.30374393394353, CurrSamplesPerSec=39.94935495671187, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:24:22,036] [INFO] [logging.py:68:log_dist] [Rank 0] step=84, skipped=0, lr=[9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:22,036] [INFO] [timer.py:197:stop] 0/84, RunningAvgSamplesPerSec=26.42541953816425, CurrSamplesPerSec=42.25964397016901, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       84/     200 | consumed samples:        21504 | consumed tokens:     22020096 | elapsed time per iteration (ms): 4885.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.733340E+00 | moe loss: 8.122761E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2579.76 | backward-compute: 1973.64 | backward-embedding-all-reduce: 0.01 | optimizer: 117.62 | batch-generator: 821.10
[2022-12-23 16:24:26,053] [INFO] [logging.py:68:log_dist] [Rank 0] step=85, skipped=0, lr=[9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:26,056] [INFO] [timer.py:197:stop] 0/85, RunningAvgSamplesPerSec=26.557733321815835, CurrSamplesPerSec=45.057347540301194, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       85/     200 | consumed samples:        21760 | consumed tokens:     22282240 | elapsed time per iteration (ms): 4003.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.724251E+00 | moe loss: 8.066205E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2033.34 | backward-compute: 1865.86 | backward-embedding-all-reduce: 0.01 | optimizer: 90.82 | batch-generator: 462.58
[2022-12-23 16:24:30,075] [INFO] [logging.py:68:log_dist] [Rank 0] step=86, skipped=0, lr=[9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:30,075] [INFO] [timer.py:197:stop] 0/86, RunningAvgSamplesPerSec=26.691074489284574, CurrSamplesPerSec=45.76083812417619, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       86/     200 | consumed samples:        22016 | consumed tokens:     22544384 | elapsed time per iteration (ms): 4022.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.735715E+00 | moe loss: 8.110397E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1981.76 | backward-compute: 1941.91 | backward-embedding-all-reduce: 0.01 | optimizer: 88.97 | batch-generator: 442.55
[2022-12-23 16:24:33,897] [INFO] [logging.py:68:log_dist] [Rank 0] step=87, skipped=0, lr=[9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:33,898] [INFO] [timer.py:197:stop] 0/87, RunningAvgSamplesPerSec=26.82538576061838, CurrSamplesPerSec=46.4664630536551, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       87/     200 | consumed samples:        22272 | consumed tokens:     22806528 | elapsed time per iteration (ms): 3822.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.708797E+00 | moe loss: 8.057868E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1887.90 | backward-compute: 1837.45 | backward-embedding-all-reduce: 0.01 | optimizer: 89.12 | batch-generator: 465.75
 iteration       88/     200 | consumed samples:        22528 | consumed tokens:     23068672 | elapsed time per iteration (ms): 4904.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.699480E+00 | moe loss: 8.046295E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2428.41 | backward-compute: 2349.16 | backward-embedding-all-reduce: 0.01 | optimizer: 110.10 | batch-generator: 575.40
[2022-12-23 16:24:38,808] [INFO] [logging.py:68:log_dist] [Rank 0] step=88, skipped=0, lr=[9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:38,819] [INFO] [timer.py:197:stop] 0/88, RunningAvgSamplesPerSec=26.863644854132676, CurrSamplesPerSec=30.569573034547254, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:24:42,707] [INFO] [logging.py:68:log_dist] [Rank 0] step=89, skipped=0, lr=[9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:42,707] [INFO] [timer.py:197:stop] 0/89, RunningAvgSamplesPerSec=26.990944263489578, CurrSamplesPerSec=45.55666592332896, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       89/     200 | consumed samples:        22784 | consumed tokens:     23330816 | elapsed time per iteration (ms): 3905.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.687611E+00 | moe loss: 8.171672E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1906.95 | backward-compute: 1883.44 | backward-embedding-all-reduce: 0.01 | optimizer: 91.55 | batch-generator: 433.45
[2022-12-23 16:24:46,723] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:46,725] [INFO] [timer.py:197:stop] 0/90, RunningAvgSamplesPerSec=27.120839728808413, CurrSamplesPerSec=46.6548925549739, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       90/     200 | consumed samples:        23040 | consumed tokens:     23592960 | elapsed time per iteration (ms): 4015.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.691442E+00 | moe loss: 8.165002E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1944.02 | backward-compute: 1963.38 | backward-embedding-all-reduce: 0.01 | optimizer: 90.57 | batch-generator: 433.18
[2022-12-23 16:24:50,616] [INFO] [logging.py:68:log_dist] [Rank 0] step=91, skipped=0, lr=[9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:50,617] [INFO] [timer.py:197:stop] 0/91, RunningAvgSamplesPerSec=27.24414621981945, CurrSamplesPerSec=45.41426516600579, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       91/     200 | consumed samples:        23296 | consumed tokens:     23855104 | elapsed time per iteration (ms): 3894.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.673038E+00 | moe loss: 8.111458E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1923.71 | backward-compute: 1868.60 | backward-embedding-all-reduce: 0.01 | optimizer: 91.33 | batch-generator: 394.74
 iteration       92/     200 | consumed samples:        23552 | consumed tokens:     24117248 | elapsed time per iteration (ms): 4003.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.674373E+00 | moe loss: 8.199982E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1912.58 | backward-compute: 1890.84 | backward-embedding-all-reduce: 0.01 | optimizer: 193.85 | batch-generator: 396.60
[2022-12-23 16:24:54,648] [INFO] [logging.py:68:log_dist] [Rank 0] step=92, skipped=0, lr=[9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:54,649] [INFO] [timer.py:197:stop] 0/92, RunningAvgSamplesPerSec=27.322625432404358, CurrSamplesPerSec=36.742354862914645, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:24:59,531] [INFO] [logging.py:68:log_dist] [Rank 0] step=93, skipped=0, lr=[9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:24:59,531] [INFO] [timer.py:197:stop] 0/93, RunningAvgSamplesPerSec=27.436648638546572, CurrSamplesPerSec=43.94007522516314, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       93/     200 | consumed samples:        23808 | consumed tokens:     24379392 | elapsed time per iteration (ms): 4924.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.645171E+00 | moe loss: 8.169740E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2813.81 | backward-compute: 1941.64 | backward-embedding-all-reduce: 0.01 | optimizer: 114.24 | batch-generator: 474.13
[2022-12-23 16:25:03,501] [INFO] [logging.py:68:log_dist] [Rank 0] step=94, skipped=0, lr=[9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:03,501] [INFO] [timer.py:197:stop] 0/94, RunningAvgSamplesPerSec=27.556465631759583, CurrSamplesPerSec=45.72934393532503, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       94/     200 | consumed samples:        24064 | consumed tokens:     24641536 | elapsed time per iteration (ms): 3956.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.648740E+00 | moe loss: 8.200287E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1970.14 | backward-compute: 1883.61 | backward-embedding-all-reduce: 0.01 | optimizer: 89.23 | batch-generator: 460.41
[2022-12-23 16:25:07,530] [INFO] [logging.py:68:log_dist] [Rank 0] step=95, skipped=0, lr=[9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:07,531] [INFO] [timer.py:197:stop] 0/95, RunningAvgSamplesPerSec=27.672533531849172, CurrSamplesPerSec=45.1799580979259, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       95/     200 | consumed samples:        24320 | consumed tokens:     24903680 | elapsed time per iteration (ms): 4029.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.639935E+00 | moe loss: 8.306003E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1997.05 | backward-compute: 1933.39 | backward-embedding-all-reduce: 0.01 | optimizer: 91.10 | batch-generator: 415.27
 iteration       96/     200 | consumed samples:        24576 | consumed tokens:     25165824 | elapsed time per iteration (ms): 4249.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.629040E+00 | moe loss: 8.301557E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2197.64 | backward-compute: 1949.31 | backward-embedding-all-reduce: 0.01 | optimizer: 89.18 | batch-generator: 417.40
[2022-12-23 16:25:11,791] [INFO] [logging.py:68:log_dist] [Rank 0] step=96, skipped=0, lr=[9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:11,791] [INFO] [timer.py:197:stop] 0/96, RunningAvgSamplesPerSec=27.781202746272946, CurrSamplesPerSec=43.764276698470375, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:25:15,777] [INFO] [logging.py:68:log_dist] [Rank 0] step=97, skipped=0, lr=[9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:15,778] [INFO] [timer.py:197:stop] 0/97, RunningAvgSamplesPerSec=27.892899315048624, CurrSamplesPerSec=44.83917423965544, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       97/     200 | consumed samples:        24832 | consumed tokens:     25427968 | elapsed time per iteration (ms): 3997.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.630140E+00 | moe loss: 8.249754E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1973.61 | backward-compute: 1912.31 | backward-embedding-all-reduce: 0.01 | optimizer: 92.56 | batch-generator: 418.96
[2022-12-23 16:25:20,759] [INFO] [logging.py:68:log_dist] [Rank 0] step=98, skipped=0, lr=[9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:20,760] [INFO] [timer.py:197:stop] 0/98, RunningAvgSamplesPerSec=27.993027060925183, CurrSamplesPerSec=42.47955680241449, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration       98/     200 | consumed samples:        25088 | consumed tokens:     25690112 | elapsed time per iteration (ms): 4995.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.602974E+00 | moe loss: 8.405506E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2790.87 | backward-compute: 2077.56 | backward-embedding-all-reduce: 0.01 | optimizer: 113.01 | batch-generator: 483.68
 iteration       99/     200 | consumed samples:        25344 | consumed tokens:     25952256 | elapsed time per iteration (ms): 4205.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.593340E+00 | moe loss: 8.368041E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1985.57 | backward-compute: 2058.95 | backward-embedding-all-reduce: 0.01 | optimizer: 147.13 | batch-generator: 408.36
[2022-12-23 16:25:25,002] [INFO] [logging.py:68:log_dist] [Rank 0] step=99, skipped=0, lr=[9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:25,007] [INFO] [timer.py:197:stop] 0/99, RunningAvgSamplesPerSec=28.053249132843874, CurrSamplesPerSec=35.355018919812395, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:25:29,225] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:29,225] [INFO] [timer.py:197:stop] 0/100, RunningAvgSamplesPerSec=28.162096389812685, CurrSamplesPerSec=45.15778002452733, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      100/     200 | consumed samples:        25600 | consumed tokens:     26214400 | elapsed time per iteration (ms): 4246.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.597268E+00 | moe loss: 8.310650E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2124.22 | backward-compute: 1997.61 | backward-embedding-all-reduce: 0.01 | optimizer: 90.33 | batch-generator: 404.13
[2022-12-23 16:25:33,125] [INFO] [logging.py:68:log_dist] [Rank 0] step=101, skipped=0, lr=[9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:33,125] [INFO] [timer.py:197:stop] 0/101, RunningAvgSamplesPerSec=28.262937318965204, CurrSamplesPerSec=43.54252004572983, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      101/     200 | consumed samples:        25856 | consumed tokens:     26476544 | elapsed time per iteration (ms): 3899.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.593423E+00 | moe loss: 8.310112E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1897.93 | backward-compute: 1904.16 | backward-embedding-all-reduce: 0.01 | optimizer: 89.07 | batch-generator: 435.29
[2022-12-23 16:25:36,980] [INFO] [logging.py:68:log_dist] [Rank 0] step=102, skipped=0, lr=[9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:36,980] [INFO] [timer.py:197:stop] 0/102, RunningAvgSamplesPerSec=28.37272613335591, CurrSamplesPerSec=46.10231920368248, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      102/     200 | consumed samples:        26112 | consumed tokens:     26738688 | elapsed time per iteration (ms): 3854.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.581092E+00 | moe loss: 8.307369E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1892.34 | backward-compute: 1862.69 | backward-embedding-all-reduce: 0.01 | optimizer: 93.74 | batch-generator: 409.62
[2022-12-23 16:25:41,150] [INFO] [logging.py:68:log_dist] [Rank 0] step=103, skipped=0, lr=[9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:41,150] [INFO] [timer.py:197:stop] 0/103, RunningAvgSamplesPerSec=28.48093538617319, CurrSamplesPerSec=46.03982158666847, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      103/     200 | consumed samples:        26368 | consumed tokens:     27000832 | elapsed time per iteration (ms): 4184.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.566422E+00 | moe loss: 8.248845E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2086.86 | backward-compute: 1979.13 | backward-embedding-all-reduce: 0.01 | optimizer: 106.54 | batch-generator: 577.35
 iteration      104/     200 | consumed samples:        26624 | consumed tokens:     27262976 | elapsed time per iteration (ms): 4276.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.574636E+00 | moe loss: 8.299091E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1895.67 | backward-compute: 2251.08 | backward-embedding-all-reduce: 0.01 | optimizer: 124.67 | batch-generator: 417.29
[2022-12-23 16:25:45,464] [INFO] [logging.py:68:log_dist] [Rank 0] step=104, skipped=0, lr=[9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:45,484] [INFO] [timer.py:197:stop] 0/104, RunningAvgSamplesPerSec=28.471434004954055, CurrSamplesPerSec=27.54338443782697, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:25:49,560] [INFO] [logging.py:68:log_dist] [Rank 0] step=105, skipped=0, lr=[9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:49,560] [INFO] [timer.py:197:stop] 0/105, RunningAvgSamplesPerSec=28.578532255987565, CurrSamplesPerSec=46.36990210056932, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      105/     200 | consumed samples:        26880 | consumed tokens:     27525120 | elapsed time per iteration (ms): 4119.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.558683E+00 | moe loss: 8.212019E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2061.69 | backward-compute: 1917.20 | backward-embedding-all-reduce: 0.01 | optimizer: 89.32 | batch-generator: 407.39
[2022-12-23 16:25:53,416] [INFO] [logging.py:68:log_dist] [Rank 0] step=106, skipped=0, lr=[9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:53,417] [INFO] [timer.py:197:stop] 0/106, RunningAvgSamplesPerSec=28.683155969990533, CurrSamplesPerSec=46.045902593187755, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      106/     200 | consumed samples:        27136 | consumed tokens:     27787264 | elapsed time per iteration (ms): 3855.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.555933E+00 | moe loss: 8.177542E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1882.71 | backward-compute: 1875.16 | backward-embedding-all-reduce: 0.01 | optimizer: 90.17 | batch-generator: 392.65
[2022-12-23 16:25:58,241] [INFO] [logging.py:68:log_dist] [Rank 0] step=107, skipped=0, lr=[9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:25:58,241] [INFO] [timer.py:197:stop] 0/107, RunningAvgSamplesPerSec=28.65156157130368, CurrSamplesPerSec=25.706712511271537, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      107/     200 | consumed samples:        27392 | consumed tokens:     28049408 | elapsed time per iteration (ms): 4872.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.557181E+00 | moe loss: 8.178946E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2097.22 | backward-compute: 2602.68 | backward-embedding-all-reduce: 0.01 | optimizer: 157.22 | batch-generator: 410.00
[2022-12-23 16:26:02,521] [INFO] [logging.py:68:log_dist] [Rank 0] step=108, skipped=0, lr=[9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:02,521] [INFO] [timer.py:197:stop] 0/108, RunningAvgSamplesPerSec=28.755269078646506, CurrSamplesPerSec=46.38389177876355, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      108/     200 | consumed samples:        27648 | consumed tokens:     28311552 | elapsed time per iteration (ms): 4232.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.535460E+00 | moe loss: 8.217842E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2149.48 | backward-compute: 1977.72 | backward-embedding-all-reduce: 0.01 | optimizer: 89.29 | batch-generator: 471.79
 iteration      109/     200 | consumed samples:        27904 | consumed tokens:     28573696 | elapsed time per iteration (ms): 4027.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.529988E+00 | moe loss: 8.270186E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1961.88 | backward-compute: 1960.30 | backward-embedding-all-reduce: 0.01 | optimizer: 98.24 | batch-generator: 389.07
[2022-12-23 16:26:06,570] [INFO] [logging.py:68:log_dist] [Rank 0] step=109, skipped=0, lr=[9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:06,583] [INFO] [timer.py:197:stop] 0/109, RunningAvgSamplesPerSec=28.847228589286757, CurrSamplesPerSec=43.64104486662789, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:26:10,526] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:10,526] [INFO] [timer.py:197:stop] 0/110, RunningAvgSamplesPerSec=28.950482649525206, CurrSamplesPerSec=46.92056404912235, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      110/     200 | consumed samples:        28160 | consumed tokens:     28835840 | elapsed time per iteration (ms): 3977.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.524988E+00 | moe loss: 8.208134E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1995.47 | backward-compute: 1852.60 | backward-embedding-all-reduce: 0.01 | optimizer: 89.29 | batch-generator: 486.15
[2022-12-23 16:26:14,226] [INFO] [logging.py:68:log_dist] [Rank 0] step=111, skipped=0, lr=[9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:14,226] [INFO] [timer.py:197:stop] 0/111, RunningAvgSamplesPerSec=29.051411971316586, CurrSamplesPerSec=46.59540155105393, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      111/     200 | consumed samples:        28416 | consumed tokens:     29097984 | elapsed time per iteration (ms): 3700.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.517615E+00 | moe loss: 8.190697E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1797.74 | backward-compute: 1798.06 | backward-embedding-all-reduce: 0.01 | optimizer: 89.56 | batch-generator: 389.37
[2022-12-23 16:26:18,864] [INFO] [logging.py:68:log_dist] [Rank 0] step=112, skipped=0, lr=[9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:18,865] [INFO] [timer.py:197:stop] 0/112, RunningAvgSamplesPerSec=28.965409404472425, CurrSamplesPerSec=21.89904730515767, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      112/     200 | consumed samples:        28672 | consumed tokens:     29360128 | elapsed time per iteration (ms): 4651.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.510996E+00 | moe loss: 8.215602E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2555.45 | backward-compute: 1975.83 | backward-embedding-all-reduce: 0.01 | optimizer: 105.02 | batch-generator: 766.25
[2022-12-23 16:26:22,717] [INFO] [logging.py:68:log_dist] [Rank 0] step=113, skipped=0, lr=[9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:22,717] [INFO] [timer.py:197:stop] 0/113, RunningAvgSamplesPerSec=29.063751045123976, CurrSamplesPerSec=46.38810796425615, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      113/     200 | consumed samples:        28928 | consumed tokens:     29622272 | elapsed time per iteration (ms): 3839.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.493868E+00 | moe loss: 8.235436E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1874.98 | backward-compute: 1868.40 | backward-embedding-all-reduce: 0.01 | optimizer: 89.28 | batch-generator: 414.54
 iteration      114/     200 | consumed samples:        29184 | consumed tokens:     29884416 | elapsed time per iteration (ms): 4000.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.502555E+00 | moe loss: 8.286957E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1998.65 | backward-compute: 1900.11 | backward-embedding-all-reduce: 0.01 | optimizer: 90.98 | batch-generator: 377.07
[2022-12-23 16:26:26,732] [INFO] [logging.py:68:log_dist] [Rank 0] step=114, skipped=0, lr=[9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:26,735] [INFO] [timer.py:197:stop] 0/114, RunningAvgSamplesPerSec=29.1496038110617, CurrSamplesPerSec=43.37016229672518, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:26:30,587] [INFO] [logging.py:68:log_dist] [Rank 0] step=115, skipped=0, lr=[9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:30,588] [INFO] [timer.py:197:stop] 0/115, RunningAvgSamplesPerSec=29.244840769612377, CurrSamplesPerSec=46.121979615540575, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      115/     200 | consumed samples:        29440 | consumed tokens:     30146560 | elapsed time per iteration (ms): 3869.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.490482E+00 | moe loss: 8.279555E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1896.71 | backward-compute: 1860.02 | backward-embedding-all-reduce: 0.01 | optimizer: 89.19 | batch-generator: 436.60
[2022-12-23 16:26:34,161] [INFO] [logging.py:68:log_dist] [Rank 0] step=116, skipped=0, lr=[9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:34,161] [INFO] [timer.py:197:stop] 0/116, RunningAvgSamplesPerSec=29.34616610294268, CurrSamplesPerSec=48.228164546273156, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      116/     200 | consumed samples:        29696 | consumed tokens:     30408704 | elapsed time per iteration (ms): 3573.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.483621E+00 | moe loss: 8.183055E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1724.54 | backward-compute: 1753.59 | backward-embedding-all-reduce: 0.01 | optimizer: 89.10 | batch-generator: 355.85
[2022-12-23 16:26:39,075] [INFO] [logging.py:68:log_dist] [Rank 0] step=117, skipped=0, lr=[9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:39,076] [INFO] [timer.py:197:stop] 0/117, RunningAvgSamplesPerSec=29.437041911023936, CurrSamplesPerSec=45.49929319302509, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      117/     200 | consumed samples:        29952 | consumed tokens:     30670848 | elapsed time per iteration (ms): 4917.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.484929E+00 | moe loss: 8.192363E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2877.81 | backward-compute: 1908.28 | backward-embedding-all-reduce: 0.01 | optimizer: 110.44 | batch-generator: 470.91
[2022-12-23 16:26:42,972] [INFO] [logging.py:68:log_dist] [Rank 0] step=118, skipped=0, lr=[9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:42,973] [INFO] [timer.py:197:stop] 0/118, RunningAvgSamplesPerSec=29.531059428378583, CurrSamplesPerSec=46.67413446912924, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      118/     200 | consumed samples:        30208 | consumed tokens:     30932992 | elapsed time per iteration (ms): 3894.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.484308E+00 | moe loss: 8.232118E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1872.58 | backward-compute: 1924.54 | backward-embedding-all-reduce: 0.01 | optimizer: 90.29 | batch-generator: 395.70
[2022-12-23 16:26:46,797] [INFO] [logging.py:68:log_dist] [Rank 0] step=119, skipped=0, lr=[9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:46,800] [INFO] [timer.py:197:stop] 0/119, RunningAvgSamplesPerSec=29.626955754437034, CurrSamplesPerSec=47.5314457602203, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      119/     200 | consumed samples:        30464 | consumed tokens:     31195136 | elapsed time per iteration (ms): 3823.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.468340E+00 | moe loss: 8.176658E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1955.46 | backward-compute: 1767.88 | backward-embedding-all-reduce: 0.01 | optimizer: 93.37 | batch-generator: 374.48
[2022-12-23 16:26:50,544] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:50,544] [INFO] [timer.py:197:stop] 0/120, RunningAvgSamplesPerSec=29.71558917507235, CurrSamplesPerSec=45.71789516997351, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      120/     200 | consumed samples:        30720 | consumed tokens:     31457280 | elapsed time per iteration (ms): 3747.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.467408E+00 | moe loss: 8.246340E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1810.96 | backward-compute: 1836.09 | backward-embedding-all-reduce: 0.01 | optimizer: 90.78 | batch-generator: 371.50
[2022-12-23 16:26:54,336] [INFO] [logging.py:68:log_dist] [Rank 0] step=121, skipped=0, lr=[9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:54,337] [INFO] [timer.py:197:stop] 0/121, RunningAvgSamplesPerSec=29.805801604464037, CurrSamplesPerSec=46.443245338122374, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      121/     200 | consumed samples:        30976 | consumed tokens:     31719424 | elapsed time per iteration (ms): 3792.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.454108E+00 | moe loss: 8.178028E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1880.11 | backward-compute: 1812.19 | backward-embedding-all-reduce: 0.01 | optimizer: 89.67 | batch-generator: 458.74
[2022-12-23 16:26:58,896] [INFO] [logging.py:68:log_dist] [Rank 0] step=122, skipped=0, lr=[9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:26:58,897] [INFO] [timer.py:197:stop] 0/122, RunningAvgSamplesPerSec=29.718203524262115, CurrSamplesPerSec=22.017784476657834, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      122/     200 | consumed samples:        31232 | consumed tokens:     31981568 | elapsed time per iteration (ms): 4563.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.450907E+00 | moe loss: 8.164991E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2495.00 | backward-compute: 1939.81 | backward-embedding-all-reduce: 0.01 | optimizer: 111.49 | batch-generator: 605.39
[2022-12-23 16:27:02,552] [INFO] [logging.py:68:log_dist] [Rank 0] step=123, skipped=0, lr=[9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:02,552] [INFO] [timer.py:197:stop] 0/123, RunningAvgSamplesPerSec=29.810444747736838, CurrSamplesPerSec=47.50392351108851, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      123/     200 | consumed samples:        31488 | consumed tokens:     32243712 | elapsed time per iteration (ms): 3651.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.445851E+00 | moe loss: 8.232313E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1774.28 | backward-compute: 1772.74 | backward-embedding-all-reduce: 0.01 | optimizer: 90.70 | batch-generator: 385.04
[2022-12-23 16:27:06,471] [INFO] [logging.py:68:log_dist] [Rank 0] step=124, skipped=0, lr=[9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:06,473] [INFO] [timer.py:197:stop] 0/124, RunningAvgSamplesPerSec=29.900108886230612, CurrSamplesPerSec=47.00868074709413, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      124/     200 | consumed samples:        31744 | consumed tokens:     32505856 | elapsed time per iteration (ms): 3919.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.433118E+00 | moe loss: 8.188130E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2040.16 | backward-compute: 1771.45 | backward-embedding-all-reduce: 0.01 | optimizer: 96.55 | batch-generator: 529.04
[2022-12-23 16:27:10,116] [INFO] [logging.py:68:log_dist] [Rank 0] step=125, skipped=0, lr=[9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:10,117] [INFO] [timer.py:197:stop] 0/125, RunningAvgSamplesPerSec=29.991644346523685, CurrSamplesPerSec=47.87080726555765, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      125/     200 | consumed samples:        32000 | consumed tokens:     32768000 | elapsed time per iteration (ms): 3646.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.434559E+00 | moe loss: 8.221629E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1826.77 | backward-compute: 1713.04 | backward-embedding-all-reduce: 0.01 | optimizer: 96.19 | batch-generator: 415.26
[2022-12-23 16:27:13,786] [INFO] [logging.py:68:log_dist] [Rank 0] step=126, skipped=0, lr=[9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:13,787] [INFO] [timer.py:197:stop] 0/126, RunningAvgSamplesPerSec=30.045997410115273, CurrSamplesPerSec=38.66474117490143, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      126/     200 | consumed samples:        32256 | consumed tokens:     33030144 | elapsed time per iteration (ms): 3669.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.439128E+00 | moe loss: 8.187465E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1763.21 | backward-compute: 1798.87 | backward-embedding-all-reduce: 0.01 | optimizer: 92.55 | batch-generator: 356.45
[2022-12-23 16:27:18,448] [INFO] [logging.py:68:log_dist] [Rank 0] step=127, skipped=0, lr=[9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:18,450] [INFO] [timer.py:197:stop] 0/127, RunningAvgSamplesPerSec=29.99362211545077, CurrSamplesPerSec=24.66270180043343, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      127/     200 | consumed samples:        32512 | consumed tokens:     33292288 | elapsed time per iteration (ms): 4676.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.435629E+00 | moe loss: 8.199271E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2222.54 | backward-compute: 2303.87 | backward-embedding-all-reduce: 0.01 | optimizer: 129.02 | batch-generator: 360.13
[2022-12-23 16:27:22,194] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:22,195] [INFO] [timer.py:197:stop] 0/128, RunningAvgSamplesPerSec=30.083470529427288, CurrSamplesPerSec=48.09107863501005, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      128/     200 | consumed samples:        32768 | consumed tokens:     33554432 | elapsed time per iteration (ms): 3731.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.418331E+00 | moe loss: 8.175840E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1921.29 | backward-compute: 1711.97 | backward-embedding-all-reduce: 0.01 | optimizer: 89.51 | batch-generator: 446.36
 iteration      129/     200 | consumed samples:        33024 | consumed tokens:     33816576 | elapsed time per iteration (ms): 3593.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.420886E+00 | moe loss: 8.200478E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1776.87 | backward-compute: 1705.51 | backward-embedding-all-reduce: 0.01 | optimizer: 97.88 | batch-generator: 350.95
[2022-12-23 16:27:25,793] [INFO] [logging.py:68:log_dist] [Rank 0] step=129, skipped=0, lr=[9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:25,802] [INFO] [timer.py:197:stop] 0/129, RunningAvgSamplesPerSec=30.172088227347675, CurrSamplesPerSec=47.98067299198628, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:27:29,344] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:29,345] [INFO] [timer.py:197:stop] 0/130, RunningAvgSamplesPerSec=30.262182934657655, CurrSamplesPerSec=48.74908217094357, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      130/     200 | consumed samples:        33280 | consumed tokens:     34078720 | elapsed time per iteration (ms): 3556.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.423852E+00 | moe loss: 8.214907E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1728.82 | backward-compute: 1717.70 | backward-embedding-all-reduce: 0.01 | optimizer: 89.53 | batch-generator: 396.66
[2022-12-23 16:27:32,906] [INFO] [logging.py:68:log_dist] [Rank 0] step=131, skipped=0, lr=[9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:32,907] [INFO] [timer.py:197:stop] 0/131, RunningAvgSamplesPerSec=30.349816465428297, CurrSamplesPerSec=48.22511470815443, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      131/     200 | consumed samples:        33536 | consumed tokens:     34340864 | elapsed time per iteration (ms): 3561.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.398363E+00 | moe loss: 8.172805E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1711.63 | backward-compute: 1751.25 | backward-embedding-all-reduce: 0.01 | optimizer: 89.12 | batch-generator: 349.45
 iteration      132/     200 | consumed samples:        33792 | consumed tokens:     34603008 | elapsed time per iteration (ms): 4087.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.396164E+00 | moe loss: 8.244511E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2178.29 | backward-compute: 1803.33 | backward-embedding-all-reduce: 0.01 | optimizer: 91.30 | batch-generator: 335.60
[2022-12-23 16:27:37,005] [INFO] [logging.py:68:log_dist] [Rank 0] step=132, skipped=0, lr=[9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:37,005] [INFO] [timer.py:197:stop] 0/132, RunningAvgSamplesPerSec=30.430333738579193, CurrSamplesPerSec=46.26313490954201, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:27:41,205] [INFO] [logging.py:68:log_dist] [Rank 0] step=133, skipped=0, lr=[9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:41,206] [INFO] [timer.py:197:stop] 0/133, RunningAvgSamplesPerSec=30.51559907712268, CurrSamplesPerSec=47.99997711182732, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      133/     200 | consumed samples:        34048 | consumed tokens:     34865152 | elapsed time per iteration (ms): 4221.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.407061E+00 | moe loss: 8.130739E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2336.65 | backward-compute: 1748.73 | backward-embedding-all-reduce: 0.01 | optimizer: 104.46 | batch-generator: 412.02
[2022-12-23 16:27:44,642] [INFO] [logging.py:68:log_dist] [Rank 0] step=134, skipped=0, lr=[9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:44,643] [INFO] [timer.py:197:stop] 0/134, RunningAvgSamplesPerSec=30.602425795585784, CurrSamplesPerSec=48.78725097979631, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      134/     200 | consumed samples:        34304 | consumed tokens:     35127296 | elapsed time per iteration (ms): 3426.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.398754E+00 | moe loss: 8.189713E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1638.44 | backward-compute: 1689.29 | backward-embedding-all-reduce: 0.01 | optimizer: 90.22 | batch-generator: 332.23
[2022-12-23 16:27:48,189] [INFO] [logging.py:68:log_dist] [Rank 0] step=135, skipped=0, lr=[9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:48,193] [INFO] [timer.py:197:stop] 0/135, RunningAvgSamplesPerSec=30.685634562647692, CurrSamplesPerSec=47.864883370612134, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      135/     200 | consumed samples:        34560 | consumed tokens:     35389440 | elapsed time per iteration (ms): 3546.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.379688E+00 | moe loss: 8.172563E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1746.00 | backward-compute: 1690.23 | backward-embedding-all-reduce: 0.01 | optimizer: 94.97 | batch-generator: 338.94
[2022-12-23 16:27:51,696] [INFO] [logging.py:68:log_dist] [Rank 0] step=136, skipped=0, lr=[9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:51,696] [INFO] [timer.py:197:stop] 0/136, RunningAvgSamplesPerSec=30.76583160284846, CurrSamplesPerSec=47.15764383465383, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      136/     200 | consumed samples:        34816 | consumed tokens:     35651584 | elapsed time per iteration (ms): 3508.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.387387E+00 | moe loss: 8.157874E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1681.69 | backward-compute: 1716.93 | backward-embedding-all-reduce: 0.01 | optimizer: 97.74 | batch-generator: 345.00
 iteration      137/     200 | consumed samples:        35072 | consumed tokens:     35913728 | elapsed time per iteration (ms): 3882.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.378030E+00 | moe loss: 8.216484E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1647.32 | backward-compute: 2118.51 | backward-embedding-all-reduce: 0.01 | optimizer: 109.50 | batch-generator: 334.04
[2022-12-23 16:27:55,597] [INFO] [logging.py:68:log_dist] [Rank 0] step=137, skipped=0, lr=[9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:55,598] [INFO] [timer.py:197:stop] 0/137, RunningAvgSamplesPerSec=30.74733033474151, CurrSamplesPerSec=28.454418523202975, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:27:59,523] [INFO] [logging.py:68:log_dist] [Rank 0] step=138, skipped=0, lr=[9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:27:59,524] [INFO] [timer.py:197:stop] 0/138, RunningAvgSamplesPerSec=30.825810162246313, CurrSamplesPerSec=47.03182463581432, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      138/     200 | consumed samples:        35328 | consumed tokens:     36175872 | elapsed time per iteration (ms): 3959.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.376946E+00 | moe loss: 8.225787E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2083.96 | backward-compute: 1740.24 | backward-embedding-all-reduce: 0.01 | optimizer: 108.14 | batch-generator: 437.80
[2022-12-23 16:28:03,510] [INFO] [logging.py:68:log_dist] [Rank 0] step=139, skipped=0, lr=[9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:03,511] [INFO] [timer.py:197:stop] 0/139, RunningAvgSamplesPerSec=30.903533858397132, CurrSamplesPerSec=47.03076989923709, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      139/     200 | consumed samples:        35584 | consumed tokens:     36438016 | elapsed time per iteration (ms): 3970.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.391067E+00 | moe loss: 8.393916E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1890.90 | backward-compute: 1984.15 | backward-embedding-all-reduce: 0.01 | optimizer: 89.32 | batch-generator: 330.14
 iteration      140/     200 | consumed samples:        35840 | consumed tokens:     36700160 | elapsed time per iteration (ms): 3832.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.379182E+00 | moe loss: 8.311505E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1828.03 | backward-compute: 1898.22 | backward-embedding-all-reduce: 0.01 | optimizer: 98.48 | batch-generator: 348.04
[2022-12-23 16:28:07,348] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:07,356] [INFO] [timer.py:197:stop] 0/140, RunningAvgSamplesPerSec=30.972942090950816, CurrSamplesPerSec=44.73900405864502, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:28:10,879] [INFO] [logging.py:68:log_dist] [Rank 0] step=141, skipped=0, lr=[9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:10,880] [INFO] [timer.py:197:stop] 0/141, RunningAvgSamplesPerSec=31.05328498297471, CurrSamplesPerSec=48.36718456923035, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      141/     200 | consumed samples:        36096 | consumed tokens:     36962304 | elapsed time per iteration (ms): 3536.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.351847E+00 | moe loss: 8.322199E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1692.60 | backward-compute: 1729.42 | backward-embedding-all-reduce: 0.01 | optimizer: 89.88 | batch-generator: 313.72
[2022-12-23 16:28:14,395] [INFO] [logging.py:68:log_dist] [Rank 0] step=142, skipped=0, lr=[9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:14,395] [INFO] [timer.py:197:stop] 0/142, RunningAvgSamplesPerSec=31.131503186206096, CurrSamplesPerSec=47.90335507615699, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      142/     200 | consumed samples:        36352 | consumed tokens:     37224448 | elapsed time per iteration (ms): 3515.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.351327E+00 | moe loss: 8.318612E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1678.09 | backward-compute: 1739.71 | backward-embedding-all-reduce: 0.01 | optimizer: 89.16 | batch-generator: 339.18
[2022-12-23 16:28:19,081] [INFO] [logging.py:68:log_dist] [Rank 0] step=143, skipped=0, lr=[9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:19,082] [INFO] [timer.py:197:stop] 0/143, RunningAvgSamplesPerSec=31.145254303523288, CurrSamplesPerSec=33.19821515142521, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      143/     200 | consumed samples:        36608 | consumed tokens:     37486592 | elapsed time per iteration (ms): 4689.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.367075E+00 | moe loss: 8.318862E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2359.90 | backward-compute: 2211.06 | backward-embedding-all-reduce: 0.01 | optimizer: 104.21 | batch-generator: 409.18
[2022-12-23 16:28:22,797] [INFO] [logging.py:68:log_dist] [Rank 0] step=144, skipped=0, lr=[9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:22,798] [INFO] [timer.py:197:stop] 0/144, RunningAvgSamplesPerSec=31.217499086824, CurrSamplesPerSec=46.390047987656764, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      144/     200 | consumed samples:        36864 | consumed tokens:     37748736 | elapsed time per iteration (ms): 3713.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.350393E+00 | moe loss: 8.384250E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1804.50 | backward-compute: 1810.76 | backward-embedding-all-reduce: 0.01 | optimizer: 89.10 | batch-generator: 375.55
[2022-12-23 16:28:26,345] [INFO] [logging.py:68:log_dist] [Rank 0] step=145, skipped=0, lr=[9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:26,346] [INFO] [timer.py:197:stop] 0/145, RunningAvgSamplesPerSec=31.29488913788964, CurrSamplesPerSec=48.296557831447124, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      145/     200 | consumed samples:        37120 | consumed tokens:     38010880 | elapsed time per iteration (ms): 3547.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.345033E+00 | moe loss: 8.284125E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1715.00 | backward-compute: 1727.92 | backward-embedding-all-reduce: 0.01 | optimizer: 90.85 | batch-generator: 341.55
[2022-12-23 16:28:29,899] [INFO] [logging.py:68:log_dist] [Rank 0] step=146, skipped=0, lr=[9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:29,900] [INFO] [timer.py:197:stop] 0/146, RunningAvgSamplesPerSec=31.37224981626013, CurrSamplesPerSec=48.52589628182374, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      146/     200 | consumed samples:        37376 | consumed tokens:     38273024 | elapsed time per iteration (ms): 3555.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.357858E+00 | moe loss: 8.294483E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1729.79 | backward-compute: 1720.16 | backward-embedding-all-reduce: 0.01 | optimizer: 94.15 | batch-generator: 377.38
 iteration      147/     200 | consumed samples:        37632 | consumed tokens:     38535168 | elapsed time per iteration (ms): 4119.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.337115E+00 | moe loss: 8.361427E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2212.38 | backward-compute: 1806.06 | backward-embedding-all-reduce: 0.01 | optimizer: 91.09 | batch-generator: 332.93
[2022-12-23 16:28:34,037] [INFO] [logging.py:68:log_dist] [Rank 0] step=147, skipped=0, lr=[9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:34,037] [INFO] [timer.py:197:stop] 0/147, RunningAvgSamplesPerSec=31.439295452555474, CurrSamplesPerSec=45.4156020900599, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:28:37,636] [INFO] [logging.py:68:log_dist] [Rank 0] step=148, skipped=0, lr=[9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:37,637] [INFO] [timer.py:197:stop] 0/148, RunningAvgSamplesPerSec=31.514278196055432, CurrSamplesPerSec=48.17408720009131, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      148/     200 | consumed samples:        37888 | consumed tokens:     38797312 | elapsed time per iteration (ms): 3816.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.339796E+00 | moe loss: 8.339891E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1703.91 | backward-compute: 1791.23 | backward-embedding-all-reduce: 0.01 | optimizer: 226.46 | batch-generator: 331.30
[2022-12-23 16:28:41,706] [INFO] [logging.py:68:log_dist] [Rank 0] step=149, skipped=0, lr=[9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:41,706] [INFO] [timer.py:197:stop] 0/149, RunningAvgSamplesPerSec=31.584943551967996, CurrSamplesPerSec=46.95807428751759, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      149/     200 | consumed samples:        38144 | consumed tokens:     39059456 | elapsed time per iteration (ms): 3869.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.328584E+00 | moe loss: 8.340366E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1956.34 | backward-compute: 1769.81 | backward-embedding-all-reduce: 0.01 | optimizer: 94.47 | batch-generator: 436.44
 iteration      150/     200 | consumed samples:        38400 | consumed tokens:     39321600 | elapsed time per iteration (ms): 3853.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.339808E+00 | moe loss: 8.319765E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1883.80 | backward-compute: 1867.47 | backward-embedding-all-reduce: 0.01 | optimizer: 90.02 | batch-generator: 325.93
[2022-12-23 16:28:45,580] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:45,581] [INFO] [timer.py:197:stop] 0/150, RunningAvgSamplesPerSec=31.62109365656519, CurrSamplesPerSec=38.017399018646174, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:28:49,154] [INFO] [logging.py:68:log_dist] [Rank 0] step=151, skipped=0, lr=[9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:49,154] [INFO] [timer.py:197:stop] 0/151, RunningAvgSamplesPerSec=31.69494584433889, CurrSamplesPerSec=48.437983210624346, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      151/     200 | consumed samples:        38656 | consumed tokens:     39583744 | elapsed time per iteration (ms): 3595.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.319063E+00 | moe loss: 8.265659E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1748.56 | backward-compute: 1726.84 | backward-embedding-all-reduce: 0.01 | optimizer: 92.73 | batch-generator: 369.24
[2022-12-23 16:28:52,728] [INFO] [logging.py:68:log_dist] [Rank 0] step=152, skipped=0, lr=[9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:52,728] [INFO] [timer.py:197:stop] 0/152, RunningAvgSamplesPerSec=31.768945867987394, CurrSamplesPerSec=48.716330192113716, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      152/     200 | consumed samples:        38912 | consumed tokens:     39845888 | elapsed time per iteration (ms): 3572.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.332182E+00 | moe loss: 8.277010E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1708.28 | backward-compute: 1766.25 | backward-embedding-all-reduce: 0.01 | optimizer: 90.99 | batch-generator: 315.99
[2022-12-23 16:28:56,378] [INFO] [logging.py:68:log_dist] [Rank 0] step=153, skipped=0, lr=[9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:28:56,379] [INFO] [timer.py:197:stop] 0/153, RunningAvgSamplesPerSec=31.842765924536753, CurrSamplesPerSec=48.87969499020168, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      153/     200 | consumed samples:        39168 | consumed tokens:     40108032 | elapsed time per iteration (ms): 3650.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.302763E+00 | moe loss: 8.269488E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1805.47 | backward-compute: 1747.41 | backward-embedding-all-reduce: 0.01 | optimizer: 91.47 | batch-generator: 327.02
[2022-12-23 16:29:00,060] [INFO] [logging.py:68:log_dist] [Rank 0] step=154, skipped=0, lr=[9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:00,061] [INFO] [timer.py:197:stop] 0/154, RunningAvgSamplesPerSec=31.911172970972416, CurrSamplesPerSec=47.2331032871235, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      154/     200 | consumed samples:        39424 | consumed tokens:     40370176 | elapsed time per iteration (ms): 3692.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.311473E+00 | moe loss: 8.267092E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1811.66 | backward-compute: 1768.17 | backward-embedding-all-reduce: 0.01 | optimizer: 103.03 | batch-generator: 415.98
[2022-12-23 16:29:03,519] [INFO] [logging.py:68:log_dist] [Rank 0] step=155, skipped=0, lr=[9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:03,520] [INFO] [timer.py:197:stop] 0/155, RunningAvgSamplesPerSec=31.980526434066398, CurrSamplesPerSec=47.75678709726545, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      155/     200 | consumed samples:        39680 | consumed tokens:     40632320 | elapsed time per iteration (ms): 3448.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.315138E+00 | moe loss: 8.252036E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1639.04 | backward-compute: 1713.41 | backward-embedding-all-reduce: 0.01 | optimizer: 89.12 | batch-generator: 297.53
 iteration      156/     200 | consumed samples:        39936 | consumed tokens:     40894464 | elapsed time per iteration (ms): 3938.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.303639E+00 | moe loss: 8.252205E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1909.82 | backward-compute: 1923.64 | backward-embedding-all-reduce: 0.01 | optimizer: 98.17 | batch-generator: 308.67
[2022-12-23 16:29:07,475] [INFO] [logging.py:68:log_dist] [Rank 0] step=156, skipped=0, lr=[9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:07,483] [INFO] [timer.py:197:stop] 0/156, RunningAvgSamplesPerSec=32.036431362217456, CurrSamplesPerSec=43.733246790739294, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:29:10,948] [INFO] [logging.py:68:log_dist] [Rank 0] step=157, skipped=0, lr=[9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:10,948] [INFO] [timer.py:197:stop] 0/157, RunningAvgSamplesPerSec=32.10974759492351, CurrSamplesPerSec=49.58522304307397, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      157/     200 | consumed samples:        40192 | consumed tokens:     41156608 | elapsed time per iteration (ms): 3490.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.297523E+00 | moe loss: 8.271749E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1655.72 | backward-compute: 1713.51 | backward-embedding-all-reduce: 0.01 | optimizer: 89.56 | batch-generator: 310.63
[2022-12-23 16:29:14,570] [INFO] [logging.py:68:log_dist] [Rank 0] step=158, skipped=0, lr=[9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:14,571] [INFO] [timer.py:197:stop] 0/158, RunningAvgSamplesPerSec=32.18252638356687, CurrSamplesPerSec=49.61212948755813, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      158/     200 | consumed samples:        40448 | consumed tokens:     41418752 | elapsed time per iteration (ms): 3625.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.303010E+00 | moe loss: 8.205520E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1716.95 | backward-compute: 1806.60 | backward-embedding-all-reduce: 0.01 | optimizer: 89.05 | batch-generator: 311.77
[2022-12-23 16:29:18,882] [INFO] [logging.py:68:log_dist] [Rank 0] step=159, skipped=0, lr=[9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:18,883] [INFO] [timer.py:197:stop] 0/159, RunningAvgSamplesPerSec=32.12507845379165, CurrSamplesPerSec=25.127747609949797, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      159/     200 | consumed samples:        40704 | consumed tokens:     41680896 | elapsed time per iteration (ms): 4330.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.293531E+00 | moe loss: 8.182040E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2316.90 | backward-compute: 1877.47 | backward-embedding-all-reduce: 0.01 | optimizer: 122.39 | batch-generator: 393.96
[2022-12-23 16:29:22,451] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:22,452] [INFO] [timer.py:197:stop] 0/160, RunningAvgSamplesPerSec=32.194049221526605, CurrSamplesPerSec=48.56329454451512, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      160/     200 | consumed samples:        40960 | consumed tokens:     41943040 | elapsed time per iteration (ms): 3547.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.289070E+00 | moe loss: 8.223473E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1719.06 | backward-compute: 1724.64 | backward-embedding-all-reduce: 0.01 | optimizer: 89.49 | batch-generator: 355.08
 iteration      161/     200 | consumed samples:        41216 | consumed tokens:     42205184 | elapsed time per iteration (ms): 3633.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.301133E+00 | moe loss: 8.214282E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1812.43 | backward-compute: 1722.25 | backward-embedding-all-reduce: 0.01 | optimizer: 89.53 | batch-generator: 325.13
[2022-12-23 16:29:26,097] [INFO] [logging.py:68:log_dist] [Rank 0] step=161, skipped=0, lr=[9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:26,098] [INFO] [timer.py:197:stop] 0/161, RunningAvgSamplesPerSec=32.26104476583451, CurrSamplesPerSec=48.06448815037399, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:29:29,639] [INFO] [logging.py:68:log_dist] [Rank 0] step=162, skipped=0, lr=[9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:29,642] [INFO] [timer.py:197:stop] 0/162, RunningAvgSamplesPerSec=32.293233782016756, CurrSamplesPerSec=38.382411624159424, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      162/     200 | consumed samples:        41472 | consumed tokens:     42467328 | elapsed time per iteration (ms): 3566.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.296416E+00 | moe loss: 8.232488E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1758.81 | backward-compute: 1684.79 | backward-embedding-all-reduce: 0.01 | optimizer: 103.32 | batch-generator: 312.35
[2022-12-23 16:29:33,680] [INFO] [logging.py:68:log_dist] [Rank 0] step=163, skipped=0, lr=[9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:33,681] [INFO] [timer.py:197:stop] 0/163, RunningAvgSamplesPerSec=32.316571230952086, CurrSamplesPerSec=36.54181688488126, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      163/     200 | consumed samples:        41728 | consumed tokens:     42729472 | elapsed time per iteration (ms): 4029.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.297427E+00 | moe loss: 8.246787E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1923.49 | backward-compute: 2002.49 | backward-embedding-all-reduce: 0.01 | optimizer: 96.80 | batch-generator: 345.61
 iteration      164/     200 | consumed samples:        41984 | consumed tokens:     42991616 | elapsed time per iteration (ms): 3485.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.287220E+00 | moe loss: 8.183875E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1690.25 | backward-compute: 1691.92 | backward-embedding-all-reduce: 0.01 | optimizer: 89.51 | batch-generator: 309.08
[2022-12-23 16:29:37,171] [INFO] [logging.py:68:log_dist] [Rank 0] step=164, skipped=0, lr=[9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:37,173] [INFO] [timer.py:197:stop] 0/164, RunningAvgSamplesPerSec=32.384029784963616, CurrSamplesPerSec=48.77671935213303, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:29:41,597] [INFO] [logging.py:68:log_dist] [Rank 0] step=165, skipped=0, lr=[9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:41,597] [INFO] [timer.py:197:stop] 0/165, RunningAvgSamplesPerSec=32.451449777269325, CurrSamplesPerSec=48.966040224922175, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      165/     200 | consumed samples:        42240 | consumed tokens:     43253760 | elapsed time per iteration (ms): 4433.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.295711E+00 | moe loss: 8.189809E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2065.87 | backward-compute: 2254.46 | backward-embedding-all-reduce: 0.01 | optimizer: 93.37 | batch-generator: 413.13
 iteration      166/     200 | consumed samples:        42496 | consumed tokens:     43515904 | elapsed time per iteration (ms): 3469.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.262628E+00 | moe loss: 8.252053E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1602.56 | backward-compute: 1698.31 | backward-embedding-all-reduce: 0.01 | optimizer: 162.37 | batch-generator: 280.29
[2022-12-23 16:29:45,096] [INFO] [logging.py:68:log_dist] [Rank 0] step=166, skipped=0, lr=[9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:45,097] [INFO] [timer.py:197:stop] 0/166, RunningAvgSamplesPerSec=32.498677390187694, CurrSamplesPerSec=42.605515828461485, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:29:49,045] [INFO] [logging.py:68:log_dist] [Rank 0] step=167, skipped=0, lr=[9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:49,046] [INFO] [timer.py:197:stop] 0/167, RunningAvgSamplesPerSec=32.550580769231765, CurrSamplesPerSec=44.10188226418467, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      167/     200 | consumed samples:        42752 | consumed tokens:     43778048 | elapsed time per iteration (ms): 3991.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.267306E+00 | moe loss: 8.178061E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2131.52 | backward-compute: 1715.01 | backward-embedding-all-reduce: 0.01 | optimizer: 105.02 | batch-generator: 353.77
[2022-12-23 16:29:52,428] [INFO] [logging.py:68:log_dist] [Rank 0] step=168, skipped=0, lr=[9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:52,428] [INFO] [timer.py:197:stop] 0/168, RunningAvgSamplesPerSec=32.6170545465194, CurrSamplesPerSec=49.19303222524354, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      168/     200 | consumed samples:        43008 | consumed tokens:     44040192 | elapsed time per iteration (ms): 3367.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.254819E+00 | moe loss: 8.151467E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1592.21 | backward-compute: 1678.79 | backward-embedding-all-reduce: 0.01 | optimizer: 89.29 | batch-generator: 299.54
[2022-12-23 16:29:55,786] [INFO] [logging.py:68:log_dist] [Rank 0] step=169, skipped=0, lr=[9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:55,787] [INFO] [timer.py:197:stop] 0/169, RunningAvgSamplesPerSec=32.68327420002375, CurrSamplesPerSec=49.29721375306093, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      169/     200 | consumed samples:        43264 | consumed tokens:     44302336 | elapsed time per iteration (ms): 3358.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.259919E+00 | moe loss: 8.213416E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1567.67 | backward-compute: 1693.78 | backward-embedding-all-reduce: 0.01 | optimizer: 90.70 | batch-generator: 287.30
[2022-12-23 16:29:59,585] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:29:59,585] [INFO] [timer.py:197:stop] 0/170, RunningAvgSamplesPerSec=32.74952956001909, CurrSamplesPerSec=49.511088354921796, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      170/     200 | consumed samples:        43520 | consumed tokens:     44564480 | elapsed time per iteration (ms): 3799.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.270633E+00 | moe loss: 8.176015E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1888.27 | backward-compute: 1806.66 | backward-embedding-all-reduce: 0.01 | optimizer: 91.07 | batch-generator: 350.30
[2022-12-23 16:30:02,909] [INFO] [logging.py:68:log_dist] [Rank 0] step=171, skipped=0, lr=[9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:02,910] [INFO] [timer.py:197:stop] 0/171, RunningAvgSamplesPerSec=32.815825724793065, CurrSamplesPerSec=49.72765589154134, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      171/     200 | consumed samples:        43776 | consumed tokens:     44826624 | elapsed time per iteration (ms): 3324.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.264196E+00 | moe loss: 8.169556E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1571.62 | backward-compute: 1657.44 | backward-embedding-all-reduce: 0.01 | optimizer: 89.11 | batch-generator: 310.75
 iteration      172/     200 | consumed samples:        44032 | consumed tokens:     45088768 | elapsed time per iteration (ms): 3721.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.262530E+00 | moe loss: 8.147253E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1899.56 | backward-compute: 1719.08 | backward-embedding-all-reduce: 0.01 | optimizer: 94.28 | batch-generator: 292.68
[2022-12-23 16:30:06,663] [INFO] [logging.py:68:log_dist] [Rank 0] step=172, skipped=0, lr=[9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:06,664] [INFO] [timer.py:197:stop] 0/172, RunningAvgSamplesPerSec=32.86889417478612, CurrSamplesPerSec=45.230353848756465, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:30:10,459] [INFO] [logging.py:68:log_dist] [Rank 0] step=173, skipped=0, lr=[9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:10,459] [INFO] [timer.py:197:stop] 0/173, RunningAvgSamplesPerSec=32.92630529795866, CurrSamplesPerSec=46.832450252327966, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      173/     200 | consumed samples:        44288 | consumed tokens:     45350912 | elapsed time per iteration (ms): 3844.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.242136E+00 | moe loss: 8.163673E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1790.09 | backward-compute: 1880.41 | backward-embedding-all-reduce: 0.01 | optimizer: 110.02 | batch-generator: 371.87
[2022-12-23 16:30:13,923] [INFO] [logging.py:68:log_dist] [Rank 0] step=174, skipped=0, lr=[9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:13,924] [INFO] [timer.py:197:stop] 0/174, RunningAvgSamplesPerSec=32.98711965992775, CurrSamplesPerSec=48.21510147543151, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      174/     200 | consumed samples:        44544 | consumed tokens:     45613056 | elapsed time per iteration (ms): 3448.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.240655E+00 | moe loss: 8.159567E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1625.28 | backward-compute: 1723.49 | backward-embedding-all-reduce: 0.01 | optimizer: 88.94 | batch-generator: 292.09
[2022-12-23 16:30:17,340] [INFO] [logging.py:68:log_dist] [Rank 0] step=175, skipped=0, lr=[9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:17,341] [INFO] [timer.py:197:stop] 0/175, RunningAvgSamplesPerSec=33.05127401948609, CurrSamplesPerSec=49.66463334987369, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      175/     200 | consumed samples:        44800 | consumed tokens:     45875200 | elapsed time per iteration (ms): 3418.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.244213E+00 | moe loss: 8.182859E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1594.44 | backward-compute: 1727.40 | backward-embedding-all-reduce: 0.01 | optimizer: 89.30 | batch-generator: 277.00
[2022-12-23 16:30:20,749] [INFO] [logging.py:68:log_dist] [Rank 0] step=176, skipped=0, lr=[9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:20,749] [INFO] [timer.py:197:stop] 0/176, RunningAvgSamplesPerSec=33.11429947368762, CurrSamplesPerSec=49.41647950775611, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      176/     200 | consumed samples:        45056 | consumed tokens:     46137344 | elapsed time per iteration (ms): 3412.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.233699E+00 | moe loss: 8.203446E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1641.81 | backward-compute: 1670.60 | backward-embedding-all-reduce: 0.01 | optimizer: 90.72 | batch-generator: 347.49
[2022-12-23 16:30:24,192] [INFO] [logging.py:68:log_dist] [Rank 0] step=177, skipped=0, lr=[9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:24,193] [INFO] [timer.py:197:stop] 0/177, RunningAvgSamplesPerSec=33.17546876250647, CurrSamplesPerSec=48.889238248189756, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      177/     200 | consumed samples:        45312 | consumed tokens:     46399488 | elapsed time per iteration (ms): 3445.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.235391E+00 | moe loss: 8.155309E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1632.39 | backward-compute: 1717.73 | backward-embedding-all-reduce: 0.01 | optimizer: 89.22 | batch-generator: 276.85
[2022-12-23 16:30:28,518] [INFO] [logging.py:68:log_dist] [Rank 0] step=178, skipped=0, lr=[9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:28,521] [INFO] [timer.py:197:stop] 0/178, RunningAvgSamplesPerSec=33.15433979114147, CurrSamplesPerSec=29.829675050617073, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      178/     200 | consumed samples:        45568 | consumed tokens:     46661632 | elapsed time per iteration (ms): 4372.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.230398E+00 | moe loss: 8.178522E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2059.34 | backward-compute: 2131.02 | backward-embedding-all-reduce: 0.01 | optimizer: 162.15 | batch-generator: 283.21
[2022-12-23 16:30:32,105] [INFO] [logging.py:68:log_dist] [Rank 0] step=179, skipped=0, lr=[9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:32,105] [INFO] [timer.py:197:stop] 0/179, RunningAvgSamplesPerSec=33.21737580380793, CurrSamplesPerSec=49.922941359822474, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      179/     200 | consumed samples:        45824 | consumed tokens:     46923776 | elapsed time per iteration (ms): 3547.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.226761E+00 | moe loss: 8.202366E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1771.96 | backward-compute: 1667.45 | backward-embedding-all-reduce: 0.01 | optimizer: 89.30 | batch-generator: 374.63
[2022-12-23 16:30:35,412] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:35,412] [INFO] [timer.py:197:stop] 0/180, RunningAvgSamplesPerSec=33.27591265779013, CurrSamplesPerSec=48.36023108957568, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      180/     200 | consumed samples:        46080 | consumed tokens:     47185920 | elapsed time per iteration (ms): 3308.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.215743E+00 | moe loss: 8.135533E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1552.90 | backward-compute: 1658.22 | backward-embedding-all-reduce: 0.01 | optimizer: 89.38 | batch-generator: 269.53
[2022-12-23 16:30:39,348] [INFO] [logging.py:68:log_dist] [Rank 0] step=181, skipped=0, lr=[9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:39,349] [INFO] [timer.py:197:stop] 0/181, RunningAvgSamplesPerSec=33.3370288036259, CurrSamplesPerSec=49.52932250474563, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      181/     200 | consumed samples:        46336 | consumed tokens:     47448064 | elapsed time per iteration (ms): 3939.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.220320E+00 | moe loss: 8.468120E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1985.96 | backward-compute: 1854.93 | backward-embedding-all-reduce: 0.01 | optimizer: 91.38 | batch-generator: 364.78
[2022-12-23 16:30:42,906] [INFO] [logging.py:68:log_dist] [Rank 0] step=182, skipped=0, lr=[9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:42,907] [INFO] [timer.py:197:stop] 0/182, RunningAvgSamplesPerSec=33.38481525291409, CurrSamplesPerSec=44.90736085659347, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      182/     200 | consumed samples:        46592 | consumed tokens:     47710208 | elapsed time per iteration (ms): 3557.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.232281E+00 | moe loss: 8.164690E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1673.76 | backward-compute: 1787.30 | backward-embedding-all-reduce: 0.01 | optimizer: 89.25 | batch-generator: 283.86
 iteration      183/     200 | consumed samples:        46848 | consumed tokens:     47972352 | elapsed time per iteration (ms): 3853.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.221376E+00 | moe loss: 8.107757E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2007.02 | backward-compute: 1742.16 | backward-embedding-all-reduce: 0.01 | optimizer: 95.13 | batch-generator: 282.75
[2022-12-23 16:30:46,775] [INFO] [logging.py:68:log_dist] [Rank 0] step=183, skipped=0, lr=[9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:46,777] [INFO] [timer.py:197:stop] 0/183, RunningAvgSamplesPerSec=33.44004160816564, CurrSamplesPerSec=47.619272117418866, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:30:50,247] [INFO] [logging.py:68:log_dist] [Rank 0] step=184, skipped=0, lr=[9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:50,248] [INFO] [timer.py:197:stop] 0/184, RunningAvgSamplesPerSec=33.497243462996224, CurrSamplesPerSec=48.51965129751234, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      184/     200 | consumed samples:        47104 | consumed tokens:     48234496 | elapsed time per iteration (ms): 3495.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.213927E+00 | moe loss: 8.116163E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1625.28 | backward-compute: 1745.77 | backward-embedding-all-reduce: 0.01 | optimizer: 99.61 | batch-generator: 336.74
[2022-12-23 16:30:53,643] [INFO] [logging.py:68:log_dist] [Rank 0] step=185, skipped=0, lr=[9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:53,644] [INFO] [timer.py:197:stop] 0/185, RunningAvgSamplesPerSec=33.557270981799675, CurrSamplesPerSec=49.79905950832336, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      185/     200 | consumed samples:        47360 | consumed tokens:     48496640 | elapsed time per iteration (ms): 3389.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.205358E+00 | moe loss: 8.120733E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1603.94 | backward-compute: 1685.23 | backward-embedding-all-reduce: 0.01 | optimizer: 90.61 | batch-generator: 292.90
[2022-12-23 16:30:57,009] [INFO] [logging.py:68:log_dist] [Rank 0] step=186, skipped=0, lr=[9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:30:57,009] [INFO] [timer.py:197:stop] 0/186, RunningAvgSamplesPerSec=33.617989229499, CurrSamplesPerSec=50.26002118730544, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      186/     200 | consumed samples:        47616 | consumed tokens:     48758784 | elapsed time per iteration (ms): 3363.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.210779E+00 | moe loss: 8.139548E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1555.52 | backward-compute: 1710.36 | backward-embedding-all-reduce: 0.01 | optimizer: 91.25 | batch-generator: 278.07
[2022-12-23 16:31:00,890] [INFO] [logging.py:68:log_dist] [Rank 0] step=187, skipped=0, lr=[9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:00,890] [INFO] [timer.py:197:stop] 0/187, RunningAvgSamplesPerSec=33.67612902023029, CurrSamplesPerSec=49.39400166636735, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      187/     200 | consumed samples:        47872 | consumed tokens:     49020928 | elapsed time per iteration (ms): 3881.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.214632E+00 | moe loss: 8.121270E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1856.23 | backward-compute: 1924.54 | backward-embedding-all-reduce: 0.01 | optimizer: 91.02 | batch-generator: 349.12
[2022-12-23 16:31:04,179] [INFO] [logging.py:68:log_dist] [Rank 0] step=188, skipped=0, lr=[9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:04,179] [INFO] [timer.py:197:stop] 0/188, RunningAvgSamplesPerSec=33.73568599640482, CurrSamplesPerSec=50.1404749043086, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      188/     200 | consumed samples:        48128 | consumed tokens:     49283072 | elapsed time per iteration (ms): 3288.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.210283E+00 | moe loss: 8.126376E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1539.25 | backward-compute: 1650.52 | backward-embedding-all-reduce: 0.01 | optimizer: 90.17 | batch-generator: 250.77
[2022-12-23 16:31:08,253] [INFO] [logging.py:68:log_dist] [Rank 0] step=189, skipped=0, lr=[9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:08,261] [INFO] [timer.py:197:stop] 0/189, RunningAvgSamplesPerSec=33.736762921543594, CurrSamplesPerSec=33.93827387755515, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      189/     200 | consumed samples:        48384 | consumed tokens:     49545216 | elapsed time per iteration (ms): 4096.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.210321E+00 | moe loss: 8.103829E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1998.73 | backward-compute: 1891.57 | backward-embedding-all-reduce: 0.01 | optimizer: 169.15 | batch-generator: 276.07
[2022-12-23 16:31:11,867] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:11,868] [INFO] [timer.py:197:stop] 0/190, RunningAvgSamplesPerSec=33.79238028183102, CurrSamplesPerSec=48.852847868593734, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      190/     200 | consumed samples:        48640 | consumed tokens:     49807360 | elapsed time per iteration (ms): 3591.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.195167E+00 | moe loss: 8.094711E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1831.26 | backward-compute: 1650.20 | backward-embedding-all-reduce: 0.01 | optimizer: 91.41 | batch-generator: 380.24
[2022-12-23 16:31:15,235] [INFO] [logging.py:68:log_dist] [Rank 0] step=191, skipped=0, lr=[9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:15,236] [INFO] [timer.py:197:stop] 0/191, RunningAvgSamplesPerSec=33.837719733530236, CurrSamplesPerSec=45.252160825570904, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      191/     200 | consumed samples:        48896 | consumed tokens:     50069504 | elapsed time per iteration (ms): 3364.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.201919E+00 | moe loss: 8.098130E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1548.90 | backward-compute: 1698.85 | backward-embedding-all-reduce: 0.01 | optimizer: 105.43 | batch-generator: 268.66
 iteration      192/     200 | consumed samples:        49152 | consumed tokens:     50331648 | elapsed time per iteration (ms): 3676.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.201332E+00 | moe loss: 8.094044E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1798.49 | backward-compute: 1773.47 | backward-embedding-all-reduce: 0.01 | optimizer: 91.76 | batch-generator: 292.74
[2022-12-23 16:31:18,917] [INFO] [logging.py:68:log_dist] [Rank 0] step=192, skipped=0, lr=[9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:18,917] [INFO] [timer.py:197:stop] 0/192, RunningAvgSamplesPerSec=33.84555449243304, CurrSamplesPerSec=35.39444776131998, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:31:22,242] [INFO] [logging.py:68:log_dist] [Rank 0] step=193, skipped=0, lr=[9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:22,242] [INFO] [timer.py:197:stop] 0/193, RunningAvgSamplesPerSec=33.90001881986575, CurrSamplesPerSec=48.82958288963825, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      193/     200 | consumed samples:        49408 | consumed tokens:     50593792 | elapsed time per iteration (ms): 3332.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.177924E+00 | moe loss: 8.101946E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1570.37 | backward-compute: 1652.30 | backward-embedding-all-reduce: 0.01 | optimizer: 94.26 | batch-generator: 266.13
[2022-12-23 16:31:25,514] [INFO] [logging.py:68:log_dist] [Rank 0] step=194, skipped=0, lr=[9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:25,515] [INFO] [timer.py:197:stop] 0/194, RunningAvgSamplesPerSec=33.948737927021526, CurrSamplesPerSec=46.79318262479962, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      194/     200 | consumed samples:        49664 | consumed tokens:     50855936 | elapsed time per iteration (ms): 3272.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.186220E+00 | moe loss: 8.117462E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1518.10 | backward-compute: 1649.72 | backward-embedding-all-reduce: 0.01 | optimizer: 92.29 | batch-generator: 271.60
[2022-12-23 16:31:29,387] [INFO] [logging.py:68:log_dist] [Rank 0] step=195, skipped=0, lr=[9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:29,388] [INFO] [timer.py:197:stop] 0/195, RunningAvgSamplesPerSec=33.990842362392655, CurrSamplesPerSec=44.61473780503209, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      195/     200 | consumed samples:        49920 | consumed tokens:     51118080 | elapsed time per iteration (ms): 3917.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.182507E+00 | moe loss: 8.130160E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2075.17 | backward-compute: 1688.80 | backward-embedding-all-reduce: 0.01 | optimizer: 143.00 | batch-generator: 361.18
[2022-12-23 16:31:32,670] [INFO] [logging.py:68:log_dist] [Rank 0] step=196, skipped=0, lr=[9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:32,671] [INFO] [timer.py:197:stop] 0/196, RunningAvgSamplesPerSec=34.047665658478586, CurrSamplesPerSec=50.26544212953514, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      196/     200 | consumed samples:        50176 | consumed tokens:     51380224 | elapsed time per iteration (ms): 3238.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.180634E+00 | moe loss: 8.100773E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1515.74 | backward-compute: 1618.79 | backward-embedding-all-reduce: 0.01 | optimizer: 91.36 | batch-generator: 276.40
 iteration      197/     200 | consumed samples:        50432 | consumed tokens:     51642368 | elapsed time per iteration (ms): 3491.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.181886E+00 | moe loss: 8.117689E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1623.35 | backward-compute: 1765.18 | backward-embedding-all-reduce: 0.01 | optimizer: 93.40 | batch-generator: 269.33
[2022-12-23 16:31:36,181] [INFO] [logging.py:68:log_dist] [Rank 0] step=197, skipped=0, lr=[9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:36,182] [INFO] [timer.py:197:stop] 0/197, RunningAvgSamplesPerSec=34.08772248761588, CurrSamplesPerSec=44.168788046121655, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
[2022-12-23 16:31:39,560] [INFO] [logging.py:68:log_dist] [Rank 0] step=198, skipped=0, lr=[9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:39,561] [INFO] [timer.py:197:stop] 0/198, RunningAvgSamplesPerSec=34.140040114309954, CurrSamplesPerSec=48.721706210616226, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      198/     200 | consumed samples:        50688 | consumed tokens:     51904512 | elapsed time per iteration (ms): 3398.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.179622E+00 | moe loss: 8.113398E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1559.45 | backward-compute: 1719.92 | backward-embedding-all-reduce: 0.01 | optimizer: 91.25 | batch-generator: 285.47
[2022-12-23 16:31:42,955] [INFO] [logging.py:68:log_dist] [Rank 0] step=199, skipped=0, lr=[9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:42,956] [INFO] [timer.py:197:stop] 0/199, RunningAvgSamplesPerSec=34.194631351974444, CurrSamplesPerSec=49.80367919683436, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      199/     200 | consumed samples:        50944 | consumed tokens:     52166656 | elapsed time per iteration (ms): 3394.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.179657E+00 | moe loss: 8.116496E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1584.54 | backward-compute: 1709.07 | backward-embedding-all-reduce: 0.01 | optimizer: 92.54 | batch-generator: 257.04
[2022-12-23 16:31:46,276] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 16:31:46,278] [INFO] [timer.py:197:stop] 0/200, RunningAvgSamplesPerSec=34.247325445101914, CurrSamplesPerSec=49.17610781848133, MemAllocated=18.13GB, MaxMemAllocated=39.06GB
 iteration      200/     200 | consumed samples:        51200 | consumed tokens:     52428800 | elapsed time per iteration (ms): 3320.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.182082E+00 | moe loss: 8.112752E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1538.42 | backward-compute: 1676.53 | backward-embedding-all-reduce: 0.01 | optimizer: 91.84 | batch-generator: 278.13
[after training is done] datetime: 2022-12-23 16:31:46 
Loading extension module utils...
Time to load utils op: 0.3774693012237549 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.369143009185791 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3710050582885742 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.36897730827331543 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.37143421173095703 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3778045177459717 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.36903858184814453 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.35407376289367676 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.34819793701171875 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3479578495025635 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.34797000885009766 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.34807538986206055 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.34962892532348633 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.34960389137268066 seconds
