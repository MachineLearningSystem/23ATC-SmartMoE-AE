--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,288] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,288] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,288] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,288] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,288] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,288] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,288] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[2022-12-23 03:47:40,291] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /mnt/cache/zhaishuming/Megatron-DeepSpeed/examples/MoE/ds_config_gpt_gpt-L16_H2560_MoE16B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-2-gpus-32-mp-1-pp-1-ep-16-mlc-0.01-cap-4.8-drop-true.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2560
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 16
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 4.8
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [16]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /mnt/cache/zhaishuming/Auto-Megatron/deepspeed/output/tensorboard/gpt-L16_H2560_MoE16B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-2-gpus-32-mp-1-pp-1-ep-16-mlc-0.01-cap-4.8-drop-true_SH-IDC1-10-140-1-46_2022.12.23-03.47.19
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... 200
  train_samples ................................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 16
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,498] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,498] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-23 03:47:40,498] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,498] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,498] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,498] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,498] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:47:40,498] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,837] [INFO] [comm.py:654:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=4, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=5, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=6, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:47:41,834] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=7, world_size=16, master_addr=10.140.1.46, master_port=29500
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.126 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 6.646 seconds
time to initialize megatron (seconds): -26.751
[after megatron is initialized] datetime: 2022-12-23 03:47:49 
building GPT model ...
[2022-12-23 03:47:49,305] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-12-23 03:47:49,306] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-12-23 03:47:49,306] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 184.5 GB, percent = 18.3%
[2022-12-23 03:47:49,353] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:47:49,367] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:47:49,382] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:47:49,396] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:47:49,406] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:47:49,421] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:47:49,436] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:47:49,450] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:47:49,486] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-12-23 03:47:49,486] [INFO] [utils.py:828:see_memory_usage] MA 2.59 GB         Max_MA 2.69 GB         CA 2.7 GB         Max_CA 3 GB 
[2022-12-23 03:47:49,487] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 184.58 GB, percent = 18.3%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1390556160
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-12-23 03:47:49,490] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_16
[2022-12-23 03:47:49,765] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 16
[2022-12-23 03:47:49,776] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [0]
[2022-12-23 03:47:49,786] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [1]
[2022-12-23 03:47:49,786] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [2]
[2022-12-23 03:47:49,796] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [3]
[2022-12-23 03:47:49,807] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [4]
[2022-12-23 03:47:49,817] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [5]
[2022-12-23 03:47:49,827] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [6]
[2022-12-23 03:47:49,837] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [7]
[2022-12-23 03:47:49,847] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [8]
[2022-12-23 03:47:49,858] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [9]
[2022-12-23 03:47:49,868] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [10]
[2022-12-23 03:47:49,878] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [11]
[2022-12-23 03:47:49,888] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [12]
[2022-12-23 03:47:49,899] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [13]
[2022-12-23 03:47:49,909] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [14]
[2022-12-23 03:47:49,909] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [15]
[2022-12-23 03:47:49,919] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_16 with ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[2022-12-23 03:47:52,449] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-12-23 03:47:52,451] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-12-23 03:47:52,451] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-12-23 03:47:52,457] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2022-12-23 03:47:52,457] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2022-12-23 03:47:52,819] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2022-12-23 03:47:52,819] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-12-23 03:47:52,819] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7fbd2755c2e0>
[2022-12-23 03:47:52,819] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:47:52,819] [INFO] [config.py:1020:print] DeepSpeedEngine configuration:
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   amp_enabled .................. False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   amp_params ................... False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   bfloat16_enabled ............. False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   checkpoint_parallel_write_pipeline  False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   checkpoint_tag_validation_enabled  True
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   checkpoint_tag_validation_fail  False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbd2755fa60>
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   communication_data_type ...... None
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   curriculum_enabled ........... False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 424592, 'difficulty_step': 8}}
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   dataloader_drop_last ......... False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   disable_allgather ............ False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   dump_state ................... False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   eigenvalue_enabled ........... False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   eigenvalue_gas_boundary_resolution  1
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   eigenvalue_layer_num ......... 0
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   eigenvalue_max_iter .......... 100
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   eigenvalue_stability ......... 1e-06
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   eigenvalue_tol ............... 0.01
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   eigenvalue_verbose ........... False
[2022-12-23 03:47:52,820] [INFO] [config.py:1024:print]   elasticity_enabled ........... False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   fp16_auto_cast ............... False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   fp16_enabled ................. True
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   fp16_master_weights_and_gradients  False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   global_rank .................. 0
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   grad_accum_dtype ............. None
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   gradient_accumulation_steps .. 8
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   gradient_clipping ............ 1.0
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   gradient_predivide_factor .... 1.0
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   initial_dynamic_scale ........ 2048
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   load_universal_checkpoint .... False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   loss_scale ................... 0
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   memory_breakdown ............. False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fbd20252710>
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   optimizer_legacy_fusion ...... False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   optimizer_name ............... None
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   optimizer_params ............. None
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   pld_enabled .................. False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   pld_params ................... False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   prescale_gradients ........... True
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   scheduler_name ............... None
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   scheduler_params ............. None
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   sparse_attention ............. None
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   sparse_gradients_enabled ..... False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   steps_per_print .............. 1
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   train_batch_size ............. 256
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   train_micro_batch_size_per_gpu  2
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   use_node_local_storage ....... False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   wall_clock_breakdown ......... False
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   world_size ................... 16
[2022-12-23 03:47:52,821] [INFO] [config.py:1024:print]   zero_allow_untested_optimizer  False
[2022-12-23 03:47:52,826] [INFO] [config.py:1024:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-12-23 03:47:52,827] [INFO] [config.py:1024:print]   zero_enabled ................. False
[2022-12-23 03:47:52,827] [INFO] [config.py:1024:print]   zero_optimization_stage ...... 0
[2022-12-23 03:47:52,827] [INFO] [config.py:1009:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 4.245920e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.30289149284362793 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-23 03:47:53 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: 25600000
    test:       25600000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.027247 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.029 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-23 03:47:55 
done with setup ...
training ...
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.27554988861083984 seconds
time (ms) | model-and-optimizer-setup: 3826.06 | train/valid/test-data-iterators-setup: 2123.95
[before the start of training step] datetime: 2022-12-23 03:47:55 
[2022-12-23 03:48:07,350] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18567.935546875 | max allocated: 31070.55419921875 | reserved: 35248.0 | max reserved: 35248.0
 iteration        1/     200 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 12095.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.115835E+01 | moe loss: 1.251762E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 7957.86 | backward-compute: 3629.85 | backward-embedding-all-reduce: 0.01 | optimizer: 504.28 | batch-generator: 1009.12
[2022-12-23 03:48:14,594] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        2/     200 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 7238.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.058015E+01 | moe loss: 1.677023E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3814.27 | backward-compute: 3317.34 | backward-embedding-all-reduce: 0.01 | optimizer: 96.21 | batch-generator: 995.85
[2022-12-23 03:48:21,355] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:48:21,355] [INFO] [timer.py:197:stop] 0/3, RunningAvgSamplesPerSec=32.59837068535458, CurrSamplesPerSec=32.59837068535458, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration        3/     200 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 6764.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.249523E+01 | moe loss: 1.755417E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3528.76 | backward-compute: 3135.15 | backward-embedding-all-reduce: 0.01 | optimizer: 90.48 | batch-generator: 968.74
[2022-12-23 03:48:28,456] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:48:28,457] [INFO] [timer.py:197:stop] 0/4, RunningAvgSamplesPerSec=31.35067179563183, CurrSamplesPerSec=30.194963056550485, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration        4/     200 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 7118.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.055035E+01 | moe loss: 1.605698E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3778.87 | backward-compute: 3185.74 | backward-embedding-all-reduce: 0.01 | optimizer: 141.40 | batch-generator: 968.33
[2022-12-23 03:48:35,831] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:48:35,831] [INFO] [timer.py:197:stop] 0/5, RunningAvgSamplesPerSec=31.239714910470394, CurrSamplesPerSec=31.02014078325448, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration        5/     200 | consumed samples:         1280 | consumed tokens:      1310720 | elapsed time per iteration (ms): 7355.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.008052E+01 | moe loss: 1.423697E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3699.80 | backward-compute: 3547.20 | backward-embedding-all-reduce: 0.01 | optimizer: 96.98 | batch-generator: 958.13
 iteration        6/     200 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 6722.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.643751E+00 | moe loss: 1.340337E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3467.98 | backward-compute: 3152.17 | backward-embedding-all-reduce: 0.01 | optimizer: 92.32 | batch-generator: 917.65
[2022-12-23 03:48:42,574] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:48:42,575] [INFO] [timer.py:197:stop] 0/6, RunningAvgSamplesPerSec=31.23440664734904, CurrSamplesPerSec=31.218492676253028, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:48:49,530] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:48:49,531] [INFO] [timer.py:197:stop] 0/7, RunningAvgSamplesPerSec=31.327865078857133, CurrSamplesPerSec=31.707359411672297, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration        7/     200 | consumed samples:         1792 | consumed tokens:      1835008 | elapsed time per iteration (ms): 6987.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.194788E+00 | moe loss: 1.259907E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3577.77 | backward-compute: 3277.22 | backward-embedding-all-reduce: 0.01 | optimizer: 102.24 | batch-generator: 979.06
 iteration        8/     200 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 7728.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.778703E+00 | moe loss: 1.264254E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4213.50 | backward-compute: 3408.91 | backward-embedding-all-reduce: 0.01 | optimizer: 98.32 | batch-generator: 937.34
[2022-12-23 03:48:57,279] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:48:57,281] [INFO] [timer.py:197:stop] 0/8, RunningAvgSamplesPerSec=30.717050673807297, CurrSamplesPerSec=27.988522467936313, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:49:04,144] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:49:04,144] [INFO] [timer.py:197:stop] 0/9, RunningAvgSamplesPerSec=30.479091652284495, CurrSamplesPerSec=29.125322381459647, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration        9/     200 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 6903.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.448478E+00 | moe loss: 1.282769E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3589.98 | backward-compute: 3171.10 | backward-embedding-all-reduce: 0.01 | optimizer: 123.53 | batch-generator: 853.02
[2022-12-23 03:49:10,835] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:49:10,836] [INFO] [timer.py:197:stop] 0/10, RunningAvgSamplesPerSec=30.67273435818611, CurrSamplesPerSec=32.100334976085776, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       10/     200 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 6664.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.109287E+00 | moe loss: 1.284564E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3438.69 | backward-compute: 3127.44 | backward-embedding-all-reduce: 0.01 | optimizer: 91.09 | batch-generator: 907.58
 iteration       11/     200 | consumed samples:         2816 | consumed tokens:      2883584 | elapsed time per iteration (ms): 7566.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.123278E+00 | moe loss: 1.253546E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4005.12 | backward-compute: 3452.83 | backward-embedding-all-reduce: 0.01 | optimizer: 101.09 | batch-generator: 905.89
[2022-12-23 03:49:18,434] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:49:18,445] [INFO] [timer.py:197:stop] 0/11, RunningAvgSamplesPerSec=30.66013174500749, CurrSamplesPerSec=30.5596824398615, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:49:25,265] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:49:25,266] [INFO] [timer.py:197:stop] 0/12, RunningAvgSamplesPerSec=30.728191949549046, CurrSamplesPerSec=31.354608355109683, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       12/     200 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 6871.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.265158E+00 | moe loss: 1.285881E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3576.82 | backward-compute: 3145.85 | backward-embedding-all-reduce: 0.01 | optimizer: 100.52 | batch-generator: 886.47
[2022-12-23 03:49:31,901] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:49:31,902] [INFO] [timer.py:197:stop] 0/13, RunningAvgSamplesPerSec=30.897171338076387, CurrSamplesPerSec=32.69513040881297, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       13/     200 | consumed samples:         3328 | consumed tokens:      3407872 | elapsed time per iteration (ms): 6627.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.960064E+00 | moe loss: 1.398365E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3412.47 | backward-compute: 3116.17 | backward-embedding-all-reduce: 0.01 | optimizer: 91.48 | batch-generator: 869.72
 iteration       14/     200 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 7338.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.731863E+00 | moe loss: 1.398584E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3980.93 | backward-compute: 3246.61 | backward-embedding-all-reduce: 0.01 | optimizer: 103.28 | batch-generator: 846.71
[2022-12-23 03:49:39,255] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:49:39,266] [INFO] [timer.py:197:stop] 0/14, RunningAvgSamplesPerSec=30.87460031165732, CurrSamplesPerSec=30.628478166513847, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:49:45,916] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:49:45,917] [INFO] [timer.py:197:stop] 0/15, RunningAvgSamplesPerSec=30.969632611575527, CurrSamplesPerSec=32.1574019101178, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       15/     200 | consumed samples:         3840 | consumed tokens:      3932160 | elapsed time per iteration (ms): 6696.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.643004E+00 | moe loss: 1.332891E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3430.58 | backward-compute: 3123.77 | backward-embedding-all-reduce: 0.01 | optimizer: 110.08 | batch-generator: 866.81
 iteration       16/     200 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 7007.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.592388E+00 | moe loss: 1.306320E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3337.50 | backward-compute: 3536.91 | backward-embedding-all-reduce: 0.01 | optimizer: 124.44 | batch-generator: 803.93
[2022-12-23 03:49:52,977] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:49:52,993] [INFO] [timer.py:197:stop] 0/16, RunningAvgSamplesPerSec=30.04458255466323, CurrSamplesPerSec=21.641203666006497, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:50:00,143] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:50:00,144] [INFO] [timer.py:197:stop] 0/17, RunningAvgSamplesPerSec=30.150632612403577, CurrSamplesPerSec=31.718029459841464, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       17/     200 | consumed samples:         4352 | consumed tokens:      4456448 | elapsed time per iteration (ms): 7199.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.537180E+00 | moe loss: 1.262800E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3874.47 | backward-compute: 3169.24 | backward-embedding-all-reduce: 0.01 | optimizer: 100.90 | batch-generator: 794.62
[2022-12-23 03:50:06,871] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:50:06,872] [INFO] [timer.py:197:stop] 0/18, RunningAvgSamplesPerSec=30.22400412203666, CurrSamplesPerSec=31.369052090288868, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       18/     200 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 6732.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.491063E+00 | moe loss: 1.233515E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3457.57 | backward-compute: 3155.93 | backward-embedding-all-reduce: 0.01 | optimizer: 104.27 | batch-generator: 849.36
 iteration       19/     200 | consumed samples:         4864 | consumed tokens:      4980736 | elapsed time per iteration (ms): 7335.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.438990E+00 | moe loss: 1.253547E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3527.44 | backward-compute: 3675.72 | backward-embedding-all-reduce: 0.01 | optimizer: 116.82 | batch-generator: 798.05
[2022-12-23 03:50:14,224] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:50:14,234] [INFO] [timer.py:197:stop] 0/19, RunningAvgSamplesPerSec=29.575458924531606, CurrSamplesPerSec=22.01657094058706, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:50:20,893] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:50:20,894] [INFO] [timer.py:197:stop] 0/20, RunningAvgSamplesPerSec=29.72947126038474, CurrSamplesPerSec=32.61693168676037, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       20/     200 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 6681.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.402234E+00 | moe loss: 1.252678E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3429.10 | backward-compute: 3130.93 | backward-embedding-all-reduce: 0.01 | optimizer: 92.48 | batch-generator: 798.51
[2022-12-23 03:50:27,573] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:50:27,573] [INFO] [timer.py:197:stop] 0/21, RunningAvgSamplesPerSec=29.828121712137765, CurrSamplesPerSec=31.722894806877523, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       21/     200 | consumed samples:         5376 | consumed tokens:      5505024 | elapsed time per iteration (ms): 6687.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.401142E+00 | moe loss: 1.242262E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3426.09 | backward-compute: 3152.67 | backward-embedding-all-reduce: 0.01 | optimizer: 99.61 | batch-generator: 818.65
 iteration       22/     200 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 7541.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.406389E+00 | moe loss: 1.163621E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3682.42 | backward-compute: 3741.07 | backward-embedding-all-reduce: 0.01 | optimizer: 100.84 | batch-generator: 809.13
[2022-12-23 03:50:35,131] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:50:35,140] [INFO] [timer.py:197:stop] 0/22, RunningAvgSamplesPerSec=29.17861662515151, CurrSamplesPerSec=20.639548979315354, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:50:41,672] [INFO] [logging.py:68:log_dist] [Rank 0] step=23, skipped=0, lr=[9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:50:41,673] [INFO] [timer.py:197:stop] 0/23, RunningAvgSamplesPerSec=29.30523045074406, CurrSamplesPerSec=32.090188776325064, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       23/     200 | consumed samples:         5888 | consumed tokens:      6029312 | elapsed time per iteration (ms): 6550.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.367725E+00 | moe loss: 1.179956E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3293.22 | backward-compute: 3134.02 | backward-embedding-all-reduce: 0.01 | optimizer: 98.12 | batch-generator: 753.35
[2022-12-23 03:50:48,386] [INFO] [logging.py:68:log_dist] [Rank 0] step=24, skipped=0, lr=[9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:50:48,387] [INFO] [timer.py:197:stop] 0/24, RunningAvgSamplesPerSec=29.39557275572709, CurrSamplesPerSec=31.430338306882568, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       24/     200 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 6726.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.369552E+00 | moe loss: 1.120538E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3469.98 | backward-compute: 3139.49 | backward-embedding-all-reduce: 0.01 | optimizer: 104.13 | batch-generator: 770.51
 iteration       25/     200 | consumed samples:         6400 | consumed tokens:      6553600 | elapsed time per iteration (ms): 7488.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.369455E+00 | moe loss: 1.078456E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3668.77 | backward-compute: 3704.71 | backward-embedding-all-reduce: 0.01 | optimizer: 99.58 | batch-generator: 781.83
[2022-12-23 03:50:55,908] [INFO] [logging.py:68:log_dist] [Rank 0] step=25, skipped=0, lr=[9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:50:55,919] [INFO] [timer.py:197:stop] 0/25, RunningAvgSamplesPerSec=29.451285611220786, CurrSamplesPerSec=30.732722454226426, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:51:02,427] [INFO] [logging.py:68:log_dist] [Rank 0] step=26, skipped=0, lr=[9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:51:02,427] [INFO] [timer.py:197:stop] 0/26, RunningAvgSamplesPerSec=29.552571188225066, CurrSamplesPerSec=32.09093302225339, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       26/     200 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 6537.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.363584E+00 | moe loss: 1.062029E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3268.84 | backward-compute: 3141.52 | backward-embedding-all-reduce: 0.01 | optimizer: 91.17 | batch-generator: 721.86
[2022-12-23 03:51:09,128] [INFO] [logging.py:68:log_dist] [Rank 0] step=27, skipped=0, lr=[9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:51:09,129] [INFO] [timer.py:197:stop] 0/27, RunningAvgSamplesPerSec=29.661744486976882, CurrSamplesPerSec=32.547430765820884, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       27/     200 | consumed samples:         6912 | consumed tokens:      7077888 | elapsed time per iteration (ms): 6709.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.356497E+00 | moe loss: 1.087932E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3469.90 | backward-compute: 3131.29 | backward-embedding-all-reduce: 0.01 | optimizer: 100.92 | batch-generator: 786.01
 iteration       28/     200 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 7293.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.356384E+00 | moe loss: 1.059017E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3763.68 | backward-compute: 3418.79 | backward-embedding-all-reduce: 0.01 | optimizer: 95.93 | batch-generator: 839.20
[2022-12-23 03:51:16,450] [INFO] [logging.py:68:log_dist] [Rank 0] step=28, skipped=0, lr=[9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:51:16,458] [INFO] [timer.py:197:stop] 0/28, RunningAvgSamplesPerSec=29.645817150262832, CurrSamplesPerSec=29.253119175894604, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:51:22,989] [INFO] [logging.py:68:log_dist] [Rank 0] step=29, skipped=0, lr=[9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:51:22,990] [INFO] [timer.py:197:stop] 0/29, RunningAvgSamplesPerSec=29.74023306536765, CurrSamplesPerSec=32.425192254543276, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       29/     200 | consumed samples:         7424 | consumed tokens:      7602176 | elapsed time per iteration (ms): 6559.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.347465E+00 | moe loss: 1.010727E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3307.19 | backward-compute: 3124.97 | backward-embedding-all-reduce: 0.01 | optimizer: 92.06 | batch-generator: 737.67
[2022-12-23 03:51:29,527] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:51:29,528] [INFO] [timer.py:197:stop] 0/30, RunningAvgSamplesPerSec=29.835316721034808, CurrSamplesPerSec=32.654109227435754, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       30/     200 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 6551.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.329430E+00 | moe loss: 9.993225E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3270.96 | backward-compute: 3167.92 | backward-embedding-all-reduce: 0.01 | optimizer: 105.28 | batch-generator: 734.68
 iteration       31/     200 | consumed samples:         7936 | consumed tokens:      8126464 | elapsed time per iteration (ms): 7247.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.320711E+00 | moe loss: 9.960512E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3619.60 | backward-compute: 3516.08 | backward-embedding-all-reduce: 0.01 | optimizer: 96.05 | batch-generator: 762.13
[2022-12-23 03:51:36,798] [INFO] [logging.py:68:log_dist] [Rank 0] step=31, skipped=0, lr=[9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:51:36,800] [INFO] [timer.py:197:stop] 0/31, RunningAvgSamplesPerSec=29.899483329921264, CurrSamplesPerSec=31.81538703687111, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:51:43,255] [INFO] [logging.py:68:log_dist] [Rank 0] step=32, skipped=0, lr=[9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:51:43,256] [INFO] [timer.py:197:stop] 0/32, RunningAvgSamplesPerSec=29.972527397885397, CurrSamplesPerSec=32.25789060299387, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       32/     200 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 6468.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.291051E+00 | moe loss: 9.846527E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3224.42 | backward-compute: 3134.79 | backward-embedding-all-reduce: 0.01 | optimizer: 90.17 | batch-generator: 679.78
[2022-12-23 03:51:49,860] [INFO] [logging.py:68:log_dist] [Rank 0] step=33, skipped=0, lr=[9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:51:49,860] [INFO] [timer.py:197:stop] 0/33, RunningAvgSamplesPerSec=30.05508109668223, CurrSamplesPerSec=32.76220102642965, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       33/     200 | consumed samples:         8448 | consumed tokens:      8650752 | elapsed time per iteration (ms): 6615.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.291414E+00 | moe loss: 9.762019E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3372.60 | backward-compute: 3130.27 | backward-embedding-all-reduce: 0.01 | optimizer: 102.52 | batch-generator: 724.92
[2022-12-23 03:51:57,217] [INFO] [logging.py:68:log_dist] [Rank 0] step=34, skipped=0, lr=[9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:51:57,219] [INFO] [timer.py:197:stop] 0/34, RunningAvgSamplesPerSec=30.096254764108735, CurrSamplesPerSec=31.43107434176755, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       34/     200 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 7338.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.279044E+00 | moe loss: 9.741821E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3662.60 | backward-compute: 3555.97 | backward-embedding-all-reduce: 0.01 | optimizer: 104.57 | batch-generator: 731.85
[2022-12-23 03:52:03,749] [INFO] [logging.py:68:log_dist] [Rank 0] step=35, skipped=0, lr=[9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:52:03,756] [INFO] [timer.py:197:stop] 0/35, RunningAvgSamplesPerSec=30.120284428333573, CurrSamplesPerSec=30.910025247529656, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       35/     200 | consumed samples:         8960 | consumed tokens:      9175040 | elapsed time per iteration (ms): 6545.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.264681E+00 | moe loss: 9.699286E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3229.58 | backward-compute: 3163.36 | backward-embedding-all-reduce: 0.01 | optimizer: 135.01 | batch-generator: 674.69
[2022-12-23 03:52:10,293] [INFO] [logging.py:68:log_dist] [Rank 0] step=36, skipped=0, lr=[9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:52:10,293] [INFO] [timer.py:197:stop] 0/36, RunningAvgSamplesPerSec=30.193489655288865, CurrSamplesPerSec=32.826295525794585, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       36/     200 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 6538.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.242126E+00 | moe loss: 9.612391E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3279.46 | backward-compute: 3150.51 | backward-embedding-all-reduce: 0.01 | optimizer: 91.25 | batch-generator: 718.77
 iteration       37/     200 | consumed samples:         9472 | consumed tokens:      9699328 | elapsed time per iteration (ms): 7190.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.245780E+00 | moe loss: 9.701395E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3701.96 | backward-compute: 3376.88 | backward-embedding-all-reduce: 0.01 | optimizer: 94.77 | batch-generator: 796.09
[2022-12-23 03:52:17,497] [INFO] [logging.py:68:log_dist] [Rank 0] step=37, skipped=0, lr=[9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:52:17,499] [INFO] [timer.py:197:stop] 0/37, RunningAvgSamplesPerSec=30.219290961491033, CurrSamplesPerSec=31.12355765306501, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:52:24,076] [INFO] [logging.py:68:log_dist] [Rank 0] step=38, skipped=0, lr=[9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:52:24,076] [INFO] [timer.py:197:stop] 0/38, RunningAvgSamplesPerSec=30.201588968050217, CurrSamplesPerSec=29.59482233986068, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       38/     200 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 6621.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.234773E+00 | moe loss: 9.571423E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3254.03 | backward-compute: 3219.41 | backward-embedding-all-reduce: 0.01 | optimizer: 124.82 | batch-generator: 647.33
[2022-12-23 03:52:30,673] [INFO] [logging.py:68:log_dist] [Rank 0] step=39, skipped=0, lr=[9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:52:30,674] [INFO] [timer.py:197:stop] 0/39, RunningAvgSamplesPerSec=30.260169854160235, CurrSamplesPerSec=32.53179502343822, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       39/     200 | consumed samples:         9984 | consumed tokens:     10223616 | elapsed time per iteration (ms): 6568.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.240269E+00 | moe loss: 9.612474E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3321.70 | backward-compute: 3150.98 | backward-embedding-all-reduce: 0.01 | optimizer: 89.62 | batch-generator: 663.42
 iteration       40/     200 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 7523.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.205981E+00 | moe loss: 9.928580E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4113.09 | backward-compute: 3278.69 | backward-embedding-all-reduce: 0.01 | optimizer: 103.20 | batch-generator: 665.90
[2022-12-23 03:52:38,232] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:52:38,239] [INFO] [timer.py:197:stop] 0/40, RunningAvgSamplesPerSec=30.21610879002299, CurrSamplesPerSec=28.671441760869037, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:52:44,849] [INFO] [logging.py:68:log_dist] [Rank 0] step=41, skipped=0, lr=[9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:52:44,850] [INFO] [timer.py:197:stop] 0/41, RunningAvgSamplesPerSec=30.16910212708283, CurrSamplesPerSec=28.485174703327466, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       41/     200 | consumed samples:        10496 | consumed tokens:     10747904 | elapsed time per iteration (ms): 6665.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.196093E+00 | moe loss: 9.551223E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3377.55 | backward-compute: 3135.24 | backward-embedding-all-reduce: 0.01 | optimizer: 104.04 | batch-generator: 737.04
[2022-12-23 03:52:51,281] [INFO] [logging.py:68:log_dist] [Rank 0] step=42, skipped=0, lr=[9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:52:51,281] [INFO] [timer.py:197:stop] 0/42, RunningAvgSamplesPerSec=30.218379556431618, CurrSamplesPerSec=32.2743040059096, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       42/     200 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 6419.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.214693E+00 | moe loss: 9.371705E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3186.47 | backward-compute: 3127.86 | backward-embedding-all-reduce: 0.01 | optimizer: 96.61 | batch-generator: 603.07
 iteration       43/     200 | consumed samples:        11008 | consumed tokens:     11272192 | elapsed time per iteration (ms): 7566.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.180332E+00 | moe loss: 9.300851E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4169.16 | backward-compute: 3274.62 | backward-embedding-all-reduce: 0.01 | optimizer: 102.71 | batch-generator: 627.17
[2022-12-23 03:52:58,860] [INFO] [logging.py:68:log_dist] [Rank 0] step=43, skipped=0, lr=[9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:52:58,864] [INFO] [timer.py:197:stop] 0/43, RunningAvgSamplesPerSec=30.23297970220971, CurrSamplesPerSec=30.828782286127733, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:53:05,504] [INFO] [logging.py:68:log_dist] [Rank 0] step=44, skipped=0, lr=[9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:53:05,504] [INFO] [timer.py:197:stop] 0/44, RunningAvgSamplesPerSec=30.24397265879791, CurrSamplesPerSec=30.70167109860637, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       44/     200 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 6678.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.169090E+00 | moe loss: 9.055360E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3344.18 | backward-compute: 3197.38 | backward-embedding-all-reduce: 0.02 | optimizer: 111.70 | batch-generator: 664.79
[2022-12-23 03:53:11,900] [INFO] [logging.py:68:log_dist] [Rank 0] step=45, skipped=0, lr=[9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:53:11,900] [INFO] [timer.py:197:stop] 0/45, RunningAvgSamplesPerSec=30.29339361719896, CurrSamplesPerSec=32.52566946894924, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       45/     200 | consumed samples:        11520 | consumed tokens:     11796480 | elapsed time per iteration (ms): 6374.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.157461E+00 | moe loss: 9.049228E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3131.08 | backward-compute: 3146.50 | backward-embedding-all-reduce: 0.01 | optimizer: 90.52 | batch-generator: 611.05
 iteration       46/     200 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 7263.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.139811E+00 | moe loss: 9.075782E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3456.28 | backward-compute: 3695.43 | backward-embedding-all-reduce: 0.01 | optimizer: 100.19 | batch-generator: 649.33
[2022-12-23 03:53:19,174] [INFO] [logging.py:68:log_dist] [Rank 0] step=46, skipped=0, lr=[9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:53:19,184] [INFO] [timer.py:197:stop] 0/46, RunningAvgSamplesPerSec=30.292841202810624, CurrSamplesPerSec=30.269106428313528, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:53:25,869] [INFO] [logging.py:68:log_dist] [Rank 0] step=47, skipped=0, lr=[9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:53:25,869] [INFO] [timer.py:197:stop] 0/47, RunningAvgSamplesPerSec=30.344038614785433, CurrSamplesPerSec=32.781813820960686, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       47/     200 | consumed samples:        12032 | consumed tokens:     12320768 | elapsed time per iteration (ms): 6716.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.125135E+00 | moe loss: 8.838765E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3364.99 | backward-compute: 3220.83 | backward-embedding-all-reduce: 0.01 | optimizer: 104.45 | batch-generator: 605.76
[2022-12-23 03:53:32,372] [INFO] [logging.py:68:log_dist] [Rank 0] step=48, skipped=0, lr=[9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:53:32,373] [INFO] [timer.py:197:stop] 0/48, RunningAvgSamplesPerSec=30.394143150935292, CurrSamplesPerSec=32.83385207392268, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       48/     200 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 6491.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.116556E+00 | moe loss: 8.787508E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3237.71 | backward-compute: 3157.38 | backward-embedding-all-reduce: 0.01 | optimizer: 89.64 | batch-generator: 600.73
 iteration       49/     200 | consumed samples:        12544 | consumed tokens:     12845056 | elapsed time per iteration (ms): 6640.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.105671E+00 | moe loss: 8.739054E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3292.57 | backward-compute: 3248.32 | backward-embedding-all-reduce: 0.01 | optimizer: 92.08 | batch-generator: 580.78
[2022-12-23 03:53:39,021] [INFO] [logging.py:68:log_dist] [Rank 0] step=49, skipped=0, lr=[9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:53:39,023] [INFO] [timer.py:197:stop] 0/49, RunningAvgSamplesPerSec=30.39920988107424, CurrSamplesPerSec=30.634119664986425, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:53:46,059] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:53:46,062] [INFO] [timer.py:197:stop] 0/50, RunningAvgSamplesPerSec=30.418468075755268, CurrSamplesPerSec=31.35197166641984, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       50/     200 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 7061.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.079702E+00 | moe loss: 8.692472E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3601.86 | backward-compute: 3322.31 | backward-embedding-all-reduce: 0.01 | optimizer: 118.55 | batch-generator: 648.56
[2022-12-23 03:53:52,388] [INFO] [logging.py:68:log_dist] [Rank 0] step=51, skipped=0, lr=[9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:53:52,388] [INFO] [timer.py:197:stop] 0/51, RunningAvgSamplesPerSec=30.45856234039508, CurrSamplesPerSec=32.51578042585685, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       51/     200 | consumed samples:        13056 | consumed tokens:     13369344 | elapsed time per iteration (ms): 6313.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.074531E+00 | moe loss: 8.641499E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3074.79 | backward-compute: 3139.20 | backward-embedding-all-reduce: 0.01 | optimizer: 89.89 | batch-generator: 575.90
 iteration       52/     200 | consumed samples:        13312 | consumed tokens:     13631488 | elapsed time per iteration (ms): 6724.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.051487E+00 | moe loss: 8.650246E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3303.38 | backward-compute: 3316.66 | backward-embedding-all-reduce: 0.01 | optimizer: 93.69 | batch-generator: 573.29
[2022-12-23 03:53:59,122] [INFO] [logging.py:68:log_dist] [Rank 0] step=52, skipped=0, lr=[9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:53:59,124] [INFO] [timer.py:197:stop] 0/52, RunningAvgSamplesPerSec=30.484399908676984, CurrSamplesPerSec=31.806467803759634, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:54:06,269] [INFO] [logging.py:68:log_dist] [Rank 0] step=53, skipped=0, lr=[9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:54:06,275] [INFO] [timer.py:197:stop] 0/53, RunningAvgSamplesPerSec=30.50988688042048, CurrSamplesPerSec=31.84094346820217, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       53/     200 | consumed samples:        13568 | consumed tokens:     13893632 | elapsed time per iteration (ms): 7179.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.059107E+00 | moe loss: 8.595610E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3817.08 | backward-compute: 3215.32 | backward-embedding-all-reduce: 0.01 | optimizer: 123.31 | batch-generator: 619.58
 iteration       54/     200 | consumed samples:        13824 | consumed tokens:     14155776 | elapsed time per iteration (ms): 6391.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.037427E+00 | moe loss: 8.368272E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3089.96 | backward-compute: 3186.60 | backward-embedding-all-reduce: 0.01 | optimizer: 106.70 | batch-generator: 550.40
[2022-12-23 03:54:12,703] [INFO] [logging.py:68:log_dist] [Rank 0] step=54, skipped=0, lr=[9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:54:12,705] [INFO] [timer.py:197:stop] 0/54, RunningAvgSamplesPerSec=30.4939278440331, CurrSamplesPerSec=29.701580149926695, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:54:19,273] [INFO] [logging.py:68:log_dist] [Rank 0] step=55, skipped=0, lr=[9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:54:19,273] [INFO] [timer.py:197:stop] 0/55, RunningAvgSamplesPerSec=30.52825886727688, CurrSamplesPerSec=32.42661801222578, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       55/     200 | consumed samples:        14080 | consumed tokens:     14417920 | elapsed time per iteration (ms): 6592.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.026151E+00 | moe loss: 8.379474E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3302.54 | backward-compute: 3167.79 | backward-embedding-all-reduce: 0.01 | optimizer: 91.66 | batch-generator: 564.71
[2022-12-23 03:54:26,522] [INFO] [logging.py:68:log_dist] [Rank 0] step=56, skipped=0, lr=[9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:54:26,530] [INFO] [timer.py:197:stop] 0/56, RunningAvgSamplesPerSec=30.529564064549596, CurrSamplesPerSec=30.598899588223304, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       56/     200 | consumed samples:        14336 | consumed tokens:     14680064 | elapsed time per iteration (ms): 7251.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.038879E+00 | moe loss: 8.501244E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3895.84 | backward-compute: 3240.13 | backward-embedding-all-reduce: 0.01 | optimizer: 108.18 | batch-generator: 579.68
 iteration       57/     200 | consumed samples:        14592 | consumed tokens:     14942208 | elapsed time per iteration (ms): 6332.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.007875E+00 | moe loss: 8.405092E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3050.57 | backward-compute: 3178.22 | backward-embedding-all-reduce: 0.01 | optimizer: 91.16 | batch-generator: 556.86
[2022-12-23 03:54:32,871] [INFO] [logging.py:68:log_dist] [Rank 0] step=57, skipped=0, lr=[9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:54:32,871] [INFO] [timer.py:197:stop] 0/57, RunningAvgSamplesPerSec=30.54155317512769, CurrSamplesPerSec=31.20325140500702, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:54:39,493] [INFO] [logging.py:68:log_dist] [Rank 0] step=58, skipped=0, lr=[9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:54:39,493] [INFO] [timer.py:197:stop] 0/58, RunningAvgSamplesPerSec=30.53848068624918, CurrSamplesPerSec=30.370440566652956, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       58/     200 | consumed samples:        14848 | consumed tokens:     15204352 | elapsed time per iteration (ms): 6636.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.002636E+00 | moe loss: 8.361590E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3324.49 | backward-compute: 3199.38 | backward-embedding-all-reduce: 0.01 | optimizer: 91.54 | batch-generator: 553.36
[2022-12-23 03:54:46,527] [INFO] [logging.py:68:log_dist] [Rank 0] step=59, skipped=0, lr=[9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:54:46,527] [INFO] [timer.py:197:stop] 0/59, RunningAvgSamplesPerSec=30.569572912397103, CurrSamplesPerSec=32.41789308628783, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       59/     200 | consumed samples:        15104 | consumed tokens:     15466496 | elapsed time per iteration (ms): 7045.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.981173E+00 | moe loss: 8.201458E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3738.27 | backward-compute: 3190.61 | backward-embedding-all-reduce: 0.01 | optimizer: 107.93 | batch-generator: 564.23
 iteration       60/     200 | consumed samples:        15360 | consumed tokens:     15728640 | elapsed time per iteration (ms): 6406.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.971731E+00 | moe loss: 8.211639E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3086.72 | backward-compute: 3214.00 | backward-embedding-all-reduce: 0.01 | optimizer: 97.91 | batch-generator: 547.59
[2022-12-23 03:54:52,964] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:54:52,965] [INFO] [timer.py:197:stop] 0/60, RunningAvgSamplesPerSec=30.54725333467839, CurrSamplesPerSec=29.326759646289954, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:54:59,583] [INFO] [logging.py:68:log_dist] [Rank 0] step=61, skipped=0, lr=[9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:54:59,584] [INFO] [timer.py:197:stop] 0/61, RunningAvgSamplesPerSec=30.571104520130543, CurrSamplesPerSec=32.02122378399837, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       61/     200 | consumed samples:        15616 | consumed tokens:     15990784 | elapsed time per iteration (ms): 6634.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.970887E+00 | moe loss: 8.211216E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3356.19 | backward-compute: 3163.58 | backward-embedding-all-reduce: 0.01 | optimizer: 91.31 | batch-generator: 571.10
[2022-12-23 03:55:06,638] [INFO] [logging.py:68:log_dist] [Rank 0] step=62, skipped=0, lr=[9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:55:06,641] [INFO] [timer.py:197:stop] 0/62, RunningAvgSamplesPerSec=30.559342426742177, CurrSamplesPerSec=29.88104329396102, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       62/     200 | consumed samples:        15872 | consumed tokens:     16252928 | elapsed time per iteration (ms): 7062.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.955186E+00 | moe loss: 8.294201E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3600.76 | backward-compute: 3343.70 | backward-embedding-all-reduce: 0.01 | optimizer: 110.47 | batch-generator: 637.99
 iteration       63/     200 | consumed samples:        16128 | consumed tokens:     16515072 | elapsed time per iteration (ms): 6357.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.924718E+00 | moe loss: 8.210008E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3039.23 | backward-compute: 3212.95 | backward-embedding-all-reduce: 0.01 | optimizer: 97.29 | batch-generator: 494.62
[2022-12-23 03:55:13,015] [INFO] [logging.py:68:log_dist] [Rank 0] step=63, skipped=0, lr=[9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:55:13,017] [INFO] [timer.py:197:stop] 0/63, RunningAvgSamplesPerSec=30.549213699893585, CurrSamplesPerSec=29.953537532047392, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:55:19,689] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:55:19,689] [INFO] [timer.py:197:stop] 0/64, RunningAvgSamplesPerSec=30.564079103466415, CurrSamplesPerSec=31.499063014916427, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       64/     200 | consumed samples:        16384 | consumed tokens:     16777216 | elapsed time per iteration (ms): 6685.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.918988E+00 | moe loss: 8.169727E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3384.49 | backward-compute: 3190.30 | backward-embedding-all-reduce: 0.01 | optimizer: 91.37 | batch-generator: 556.65
[2022-12-23 03:55:26,590] [INFO] [logging.py:68:log_dist] [Rank 0] step=65, skipped=0, lr=[9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:55:26,598] [INFO] [timer.py:197:stop] 0/65, RunningAvgSamplesPerSec=30.581997739691904, CurrSamplesPerSec=31.73553368830506, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       65/     200 | consumed samples:        16640 | consumed tokens:     17039360 | elapsed time per iteration (ms): 6906.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.921507E+00 | moe loss: 8.327783E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3431.90 | backward-compute: 3359.20 | backward-embedding-all-reduce: 0.01 | optimizer: 106.39 | batch-generator: 554.47
 iteration       66/     200 | consumed samples:        16896 | consumed tokens:     17301504 | elapsed time per iteration (ms): 6470.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.893635E+00 | moe loss: 8.221187E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3195.88 | backward-compute: 3168.63 | backward-embedding-all-reduce: 0.01 | optimizer: 91.52 | batch-generator: 539.38
[2022-12-23 03:55:33,076] [INFO] [logging.py:68:log_dist] [Rank 0] step=66, skipped=0, lr=[9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:55:33,077] [INFO] [timer.py:197:stop] 0/66, RunningAvgSamplesPerSec=30.596748756884526, CurrSamplesPerSec=31.55564980770524, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:55:39,508] [INFO] [logging.py:68:log_dist] [Rank 0] step=67, skipped=0, lr=[9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:55:39,508] [INFO] [timer.py:197:stop] 0/67, RunningAvgSamplesPerSec=30.625596002167146, CurrSamplesPerSec=32.5922279503362, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       67/     200 | consumed samples:        17152 | consumed tokens:     17563648 | elapsed time per iteration (ms): 6441.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.889065E+00 | moe loss: 8.121738E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3196.64 | backward-compute: 3136.82 | backward-embedding-all-reduce: 0.01 | optimizer: 91.34 | batch-generator: 521.12
[2022-12-23 03:55:46,707] [INFO] [logging.py:68:log_dist] [Rank 0] step=68, skipped=0, lr=[9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:55:46,708] [INFO] [timer.py:197:stop] 0/68, RunningAvgSamplesPerSec=30.64120514483845, CurrSamplesPerSec=31.691098409586164, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       68/     200 | consumed samples:        17408 | consumed tokens:     17825792 | elapsed time per iteration (ms): 7203.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.876100E+00 | moe loss: 8.107157E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3862.19 | backward-compute: 3225.97 | backward-embedding-all-reduce: 0.01 | optimizer: 107.76 | batch-generator: 544.83
 iteration       69/     200 | consumed samples:        17664 | consumed tokens:     18087936 | elapsed time per iteration (ms): 6360.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.861467E+00 | moe loss: 8.109526E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3048.89 | backward-compute: 3203.16 | backward-embedding-all-reduce: 0.01 | optimizer: 97.13 | batch-generator: 483.71
[2022-12-23 03:55:53,078] [INFO] [logging.py:68:log_dist] [Rank 0] step=69, skipped=0, lr=[9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:55:53,079] [INFO] [timer.py:197:stop] 0/69, RunningAvgSamplesPerSec=30.636397136538537, CurrSamplesPerSec=30.322370525668948, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:55:59,530] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:55:59,530] [INFO] [timer.py:197:stop] 0/70, RunningAvgSamplesPerSec=30.663182383190563, CurrSamplesPerSec=32.571125991868485, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       70/     200 | consumed samples:        17920 | consumed tokens:     18350080 | elapsed time per iteration (ms): 6459.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.852602E+00 | moe loss: 8.091243E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3197.89 | backward-compute: 3147.12 | backward-embedding-all-reduce: 0.01 | optimizer: 93.81 | batch-generator: 511.54
[2022-12-23 03:56:06,784] [INFO] [logging.py:68:log_dist] [Rank 0] step=71, skipped=0, lr=[9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:56:06,785] [INFO] [timer.py:197:stop] 0/71, RunningAvgSamplesPerSec=30.66405170717632, CurrSamplesPerSec=30.72328160051275, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       71/     200 | consumed samples:        18176 | consumed tokens:     18612224 | elapsed time per iteration (ms): 7257.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.838958E+00 | moe loss: 8.102611E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3903.88 | backward-compute: 3230.18 | backward-embedding-all-reduce: 0.01 | optimizer: 116.42 | batch-generator: 550.02
[2022-12-23 03:56:13,093] [INFO] [logging.py:68:log_dist] [Rank 0] step=72, skipped=0, lr=[9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:56:13,094] [INFO] [timer.py:197:stop] 0/72, RunningAvgSamplesPerSec=30.67329218468445, CurrSamplesPerSec=31.324620233311926, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       72/     200 | consumed samples:        18432 | consumed tokens:     18874368 | elapsed time per iteration (ms): 6303.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.845152E+00 | moe loss: 8.136500E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3020.05 | backward-compute: 3176.31 | backward-embedding-all-reduce: 0.01 | optimizer: 92.61 | batch-generator: 510.24
[2022-12-23 03:56:19,362] [INFO] [logging.py:68:log_dist] [Rank 0] step=73, skipped=0, lr=[9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:56:19,363] [INFO] [timer.py:197:stop] 0/73, RunningAvgSamplesPerSec=30.69885944166272, CurrSamplesPerSec=32.60104698205673, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       73/     200 | consumed samples:        18688 | consumed tokens:     19136512 | elapsed time per iteration (ms): 6270.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.846514E+00 | moe loss: 7.986617E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3051.65 | backward-compute: 3115.75 | backward-embedding-all-reduce: 0.01 | optimizer: 92.77 | batch-generator: 495.33
[2022-12-23 03:56:26,058] [INFO] [logging.py:68:log_dist] [Rank 0] step=74, skipped=0, lr=[9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:56:26,058] [INFO] [timer.py:197:stop] 0/74, RunningAvgSamplesPerSec=30.714593334543522, CurrSamplesPerSec=31.874479584860055, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       74/     200 | consumed samples:        18944 | consumed tokens:     19398656 | elapsed time per iteration (ms): 6698.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.825084E+00 | moe loss: 8.143327E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3313.81 | backward-compute: 3275.19 | backward-embedding-all-reduce: 0.01 | optimizer: 100.96 | batch-generator: 533.77
[2022-12-23 03:56:32,328] [INFO] [logging.py:68:log_dist] [Rank 0] step=75, skipped=0, lr=[9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:56:32,329] [INFO] [timer.py:197:stop] 0/75, RunningAvgSamplesPerSec=30.740765429044867, CurrSamplesPerSec=32.750033855938895, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       75/     200 | consumed samples:        19200 | consumed tokens:     19660800 | elapsed time per iteration (ms): 6267.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.815032E+00 | moe loss: 8.149985E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3054.62 | backward-compute: 3116.44 | backward-embedding-all-reduce: 0.01 | optimizer: 89.63 | batch-generator: 472.65
 iteration       76/     200 | consumed samples:        19456 | consumed tokens:     19922944 | elapsed time per iteration (ms): 6329.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.794747E+00 | moe loss: 8.054628E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3089.91 | backward-compute: 3141.33 | backward-embedding-all-reduce: 0.01 | optimizer: 90.85 | batch-generator: 472.99
[2022-12-23 03:56:38,698] [INFO] [logging.py:68:log_dist] [Rank 0] step=76, skipped=0, lr=[9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:56:38,702] [INFO] [timer.py:197:stop] 0/76, RunningAvgSamplesPerSec=30.7490406768044, CurrSamplesPerSec=31.365408757411494, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:56:45,622] [INFO] [logging.py:68:log_dist] [Rank 0] step=77, skipped=0, lr=[9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:56:45,622] [INFO] [timer.py:197:stop] 0/77, RunningAvgSamplesPerSec=30.69496133093605, CurrSamplesPerSec=27.160167974175753, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       77/     200 | consumed samples:        19712 | consumed tokens:     20185088 | elapsed time per iteration (ms): 6966.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.816034E+00 | moe loss: 8.042823E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3417.28 | backward-compute: 3386.92 | backward-embedding-all-reduce: 0.01 | optimizer: 109.47 | batch-generator: 579.67
[2022-12-23 03:56:51,853] [INFO] [logging.py:68:log_dist] [Rank 0] step=78, skipped=0, lr=[9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:56:51,854] [INFO] [timer.py:197:stop] 0/78, RunningAvgSamplesPerSec=30.716283015582608, CurrSamplesPerSec=32.4044702605125, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       78/     200 | consumed samples:        19968 | consumed tokens:     20447232 | elapsed time per iteration (ms): 6229.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.790793E+00 | moe loss: 8.092283E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3013.64 | backward-compute: 3117.63 | backward-embedding-all-reduce: 0.01 | optimizer: 89.81 | batch-generator: 484.97
[2022-12-23 03:56:58,280] [INFO] [logging.py:68:log_dist] [Rank 0] step=79, skipped=0, lr=[9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       79/     200 | consumed samples:        20224 | consumed tokens:     20709376 | elapsed time per iteration (ms): 6423.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.769840E+00 | moe loss: 8.164515E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3131.14 | backward-compute: 3193.43 | backward-embedding-all-reduce: 0.01 | optimizer: 91.52 | batch-generator: 464.42
[2022-12-23 03:56:58,282] [INFO] [timer.py:197:stop] 0/79, RunningAvgSamplesPerSec=30.735606737599202, CurrSamplesPerSec=32.27892231499249, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:57:05,429] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:57:05,430] [INFO] [timer.py:197:stop] 0/80, RunningAvgSamplesPerSec=30.617182955185886, CurrSamplesPerSec=23.611985634127656, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       80/     200 | consumed samples:        20480 | consumed tokens:     20971520 | elapsed time per iteration (ms): 7157.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.777031E+00 | moe loss: 8.102736E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3790.14 | backward-compute: 3248.23 | backward-embedding-all-reduce: 0.01 | optimizer: 106.57 | batch-generator: 447.30
[2022-12-23 03:57:11,670] [INFO] [logging.py:68:log_dist] [Rank 0] step=81, skipped=0, lr=[9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:57:11,670] [INFO] [timer.py:197:stop] 0/81, RunningAvgSamplesPerSec=30.64031086459476, CurrSamplesPerSec=32.558681697960836, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       81/     200 | consumed samples:        20736 | consumed tokens:     21233664 | elapsed time per iteration (ms): 6235.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.764062E+00 | moe loss: 8.016225E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3023.67 | backward-compute: 3114.81 | backward-embedding-all-reduce: 0.01 | optimizer: 89.72 | batch-generator: 468.22
 iteration       82/     200 | consumed samples:        20992 | consumed tokens:     21495808 | elapsed time per iteration (ms): 6341.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.740961E+00 | moe loss: 8.196779E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3113.38 | backward-compute: 3130.89 | backward-embedding-all-reduce: 0.01 | optimizer: 90.83 | batch-generator: 427.21
[2022-12-23 03:57:18,046] [INFO] [logging.py:68:log_dist] [Rank 0] step=82, skipped=0, lr=[9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:57:18,048] [INFO] [timer.py:197:stop] 0/82, RunningAvgSamplesPerSec=30.650422824624947, CurrSamplesPerSec=31.470923136913026, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:57:24,873] [INFO] [logging.py:68:log_dist] [Rank 0] step=83, skipped=0, lr=[9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:57:24,873] [INFO] [timer.py:197:stop] 0/83, RunningAvgSamplesPerSec=30.57225394065421, CurrSamplesPerSec=25.39167021068897, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       83/     200 | consumed samples:        21248 | consumed tokens:     21757952 | elapsed time per iteration (ms): 6871.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.762596E+00 | moe loss: 8.101004E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3289.65 | backward-compute: 3410.42 | backward-embedding-all-reduce: 0.01 | optimizer: 125.34 | batch-generator: 491.51
[2022-12-23 03:57:31,206] [INFO] [logging.py:68:log_dist] [Rank 0] step=84, skipped=0, lr=[9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:57:31,207] [INFO] [timer.py:197:stop] 0/84, RunningAvgSamplesPerSec=30.59467089122994, CurrSamplesPerSec=32.52651287681059, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       84/     200 | consumed samples:        21504 | consumed tokens:     22020096 | elapsed time per iteration (ms): 6322.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.719806E+00 | moe loss: 8.182800E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3082.01 | backward-compute: 3134.59 | backward-embedding-all-reduce: 0.01 | optimizer: 93.05 | batch-generator: 503.99
 iteration       85/     200 | consumed samples:        21760 | consumed tokens:     22282240 | elapsed time per iteration (ms): 6381.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.710574E+00 | moe loss: 8.199929E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3126.01 | backward-compute: 3149.05 | backward-embedding-all-reduce: 0.01 | optimizer: 97.45 | batch-generator: 424.85
[2022-12-23 03:57:37,594] [INFO] [logging.py:68:log_dist] [Rank 0] step=85, skipped=0, lr=[9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:57:37,607] [INFO] [timer.py:197:stop] 0/85, RunningAvgSamplesPerSec=30.601230901074278, CurrSamplesPerSec=31.148896193652778, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:57:44,509] [INFO] [logging.py:68:log_dist] [Rank 0] step=86, skipped=0, lr=[9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:57:44,509] [INFO] [timer.py:197:stop] 0/86, RunningAvgSamplesPerSec=30.439810644070324, CurrSamplesPerSec=21.170783382720046, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       86/     200 | consumed samples:        22016 | consumed tokens:     22544384 | elapsed time per iteration (ms): 6937.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.721569E+00 | moe loss: 8.074234E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3386.39 | backward-compute: 3366.70 | backward-embedding-all-reduce: 0.01 | optimizer: 149.65 | batch-generator: 404.10
[2022-12-23 03:57:50,975] [INFO] [logging.py:68:log_dist] [Rank 0] step=87, skipped=0, lr=[9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:57:50,976] [INFO] [timer.py:197:stop] 0/87, RunningAvgSamplesPerSec=30.46293028102068, CurrSamplesPerSec=32.53890103347064, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       87/     200 | consumed samples:        22272 | consumed tokens:     22806528 | elapsed time per iteration (ms): 6456.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.700205E+00 | moe loss: 8.056293E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3179.68 | backward-compute: 3162.01 | backward-embedding-all-reduce: 0.01 | optimizer: 95.97 | batch-generator: 512.14
 iteration       88/     200 | consumed samples:        22528 | consumed tokens:     23068672 | elapsed time per iteration (ms): 6405.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.690630E+00 | moe loss: 8.186642E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3158.43 | backward-compute: 3144.23 | backward-embedding-all-reduce: 0.01 | optimizer: 96.36 | batch-generator: 434.13
[2022-12-23 03:57:57,425] [INFO] [logging.py:68:log_dist] [Rank 0] step=88, skipped=0, lr=[9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:57:57,446] [INFO] [timer.py:197:stop] 0/88, RunningAvgSamplesPerSec=30.463507795175577, CurrSamplesPerSec=30.51267666086275, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       89/     200 | consumed samples:        22784 | consumed tokens:     23330816 | elapsed time per iteration (ms): 6633.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.675155E+00 | moe loss: 8.080369E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[2022-12-23 03:58:04,025] [INFO] [logging.py:68:log_dist] [Rank 0] step=89, skipped=0, lr=[9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
time (ms) | forward-compute: 3300.27 | backward-compute: 3170.67 | backward-embedding-all-reduce: 0.01 | optimizer: 95.15 | batch-generator: 474.57
[2022-12-23 03:58:04,025] [INFO] [timer.py:197:stop] 0/89, RunningAvgSamplesPerSec=30.370690019658724, CurrSamplesPerSec=24.064966109097796, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:58:10,683] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:58:10,684] [INFO] [timer.py:197:stop] 0/90, RunningAvgSamplesPerSec=30.38492942742361, CurrSamplesPerSec=31.677044445378005, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       90/     200 | consumed samples:        23040 | consumed tokens:     23592960 | elapsed time per iteration (ms): 6671.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.676606E+00 | moe loss: 8.099678E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3365.89 | backward-compute: 3184.75 | backward-embedding-all-reduce: 0.01 | optimizer: 107.21 | batch-generator: 473.14
 iteration       91/     200 | consumed samples:        23296 | consumed tokens:     23855104 | elapsed time per iteration (ms): 6434.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.656245E+00 | moe loss: 8.281474E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3053.46 | backward-compute: 3273.03 | backward-embedding-all-reduce: 0.01 | optimizer: 96.89 | batch-generator: 399.44
[2022-12-23 03:58:17,138] [INFO] [logging.py:68:log_dist] [Rank 0] step=91, skipped=0, lr=[9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:58:17,152] [INFO] [timer.py:197:stop] 0/91, RunningAvgSamplesPerSec=30.394902310400322, CurrSamplesPerSec=31.298914826660262, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       92/     200 | consumed samples:        23552 | consumed tokens:     24117248 | elapsed time per iteration (ms): 6334.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.646682E+00 | moe loss: 8.163148E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2943.80 | backward-compute: 3265.24 | backward-embedding-all-reduce: 0.01 | optimizer: 93.55 | batch-generator: 385.75
[2022-12-23 03:58:23,474] [INFO] [logging.py:68:log_dist] [Rank 0] step=92, skipped=0, lr=[9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:58:23,474] [INFO] [timer.py:197:stop] 0/92, RunningAvgSamplesPerSec=30.366988011860546, CurrSamplesPerSec=28.072444950137207, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:58:30,193] [INFO] [logging.py:68:log_dist] [Rank 0] step=93, skipped=0, lr=[9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:58:30,194] [INFO] [timer.py:197:stop] 0/93, RunningAvgSamplesPerSec=30.381180465856932, CurrSamplesPerSec=31.715211389880874, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       93/     200 | consumed samples:        23808 | consumed tokens:     24379392 | elapsed time per iteration (ms): 6747.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.629041E+00 | moe loss: 8.194395E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3200.65 | backward-compute: 3407.82 | backward-embedding-all-reduce: 0.01 | optimizer: 114.73 | batch-generator: 493.35
 iteration       94/     200 | consumed samples:        24064 | consumed tokens:     24641536 | elapsed time per iteration (ms): 6374.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.619081E+00 | moe loss: 8.141039E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3035.24 | backward-compute: 3230.22 | backward-embedding-all-reduce: 0.01 | optimizer: 96.46 | batch-generator: 414.49
[2022-12-23 03:58:36,615] [INFO] [logging.py:68:log_dist] [Rank 0] step=94, skipped=0, lr=[9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:58:36,618] [INFO] [timer.py:197:stop] 0/94, RunningAvgSamplesPerSec=30.391164429547764, CurrSamplesPerSec=31.328020099568864, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:58:42,767] [INFO] [logging.py:68:log_dist] [Rank 0] step=95, skipped=0, lr=[9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:58:42,767] [INFO] [timer.py:197:stop] 0/95, RunningAvgSamplesPerSec=30.417312025201817, CurrSamplesPerSec=33.03191662683836, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       95/     200 | consumed samples:        24320 | consumed tokens:     24903680 | elapsed time per iteration (ms): 6184.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.618183E+00 | moe loss: 8.275551E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2939.88 | backward-compute: 3112.07 | backward-embedding-all-reduce: 0.01 | optimizer: 89.82 | batch-generator: 408.91
[2022-12-23 03:58:49,521] [INFO] [logging.py:68:log_dist] [Rank 0] step=96, skipped=0, lr=[9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:58:49,522] [INFO] [timer.py:197:stop] 0/96, RunningAvgSamplesPerSec=30.42045829619547, CurrSamplesPerSec=30.715934131948934, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       96/     200 | consumed samples:        24576 | consumed tokens:     25165824 | elapsed time per iteration (ms): 6759.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.607555E+00 | moe loss: 8.212252E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3281.90 | backward-compute: 3352.89 | backward-embedding-all-reduce: 0.01 | optimizer: 114.25 | batch-generator: 463.20
[2022-12-23 03:58:55,835] [INFO] [logging.py:68:log_dist] [Rank 0] step=97, skipped=0, lr=[9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:58:55,837] [INFO] [timer.py:197:stop] 0/97, RunningAvgSamplesPerSec=30.438289449129776, CurrSamplesPerSec=32.21319534530408, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       97/     200 | consumed samples:        24832 | consumed tokens:     25427968 | elapsed time per iteration (ms): 6307.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.607607E+00 | moe loss: 8.178114E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3035.84 | backward-compute: 3168.81 | backward-embedding-all-reduce: 0.01 | optimizer: 91.07 | batch-generator: 413.96
[2022-12-23 03:59:02,048] [INFO] [logging.py:68:log_dist] [Rank 0] step=98, skipped=0, lr=[9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:59:02,049] [INFO] [timer.py:197:stop] 0/98, RunningAvgSamplesPerSec=30.460892673694325, CurrSamplesPerSec=32.772896731640564, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       98/     200 | consumed samples:        25088 | consumed tokens:     25690112 | elapsed time per iteration (ms): 6214.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.579611E+00 | moe loss: 8.232003E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2985.97 | backward-compute: 3129.01 | backward-embedding-all-reduce: 0.01 | optimizer: 90.06 | batch-generator: 439.72
[2022-12-23 03:59:09,013] [INFO] [logging.py:68:log_dist] [Rank 0] step=99, skipped=0, lr=[9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:59:09,018] [INFO] [timer.py:197:stop] 0/99, RunningAvgSamplesPerSec=30.465110982333623, CurrSamplesPerSec=30.87558162993717, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration       99/     200 | consumed samples:        25344 | consumed tokens:     25952256 | elapsed time per iteration (ms): 6961.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.570577E+00 | moe loss: 8.156683E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3462.62 | backward-compute: 3370.28 | backward-embedding-all-reduce: 0.01 | optimizer: 118.91 | batch-generator: 480.76
 iteration      100/     200 | consumed samples:        25600 | consumed tokens:     26214400 | elapsed time per iteration (ms): 6421.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.579503E+00 | moe loss: 8.101754E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3154.04 | backward-compute: 3160.02 | backward-embedding-all-reduce: 0.01 | optimizer: 93.67 | batch-generator: 475.77
[2022-12-23 03:59:15,465] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:59:15,467] [INFO] [timer.py:197:stop] 0/100, RunningAvgSamplesPerSec=30.439229111221927, CurrSamplesPerSec=28.121793606594128, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 03:59:21,643] [INFO] [logging.py:68:log_dist] [Rank 0] step=101, skipped=0, lr=[9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:59:21,644] [INFO] [timer.py:197:stop] 0/101, RunningAvgSamplesPerSec=30.454859384841527, CurrSamplesPerSec=32.068620650849674, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      101/     200 | consumed samples:        25856 | consumed tokens:     26476544 | elapsed time per iteration (ms): 6210.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.573600E+00 | moe loss: 8.144335E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2926.01 | backward-compute: 3146.07 | backward-embedding-all-reduce: 0.01 | optimizer: 96.05 | batch-generator: 429.31
[2022-12-23 03:59:28,851] [INFO] [logging.py:68:log_dist] [Rank 0] step=102, skipped=0, lr=[9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:59:28,855] [INFO] [timer.py:197:stop] 0/102, RunningAvgSamplesPerSec=30.453718026044168, CurrSamplesPerSec=30.341145410061124, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      102/     200 | consumed samples:        26112 | consumed tokens:     26738688 | elapsed time per iteration (ms): 7211.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.560465E+00 | moe loss: 8.216686E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3353.08 | backward-compute: 3730.69 | backward-embedding-all-reduce: 0.01 | optimizer: 116.30 | batch-generator: 478.24
[2022-12-23 03:59:35,139] [INFO] [logging.py:68:log_dist] [Rank 0] step=103, skipped=0, lr=[9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:59:35,141] [INFO] [timer.py:197:stop] 0/103, RunningAvgSamplesPerSec=30.46709856603453, CurrSamplesPerSec=31.867259765775714, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      103/     200 | consumed samples:        26368 | consumed tokens:     27000832 | elapsed time per iteration (ms): 6286.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.548502E+00 | moe loss: 8.187065E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2993.75 | backward-compute: 3182.82 | backward-embedding-all-reduce: 0.01 | optimizer: 91.65 | batch-generator: 390.62
[2022-12-23 03:59:41,306] [INFO] [logging.py:68:log_dist] [Rank 0] step=104, skipped=0, lr=[9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:59:41,306] [INFO] [timer.py:197:stop] 0/104, RunningAvgSamplesPerSec=30.486665141777603, CurrSamplesPerSec=32.601324138480486, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      104/     200 | consumed samples:        26624 | consumed tokens:     27262976 | elapsed time per iteration (ms): 6166.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.555411E+00 | moe loss: 8.216332E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2934.37 | backward-compute: 3133.63 | backward-embedding-all-reduce: 0.01 | optimizer: 89.63 | batch-generator: 415.04
[2022-12-23 03:59:48,465] [INFO] [logging.py:68:log_dist] [Rank 0] step=105, skipped=0, lr=[9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:59:48,472] [INFO] [timer.py:197:stop] 0/105, RunningAvgSamplesPerSec=30.49309060298873, CurrSamplesPerSec=31.163027977545152, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      105/     200 | consumed samples:        26880 | consumed tokens:     27525120 | elapsed time per iteration (ms): 7156.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.548418E+00 | moe loss: 8.133177E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3801.07 | backward-compute: 3232.27 | backward-embedding-all-reduce: 0.01 | optimizer: 113.22 | batch-generator: 443.34
 iteration      106/     200 | consumed samples:        27136 | consumed tokens:     27787264 | elapsed time per iteration (ms): 6302.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.550516E+00 | moe loss: 8.271383E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2966.83 | backward-compute: 3189.05 | backward-embedding-all-reduce: 0.01 | optimizer: 117.95 | batch-generator: 386.78
[2022-12-23 03:59:54,783] [INFO] [logging.py:68:log_dist] [Rank 0] step=106, skipped=0, lr=[9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:59:54,785] [INFO] [timer.py:197:stop] 0/106, RunningAvgSamplesPerSec=30.47572369694081, CurrSamplesPerSec=28.787014524583693, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:00:01,141] [INFO] [logging.py:68:log_dist] [Rank 0] step=107, skipped=0, lr=[9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:00:01,142] [INFO] [timer.py:197:stop] 0/107, RunningAvgSamplesPerSec=30.4923396347077, CurrSamplesPerSec=32.32527162767555, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      107/     200 | consumed samples:        27392 | consumed tokens:     28049408 | elapsed time per iteration (ms): 6378.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.545091E+00 | moe loss: 8.205295E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3104.39 | backward-compute: 3153.27 | backward-embedding-all-reduce: 0.01 | optimizer: 91.55 | batch-generator: 456.40
[2022-12-23 04:00:07,995] [INFO] [logging.py:68:log_dist] [Rank 0] step=108, skipped=0, lr=[9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:00:07,996] [INFO] [timer.py:197:stop] 0/108, RunningAvgSamplesPerSec=30.493727716026562, CurrSamplesPerSec=30.640182921929448, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      108/     200 | consumed samples:        27648 | consumed tokens:     28311552 | elapsed time per iteration (ms): 6856.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.528860E+00 | moe loss: 8.183613E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3427.00 | backward-compute: 3314.17 | backward-embedding-all-reduce: 0.01 | optimizer: 107.86 | batch-generator: 534.69
[2022-12-23 04:00:14,256] [INFO] [logging.py:68:log_dist] [Rank 0] step=109, skipped=0, lr=[9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      109/     200 | consumed samples:        27904 | consumed tokens:     28573696 | elapsed time per iteration (ms): 6253.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.526122E+00 | moe loss: 8.292881E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2980.13 | backward-compute: 3173.06 | backward-embedding-all-reduce: 0.01 | optimizer: 92.37 | batch-generator: 378.53
[2022-12-23 04:00:14,258] [INFO] [timer.py:197:stop] 0/109, RunningAvgSamplesPerSec=30.5052075068159, CurrSamplesPerSec=31.773119703502676, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:00:20,455] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:00:20,456] [INFO] [timer.py:197:stop] 0/110, RunningAvgSamplesPerSec=30.523089192847024, CurrSamplesPerSec=32.56566513379154, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      110/     200 | consumed samples:        28160 | consumed tokens:     28835840 | elapsed time per iteration (ms): 6202.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.516355E+00 | moe loss: 8.271746E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2954.44 | backward-compute: 3147.17 | backward-embedding-all-reduce: 0.01 | optimizer: 89.74 | batch-generator: 410.58
[2022-12-23 04:00:27,403] [INFO] [logging.py:68:log_dist] [Rank 0] step=111, skipped=0, lr=[9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:00:27,407] [INFO] [timer.py:197:stop] 0/111, RunningAvgSamplesPerSec=30.509801461373655, CurrSamplesPerSec=29.139764902250306, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      111/     200 | consumed samples:        28416 | consumed tokens:     29097984 | elapsed time per iteration (ms): 6949.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.513313E+00 | moe loss: 8.255560E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3231.65 | backward-compute: 3561.91 | backward-embedding-all-reduce: 0.01 | optimizer: 147.30 | batch-generator: 437.66
[2022-12-23 04:00:33,574] [INFO] [logging.py:68:log_dist] [Rank 0] step=112, skipped=0, lr=[9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:00:33,575] [INFO] [timer.py:197:stop] 0/112, RunningAvgSamplesPerSec=30.519674396639612, CurrSamplesPerSec=31.6355313902286, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      112/     200 | consumed samples:        28672 | consumed tokens:     29360128 | elapsed time per iteration (ms): 6166.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.505045E+00 | moe loss: 8.271892E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2891.26 | backward-compute: 3176.76 | backward-embedding-all-reduce: 0.01 | optimizer: 91.02 | batch-generator: 369.15
[2022-12-23 04:00:39,738] [INFO] [logging.py:68:log_dist] [Rank 0] step=113, skipped=0, lr=[9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:00:39,739] [INFO] [timer.py:197:stop] 0/113, RunningAvgSamplesPerSec=30.537204099198576, CurrSamplesPerSec=32.596700202184564, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      113/     200 | consumed samples:        28928 | consumed tokens:     29622272 | elapsed time per iteration (ms): 6167.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.489242E+00 | moe loss: 8.275575E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2937.06 | backward-compute: 3127.90 | backward-embedding-all-reduce: 0.01 | optimizer: 91.80 | batch-generator: 400.36
[2022-12-23 04:00:46,563] [INFO] [logging.py:68:log_dist] [Rank 0] step=114, skipped=0, lr=[9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:00:46,563] [INFO] [timer.py:197:stop] 0/114, RunningAvgSamplesPerSec=30.531129557381835, CurrSamplesPerSec=29.87155326249087, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      114/     200 | consumed samples:        29184 | consumed tokens:     29884416 | elapsed time per iteration (ms): 6831.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.494070E+00 | moe loss: 8.263445E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3365.40 | backward-compute: 3342.81 | backward-embedding-all-reduce: 0.01 | optimizer: 111.88 | batch-generator: 450.40
[2022-12-23 04:00:52,763] [INFO] [logging.py:68:log_dist] [Rank 0] step=115, skipped=0, lr=[9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:00:52,763] [INFO] [timer.py:197:stop] 0/115, RunningAvgSamplesPerSec=30.546981452196132, CurrSamplesPerSec=32.43298845011332, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      115/     200 | consumed samples:        29440 | consumed tokens:     30146560 | elapsed time per iteration (ms): 6191.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.484571E+00 | moe loss: 8.221814E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2945.73 | backward-compute: 3144.15 | backward-embedding-all-reduce: 0.01 | optimizer: 89.79 | batch-generator: 448.19
[2022-12-23 04:00:59,008] [INFO] [logging.py:68:log_dist] [Rank 0] step=116, skipped=0, lr=[9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:00:59,011] [INFO] [timer.py:197:stop] 0/116, RunningAvgSamplesPerSec=30.562645698471616, CurrSamplesPerSec=32.442544880580314, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      116/     200 | consumed samples:        29696 | consumed tokens:     30408704 | elapsed time per iteration (ms): 6244.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.478877E+00 | moe loss: 8.205792E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3008.83 | backward-compute: 3137.06 | backward-embedding-all-reduce: 0.01 | optimizer: 91.13 | batch-generator: 355.75
 iteration      117/     200 | consumed samples:        29952 | consumed tokens:     30670848 | elapsed time per iteration (ms): 7085.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.480630E+00 | moe loss: 8.215453E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3711.24 | backward-compute: 3259.54 | backward-embedding-all-reduce: 0.02 | optimizer: 103.82 | batch-generator: 385.25
[2022-12-23 04:01:06,095] [INFO] [logging.py:68:log_dist] [Rank 0] step=117, skipped=0, lr=[9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:01:06,101] [INFO] [timer.py:197:stop] 0/117, RunningAvgSamplesPerSec=30.563012443259222, CurrSamplesPerSec=30.60487912327463, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:01:12,214] [INFO] [logging.py:68:log_dist] [Rank 0] step=118, skipped=0, lr=[9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:01:12,215] [INFO] [timer.py:197:stop] 0/118, RunningAvgSamplesPerSec=30.581707346575048, CurrSamplesPerSec=32.895711818686706, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      118/     200 | consumed samples:        30208 | consumed tokens:     30932992 | elapsed time per iteration (ms): 6120.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.481456E+00 | moe loss: 8.272357E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2878.19 | backward-compute: 3137.64 | backward-embedding-all-reduce: 0.01 | optimizer: 91.43 | batch-generator: 385.76
[2022-12-23 04:01:18,604] [INFO] [logging.py:68:log_dist] [Rank 0] step=119, skipped=0, lr=[9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:01:18,606] [INFO] [timer.py:197:stop] 0/119, RunningAvgSamplesPerSec=30.59896487585548, CurrSamplesPerSec=32.74226822231259, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      119/     200 | consumed samples:        30464 | consumed tokens:     31195136 | elapsed time per iteration (ms): 6388.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.466093E+00 | moe loss: 8.244211E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3063.44 | backward-compute: 3227.52 | backward-embedding-all-reduce: 0.01 | optimizer: 91.03 | batch-generator: 406.38
[2022-12-23 04:01:25,092] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:01:25,093] [INFO] [timer.py:197:stop] 0/120, RunningAvgSamplesPerSec=30.587940495552854, CurrSamplesPerSec=29.35070642323814, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      120/     200 | consumed samples:        30720 | consumed tokens:     31457280 | elapsed time per iteration (ms): 6494.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.462124E+00 | moe loss: 8.240950E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3156.11 | backward-compute: 3212.45 | backward-embedding-all-reduce: 0.01 | optimizer: 107.63 | batch-generator: 386.03
[2022-12-23 04:01:31,369] [INFO] [logging.py:68:log_dist] [Rank 0] step=121, skipped=0, lr=[9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:01:31,369] [INFO] [timer.py:197:stop] 0/121, RunningAvgSamplesPerSec=30.598107225857277, CurrSamplesPerSec=31.847168966067077, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      121/     200 | consumed samples:        30976 | consumed tokens:     31719424 | elapsed time per iteration (ms): 6273.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.449804E+00 | moe loss: 8.268588E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3055.32 | backward-compute: 3115.55 | backward-embedding-all-reduce: 0.01 | optimizer: 91.79 | batch-generator: 465.89
[2022-12-23 04:01:37,768] [INFO] [logging.py:68:log_dist] [Rank 0] step=122, skipped=0, lr=[9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:01:37,769] [INFO] [timer.py:197:stop] 0/122, RunningAvgSamplesPerSec=30.61131083590706, CurrSamplesPerSec=32.268306058355606, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      122/     200 | consumed samples:        31232 | consumed tokens:     31981568 | elapsed time per iteration (ms): 6397.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.449151E+00 | moe loss: 8.322626E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3145.17 | backward-compute: 3154.71 | backward-embedding-all-reduce: 0.01 | optimizer: 90.97 | batch-generator: 383.79
 iteration      123/     200 | consumed samples:        31488 | consumed tokens:     32243712 | elapsed time per iteration (ms): 6352.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.439549E+00 | moe loss: 8.341693E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2883.00 | backward-compute: 3365.77 | backward-embedding-all-reduce: 0.01 | optimizer: 95.59 | batch-generator: 357.64
[2022-12-23 04:01:44,142] [INFO] [logging.py:68:log_dist] [Rank 0] step=123, skipped=0, lr=[9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:01:44,144] [INFO] [timer.py:197:stop] 0/123, RunningAvgSamplesPerSec=30.559745982552315, CurrSamplesPerSec=25.4211123430662, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:01:50,712] [INFO] [logging.py:68:log_dist] [Rank 0] step=124, skipped=0, lr=[9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:01:50,712] [INFO] [timer.py:197:stop] 0/124, RunningAvgSamplesPerSec=30.570921890043685, CurrSamplesPerSec=31.9863339637606, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      124/     200 | consumed samples:        31744 | consumed tokens:     32505856 | elapsed time per iteration (ms): 6600.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.428619E+00 | moe loss: 8.262355E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3312.05 | backward-compute: 3149.88 | backward-embedding-all-reduce: 0.01 | optimizer: 103.23 | batch-generator: 449.31
[2022-12-23 04:01:57,166] [INFO] [logging.py:68:log_dist] [Rank 0] step=125, skipped=0, lr=[9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      125/     200 | consumed samples:        32000 | consumed tokens:     32768000 | elapsed time per iteration (ms): 6438.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.431037E+00 | moe loss: 8.229923E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3181.68 | backward-compute: 3152.23 | backward-embedding-all-reduce: 0.01 | optimizer: 97.64 | batch-generator: 350.49
[2022-12-23 04:01:57,172] [INFO] [timer.py:197:stop] 0/125, RunningAvgSamplesPerSec=30.577836258427773, CurrSamplesPerSec=31.44552232266904, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      126/     200 | consumed samples:        32256 | consumed tokens:     33030144 | elapsed time per iteration (ms): 6110.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.436872E+00 | moe loss: 8.277803E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2864.25 | backward-compute: 3136.43 | backward-embedding-all-reduce: 0.01 | optimizer: 89.77 | batch-generator: 343.16
[2022-12-23 04:02:03,390] [INFO] [logging.py:68:log_dist] [Rank 0] step=126, skipped=0, lr=[9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:02:03,391] [INFO] [timer.py:197:stop] 0/126, RunningAvgSamplesPerSec=30.566068068952845, CurrSamplesPerSec=29.184536500096872, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:02:10,206] [INFO] [logging.py:68:log_dist] [Rank 0] step=127, skipped=0, lr=[9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:02:10,207] [INFO] [timer.py:197:stop] 0/127, RunningAvgSamplesPerSec=30.574557479480873, CurrSamplesPerSec=31.665094552176217, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      127/     200 | consumed samples:        32512 | consumed tokens:     33292288 | elapsed time per iteration (ms): 6948.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.432987E+00 | moe loss: 8.267711E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3382.18 | backward-compute: 3283.48 | backward-embedding-all-reduce: 0.01 | optimizer: 107.41 | batch-generator: 433.02
[2022-12-23 04:02:16,673] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:02:16,677] [INFO] [timer.py:197:stop] 0/128, RunningAvgSamplesPerSec=30.58710757018116, CurrSamplesPerSec=32.241393201519806, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      128/     200 | consumed samples:        32768 | consumed tokens:     33554432 | elapsed time per iteration (ms): 6448.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.418822E+00 | moe loss: 8.223371E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3045.92 | backward-compute: 3296.03 | backward-embedding-all-reduce: 0.01 | optimizer: 97.94 | batch-generator: 370.41
[2022-12-23 04:02:22,796] [INFO] [logging.py:68:log_dist] [Rank 0] step=129, skipped=0, lr=[9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:02:22,797] [INFO] [timer.py:197:stop] 0/129, RunningAvgSamplesPerSec=30.60256958402886, CurrSamplesPerSec=32.68436600583322, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      129/     200 | consumed samples:        33024 | consumed tokens:     33816576 | elapsed time per iteration (ms): 6128.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.420966E+00 | moe loss: 8.290645E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2898.16 | backward-compute: 3120.47 | backward-embedding-all-reduce: 0.01 | optimizer: 90.11 | batch-generator: 336.82
[2022-12-23 04:02:29,393] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:02:29,393] [INFO] [timer.py:197:stop] 0/130, RunningAvgSamplesPerSec=30.60727745905327, CurrSamplesPerSec=31.217185698184107, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      130/     200 | consumed samples:        33280 | consumed tokens:     34078720 | elapsed time per iteration (ms): 6593.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.422952E+00 | moe loss: 8.312640E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3163.19 | backward-compute: 3311.81 | backward-embedding-all-reduce: 0.01 | optimizer: 107.48 | batch-generator: 435.07
[2022-12-23 04:02:36,074] [INFO] [logging.py:68:log_dist] [Rank 0] step=131, skipped=0, lr=[9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      131/     200 | consumed samples:        33536 | consumed tokens:     34340864 | elapsed time per iteration (ms): 6684.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.398853E+00 | moe loss: 8.279616E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3376.78 | backward-compute: 3189.16 | backward-embedding-all-reduce: 0.01 | optimizer: 99.81 | batch-generator: 501.33
[2022-12-23 04:02:36,081] [INFO] [timer.py:197:stop] 0/131, RunningAvgSamplesPerSec=30.52356164286335, CurrSamplesPerSec=22.60836186658415, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:02:42,201] [INFO] [logging.py:68:log_dist] [Rank 0] step=132, skipped=0, lr=[9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:02:42,201] [INFO] [timer.py:197:stop] 0/132, RunningAvgSamplesPerSec=30.53758258009326, CurrSamplesPerSec=32.46109367343809, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      132/     200 | consumed samples:        33792 | consumed tokens:     34603008 | elapsed time per iteration (ms): 6126.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.396854E+00 | moe loss: 8.300813E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2901.28 | backward-compute: 3121.97 | backward-embedding-all-reduce: 0.01 | optimizer: 90.13 | batch-generator: 350.62
[2022-12-23 04:02:48,550] [INFO] [logging.py:68:log_dist] [Rank 0] step=133, skipped=0, lr=[9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:02:48,550] [INFO] [timer.py:197:stop] 0/133, RunningAvgSamplesPerSec=30.54711000236338, CurrSamplesPerSec=31.83843582437373, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      133/     200 | consumed samples:        34048 | consumed tokens:     34865152 | elapsed time per iteration (ms): 6351.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.407649E+00 | moe loss: 8.181009E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3028.64 | backward-compute: 3204.27 | backward-embedding-all-reduce: 0.01 | optimizer: 107.50 | batch-generator: 377.54
[2022-12-23 04:02:54,820] [INFO] [logging.py:68:log_dist] [Rank 0] step=134, skipped=0, lr=[9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:02:54,821] [INFO] [timer.py:197:stop] 0/134, RunningAvgSamplesPerSec=30.53634425081941, CurrSamplesPerSec=29.188744462591366, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      134/     200 | consumed samples:        34304 | consumed tokens:     35127296 | elapsed time per iteration (ms): 6298.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.400149E+00 | moe loss: 8.215198E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2895.51 | backward-compute: 3174.56 | backward-embedding-all-reduce: 0.01 | optimizer: 211.64 | batch-generator: 342.95
[2022-12-23 04:03:01,215] [INFO] [logging.py:68:log_dist] [Rank 0] step=135, skipped=0, lr=[9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:01,216] [INFO] [timer.py:197:stop] 0/135, RunningAvgSamplesPerSec=30.55083231866941, CurrSamplesPerSec=32.5919984342366, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      135/     200 | consumed samples:        34560 | consumed tokens:     35389440 | elapsed time per iteration (ms): 6364.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.385807E+00 | moe loss: 8.245154E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3101.81 | backward-compute: 3164.25 | backward-embedding-all-reduce: 0.01 | optimizer: 89.59 | batch-generator: 367.57
[2022-12-23 04:03:07,707] [INFO] [logging.py:68:log_dist] [Rank 0] step=136, skipped=0, lr=[9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:07,708] [INFO] [timer.py:197:stop] 0/136, RunningAvgSamplesPerSec=30.550446079041652, CurrSamplesPerSec=30.499163088070947, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      136/     200 | consumed samples:        34816 | consumed tokens:     35651584 | elapsed time per iteration (ms): 6483.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.387051E+00 | moe loss: 8.205394E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3024.12 | backward-compute: 3317.88 | backward-embedding-all-reduce: 0.01 | optimizer: 130.84 | batch-generator: 375.37
 iteration      137/     200 | consumed samples:        35072 | consumed tokens:     35913728 | elapsed time per iteration (ms): 6242.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.377395E+00 | moe loss: 8.204959E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2884.43 | backward-compute: 3226.03 | backward-embedding-all-reduce: 0.01 | optimizer: 113.68 | batch-generator: 323.93
[2022-12-23 04:03:13,947] [INFO] [logging.py:68:log_dist] [Rank 0] step=137, skipped=0, lr=[9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:13,959] [INFO] [timer.py:197:stop] 0/137, RunningAvgSamplesPerSec=30.532037352109707, CurrSamplesPerSec=28.250939609068745, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:03:20,443] [INFO] [logging.py:68:log_dist] [Rank 0] step=138, skipped=0, lr=[9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:20,443] [INFO] [timer.py:197:stop] 0/138, RunningAvgSamplesPerSec=30.54463809537797, CurrSamplesPerSec=32.34685126197513, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      138/     200 | consumed samples:        35328 | consumed tokens:     36175872 | elapsed time per iteration (ms): 6502.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.371735E+00 | moe loss: 8.225666E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3058.23 | backward-compute: 3323.57 | backward-embedding-all-reduce: 0.01 | optimizer: 94.47 | batch-generator: 394.84
[2022-12-23 04:03:27,148] [INFO] [logging.py:68:log_dist] [Rank 0] step=139, skipped=0, lr=[9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:27,150] [INFO] [timer.py:197:stop] 0/139, RunningAvgSamplesPerSec=30.55404877543365, CurrSamplesPerSec=31.890285297193884, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      139/     200 | consumed samples:        35584 | consumed tokens:     36438016 | elapsed time per iteration (ms): 6707.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.385355E+00 | moe loss: 8.185315E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3401.77 | backward-compute: 3191.43 | backward-embedding-all-reduce: 0.01 | optimizer: 107.36 | batch-generator: 376.97
[2022-12-23 04:03:33,256] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:33,257] [INFO] [timer.py:197:stop] 0/140, RunningAvgSamplesPerSec=30.56486444673391, CurrSamplesPerSec=32.12268382841285, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      140/     200 | consumed samples:        35840 | consumed tokens:     36700160 | elapsed time per iteration (ms): 6104.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.380933E+00 | moe loss: 8.171192E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2865.79 | backward-compute: 3141.11 | backward-embedding-all-reduce: 0.01 | optimizer: 89.84 | batch-generator: 354.03
 iteration      141/     200 | consumed samples:        36096 | consumed tokens:     36962304 | elapsed time per iteration (ms): 6596.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.353686E+00 | moe loss: 8.146399E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3144.07 | backward-compute: 3344.94 | backward-embedding-all-reduce: 0.01 | optimizer: 98.06 | batch-generator: 360.73
[2022-12-23 04:03:39,858] [INFO] [logging.py:68:log_dist] [Rank 0] step=141, skipped=0, lr=[9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:39,864] [INFO] [timer.py:197:stop] 0/141, RunningAvgSamplesPerSec=30.574953935532882, CurrSamplesPerSec=32.034239171554105, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:03:46,612] [INFO] [logging.py:68:log_dist] [Rank 0] step=142, skipped=0, lr=[9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:46,612] [INFO] [timer.py:197:stop] 0/142, RunningAvgSamplesPerSec=30.571679950515428, CurrSamplesPerSec=30.123318271419237, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      142/     200 | consumed samples:        36352 | consumed tokens:     37224448 | elapsed time per iteration (ms): 6759.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.351478E+00 | moe loss: 8.177957E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3203.03 | backward-compute: 3427.45 | backward-embedding-all-reduce: 0.01 | optimizer: 106.38 | batch-generator: 375.86
[2022-12-23 04:03:52,686] [INFO] [logging.py:68:log_dist] [Rank 0] step=143, skipped=0, lr=[9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:52,687] [INFO] [timer.py:197:stop] 0/143, RunningAvgSamplesPerSec=30.58654502244251, CurrSamplesPerSec=32.8207568024383, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      143/     200 | consumed samples:        36608 | consumed tokens:     37486592 | elapsed time per iteration (ms): 6075.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.367054E+00 | moe loss: 8.208916E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2841.92 | backward-compute: 3132.72 | backward-embedding-all-reduce: 0.01 | optimizer: 90.30 | batch-generator: 315.14
 iteration      144/     200 | consumed samples:        36864 | consumed tokens:     37748736 | elapsed time per iteration (ms): 6637.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.348445E+00 | moe loss: 8.228771E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3136.92 | backward-compute: 3391.88 | backward-embedding-all-reduce: 0.01 | optimizer: 97.44 | batch-generator: 380.87
[2022-12-23 04:03:59,327] [INFO] [logging.py:68:log_dist] [Rank 0] step=144, skipped=0, lr=[9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:03:59,337] [INFO] [timer.py:197:stop] 0/144, RunningAvgSamplesPerSec=30.594624189965128, CurrSamplesPerSec=31.778167494281515, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:04:06,015] [INFO] [logging.py:68:log_dist] [Rank 0] step=145, skipped=0, lr=[9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:04:06,016] [INFO] [timer.py:197:stop] 0/145, RunningAvgSamplesPerSec=30.583549332810854, CurrSamplesPerSec=29.088345588052032, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      145/     200 | consumed samples:        37120 | consumed tokens:     38010880 | elapsed time per iteration (ms): 6693.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.344509E+00 | moe loss: 8.231082E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3099.62 | backward-compute: 3466.42 | backward-embedding-all-reduce: 0.01 | optimizer: 106.77 | batch-generator: 377.19
[2022-12-23 04:04:12,127] [INFO] [logging.py:68:log_dist] [Rank 0] step=146, skipped=0, lr=[9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:04:12,127] [INFO] [timer.py:197:stop] 0/146, RunningAvgSamplesPerSec=30.596802062012355, CurrSamplesPerSec=32.61800971363372, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      146/     200 | consumed samples:        37376 | consumed tokens:     38273024 | elapsed time per iteration (ms): 6108.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.357677E+00 | moe loss: 8.191815E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2863.10 | backward-compute: 3137.04 | backward-embedding-all-reduce: 0.01 | optimizer: 90.91 | batch-generator: 331.90
 iteration      147/     200 | consumed samples:        37632 | consumed tokens:     38535168 | elapsed time per iteration (ms): 6898.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.337357E+00 | moe loss: 8.205134E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3260.24 | backward-compute: 3525.22 | backward-embedding-all-reduce: 0.01 | optimizer: 99.01 | batch-generator: 385.19
[2022-12-23 04:04:19,039] [INFO] [logging.py:68:log_dist] [Rank 0] step=147, skipped=0, lr=[9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:04:19,045] [INFO] [timer.py:197:stop] 0/147, RunningAvgSamplesPerSec=30.600684181740952, CurrSamplesPerSec=31.17018554493592, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:04:25,791] [INFO] [logging.py:68:log_dist] [Rank 0] step=148, skipped=0, lr=[9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:04:25,792] [INFO] [timer.py:197:stop] 0/148, RunningAvgSamplesPerSec=30.578591492383104, CurrSamplesPerSec=27.680818386050216, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      148/     200 | consumed samples:        37888 | consumed tokens:     38797312 | elapsed time per iteration (ms): 6781.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.347801E+00 | moe loss: 8.216882E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3421.67 | backward-compute: 3221.45 | backward-embedding-all-reduce: 0.01 | optimizer: 109.77 | batch-generator: 325.15
[2022-12-23 04:04:31,927] [INFO] [logging.py:68:log_dist] [Rank 0] step=149, skipped=0, lr=[9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:04:31,928] [INFO] [timer.py:197:stop] 0/149, RunningAvgSamplesPerSec=30.589641115928128, CurrSamplesPerSec=32.29335236993958, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      149/     200 | consumed samples:        38144 | consumed tokens:     39059456 | elapsed time per iteration (ms): 6121.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.328425E+00 | moe loss: 8.220520E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2906.38 | backward-compute: 3113.54 | backward-embedding-all-reduce: 0.01 | optimizer: 89.66 | batch-generator: 375.84
 iteration      150/     200 | consumed samples:        38400 | consumed tokens:     39321600 | elapsed time per iteration (ms): 6579.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.340942E+00 | moe loss: 8.205155E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3180.12 | backward-compute: 3294.48 | backward-embedding-all-reduce: 0.01 | optimizer: 95.97 | batch-generator: 380.06
[2022-12-23 04:04:38,527] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:04:38,537] [INFO] [timer.py:197:stop] 0/150, RunningAvgSamplesPerSec=30.592394131456786, CurrSamplesPerSec=31.00255009646017, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:04:44,899] [INFO] [logging.py:68:log_dist] [Rank 0] step=151, skipped=0, lr=[9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:04:44,899] [INFO] [timer.py:197:stop] 0/151, RunningAvgSamplesPerSec=30.58758695084464, CurrSamplesPerSec=29.892403343587166, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      151/     200 | consumed samples:        38656 | consumed tokens:     39583744 | elapsed time per iteration (ms): 6482.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.329525E+00 | moe loss: 8.176788E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2949.66 | backward-compute: 3298.19 | backward-embedding-all-reduce: 0.01 | optimizer: 194.53 | batch-generator: 326.34
[2022-12-23 04:04:51,134] [INFO] [logging.py:68:log_dist] [Rank 0] step=152, skipped=0, lr=[9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:04:51,135] [INFO] [timer.py:197:stop] 0/152, RunningAvgSamplesPerSec=30.596061534069214, CurrSamplesPerSec=31.913511066367292, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      152/     200 | consumed samples:        38912 | consumed tokens:     39845888 | elapsed time per iteration (ms): 6144.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.335092E+00 | moe loss: 8.197557E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2881.15 | backward-compute: 3151.37 | backward-embedding-all-reduce: 0.01 | optimizer: 92.78 | batch-generator: 347.95
 iteration      153/     200 | consumed samples:        39168 | consumed tokens:     40108032 | elapsed time per iteration (ms): 6779.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.309830E+00 | moe loss: 8.200388E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3257.56 | backward-compute: 3409.22 | backward-embedding-all-reduce: 0.01 | optimizer: 99.99 | batch-generator: 351.76
[2022-12-23 04:04:57,935] [INFO] [logging.py:68:log_dist] [Rank 0] step=153, skipped=0, lr=[9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:04:57,943] [INFO] [timer.py:197:stop] 0/153, RunningAvgSamplesPerSec=30.5975271945006, CurrSamplesPerSec=30.818978033471602, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      154/     200 | consumed samples:        39424 | consumed tokens:     40370176 | elapsed time per iteration (ms): 6321.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.312778E+00 | moe loss: 8.186869E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2838.12 | backward-compute: 3346.00 | backward-embedding-all-reduce: 0.01 | optimizer: 98.27 | batch-generator: 302.97
[2022-12-23 04:05:04,240] [INFO] [logging.py:68:log_dist] [Rank 0] step=154, skipped=0, lr=[9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:05:04,240] [INFO] [timer.py:197:stop] 0/154, RunningAvgSamplesPerSec=30.56486564994052, CurrSamplesPerSec=26.32210986793812, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:05:10,577] [INFO] [logging.py:68:log_dist] [Rank 0] step=155, skipped=0, lr=[9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:05:10,578] [INFO] [timer.py:197:stop] 0/155, RunningAvgSamplesPerSec=30.573367553842868, CurrSamplesPerSec=31.923082667960546, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      155/     200 | consumed samples:        39680 | consumed tokens:     40632320 | elapsed time per iteration (ms): 6351.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.319372E+00 | moe loss: 8.138135E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3008.93 | backward-compute: 3220.32 | backward-embedding-all-reduce: 0.01 | optimizer: 105.19 | batch-generator: 364.11
 iteration      156/     200 | consumed samples:        39936 | consumed tokens:     40894464 | elapsed time per iteration (ms): 7104.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.306010E+00 | moe loss: 8.149538E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3766.32 | backward-compute: 3215.99 | backward-embedding-all-reduce: 0.01 | optimizer: 100.95 | batch-generator: 341.74
[2022-12-23 04:05:17,706] [INFO] [logging.py:68:log_dist] [Rank 0] step=156, skipped=0, lr=[9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:05:17,716] [INFO] [timer.py:197:stop] 0/156, RunningAvgSamplesPerSec=30.575952973677396, CurrSamplesPerSec=30.976741201817276, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      157/     200 | consumed samples:        40192 | consumed tokens:     41156608 | elapsed time per iteration (ms): 6234.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.303475E+00 | moe loss: 8.211888E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2833.58 | backward-compute: 3250.87 | backward-embedding-all-reduce: 0.01 | optimizer: 119.18 | batch-generator: 310.36
[2022-12-23 04:05:23,938] [INFO] [logging.py:68:log_dist] [Rank 0] step=157, skipped=0, lr=[9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:05:23,939] [INFO] [timer.py:197:stop] 0/157, RunningAvgSamplesPerSec=30.562267328624845, CurrSamplesPerSec=28.59146783096537, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:05:30,491] [INFO] [logging.py:68:log_dist] [Rank 0] step=158, skipped=0, lr=[9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:05:30,491] [INFO] [timer.py:197:stop] 0/158, RunningAvgSamplesPerSec=30.571523478762767, CurrSamplesPerSec=32.07735021593188, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      158/     200 | consumed samples:        40448 | consumed tokens:     41418752 | elapsed time per iteration (ms): 6583.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.304108E+00 | moe loss: 8.144783E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3200.33 | backward-compute: 3247.69 | backward-embedding-all-reduce: 0.01 | optimizer: 109.76 | batch-generator: 430.14
 iteration      159/     200 | consumed samples:        40704 | consumed tokens:     41680896 | elapsed time per iteration (ms): 6478.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.293921E+00 | moe loss: 8.121228E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3034.92 | backward-compute: 3340.13 | backward-embedding-all-reduce: 0.01 | optimizer: 95.10 | batch-generator: 309.67
[2022-12-23 04:05:37,011] [INFO] [logging.py:68:log_dist] [Rank 0] step=159, skipped=0, lr=[9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:05:37,013] [INFO] [timer.py:197:stop] 0/159, RunningAvgSamplesPerSec=30.574599897035938, CurrSamplesPerSec=31.062224321564035, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:05:43,067] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:05:43,068] [INFO] [timer.py:197:stop] 0/160, RunningAvgSamplesPerSec=30.58782119758321, CurrSamplesPerSec=32.81571738592543, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      160/     200 | consumed samples:        40960 | consumed tokens:     41943040 | elapsed time per iteration (ms): 6080.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.292504E+00 | moe loss: 8.129624E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2838.52 | backward-compute: 3115.62 | backward-embedding-all-reduce: 0.01 | optimizer: 89.71 | batch-generator: 292.87
[2022-12-23 04:05:49,553] [INFO] [logging.py:68:log_dist] [Rank 0] step=161, skipped=0, lr=[9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:05:49,553] [INFO] [timer.py:197:stop] 0/161, RunningAvgSamplesPerSec=30.59585606285167, CurrSamplesPerSec=31.920683535621585, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      161/     200 | consumed samples:        41216 | consumed tokens:     42205184 | elapsed time per iteration (ms): 6495.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.303039E+00 | moe loss: 8.117243E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3053.79 | backward-compute: 3331.96 | backward-embedding-all-reduce: 0.01 | optimizer: 101.97 | batch-generator: 338.48
 iteration      162/     200 | consumed samples:        41472 | consumed tokens:     42467328 | elapsed time per iteration (ms): 6609.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.296387E+00 | moe loss: 8.096687E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3165.64 | backward-compute: 3338.52 | backward-embedding-all-reduce: 0.01 | optimizer: 97.56 | batch-generator: 352.48
[2022-12-23 04:05:56,186] [INFO] [logging.py:68:log_dist] [Rank 0] step=162, skipped=0, lr=[9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:05:56,193] [INFO] [timer.py:197:stop] 0/162, RunningAvgSamplesPerSec=30.56091463043126, CurrSamplesPerSec=25.864382165722926, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:06:02,254] [INFO] [logging.py:68:log_dist] [Rank 0] step=163, skipped=0, lr=[9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:06:02,255] [INFO] [timer.py:197:stop] 0/163, RunningAvgSamplesPerSec=30.570180259560395, CurrSamplesPerSec=32.12873542967585, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      163/     200 | consumed samples:        41728 | consumed tokens:     42729472 | elapsed time per iteration (ms): 6081.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.298059E+00 | moe loss: 8.089426E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2819.69 | backward-compute: 3143.41 | backward-embedding-all-reduce: 0.01 | optimizer: 90.91 | batch-generator: 293.57
[2022-12-23 04:06:08,691] [INFO] [logging.py:68:log_dist] [Rank 0] step=164, skipped=0, lr=[9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:06:08,691] [INFO] [timer.py:197:stop] 0/164, RunningAvgSamplesPerSec=30.578118783096212, CurrSamplesPerSec=31.91233493729888, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      164/     200 | consumed samples:        41984 | consumed tokens:     42991616 | elapsed time per iteration (ms): 6449.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.286495E+00 | moe loss: 8.121696E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3054.40 | backward-compute: 3278.80 | backward-embedding-all-reduce: 0.01 | optimizer: 107.47 | batch-generator: 333.96
[2022-12-23 04:06:15,401] [INFO] [logging.py:68:log_dist] [Rank 0] step=165, skipped=0, lr=[9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:06:15,410] [INFO] [timer.py:197:stop] 0/165, RunningAvgSamplesPerSec=30.533212064052588, CurrSamplesPerSec=24.665099359472066, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      165/     200 | consumed samples:        42240 | consumed tokens:     43253760 | elapsed time per iteration (ms): 6715.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.292802E+00 | moe loss: 8.145180E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2992.00 | backward-compute: 3541.84 | backward-embedding-all-reduce: 0.01 | optimizer: 145.95 | batch-generator: 293.16
[2022-12-23 04:06:21,804] [INFO] [logging.py:68:log_dist] [Rank 0] step=166, skipped=0, lr=[9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:06:21,805] [INFO] [timer.py:197:stop] 0/166, RunningAvgSamplesPerSec=30.54367237343613, CurrSamplesPerSec=32.350164766405776, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      166/     200 | consumed samples:        42496 | consumed tokens:     43515904 | elapsed time per iteration (ms): 6384.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.261470E+00 | moe loss: 8.150148E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3124.81 | backward-compute: 3158.28 | backward-embedding-all-reduce: 0.01 | optimizer: 93.34 | batch-generator: 336.45
[2022-12-23 04:06:28,158] [INFO] [logging.py:68:log_dist] [Rank 0] step=167, skipped=0, lr=[9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:06:28,159] [INFO] [timer.py:197:stop] 0/167, RunningAvgSamplesPerSec=30.55290359444289, CurrSamplesPerSec=32.14625716818343, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      167/     200 | consumed samples:        42752 | consumed tokens:     43778048 | elapsed time per iteration (ms): 6369.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.268501E+00 | moe loss: 8.117321E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3076.63 | backward-compute: 3167.55 | backward-embedding-all-reduce: 0.01 | optimizer: 112.85 | batch-generator: 398.54
 iteration      168/     200 | consumed samples:        43008 | consumed tokens:     44040192 | elapsed time per iteration (ms): 6199.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.254560E+00 | moe loss: 8.102476E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2819.63 | backward-compute: 3265.57 | backward-embedding-all-reduce: 0.01 | optimizer: 103.56 | batch-generator: 283.63
[2022-12-23 04:06:34,389] [INFO] [logging.py:68:log_dist] [Rank 0] step=168, skipped=0, lr=[9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:06:34,390] [INFO] [timer.py:197:stop] 0/168, RunningAvgSamplesPerSec=30.535529516401994, CurrSamplesPerSec=27.91620338199027, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:06:40,916] [INFO] [logging.py:68:log_dist] [Rank 0] step=169, skipped=0, lr=[9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:06:40,916] [INFO] [timer.py:197:stop] 0/169, RunningAvgSamplesPerSec=30.543095239369, CurrSamplesPerSec=31.8532003491523, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      169/     200 | consumed samples:        43264 | consumed tokens:     44302336 | elapsed time per iteration (ms): 6543.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.256833E+00 | moe loss: 8.118097E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3083.91 | backward-compute: 3337.02 | backward-embedding-all-reduce: 0.01 | optimizer: 95.77 | batch-generator: 353.85
[2022-12-23 04:06:47,169] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:06:47,170] [INFO] [timer.py:197:stop] 0/170, RunningAvgSamplesPerSec=30.550909858852563, CurrSamplesPerSec=31.9145506848833, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      170/     200 | consumed samples:        43520 | consumed tokens:     44564480 | elapsed time per iteration (ms): 6285.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.269205E+00 | moe loss: 8.130942E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2886.43 | backward-compute: 3263.36 | backward-embedding-all-reduce: 0.01 | optimizer: 128.29 | batch-generator: 328.87
[2022-12-23 04:06:53,274] [INFO] [logging.py:68:log_dist] [Rank 0] step=171, skipped=0, lr=[9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:06:53,274] [INFO] [timer.py:197:stop] 0/171, RunningAvgSamplesPerSec=30.560690773373143, CurrSamplesPerSec=32.29784400661754, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      171/     200 | consumed samples:        43776 | consumed tokens:     44826624 | elapsed time per iteration (ms): 6070.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.260740E+00 | moe loss: 8.085237E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2835.92 | backward-compute: 3131.73 | backward-embedding-all-reduce: 0.01 | optimizer: 90.64 | batch-generator: 320.26
 iteration      172/     200 | consumed samples:        44032 | consumed tokens:     45088768 | elapsed time per iteration (ms): 6783.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.261094E+00 | moe loss: 8.092321E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3122.35 | backward-compute: 3548.78 | backward-embedding-all-reduce: 0.01 | optimizer: 101.78 | batch-generator: 345.21
[2022-12-23 04:07:00,071] [INFO] [logging.py:68:log_dist] [Rank 0] step=172, skipped=0, lr=[9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:00,077] [INFO] [timer.py:197:stop] 0/172, RunningAvgSamplesPerSec=30.565291251269688, CurrSamplesPerSec=31.36318798579356, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:07:06,342] [INFO] [logging.py:68:log_dist] [Rank 0] step=173, skipped=0, lr=[9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:06,342] [INFO] [timer.py:197:stop] 0/173, RunningAvgSamplesPerSec=30.56588755740698, CurrSamplesPerSec=30.66759891164968, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      173/     200 | consumed samples:        44288 | consumed tokens:     45350912 | elapsed time per iteration (ms): 6295.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.238546E+00 | moe loss: 8.080733E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2938.67 | backward-compute: 3225.72 | backward-embedding-all-reduce: 0.01 | optimizer: 102.63 | batch-generator: 353.86
[2022-12-23 04:07:12,378] [INFO] [logging.py:68:log_dist] [Rank 0] step=174, skipped=0, lr=[9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:12,378] [INFO] [timer.py:197:stop] 0/174, RunningAvgSamplesPerSec=30.577065976547484, CurrSamplesPerSec=32.61683657016323, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      174/     200 | consumed samples:        44544 | consumed tokens:     45613056 | elapsed time per iteration (ms): 6024.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.241759E+00 | moe loss: 8.102360E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2807.96 | backward-compute: 3118.31 | backward-embedding-all-reduce: 0.01 | optimizer: 89.56 | batch-generator: 307.48
 iteration      175/     200 | consumed samples:        44800 | consumed tokens:     45875200 | elapsed time per iteration (ms): 6831.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.246161E+00 | moe loss: 8.147394E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3395.46 | backward-compute: 3322.93 | backward-embedding-all-reduce: 0.01 | optimizer: 100.70 | batch-generator: 347.53
[2022-12-23 04:07:19,232] [INFO] [logging.py:68:log_dist] [Rank 0] step=175, skipped=0, lr=[9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:19,250] [INFO] [timer.py:197:stop] 0/175, RunningAvgSamplesPerSec=30.577217822901044, CurrSamplesPerSec=30.603357853068104, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:07:25,689] [INFO] [logging.py:68:log_dist] [Rank 0] step=176, skipped=0, lr=[9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:25,689] [INFO] [timer.py:197:stop] 0/176, RunningAvgSamplesPerSec=30.567792643950156, CurrSamplesPerSec=29.02026263544424, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      176/     200 | consumed samples:        45056 | consumed tokens:     46137344 | elapsed time per iteration (ms): 6492.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.236880E+00 | moe loss: 8.134960E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3056.82 | backward-compute: 3270.29 | backward-embedding-all-reduce: 0.01 | optimizer: 115.32 | batch-generator: 309.05
[2022-12-23 04:07:31,852] [INFO] [logging.py:68:log_dist] [Rank 0] step=177, skipped=0, lr=[9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:31,853] [INFO] [timer.py:197:stop] 0/177, RunningAvgSamplesPerSec=30.58054343747927, CurrSamplesPerSec=32.97381262047241, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      177/     200 | consumed samples:        45312 | consumed tokens:     46399488 | elapsed time per iteration (ms): 6151.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.244151E+00 | moe loss: 8.118033E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2911.16 | backward-compute: 3136.62 | backward-embedding-all-reduce: 0.01 | optimizer: 89.58 | batch-generator: 399.30
 iteration      178/     200 | consumed samples:        45568 | consumed tokens:     46661632 | elapsed time per iteration (ms): 6881.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.242981E+00 | moe loss: 8.115909E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3360.87 | backward-compute: 3405.82 | backward-embedding-all-reduce: 0.01 | optimizer: 99.27 | batch-generator: 443.14
[2022-12-23 04:07:38,743] [INFO] [logging.py:68:log_dist] [Rank 0] step=178, skipped=0, lr=[9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:38,752] [INFO] [timer.py:197:stop] 0/178, RunningAvgSamplesPerSec=30.584380902480078, CurrSamplesPerSec=31.27110211881496, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:07:45,038] [INFO] [logging.py:68:log_dist] [Rank 0] step=179, skipped=0, lr=[9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:45,039] [INFO] [timer.py:197:stop] 0/179, RunningAvgSamplesPerSec=30.573146582241222, CurrSamplesPerSec=28.716652431478536, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      179/     200 | consumed samples:        45824 | consumed tokens:     46923776 | elapsed time per iteration (ms): 6303.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.231884E+00 | moe loss: 8.103948E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3042.86 | backward-compute: 3143.58 | backward-embedding-all-reduce: 0.01 | optimizer: 91.53 | batch-generator: 301.52
[2022-12-23 04:07:51,241] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:51,242] [INFO] [timer.py:197:stop] 0/180, RunningAvgSamplesPerSec=30.580840885034643, CurrSamplesPerSec=32.00658552248619, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      180/     200 | consumed samples:        46080 | consumed tokens:     47185920 | elapsed time per iteration (ms): 6214.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.224385E+00 | moe loss: 8.092950E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2968.54 | backward-compute: 3130.46 | backward-embedding-all-reduce: 0.01 | optimizer: 103.96 | batch-generator: 339.94
 iteration      181/     200 | consumed samples:        46336 | consumed tokens:     47448064 | elapsed time per iteration (ms): 6720.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.228617E+00 | moe loss: 8.536971E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3090.84 | backward-compute: 3511.08 | backward-embedding-all-reduce: 0.01 | optimizer: 107.34 | batch-generator: 314.02
[2022-12-23 04:07:57,986] [INFO] [logging.py:68:log_dist] [Rank 0] step=181, skipped=0, lr=[9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:07:57,997] [INFO] [timer.py:197:stop] 0/181, RunningAvgSamplesPerSec=30.58305109739111, CurrSamplesPerSec=30.981624921632843, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      182/     200 | consumed samples:        46592 | consumed tokens:     47710208 | elapsed time per iteration (ms): 6162.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.238194E+00 | moe loss: 8.075756E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2819.58 | backward-compute: 3191.94 | backward-embedding-all-reduce: 0.01 | optimizer: 112.92 | batch-generator: 295.08
[2022-12-23 04:08:04,145] [INFO] [logging.py:68:log_dist] [Rank 0] step=182, skipped=0, lr=[9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:08:04,145] [INFO] [timer.py:197:stop] 0/182, RunningAvgSamplesPerSec=30.578032330532423, CurrSamplesPerSec=29.7054520231787, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:08:10,527] [INFO] [logging.py:68:log_dist] [Rank 0] step=183, skipped=0, lr=[9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:08:10,528] [INFO] [timer.py:197:stop] 0/183, RunningAvgSamplesPerSec=30.586691583629662, CurrSamplesPerSec=32.22954006692869, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      183/     200 | consumed samples:        46848 | consumed tokens:     47972352 | elapsed time per iteration (ms): 6404.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.228680E+00 | moe loss: 8.082946E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3122.89 | backward-compute: 3148.89 | backward-embedding-all-reduce: 0.01 | optimizer: 106.28 | batch-generator: 352.84
 iteration      184/     200 | consumed samples:        47104 | consumed tokens:     48234496 | elapsed time per iteration (ms): 6938.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.216688E+00 | moe loss: 8.116388E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3168.01 | backward-compute: 3649.24 | backward-embedding-all-reduce: 0.01 | optimizer: 98.40 | batch-generator: 354.71
[2022-12-23 04:08:17,487] [INFO] [logging.py:68:log_dist] [Rank 0] step=184, skipped=0, lr=[9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:08:17,490] [INFO] [timer.py:197:stop] 0/184, RunningAvgSamplesPerSec=30.586130392665908, CurrSamplesPerSec=30.484892892234466, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:08:23,547] [INFO] [logging.py:68:log_dist] [Rank 0] step=185, skipped=0, lr=[9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:08:23,547] [INFO] [timer.py:197:stop] 0/185, RunningAvgSamplesPerSec=30.59676220771592, CurrSamplesPerSec=32.663152556198995, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      185/     200 | consumed samples:        47360 | consumed tokens:     48496640 | elapsed time per iteration (ms): 6070.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.210052E+00 | moe loss: 8.104499E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2831.44 | backward-compute: 3128.51 | backward-embedding-all-reduce: 0.01 | optimizer: 90.15 | batch-generator: 292.20
[2022-12-23 04:08:29,998] [INFO] [logging.py:68:log_dist] [Rank 0] step=186, skipped=0, lr=[9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:08:29,998] [INFO] [timer.py:197:stop] 0/186, RunningAvgSamplesPerSec=30.606004737571105, CurrSamplesPerSec=32.39689890147502, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      186/     200 | consumed samples:        47616 | consumed tokens:     48758784 | elapsed time per iteration (ms): 6459.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.215687E+00 | moe loss: 8.103869E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3072.99 | backward-compute: 3269.18 | backward-embedding-all-reduce: 0.01 | optimizer: 105.50 | batch-generator: 320.20
 iteration      187/     200 | consumed samples:        47872 | consumed tokens:     49020928 | elapsed time per iteration (ms): 6548.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.220792E+00 | moe loss: 8.101232E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3213.21 | backward-compute: 3229.53 | backward-embedding-all-reduce: 0.01 | optimizer: 98.92 | batch-generator: 320.57
[2022-12-23 04:08:36,568] [INFO] [logging.py:68:log_dist] [Rank 0] step=187, skipped=0, lr=[9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:08:36,579] [INFO] [timer.py:197:stop] 0/187, RunningAvgSamplesPerSec=30.601949389017825, CurrSamplesPerSec=29.873621003678956, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:08:42,637] [INFO] [logging.py:68:log_dist] [Rank 0] step=188, skipped=0, lr=[9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:08:42,637] [INFO] [timer.py:197:stop] 0/188, RunningAvgSamplesPerSec=30.611345683810416, CurrSamplesPerSec=32.45491623409236, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      188/     200 | consumed samples:        48128 | consumed tokens:     49283072 | elapsed time per iteration (ms): 6081.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.211679E+00 | moe loss: 8.106121E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2813.15 | backward-compute: 3143.21 | backward-embedding-all-reduce: 0.01 | optimizer: 91.44 | batch-generator: 267.37
[2022-12-23 04:08:49,016] [INFO] [logging.py:68:log_dist] [Rank 0] step=189, skipped=0, lr=[9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:08:49,016] [INFO] [timer.py:197:stop] 0/189, RunningAvgSamplesPerSec=30.615446357787555, CurrSamplesPerSec=31.397766514751492, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      189/     200 | consumed samples:        48384 | consumed tokens:     49545216 | elapsed time per iteration (ms): 6387.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.213351E+00 | moe loss: 8.094840E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2934.01 | backward-compute: 3338.00 | backward-embedding-all-reduce: 0.01 | optimizer: 105.49 | batch-generator: 351.31
[2022-12-23 04:08:55,641] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:08:55,654] [INFO] [timer.py:197:stop] 0/190, RunningAvgSamplesPerSec=30.588368455677653, CurrSamplesPerSec=26.247265472449975, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      190/     200 | consumed samples:        48640 | consumed tokens:     49807360 | elapsed time per iteration (ms): 6625.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.201251E+00 | moe loss: 8.103301E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3011.01 | backward-compute: 3457.77 | backward-embedding-all-reduce: 0.01 | optimizer: 125.77 | batch-generator: 306.11
[2022-12-23 04:09:01,950] [INFO] [logging.py:68:log_dist] [Rank 0] step=191, skipped=0, lr=[9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:09:01,950] [INFO] [timer.py:197:stop] 0/191, RunningAvgSamplesPerSec=30.599364619974004, CurrSamplesPerSec=32.81728200723303, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      191/     200 | consumed samples:        48896 | consumed tokens:     50069504 | elapsed time per iteration (ms): 6299.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.206624E+00 | moe loss: 8.089786E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3031.10 | backward-compute: 3168.47 | backward-embedding-all-reduce: 0.01 | optimizer: 89.64 | batch-generator: 325.85
[2022-12-23 04:09:08,604] [INFO] [logging.py:68:log_dist] [Rank 0] step=192, skipped=0, lr=[9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:09:08,604] [INFO] [timer.py:197:stop] 0/192, RunningAvgSamplesPerSec=30.552269113822113, CurrSamplesPerSec=23.667599843976703, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      192/     200 | consumed samples:        49152 | consumed tokens:     50331648 | elapsed time per iteration (ms): 6655.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.204752E+00 | moe loss: 8.081181E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3216.28 | backward-compute: 3321.39 | backward-embedding-all-reduce: 0.01 | optimizer: 107.53 | batch-generator: 339.80
[2022-12-23 04:09:14,780] [INFO] [logging.py:68:log_dist] [Rank 0] step=193, skipped=0, lr=[9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      193/     200 | consumed samples:        49408 | consumed tokens:     50593792 | elapsed time per iteration (ms): 6170.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.182752E+00 | moe loss: 8.092305E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2930.84 | backward-compute: 3136.17 | backward-embedding-all-reduce: 0.01 | optimizer: 91.78 | batch-generator: 276.41
[2022-12-23 04:09:14,782] [INFO] [timer.py:197:stop] 0/193, RunningAvgSamplesPerSec=30.545284815972906, CurrSamplesPerSec=29.273797740849876, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
[2022-12-23 04:09:21,031] [INFO] [logging.py:68:log_dist] [Rank 0] step=194, skipped=0, lr=[9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:09:21,031] [INFO] [timer.py:197:stop] 0/194, RunningAvgSamplesPerSec=30.554943290589414, CurrSamplesPerSec=32.518908022653626, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      194/     200 | consumed samples:        49664 | consumed tokens:     50855936 | elapsed time per iteration (ms): 6255.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.191811E+00 | moe loss: 8.093660E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2858.20 | backward-compute: 3289.33 | backward-embedding-all-reduce: 0.01 | optimizer: 94.55 | batch-generator: 286.00
[2022-12-23 04:09:27,632] [INFO] [logging.py:68:log_dist] [Rank 0] step=195, skipped=0, lr=[9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:09:27,633] [INFO] [timer.py:197:stop] 0/195, RunningAvgSamplesPerSec=30.554178348891483, CurrSamplesPerSec=30.40801578108033, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      195/     200 | consumed samples:        49920 | consumed tokens:     51118080 | elapsed time per iteration (ms): 6598.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.186118E+00 | moe loss: 8.088695E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3122.93 | backward-compute: 3329.94 | backward-embedding-all-reduce: 0.01 | optimizer: 134.87 | batch-generator: 348.10
[2022-12-23 04:09:33,657] [INFO] [logging.py:68:log_dist] [Rank 0] step=196, skipped=0, lr=[9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:09:33,658] [INFO] [timer.py:197:stop] 0/196, RunningAvgSamplesPerSec=30.564687176118156, CurrSamplesPerSec=32.737843778179915, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      196/     200 | consumed samples:        50176 | consumed tokens:     51380224 | elapsed time per iteration (ms): 6027.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.185569E+00 | moe loss: 8.066245E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2808.88 | backward-compute: 3114.44 | backward-embedding-all-reduce: 0.01 | optimizer: 91.13 | batch-generator: 276.47
[2022-12-23 04:09:40,131] [INFO] [logging.py:68:log_dist] [Rank 0] step=197, skipped=0, lr=[9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:09:40,133] [INFO] [timer.py:197:stop] 0/197, RunningAvgSamplesPerSec=30.57524232255542, CurrSamplesPerSec=32.770736228464386, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      197/     200 | consumed samples:        50432 | consumed tokens:     51642368 | elapsed time per iteration (ms): 6478.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.184929E+00 | moe loss: 8.096315E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3206.72 | backward-compute: 3163.17 | backward-embedding-all-reduce: 0.01 | optimizer: 93.73 | batch-generator: 279.26
[2022-12-23 04:09:47,017] [INFO] [logging.py:68:log_dist] [Rank 0] step=198, skipped=0, lr=[9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:09:47,021] [INFO] [timer.py:197:stop] 0/198, RunningAvgSamplesPerSec=30.568493378772228, CurrSamplesPerSec=29.30703647174166, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      198/     200 | consumed samples:        50688 | consumed tokens:     51904512 | elapsed time per iteration (ms): 6879.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.182812E+00 | moe loss: 8.097609E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3551.88 | backward-compute: 3206.52 | backward-embedding-all-reduce: 0.01 | optimizer: 114.09 | batch-generator: 328.63
[2022-12-23 04:09:53,032] [INFO] [logging.py:68:log_dist] [Rank 0] step=199, skipped=0, lr=[9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:09:53,033] [INFO] [timer.py:197:stop] 0/199, RunningAvgSamplesPerSec=30.578712410148633, CurrSamplesPerSec=32.722798439070246, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      199/     200 | consumed samples:        50944 | consumed tokens:     52166656 | elapsed time per iteration (ms): 6017.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.186770E+00 | moe loss: 8.075105E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2790.35 | backward-compute: 3124.90 | backward-embedding-all-reduce: 0.01 | optimizer: 89.78 | batch-generator: 252.84
[2022-12-23 04:09:59,316] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 04:09:59,317] [INFO] [timer.py:197:stop] 0/200, RunningAvgSamplesPerSec=30.58698727086049, CurrSamplesPerSec=32.30939744673926, MemAllocated=18.13GB, MaxMemAllocated=30.34GB
 iteration      200/     200 | consumed samples:        51200 | consumed tokens:     52428800 | elapsed time per iteration (ms): 6283.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.186922E+00 | moe loss: 8.081277E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3019.03 | backward-compute: 3160.70 | backward-embedding-all-reduce: 0.01 | optimizer: 93.29 | batch-generator: 290.73
[after training is done] datetime: 2022-12-23 04:09:59 
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.275684118270874 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.284074068069458 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2756626605987549 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.27824902534484863 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.27585792541503906 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.27605772018432617 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.28500843048095703 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3036367893218994 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3106672763824463 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3032071590423584 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.30295872688293457 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.30301499366760254 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3045542240142822 seconds
Loading extension module utils...
Time to load utils op: 0.26023030281066895 seconds
