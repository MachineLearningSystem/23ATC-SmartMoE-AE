--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /mnt/cache/zhaishuming/Megatron-DeepSpeed/examples/MoE/ds_config_gpt_gpt-L16_H2560_MoE16B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-2-gpus-32-mp-1-pp-1-ep-16-mlc-0.01-cap-2.4-drop-true.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2560
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 16
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 2.4
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [16]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /mnt/cache/zhaishuming/Auto-Megatron/deepspeed/output/tensorboard/gpt-L16_H2560_MoE16B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-2-gpus-32-mp-1-pp-1-ep-16-mlc-0.01-cap-2.4-drop-true_SH-IDC1-10-140-1-46_2022.12.23-03.31.47
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... 200
  train_samples ................................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 16
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:08,953] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:08,953] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:08,953] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:08,953] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:08,953] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
> initializing torch distributed ...
[2022-12-23 03:32:08,953] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:08,953] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:08,953] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:09,207] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:09,207] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:09,208] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:09,208] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:09,208] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:09,208] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:32:09,208] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[2022-12-23 03:32:09,213] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:654:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=4, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=5, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=6, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=7, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,542] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,541] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:32:10,542] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=16, master_addr=10.140.1.46, master_port=29500
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.148 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 6.767 seconds
time to initialize megatron (seconds): 67.081
[after megatron is initialized] datetime: 2022-12-23 03:32:19 
building GPT model ...
[2022-12-23 03:32:19,186] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-12-23 03:32:19,187] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-12-23 03:32:19,188] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 184.54 GB, percent = 18.3%
[2022-12-23 03:32:19,237] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:32:19,247] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:32:19,261] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:32:19,276] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:32:19,291] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:32:19,306] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:32:19,313] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:32:19,321] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:32:19,383] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-12-23 03:32:19,384] [INFO] [utils.py:828:see_memory_usage] MA 2.59 GB         Max_MA 2.69 GB         CA 2.7 GB         Max_CA 3 GB 
[2022-12-23 03:32:19,384] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 184.59 GB, percent = 18.3%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1390556160
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-12-23 03:32:19,389] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_16
[2022-12-23 03:32:19,667] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 16
[2022-12-23 03:32:19,667] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [0]
[2022-12-23 03:32:19,678] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [1]
[2022-12-23 03:32:19,688] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [2]
[2022-12-23 03:32:19,698] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [3]
[2022-12-23 03:32:19,708] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [4]
[2022-12-23 03:32:19,719] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [5]
[2022-12-23 03:32:19,729] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [6]
[2022-12-23 03:32:19,739] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [7]
[2022-12-23 03:32:19,749] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [8]
[2022-12-23 03:32:19,759] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [9]
[2022-12-23 03:32:19,770] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [10]
[2022-12-23 03:32:19,780] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [11]
[2022-12-23 03:32:19,790] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [12]
[2022-12-23 03:32:19,800] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [13]
[2022-12-23 03:32:19,810] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [14]
[2022-12-23 03:32:19,820] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [15]
[2022-12-23 03:32:19,831] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_16 with ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[2022-12-23 03:32:22,304] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-12-23 03:32:22,304] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-12-23 03:32:22,305] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-12-23 03:32:22,310] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2022-12-23 03:32:22,310] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2022-12-23 03:32:22,686] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2022-12-23 03:32:22,686] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-12-23 03:32:22,687] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7fa5ee2e6620>
[2022-12-23 03:32:22,687] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:32:22,687] [INFO] [config.py:1020:print] DeepSpeedEngine configuration:
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   amp_enabled .................. False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   amp_params ................... False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   bfloat16_enabled ............. False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   checkpoint_parallel_write_pipeline  False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   checkpoint_tag_validation_enabled  True
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   checkpoint_tag_validation_fail  False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa5ee2e7c10>
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   communication_data_type ...... None
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   curriculum_enabled ........... False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 424592, 'difficulty_step': 8}}
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   dataloader_drop_last ......... False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   disable_allgather ............ False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   dump_state ................... False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   eigenvalue_enabled ........... False
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   eigenvalue_gas_boundary_resolution  1
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   eigenvalue_layer_num ......... 0
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   eigenvalue_max_iter .......... 100
[2022-12-23 03:32:22,688] [INFO] [config.py:1024:print]   eigenvalue_stability ......... 1e-06
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   eigenvalue_tol ............... 0.01
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   eigenvalue_verbose ........... False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   elasticity_enabled ........... False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   fp16_auto_cast ............... False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   fp16_enabled ................. True
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   fp16_master_weights_and_gradients  False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   global_rank .................. 0
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   grad_accum_dtype ............. None
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   gradient_accumulation_steps .. 8
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   gradient_clipping ............ 1.0
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   gradient_predivide_factor .... 1.0
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   initial_dynamic_scale ........ 2048
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   load_universal_checkpoint .... False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   loss_scale ................... 0
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   memory_breakdown ............. False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7fa5e718a710>
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   optimizer_legacy_fusion ...... False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   optimizer_name ............... None
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   optimizer_params ............. None
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   pld_enabled .................. False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   pld_params ................... False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   prescale_gradients ........... True
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   scheduler_name ............... None
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   scheduler_params ............. None
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   sparse_attention ............. None
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   sparse_gradients_enabled ..... False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   steps_per_print .............. 1
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   train_batch_size ............. 256
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   train_micro_batch_size_per_gpu  2
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   use_node_local_storage ....... False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   wall_clock_breakdown ......... False
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   world_size ................... 16
[2022-12-23 03:32:22,689] [INFO] [config.py:1024:print]   zero_allow_untested_optimizer  False
[2022-12-23 03:32:22,694] [INFO] [config.py:1024:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-12-23 03:32:22,694] [INFO] [config.py:1024:print]   zero_enabled ................. False
[2022-12-23 03:32:22,694] [INFO] [config.py:1024:print]   zero_optimization_stage ...... 0
[2022-12-23 03:32:22,694] [INFO] [config.py:1009:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 4.245920e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.15462565422058105 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-23 03:32:22 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: 25600000
    test:       25600000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.032931 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.033 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-23 03:32:24 
done with setup ...
training ...
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20365166664123535 seconds
time (ms) | model-and-optimizer-setup: 3759.91 | train/valid/test-data-iterators-setup: 2010.85
[before the start of training step] datetime: 2022-12-23 03:32:24 
 iteration        1/     200 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 10049.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.115822E+01 | moe loss: 1.250776E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 7160.69 | backward-compute: 2397.76 | backward-embedding-all-reduce: 0.01 | optimizer: 489.51 | batch-generator: 1033.33
[2022-12-23 03:32:34,961] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18567.935546875 | max allocated: 28830.86669921875 | reserved: 32862.0 | max reserved: 32862.0
[2022-12-23 03:32:39,742] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        2/     200 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 4789.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.057116E+01 | moe loss: 1.689205E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2622.79 | backward-compute: 2049.32 | backward-embedding-all-reduce: 0.01 | optimizer: 93.54 | batch-generator: 948.09
[2022-12-23 03:32:44,650] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:32:44,651] [INFO] [timer.py:197:stop] 0/3, RunningAvgSamplesPerSec=42.210005091577635, CurrSamplesPerSec=42.210005091577635, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration        3/     200 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 4911.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.250860E+01 | moe loss: 1.687372E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2604.75 | backward-compute: 2199.56 | backward-embedding-all-reduce: 0.01 | optimizer: 96.30 | batch-generator: 951.49
[2022-12-23 03:32:49,331] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:32:49,331] [INFO] [timer.py:197:stop] 0/4, RunningAvgSamplesPerSec=42.98045384182336, CurrSamplesPerSec=43.779551126424316, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration        4/     200 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 4680.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.053935E+01 | moe loss: 1.613683E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2524.96 | backward-compute: 2049.90 | backward-embedding-all-reduce: 0.01 | optimizer: 96.01 | batch-generator: 963.90
[2022-12-23 03:32:54,211] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:32:54,213] [INFO] [timer.py:197:stop] 0/5, RunningAvgSamplesPerSec=42.83235115250718, CurrSamplesPerSec=42.539186822834374, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration        5/     200 | consumed samples:         1280 | consumed tokens:      1310720 | elapsed time per iteration (ms): 4881.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.007939E+01 | moe loss: 1.387780E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2707.86 | backward-compute: 2061.27 | backward-embedding-all-reduce: 0.01 | optimizer: 105.65 | batch-generator: 994.02
[2022-12-23 03:32:58,891] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:32:58,892] [INFO] [timer.py:197:stop] 0/6, RunningAvgSamplesPerSec=43.12745467877503, CurrSamplesPerSec=44.037678411886894, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration        6/     200 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 4679.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.649286E+00 | moe loss: 1.343589E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2534.33 | backward-compute: 2046.06 | backward-embedding-all-reduce: 0.01 | optimizer: 92.15 | batch-generator: 964.60
[2022-12-23 03:33:03,922] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:03,922] [INFO] [timer.py:197:stop] 0/7, RunningAvgSamplesPerSec=43.15435857075438, CurrSamplesPerSec=43.26231064360465, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration        7/     200 | consumed samples:         1792 | consumed tokens:      1835008 | elapsed time per iteration (ms): 5032.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.205066E+00 | moe loss: 1.227082E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2803.52 | backward-compute: 2128.33 | backward-embedding-all-reduce: 0.01 | optimizer: 92.27 | batch-generator: 1117.71
[2022-12-23 03:33:08,708] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:08,709] [INFO] [timer.py:197:stop] 0/8, RunningAvgSamplesPerSec=41.53856471647407, CurrSamplesPerSec=34.988359956080025, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration        8/     200 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 4784.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.784460E+00 | moe loss: 1.257292E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2618.18 | backward-compute: 2052.45 | backward-embedding-all-reduce: 0.01 | optimizer: 103.42 | batch-generator: 878.24
[2022-12-23 03:33:13,499] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:13,499] [INFO] [timer.py:197:stop] 0/9, RunningAvgSamplesPerSec=41.80203464572339, CurrSamplesPerSec=43.45581852242535, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration        9/     200 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 4790.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.455661E+00 | moe loss: 1.270649E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2642.68 | backward-compute: 2047.53 | backward-embedding-all-reduce: 0.01 | optimizer: 92.53 | batch-generator: 913.35
[2022-12-23 03:33:18,102] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:18,102] [INFO] [timer.py:197:stop] 0/10, RunningAvgSamplesPerSec=41.93974379826086, CurrSamplesPerSec=42.929712360940655, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       10/     200 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 4603.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.123021E+00 | moe loss: 1.255294E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2462.46 | backward-compute: 2042.96 | backward-embedding-all-reduce: 0.01 | optimizer: 91.02 | batch-generator: 904.23
[2022-12-23 03:33:23,355] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       11/     200 | consumed samples:         2816 | consumed tokens:      2883584 | elapsed time per iteration (ms): 5251.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.060434E+00 | moe loss: 1.274910E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 3052.15 | backward-compute: 2086.55 | backward-embedding-all-reduce: 0.01 | optimizer: 105.85 | batch-generator: 910.84
[2022-12-23 03:33:23,364] [INFO] [timer.py:197:stop] 0/11, RunningAvgSamplesPerSec=41.81163242716969, CurrSamplesPerSec=40.81424551361076, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:33:27,958] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:27,959] [INFO] [timer.py:197:stop] 0/12, RunningAvgSamplesPerSec=42.00656968582861, CurrSamplesPerSec=43.84638413114283, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       12/     200 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 4606.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.162171E+00 | moe loss: 1.267001E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2453.93 | backward-compute: 2040.72 | backward-embedding-all-reduce: 0.01 | optimizer: 95.78 | batch-generator: 897.18
[2022-12-23 03:33:32,651] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:32,651] [INFO] [timer.py:197:stop] 0/13, RunningAvgSamplesPerSec=41.66529112041462, CurrSamplesPerSec=38.534583809688414, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       13/     200 | consumed samples:         3328 | consumed tokens:      3407872 | elapsed time per iteration (ms): 4692.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.907353E+00 | moe loss: 1.300287E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2539.19 | backward-compute: 2053.04 | backward-embedding-all-reduce: 0.01 | optimizer: 92.94 | batch-generator: 832.64
[2022-12-23 03:33:37,207] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:37,207] [INFO] [timer.py:197:stop] 0/14, RunningAvgSamplesPerSec=41.863489558084275, CurrSamplesPerSec=44.17499544978654, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       14/     200 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 4558.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.718200E+00 | moe loss: 1.298549E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2412.12 | backward-compute: 2046.47 | backward-embedding-all-reduce: 0.01 | optimizer: 92.80 | batch-generator: 843.85
[2022-12-23 03:33:42,393] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:42,403] [INFO] [timer.py:197:stop] 0/15, RunningAvgSamplesPerSec=41.21051808780205, CurrSamplesPerSec=34.713193098832114, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       15/     200 | consumed samples:         3840 | consumed tokens:      3932160 | elapsed time per iteration (ms): 5195.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.630754E+00 | moe loss: 1.280930E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2823.56 | backward-compute: 2240.27 | backward-embedding-all-reduce: 0.01 | optimizer: 126.26 | batch-generator: 814.83
[2022-12-23 03:33:46,956] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:46,957] [INFO] [timer.py:197:stop] 0/16, RunningAvgSamplesPerSec=41.408093644503246, CurrSamplesPerSec=44.16043188593768, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       16/     200 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 4549.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.588828E+00 | moe loss: 1.213254E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2409.09 | backward-compute: 2044.65 | backward-embedding-all-reduce: 0.01 | optimizer: 90.95 | batch-generator: 835.98
[2022-12-23 03:33:51,500] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:51,501] [INFO] [timer.py:197:stop] 0/17, RunningAvgSamplesPerSec=41.56754220545838, CurrSamplesPerSec=43.93610530702273, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       17/     200 | consumed samples:         4352 | consumed tokens:      4456448 | elapsed time per iteration (ms): 4544.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.535306E+00 | moe loss: 1.221147E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2390.07 | backward-compute: 2051.98 | backward-embedding-all-reduce: 0.01 | optimizer: 94.03 | batch-generator: 806.78
[2022-12-23 03:33:56,112] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:33:56,112] [INFO] [timer.py:197:stop] 0/18, RunningAvgSamplesPerSec=41.71388898741449, CurrSamplesPerSec=44.03964356881368, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       18/     200 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 4613.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.490543E+00 | moe loss: 1.232674E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2411.34 | backward-compute: 2099.58 | backward-embedding-all-reduce: 0.01 | optimizer: 93.16 | batch-generator: 799.61
 iteration       19/     200 | consumed samples:         4864 | consumed tokens:      4980736 | elapsed time per iteration (ms): 4760.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.435182E+00 | moe loss: 1.273933E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2397.76 | backward-compute: 2199.55 | backward-embedding-all-reduce: 0.01 | optimizer: 157.87 | batch-generator: 839.68
[2022-12-23 03:34:00,915] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:00,918] [INFO] [timer.py:197:stop] 0/19, RunningAvgSamplesPerSec=41.02674608868371, CurrSamplesPerSec=32.46906423792084, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:34:05,831] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:05,831] [INFO] [timer.py:197:stop] 0/20, RunningAvgSamplesPerSec=41.158475146007596, CurrSamplesPerSec=43.53476628778259, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       20/     200 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 4964.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.402618E+00 | moe loss: 1.226161E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2722.91 | backward-compute: 2082.28 | backward-embedding-all-reduce: 0.03 | optimizer: 100.02 | batch-generator: 780.42
[2022-12-23 03:34:10,335] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:10,335] [INFO] [timer.py:197:stop] 0/21, RunningAvgSamplesPerSec=41.25924002875114, CurrSamplesPerSec=43.161266754135475, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       21/     200 | consumed samples:         5376 | consumed tokens:      5505024 | elapsed time per iteration (ms): 4495.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.406174E+00 | moe loss: 1.145003E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2352.58 | backward-compute: 2044.61 | backward-embedding-all-reduce: 0.01 | optimizer: 90.81 | batch-generator: 783.13
[2022-12-23 03:34:14,920] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:14,921] [INFO] [timer.py:197:stop] 0/22, RunningAvgSamplesPerSec=41.37584122012645, CurrSamplesPerSec=43.72358745370456, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       22/     200 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 4587.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.395103E+00 | moe loss: 1.152600E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2423.56 | backward-compute: 2056.90 | backward-embedding-all-reduce: 0.01 | optimizer: 94.04 | batch-generator: 789.13
[2022-12-23 03:34:19,506] [INFO] [logging.py:68:log_dist] [Rank 0] step=23, skipped=0, lr=[9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:19,507] [INFO] [timer.py:197:stop] 0/23, RunningAvgSamplesPerSec=41.48907298433338, CurrSamplesPerSec=43.89139263513379, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       23/     200 | consumed samples:         5888 | consumed tokens:      6029312 | elapsed time per iteration (ms): 4584.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.362301E+00 | moe loss: 1.134329E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2442.14 | backward-compute: 2043.58 | backward-embedding-all-reduce: 0.01 | optimizer: 90.00 | batch-generator: 884.09
[2022-12-23 03:34:24,763] [INFO] [logging.py:68:log_dist] [Rank 0] step=24, skipped=0, lr=[9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:24,765] [INFO] [timer.py:197:stop] 0/24, RunningAvgSamplesPerSec=41.515950555809184, CurrSamplesPerSec=42.08853480374971, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       24/     200 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 5277.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.363340E+00 | moe loss: 1.080684E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2910.17 | backward-compute: 2228.16 | backward-embedding-all-reduce: 0.01 | optimizer: 126.71 | batch-generator: 800.70
[2022-12-23 03:34:29,271] [INFO] [logging.py:68:log_dist] [Rank 0] step=25, skipped=0, lr=[9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:29,271] [INFO] [timer.py:197:stop] 0/25, RunningAvgSamplesPerSec=41.61065342203025, CurrSamplesPerSec=43.809202503649196, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       25/     200 | consumed samples:         6400 | consumed tokens:      6553600 | elapsed time per iteration (ms): 4487.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.356815E+00 | moe loss: 1.111687E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2336.87 | backward-compute: 2050.15 | backward-embedding-all-reduce: 0.01 | optimizer: 91.60 | batch-generator: 769.16
[2022-12-23 03:34:33,840] [INFO] [logging.py:68:log_dist] [Rank 0] step=26, skipped=0, lr=[9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:33,841] [INFO] [timer.py:197:stop] 0/26, RunningAvgSamplesPerSec=41.67951745199541, CurrSamplesPerSec=43.32878946894378, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       26/     200 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 4570.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.357192E+00 | moe loss: 1.109514E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2408.50 | backward-compute: 2060.05 | backward-embedding-all-reduce: 0.01 | optimizer: 93.06 | batch-generator: 753.62
[2022-12-23 03:34:38,296] [INFO] [logging.py:68:log_dist] [Rank 0] step=27, skipped=0, lr=[9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:38,297] [INFO] [timer.py:197:stop] 0/27, RunningAvgSamplesPerSec=41.752183462759625, CurrSamplesPerSec=43.57550081668923, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       27/     200 | consumed samples:         6912 | consumed tokens:      7077888 | elapsed time per iteration (ms): 4455.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.348668E+00 | moe loss: 1.054991E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2304.20 | backward-compute: 2051.77 | backward-embedding-all-reduce: 0.01 | optimizer: 92.83 | batch-generator: 720.92
[2022-12-23 03:34:43,153] [INFO] [logging.py:68:log_dist] [Rank 0] step=28, skipped=0, lr=[9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:43,155] [INFO] [timer.py:197:stop] 0/28, RunningAvgSamplesPerSec=41.64830167681256, CurrSamplesPerSec=39.209420469580344, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       28/     200 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 4856.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.349172E+00 | moe loss: 1.025020E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2500.52 | backward-compute: 2246.28 | backward-embedding-all-reduce: 0.01 | optimizer: 99.80 | batch-generator: 777.71
[2022-12-23 03:34:47,597] [INFO] [logging.py:68:log_dist] [Rank 0] step=29, skipped=0, lr=[9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:47,598] [INFO] [timer.py:197:stop] 0/29, RunningAvgSamplesPerSec=41.74661829549209, CurrSamplesPerSec=44.476431433327136, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       29/     200 | consumed samples:         7424 | consumed tokens:      7602176 | elapsed time per iteration (ms): 4445.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.338841E+00 | moe loss: 1.008863E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2296.62 | backward-compute: 2045.68 | backward-embedding-all-reduce: 0.01 | optimizer: 91.28 | batch-generator: 712.51
[2022-12-23 03:34:52,057] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:52,058] [INFO] [timer.py:197:stop] 0/30, RunningAvgSamplesPerSec=41.782258047211606, CurrSamplesPerSec=42.76807628664082, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       30/     200 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 4463.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.315469E+00 | moe loss: 9.957007E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2295.21 | backward-compute: 2048.44 | backward-embedding-all-reduce: 0.01 | optimizer: 110.01 | batch-generator: 728.50
[2022-12-23 03:34:56,498] [INFO] [logging.py:68:log_dist] [Rank 0] step=31, skipped=0, lr=[9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:34:56,498] [INFO] [timer.py:197:stop] 0/31, RunningAvgSamplesPerSec=41.86967107153774, CurrSamplesPerSec=44.47497238078007, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       31/     200 | consumed samples:         7936 | consumed tokens:      8126464 | elapsed time per iteration (ms): 4436.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.309927E+00 | moe loss: 9.790117E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2290.96 | backward-compute: 2050.26 | backward-embedding-all-reduce: 0.01 | optimizer: 89.89 | batch-generator: 718.99
 iteration       32/     200 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 4635.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.283735E+00 | moe loss: 9.743813E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2262.81 | backward-compute: 2222.48 | backward-embedding-all-reduce: 0.01 | optimizer: 141.38 | batch-generator: 700.72
[2022-12-23 03:35:01,163] [INFO] [logging.py:68:log_dist] [Rank 0] step=32, skipped=0, lr=[9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:01,165] [INFO] [timer.py:197:stop] 0/32, RunningAvgSamplesPerSec=41.46263746997882, CurrSamplesPerSec=32.344123003249166, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:35:06,482] [INFO] [logging.py:68:log_dist] [Rank 0] step=33, skipped=0, lr=[9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:06,483] [INFO] [timer.py:197:stop] 0/33, RunningAvgSamplesPerSec=41.50091179330508, CurrSamplesPerSec=42.682935345802186, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       33/     200 | consumed samples:         8448 | consumed tokens:      8650752 | elapsed time per iteration (ms): 5370.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.282670E+00 | moe loss: 9.860633E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2769.71 | backward-compute: 2429.76 | backward-embedding-all-reduce: 0.01 | optimizer: 119.66 | batch-generator: 774.70
 iteration       34/     200 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 4455.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.270608E+00 | moe loss: 9.923597E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2265.24 | backward-compute: 2063.31 | backward-embedding-all-reduce: 0.01 | optimizer: 115.78 | batch-generator: 663.35
[2022-12-23 03:35:10,968] [INFO] [logging.py:68:log_dist] [Rank 0] step=34, skipped=0, lr=[9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:10,974] [INFO] [timer.py:197:stop] 0/34, RunningAvgSamplesPerSec=41.46239768242836, CurrSamplesPerSec=40.3029250593729, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:35:15,453] [INFO] [logging.py:68:log_dist] [Rank 0] step=35, skipped=0, lr=[9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:15,454] [INFO] [timer.py:197:stop] 0/35, RunningAvgSamplesPerSec=41.538778393591194, CurrSamplesPerSec=44.14085452905265, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       35/     200 | consumed samples:         8960 | consumed tokens:      9175040 | elapsed time per iteration (ms): 4495.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.257734E+00 | moe loss: 9.905393E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2326.91 | backward-compute: 2052.62 | backward-embedding-all-reduce: 0.01 | optimizer: 93.92 | batch-generator: 700.69
[2022-12-23 03:35:19,846] [INFO] [logging.py:68:log_dist] [Rank 0] step=36, skipped=0, lr=[9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:19,847] [INFO] [timer.py:197:stop] 0/36, RunningAvgSamplesPerSec=41.62094514566309, CurrSamplesPerSec=44.52754388054393, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       36/     200 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 4392.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.239122E+00 | moe loss: 9.628704E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2251.94 | backward-compute: 2044.14 | backward-embedding-all-reduce: 0.01 | optimizer: 89.61 | batch-generator: 685.91
[2022-12-23 03:35:25,349] [INFO] [logging.py:68:log_dist] [Rank 0] step=37, skipped=0, lr=[9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:25,351] [INFO] [timer.py:197:stop] 0/37, RunningAvgSamplesPerSec=41.58901646796052, CurrSamplesPerSec=40.531847724265226, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       37/     200 | consumed samples:         9472 | consumed tokens:      9699328 | elapsed time per iteration (ms): 5501.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.246829E+00 | moe loss: 9.620886E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2927.24 | backward-compute: 2455.04 | backward-embedding-all-reduce: 0.01 | optimizer: 103.87 | batch-generator: 707.01
[2022-12-23 03:35:29,702] [INFO] [logging.py:68:log_dist] [Rank 0] step=38, skipped=0, lr=[9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:29,702] [INFO] [timer.py:197:stop] 0/38, RunningAvgSamplesPerSec=41.66397239821164, CurrSamplesPerSec=44.46910766066402, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       38/     200 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 4354.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.234404E+00 | moe loss: 9.737568E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2211.42 | backward-compute: 2042.82 | backward-embedding-all-reduce: 0.01 | optimizer: 89.98 | batch-generator: 639.38
[2022-12-23 03:35:34,163] [INFO] [logging.py:68:log_dist] [Rank 0] step=39, skipped=0, lr=[9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:34,164] [INFO] [timer.py:197:stop] 0/39, RunningAvgSamplesPerSec=41.71819864090595, CurrSamplesPerSec=43.76897209330234, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       39/     200 | consumed samples:         9984 | consumed tokens:     10223616 | elapsed time per iteration (ms): 4461.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.238988E+00 | moe loss: 9.400244E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2319.97 | backward-compute: 2042.70 | backward-embedding-all-reduce: 0.01 | optimizer: 90.70 | batch-generator: 626.76
[2022-12-23 03:35:38,517] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:38,517] [INFO] [timer.py:197:stop] 0/40, RunningAvgSamplesPerSec=41.7890624260025, CurrSamplesPerSec=44.591614694285454, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       40/     200 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 4354.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.204907E+00 | moe loss: 9.381555E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2207.29 | backward-compute: 2047.66 | backward-embedding-all-reduce: 0.01 | optimizer: 89.79 | batch-generator: 648.80
[2022-12-23 03:35:43,909] [INFO] [logging.py:68:log_dist] [Rank 0] step=41, skipped=0, lr=[9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:43,911] [INFO] [timer.py:197:stop] 0/41, RunningAvgSamplesPerSec=41.786997040235185, CurrSamplesPerSec=41.70866337972058, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       41/     200 | consumed samples:        10496 | consumed tokens:     10747904 | elapsed time per iteration (ms): 5390.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.192355E+00 | moe loss: 9.256025E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2847.30 | backward-compute: 2424.56 | backward-embedding-all-reduce: 0.01 | optimizer: 106.12 | batch-generator: 709.50
[2022-12-23 03:35:48,254] [INFO] [logging.py:68:log_dist] [Rank 0] step=42, skipped=0, lr=[9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:48,254] [INFO] [timer.py:197:stop] 0/42, RunningAvgSamplesPerSec=41.83074666778118, CurrSamplesPerSec=43.61147889671683, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       42/     200 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 4346.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.210419E+00 | moe loss: 9.391821E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2196.70 | backward-compute: 2050.24 | backward-embedding-all-reduce: 0.01 | optimizer: 89.58 | batch-generator: 584.01
[2022-12-23 03:35:52,638] [INFO] [logging.py:68:log_dist] [Rank 0] step=43, skipped=0, lr=[9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:52,638] [INFO] [timer.py:197:stop] 0/43, RunningAvgSamplesPerSec=41.79727233132129, CurrSamplesPerSec=40.500867394680725, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       43/     200 | consumed samples:        11008 | consumed tokens:     11272192 | elapsed time per iteration (ms): 4385.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.178347E+00 | moe loss: 9.145795E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2177.14 | backward-compute: 2104.87 | backward-embedding-all-reduce: 0.01 | optimizer: 94.78 | batch-generator: 601.99
[2022-12-23 03:35:57,045] [INFO] [logging.py:68:log_dist] [Rank 0] step=44, skipped=0, lr=[9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:35:57,046] [INFO] [timer.py:197:stop] 0/44, RunningAvgSamplesPerSec=41.84258324272683, CurrSamplesPerSec=43.788849477604735, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       44/     200 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 4406.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.163560E+00 | moe loss: 8.969943E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2256.37 | backward-compute: 2053.51 | backward-embedding-all-reduce: 0.01 | optimizer: 89.64 | batch-generator: 636.82
 iteration       45/     200 | consumed samples:        11520 | consumed tokens:     11796480 | elapsed time per iteration (ms): 4748.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.150581E+00 | moe loss: 9.110247E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2547.78 | backward-compute: 2100.14 | backward-embedding-all-reduce: 0.01 | optimizer: 93.79 | batch-generator: 618.03
[2022-12-23 03:36:01,805] [INFO] [logging.py:68:log_dist] [Rank 0] step=45, skipped=0, lr=[9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:01,805] [INFO] [timer.py:197:stop] 0/45, RunningAvgSamplesPerSec=41.34577118881273, CurrSamplesPerSec=27.588104396699894, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:36:06,802] [INFO] [logging.py:68:log_dist] [Rank 0] step=46, skipped=0, lr=[9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:06,803] [INFO] [timer.py:197:stop] 0/46, RunningAvgSamplesPerSec=41.33742499814193, CurrSamplesPerSec=40.981698989180366, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       46/     200 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 5019.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.125925E+00 | moe loss: 8.934508E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2744.04 | backward-compute: 2143.50 | backward-embedding-all-reduce: 0.01 | optimizer: 106.11 | batch-generator: 664.48
 iteration       47/     200 | consumed samples:        12032 | consumed tokens:     12320768 | elapsed time per iteration (ms): 4404.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.110015E+00 | moe loss: 8.939637E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2208.89 | backward-compute: 2086.17 | backward-embedding-all-reduce: 0.01 | optimizer: 102.42 | batch-generator: 603.70
[2022-12-23 03:36:11,221] [INFO] [logging.py:68:log_dist] [Rank 0] step=47, skipped=0, lr=[9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:11,229] [INFO] [timer.py:197:stop] 0/47, RunningAvgSamplesPerSec=41.3181093349354, CurrSamplesPerSec=40.48573081570687, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       48/     200 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 4740.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.098710E+00 | moe loss: 8.985326E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2494.82 | backward-compute: 2133.39 | backward-embedding-all-reduce: 0.01 | optimizer: 95.32 | batch-generator: 659.84
[2022-12-23 03:36:15,968] [INFO] [logging.py:68:log_dist] [Rank 0] step=48, skipped=0, lr=[9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:15,968] [INFO] [timer.py:197:stop] 0/48, RunningAvgSamplesPerSec=41.35079797161919, CurrSamplesPerSec=42.87729608420985, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:36:20,274] [INFO] [logging.py:68:log_dist] [Rank 0] step=49, skipped=0, lr=[9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:20,274] [INFO] [timer.py:197:stop] 0/49, RunningAvgSamplesPerSec=41.399977447428206, CurrSamplesPerSec=43.79600804147737, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       49/     200 | consumed samples:        12544 | consumed tokens:     12845056 | elapsed time per iteration (ms): 4316.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.091636E+00 | moe loss: 8.821565E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2154.78 | backward-compute: 2047.50 | backward-embedding-all-reduce: 0.01 | optimizer: 94.28 | batch-generator: 567.55
[2022-12-23 03:36:25,447] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:25,449] [INFO] [timer.py:197:stop] 0/50, RunningAvgSamplesPerSec=41.42227090416456, CurrSamplesPerSec=42.497849424233365, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       50/     200 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 5193.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.072077E+00 | moe loss: 8.951284E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2719.04 | backward-compute: 2337.07 | backward-embedding-all-reduce: 0.01 | optimizer: 120.04 | batch-generator: 695.52
[2022-12-23 03:36:29,766] [INFO] [logging.py:68:log_dist] [Rank 0] step=51, skipped=0, lr=[9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:29,766] [INFO] [timer.py:197:stop] 0/51, RunningAvgSamplesPerSec=41.47433923294173, CurrSamplesPerSec=44.13744333921755, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       51/     200 | consumed samples:        13056 | consumed tokens:     13369344 | elapsed time per iteration (ms): 4297.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.061610E+00 | moe loss: 8.715125E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2147.01 | backward-compute: 2048.49 | backward-embedding-all-reduce: 0.01 | optimizer: 90.39 | batch-generator: 594.23
 iteration       52/     200 | consumed samples:        13312 | consumed tokens:     13631488 | elapsed time per iteration (ms): 4622.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.044334E+00 | moe loss: 8.603279E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2444.16 | backward-compute: 2076.35 | backward-embedding-all-reduce: 0.01 | optimizer: 92.16 | batch-generator: 674.23
[2022-12-23 03:36:34,394] [INFO] [logging.py:68:log_dist] [Rank 0] step=52, skipped=0, lr=[9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:34,394] [INFO] [timer.py:197:stop] 0/52, RunningAvgSamplesPerSec=41.45508335488332, CurrSamplesPerSec=40.53296159873259, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:36:38,721] [INFO] [logging.py:68:log_dist] [Rank 0] step=53, skipped=0, lr=[9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:38,722] [INFO] [timer.py:197:stop] 0/53, RunningAvgSamplesPerSec=41.50726509194844, CurrSamplesPerSec=44.295095741664994, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       53/     200 | consumed samples:        13568 | consumed tokens:     13893632 | elapsed time per iteration (ms): 4333.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.049127E+00 | moe loss: 8.676111E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2176.01 | backward-compute: 2051.93 | backward-embedding-all-reduce: 0.01 | optimizer: 91.19 | batch-generator: 602.55
[2022-12-23 03:36:43,514] [INFO] [logging.py:68:log_dist] [Rank 0] step=54, skipped=0, lr=[9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:43,515] [INFO] [timer.py:197:stop] 0/54, RunningAvgSamplesPerSec=41.360887251871084, CurrSamplesPerSec=35.055920912587254, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       54/     200 | consumed samples:        13824 | consumed tokens:     14155776 | elapsed time per iteration (ms): 4805.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.024560E+00 | moe loss: 8.489192E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2382.56 | backward-compute: 2301.54 | backward-embedding-all-reduce: 0.01 | optimizer: 111.61 | batch-generator: 615.97
[2022-12-23 03:36:47,790] [INFO] [logging.py:68:log_dist] [Rank 0] step=55, skipped=0, lr=[9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:47,790] [INFO] [timer.py:197:stop] 0/55, RunningAvgSamplesPerSec=41.40150658507857, CurrSamplesPerSec=43.62956822834689, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       55/     200 | consumed samples:        14080 | consumed tokens:     14417920 | elapsed time per iteration (ms): 4263.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.016716E+00 | moe loss: 8.417141E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2112.79 | backward-compute: 2046.77 | backward-embedding-all-reduce: 0.01 | optimizer: 91.52 | batch-generator: 565.92
 iteration       56/     200 | consumed samples:        14336 | consumed tokens:     14680064 | elapsed time per iteration (ms): 4673.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.028432E+00 | moe loss: 8.413327E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2464.81 | backward-compute: 2104.08 | backward-embedding-all-reduce: 0.01 | optimizer: 96.73 | batch-generator: 539.28
[2022-12-23 03:36:52,486] [INFO] [logging.py:68:log_dist] [Rank 0] step=56, skipped=0, lr=[9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:52,490] [INFO] [timer.py:197:stop] 0/56, RunningAvgSamplesPerSec=41.328441116780226, CurrSamplesPerSec=37.79344919353181, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:36:56,745] [INFO] [logging.py:68:log_dist] [Rank 0] step=57, skipped=0, lr=[9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:36:56,746] [INFO] [timer.py:197:stop] 0/57, RunningAvgSamplesPerSec=41.382684824050926, CurrSamplesPerSec=44.53942396428822, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       57/     200 | consumed samples:        14592 | consumed tokens:     14942208 | elapsed time per iteration (ms): 4281.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.996667E+00 | moe loss: 8.330644E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2108.08 | backward-compute: 2051.60 | backward-embedding-all-reduce: 0.01 | optimizer: 89.86 | batch-generator: 559.12
 iteration       58/     200 | consumed samples:        14848 | consumed tokens:     15204352 | elapsed time per iteration (ms): 4344.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.985040E+00 | moe loss: 8.255045E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2111.98 | backward-compute: 2113.26 | backward-embedding-all-reduce: 0.01 | optimizer: 111.28 | batch-generator: 533.01
[2022-12-23 03:37:01,097] [INFO] [logging.py:68:log_dist] [Rank 0] step=58, skipped=0, lr=[9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:01,098] [INFO] [timer.py:197:stop] 0/58, RunningAvgSamplesPerSec=41.33107917194636, CurrSamplesPerSec=38.678256699791476, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:37:06,181] [INFO] [logging.py:68:log_dist] [Rank 0] step=59, skipped=0, lr=[9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:06,182] [INFO] [timer.py:197:stop] 0/59, RunningAvgSamplesPerSec=41.31031276870512, CurrSamplesPerSec=40.17978768595291, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       59/     200 | consumed samples:        15104 | consumed tokens:     15466496 | elapsed time per iteration (ms): 5107.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.969955E+00 | moe loss: 8.258462E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2480.64 | backward-compute: 2470.33 | backward-embedding-all-reduce: 0.01 | optimizer: 118.93 | batch-generator: 635.63
[2022-12-23 03:37:10,450] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:10,450] [INFO] [timer.py:197:stop] 0/60, RunningAvgSamplesPerSec=41.3546511042931, CurrSamplesPerSec=44.049515355234405, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       60/     200 | consumed samples:        15360 | consumed tokens:     15728640 | elapsed time per iteration (ms): 4252.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.956845E+00 | moe loss: 8.259177E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2104.71 | backward-compute: 2047.18 | backward-embedding-all-reduce: 0.01 | optimizer: 89.56 | batch-generator: 539.98
 iteration       61/     200 | consumed samples:        15616 | consumed tokens:     15990784 | elapsed time per iteration (ms): 4610.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.961382E+00 | moe loss: 8.272793E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2197.55 | backward-compute: 2311.05 | backward-embedding-all-reduce: 0.01 | optimizer: 93.98 | batch-generator: 528.36
[2022-12-23 03:37:15,068] [INFO] [logging.py:68:log_dist] [Rank 0] step=61, skipped=0, lr=[9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:15,071] [INFO] [timer.py:197:stop] 0/61, RunningAvgSamplesPerSec=41.376162683514806, CurrSamplesPerSec=42.66331677778749, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:37:19,373] [INFO] [logging.py:68:log_dist] [Rank 0] step=62, skipped=0, lr=[9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:19,373] [INFO] [timer.py:197:stop] 0/62, RunningAvgSamplesPerSec=41.419168842407366, CurrSamplesPerSec=44.12510939411931, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       62/     200 | consumed samples:        15872 | consumed tokens:     16252928 | elapsed time per iteration (ms): 4313.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.942757E+00 | moe loss: 8.328941E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2154.09 | backward-compute: 2050.88 | backward-embedding-all-reduce: 0.01 | optimizer: 89.54 | batch-generator: 591.90
[2022-12-23 03:37:24,372] [INFO] [logging.py:68:log_dist] [Rank 0] step=63, skipped=0, lr=[9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:24,372] [INFO] [timer.py:197:stop] 0/63, RunningAvgSamplesPerSec=41.44287649659158, CurrSamplesPerSec=42.91676781578028, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       63/     200 | consumed samples:        16128 | consumed tokens:     16515072 | elapsed time per iteration (ms): 5020.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.913912E+00 | moe loss: 8.339275E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2752.03 | backward-compute: 2133.24 | backward-embedding-all-reduce: 0.01 | optimizer: 119.43 | batch-generator: 580.33
[2022-12-23 03:37:28,635] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:28,635] [INFO] [timer.py:197:stop] 0/64, RunningAvgSamplesPerSec=41.475670239225046, CurrSamplesPerSec=43.579207735943584, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       64/     200 | consumed samples:        16384 | consumed tokens:     16777216 | elapsed time per iteration (ms): 4241.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.908849E+00 | moe loss: 8.212093E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2078.73 | backward-compute: 2052.07 | backward-embedding-all-reduce: 0.01 | optimizer: 89.58 | batch-generator: 521.63
 iteration       65/     200 | consumed samples:        16640 | consumed tokens:     17039360 | elapsed time per iteration (ms): 4664.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.908953E+00 | moe loss: 8.297659E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2257.18 | backward-compute: 2299.57 | backward-embedding-all-reduce: 0.01 | optimizer: 99.20 | batch-generator: 523.52
[2022-12-23 03:37:33,321] [INFO] [logging.py:68:log_dist] [Rank 0] step=65, skipped=0, lr=[9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:33,332] [INFO] [timer.py:197:stop] 0/65, RunningAvgSamplesPerSec=41.4751743174453, CurrSamplesPerSec=41.444450311363575, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:37:37,769] [INFO] [logging.py:68:log_dist] [Rank 0] step=66, skipped=0, lr=[9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:37,770] [INFO] [timer.py:197:stop] 0/66, RunningAvgSamplesPerSec=41.510280930528026, CurrSamplesPerSec=43.84856145403795, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       66/     200 | consumed samples:        16896 | consumed tokens:     17301504 | elapsed time per iteration (ms): 4479.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.881793E+00 | moe loss: 8.326834E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2280.31 | backward-compute: 2059.58 | backward-embedding-all-reduce: 0.01 | optimizer: 101.00 | batch-generator: 535.18
[2022-12-23 03:37:41,999] [INFO] [logging.py:68:log_dist] [Rank 0] step=67, skipped=0, lr=[9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:42,000] [INFO] [timer.py:197:stop] 0/67, RunningAvgSamplesPerSec=41.54576639864417, CurrSamplesPerSec=43.950334168344874, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       67/     200 | consumed samples:        17152 | consumed tokens:     17563648 | elapsed time per iteration (ms): 4220.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.880037E+00 | moe loss: 8.216383E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2080.77 | backward-compute: 2043.17 | backward-embedding-all-reduce: 0.01 | optimizer: 89.92 | batch-generator: 508.71
[2022-12-23 03:37:46,282] [INFO] [logging.py:68:log_dist] [Rank 0] step=68, skipped=0, lr=[9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:46,282] [INFO] [timer.py:197:stop] 0/68, RunningAvgSamplesPerSec=41.58585843184016, CurrSamplesPerSec=44.36892483039062, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       68/     200 | consumed samples:        17408 | consumed tokens:     17825792 | elapsed time per iteration (ms): 4282.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.869380E+00 | moe loss: 8.165897E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2120.30 | backward-compute: 2061.81 | backward-embedding-all-reduce: 0.01 | optimizer: 91.81 | batch-generator: 497.83
[2022-12-23 03:37:50,476] [INFO] [logging.py:68:log_dist] [Rank 0] step=69, skipped=0, lr=[9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:50,476] [INFO] [timer.py:197:stop] 0/69, RunningAvgSamplesPerSec=41.62542018235445, CurrSamplesPerSec=44.41407243765975, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       69/     200 | consumed samples:        17664 | consumed tokens:     18087936 | elapsed time per iteration (ms): 4195.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.855235E+00 | moe loss: 8.132175E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2051.59 | backward-compute: 2039.30 | backward-embedding-all-reduce: 0.01 | optimizer: 91.22 | batch-generator: 490.51
[2022-12-23 03:37:55,140] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:55,143] [INFO] [timer.py:197:stop] 0/70, RunningAvgSamplesPerSec=41.6313507072123, CurrSamplesPerSec=42.032582535983686, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       70/     200 | consumed samples:        17920 | consumed tokens:     18350080 | elapsed time per iteration (ms): 4664.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.844456E+00 | moe loss: 8.216009E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2279.84 | backward-compute: 2270.46 | backward-embedding-all-reduce: 0.01 | optimizer: 107.39 | batch-generator: 545.09
[2022-12-23 03:37:59,445] [INFO] [logging.py:68:log_dist] [Rank 0] step=71, skipped=0, lr=[9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:37:59,445] [INFO] [timer.py:197:stop] 0/71, RunningAvgSamplesPerSec=41.66140606474377, CurrSamplesPerSec=43.812234207155534, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       71/     200 | consumed samples:        18176 | consumed tokens:     18612224 | elapsed time per iteration (ms): 4303.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.829461E+00 | moe loss: 8.148447E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2160.46 | backward-compute: 2043.13 | backward-embedding-all-reduce: 0.01 | optimizer: 89.54 | batch-generator: 594.32
[2022-12-23 03:38:04,059] [INFO] [logging.py:68:log_dist] [Rank 0] step=72, skipped=0, lr=[9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:04,060] [INFO] [timer.py:197:stop] 0/72, RunningAvgSamplesPerSec=41.694987029108745, CurrSamplesPerSec=44.150510328275885, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       72/     200 | consumed samples:        18432 | consumed tokens:     18874368 | elapsed time per iteration (ms): 4616.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.839296E+00 | moe loss: 8.186945E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2203.91 | backward-compute: 2303.95 | backward-embedding-all-reduce: 0.01 | optimizer: 93.99 | batch-generator: 542.26
[2022-12-23 03:38:08,235] [INFO] [logging.py:68:log_dist] [Rank 0] step=73, skipped=0, lr=[9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:08,236] [INFO] [timer.py:197:stop] 0/73, RunningAvgSamplesPerSec=41.72839187044116, CurrSamplesPerSec=44.20764572680187, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       73/     200 | consumed samples:        18688 | consumed tokens:     19136512 | elapsed time per iteration (ms): 4173.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.838512E+00 | moe loss: 8.244535E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2041.79 | backward-compute: 2035.09 | backward-embedding-all-reduce: 0.01 | optimizer: 90.70 | batch-generator: 479.57
 iteration       74/     200 | consumed samples:        18944 | consumed tokens:     19398656 | elapsed time per iteration (ms): 4927.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.823563E+00 | moe loss: 8.370616E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2622.02 | backward-compute: 2169.78 | backward-embedding-all-reduce: 0.01 | optimizer: 123.86 | batch-generator: 546.53
[2022-12-23 03:38:13,162] [INFO] [logging.py:68:log_dist] [Rank 0] step=74, skipped=0, lr=[9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:13,174] [INFO] [timer.py:197:stop] 0/74, RunningAvgSamplesPerSec=41.67802258728602, CurrSamplesPerSec=38.388076213433536, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:38:17,379] [INFO] [logging.py:68:log_dist] [Rank 0] step=75, skipped=0, lr=[9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:17,380] [INFO] [timer.py:197:stop] 0/75, RunningAvgSamplesPerSec=41.71247927061515, CurrSamplesPerSec=44.35256224482883, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       75/     200 | consumed samples:        19200 | consumed tokens:     19660800 | elapsed time per iteration (ms): 4217.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.816525E+00 | moe loss: 8.458663E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2063.22 | backward-compute: 2044.20 | backward-embedding-all-reduce: 0.01 | optimizer: 89.84 | batch-generator: 495.75
[2022-12-23 03:38:21,634] [INFO] [logging.py:68:log_dist] [Rank 0] step=76, skipped=0, lr=[9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:21,634] [INFO] [timer.py:197:stop] 0/76, RunningAvgSamplesPerSec=41.739310390730054, CurrSamplesPerSec=43.79580796998001, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       76/     200 | consumed samples:        19456 | consumed tokens:     19922944 | elapsed time per iteration (ms): 4254.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.791533E+00 | moe loss: 8.319513E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2113.34 | backward-compute: 2039.36 | backward-embedding-all-reduce: 0.01 | optimizer: 94.13 | batch-generator: 542.52
[2022-12-23 03:38:26,466] [INFO] [logging.py:68:log_dist] [Rank 0] step=77, skipped=0, lr=[9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:26,467] [INFO] [timer.py:197:stop] 0/77, RunningAvgSamplesPerSec=41.761883388152555, CurrSamplesPerSec=43.50286231768971, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       77/     200 | consumed samples:        19712 | consumed tokens:     20185088 | elapsed time per iteration (ms): 4835.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.815423E+00 | moe loss: 8.354136E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2603.74 | backward-compute: 2109.63 | backward-embedding-all-reduce: 0.01 | optimizer: 102.26 | batch-generator: 514.79
[2022-12-23 03:38:30,635] [INFO] [logging.py:68:log_dist] [Rank 0] step=78, skipped=0, lr=[9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:30,636] [INFO] [timer.py:197:stop] 0/78, RunningAvgSamplesPerSec=41.791216908048526, CurrSamplesPerSec=44.11520370019166, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       78/     200 | consumed samples:        19968 | consumed tokens:     20447232 | elapsed time per iteration (ms): 4164.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.794105E+00 | moe loss: 8.510218E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2023.61 | backward-compute: 2039.60 | backward-embedding-all-reduce: 0.01 | optimizer: 89.75 | batch-generator: 486.31
[2022-12-23 03:38:35,096] [INFO] [logging.py:68:log_dist] [Rank 0] step=79, skipped=0, lr=[9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:35,099] [INFO] [timer.py:197:stop] 0/79, RunningAvgSamplesPerSec=41.7765303470928, CurrSamplesPerSec=40.68976966326539, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       79/     200 | consumed samples:        20224 | consumed tokens:     20709376 | elapsed time per iteration (ms): 4461.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.774324E+00 | moe loss: 8.537317E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2191.54 | backward-compute: 2161.02 | backward-embedding-all-reduce: 0.01 | optimizer: 101.43 | batch-generator: 496.94
[2022-12-23 03:38:39,276] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:39,276] [INFO] [timer.py:197:stop] 0/80, RunningAvgSamplesPerSec=41.80998959109663, CurrSamplesPerSec=44.557877253618685, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       80/     200 | consumed samples:        20480 | consumed tokens:     20971520 | elapsed time per iteration (ms): 4180.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.783345E+00 | moe loss: 8.501641E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2031.17 | backward-compute: 2042.59 | backward-embedding-all-reduce: 0.01 | optimizer: 89.75 | batch-generator: 455.68
[2022-12-23 03:38:44,097] [INFO] [logging.py:68:log_dist] [Rank 0] step=81, skipped=0, lr=[9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:44,098] [INFO] [timer.py:197:stop] 0/81, RunningAvgSamplesPerSec=41.793198488365874, CurrSamplesPerSec=40.523783141253226, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       81/     200 | consumed samples:        20736 | consumed tokens:     21233664 | elapsed time per iteration (ms): 4824.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.771118E+00 | moe loss: 8.500431E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2602.24 | backward-compute: 2104.68 | backward-embedding-all-reduce: 0.01 | optimizer: 98.93 | batch-generator: 458.34
[2022-12-23 03:38:48,236] [INFO] [logging.py:68:log_dist] [Rank 0] step=82, skipped=0, lr=[9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:48,237] [INFO] [timer.py:197:stop] 0/82, RunningAvgSamplesPerSec=41.823242712527346, CurrSamplesPerSec=44.34145546964845, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       82/     200 | consumed samples:        20992 | consumed tokens:     21495808 | elapsed time per iteration (ms): 4136.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.744533E+00 | moe loss: 8.445007E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2000.92 | backward-compute: 2036.61 | backward-embedding-all-reduce: 0.01 | optimizer: 91.18 | batch-generator: 428.56
[2022-12-23 03:38:52,952] [INFO] [logging.py:68:log_dist] [Rank 0] step=83, skipped=0, lr=[9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:52,952] [INFO] [timer.py:197:stop] 0/83, RunningAvgSamplesPerSec=41.81727036474443, CurrSamplesPerSec=41.344946586735446, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       83/     200 | consumed samples:        21248 | consumed tokens:     21757952 | elapsed time per iteration (ms): 4715.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.770727E+00 | moe loss: 8.460757E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2236.70 | backward-compute: 2358.18 | backward-embedding-all-reduce: 0.01 | optimizer: 109.96 | batch-generator: 450.53
[2022-12-23 03:38:57,143] [INFO] [logging.py:68:log_dist] [Rank 0] step=84, skipped=0, lr=[9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:38:57,144] [INFO] [timer.py:197:stop] 0/84, RunningAvgSamplesPerSec=41.84617296699542, CurrSamplesPerSec=44.32783602055985, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       84/     200 | consumed samples:        21504 | consumed tokens:     22020096 | elapsed time per iteration (ms): 4190.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.723596E+00 | moe loss: 8.351535E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2051.00 | backward-compute: 2036.47 | backward-embedding-all-reduce: 0.01 | optimizer: 89.69 | batch-generator: 501.55
 iteration       85/     200 | consumed samples:        21760 | consumed tokens:     22282240 | elapsed time per iteration (ms): 4194.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.717042E+00 | moe loss: 8.300039E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1986.33 | backward-compute: 2086.12 | backward-embedding-all-reduce: 0.01 | optimizer: 110.37 | batch-generator: 438.11
[2022-12-23 03:39:01,343] [INFO] [logging.py:68:log_dist] [Rank 0] step=85, skipped=0, lr=[9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:01,352] [INFO] [timer.py:197:stop] 0/85, RunningAvgSamplesPerSec=41.8165108897135, CurrSamplesPerSec=39.519459732575164, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:39:05,892] [INFO] [logging.py:68:log_dist] [Rank 0] step=86, skipped=0, lr=[9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:05,892] [INFO] [timer.py:197:stop] 0/86, RunningAvgSamplesPerSec=41.83572187940969, CurrSamplesPerSec=43.494206507325636, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       86/     200 | consumed samples:        22016 | consumed tokens:     22544384 | elapsed time per iteration (ms): 4555.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.730220E+00 | moe loss: 8.367899E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2361.20 | backward-compute: 2076.73 | backward-embedding-all-reduce: 0.01 | optimizer: 96.41 | batch-generator: 448.63
[2022-12-23 03:39:10,049] [INFO] [logging.py:68:log_dist] [Rank 0] step=87, skipped=0, lr=[9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:10,049] [INFO] [timer.py:197:stop] 0/87, RunningAvgSamplesPerSec=41.85641906405031, CurrSamplesPerSec=43.67126195554384, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       87/     200 | consumed samples:        22272 | consumed tokens:     22806528 | elapsed time per iteration (ms): 4155.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.704469E+00 | moe loss: 8.362310E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2013.45 | backward-compute: 2040.69 | backward-embedding-all-reduce: 0.01 | optimizer: 92.27 | batch-generator: 462.10
[2022-12-23 03:39:14,861] [INFO] [logging.py:68:log_dist] [Rank 0] step=88, skipped=0, lr=[9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:14,863] [INFO] [timer.py:197:stop] 0/88, RunningAvgSamplesPerSec=41.86088952541428, CurrSamplesPerSec=42.244400995602376, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       88/     200 | consumed samples:        22528 | consumed tokens:     23068672 | elapsed time per iteration (ms): 4824.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.698641E+00 | moe loss: 8.334672E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2572.30 | backward-compute: 2122.93 | backward-embedding-all-reduce: 0.01 | optimizer: 118.24 | batch-generator: 482.37
[2022-12-23 03:39:19,111] [INFO] [logging.py:68:log_dist] [Rank 0] step=89, skipped=0, lr=[9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:19,112] [INFO] [timer.py:197:stop] 0/89, RunningAvgSamplesPerSec=41.88457971821629, CurrSamplesPerSec=44.02737865695524, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       89/     200 | consumed samples:        22784 | consumed tokens:     23330816 | elapsed time per iteration (ms): 4238.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.685359E+00 | moe loss: 8.408253E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2091.84 | backward-compute: 2049.96 | backward-embedding-all-reduce: 0.01 | optimizer: 90.53 | batch-generator: 531.12
[2022-12-23 03:39:23,784] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:23,784] [INFO] [timer.py:197:stop] 0/90, RunningAvgSamplesPerSec=41.754799695313935, CurrSamplesPerSec=32.888908492205964, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       90/     200 | consumed samples:        23040 | consumed tokens:     23592960 | elapsed time per iteration (ms): 4676.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.682916E+00 | moe loss: 8.307004E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2178.31 | backward-compute: 2373.10 | backward-embedding-all-reduce: 0.01 | optimizer: 105.41 | batch-generator: 501.53
[2022-12-23 03:39:27,921] [INFO] [logging.py:68:log_dist] [Rank 0] step=91, skipped=0, lr=[9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:27,922] [INFO] [timer.py:197:stop] 0/91, RunningAvgSamplesPerSec=41.7754170193101, CurrSamplesPerSec=43.67309506982217, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       91/     200 | consumed samples:        23296 | consumed tokens:     23855104 | elapsed time per iteration (ms): 4133.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.665995E+00 | moe loss: 8.321258E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1975.11 | backward-compute: 2054.07 | backward-embedding-all-reduce: 0.01 | optimizer: 92.14 | batch-generator: 384.65
 iteration       92/     200 | consumed samples:        23552 | consumed tokens:     24117248 | elapsed time per iteration (ms): 4270.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.654574E+00 | moe loss: 8.266745E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2038.59 | backward-compute: 2130.38 | backward-embedding-all-reduce: 0.01 | optimizer: 93.61 | batch-generator: 396.68
[2022-12-23 03:39:32,203] [INFO] [logging.py:68:log_dist] [Rank 0] step=92, skipped=0, lr=[9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:32,203] [INFO] [timer.py:197:stop] 0/92, RunningAvgSamplesPerSec=41.77573095356876, CurrSamplesPerSec=41.80369001207537, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:39:36,449] [INFO] [logging.py:68:log_dist] [Rank 0] step=93, skipped=0, lr=[9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:36,449] [INFO] [timer.py:197:stop] 0/93, RunningAvgSamplesPerSec=41.772336457682144, CurrSamplesPerSec=41.46907439780459, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       93/     200 | consumed samples:        23808 | consumed tokens:     24379392 | elapsed time per iteration (ms): 4269.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.640159E+00 | moe loss: 8.352162E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2093.34 | backward-compute: 2056.04 | backward-embedding-all-reduce: 0.01 | optimizer: 103.04 | batch-generator: 464.25
[2022-12-23 03:39:40,635] [INFO] [logging.py:68:log_dist] [Rank 0] step=94, skipped=0, lr=[9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:40,636] [INFO] [timer.py:197:stop] 0/94, RunningAvgSamplesPerSec=41.79958804490233, CurrSamplesPerSec=44.437717937885466, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       94/     200 | consumed samples:        24064 | consumed tokens:     24641536 | elapsed time per iteration (ms): 4173.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.626869E+00 | moe loss: 8.235154E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2029.15 | backward-compute: 2045.00 | backward-embedding-all-reduce: 0.01 | optimizer: 90.09 | batch-generator: 464.20
[2022-12-23 03:39:45,286] [INFO] [logging.py:68:log_dist] [Rank 0] step=95, skipped=0, lr=[9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:45,286] [INFO] [timer.py:197:stop] 0/95, RunningAvgSamplesPerSec=41.817655605146705, CurrSamplesPerSec=43.54945698534148, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       95/     200 | consumed samples:        24320 | consumed tokens:     24903680 | elapsed time per iteration (ms): 4652.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.630796E+00 | moe loss: 8.391348E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2127.21 | backward-compute: 2408.41 | backward-embedding-all-reduce: 0.01 | optimizer: 105.25 | batch-generator: 447.24
 iteration       96/     200 | consumed samples:        24576 | consumed tokens:     25165824 | elapsed time per iteration (ms): 4648.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.619061E+00 | moe loss: 8.416103E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2424.42 | backward-compute: 2119.98 | backward-embedding-all-reduce: 0.01 | optimizer: 91.19 | batch-generator: 388.36
[2022-12-23 03:39:49,944] [INFO] [logging.py:68:log_dist] [Rank 0] step=96, skipped=0, lr=[9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:49,945] [INFO] [timer.py:197:stop] 0/96, RunningAvgSamplesPerSec=41.82672111224093, CurrSamplesPerSec=42.68734725863843, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:39:54,204] [INFO] [logging.py:68:log_dist] [Rank 0] step=97, skipped=0, lr=[9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:54,205] [INFO] [timer.py:197:stop] 0/97, RunningAvgSamplesPerSec=41.84188957180268, CurrSamplesPerSec=43.31858092613996, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       97/     200 | consumed samples:        24832 | consumed tokens:     25427968 | elapsed time per iteration (ms): 4277.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.615469E+00 | moe loss: 8.337439E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2094.57 | backward-compute: 2062.78 | backward-embedding-all-reduce: 0.01 | optimizer: 101.13 | batch-generator: 441.54
[2022-12-23 03:39:58,342] [INFO] [logging.py:68:log_dist] [Rank 0] step=98, skipped=0, lr=[9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:39:58,343] [INFO] [timer.py:197:stop] 0/98, RunningAvgSamplesPerSec=41.858768841945654, CurrSamplesPerSec=43.526874132496204, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       98/     200 | consumed samples:        25088 | consumed tokens:     25690112 | elapsed time per iteration (ms): 4128.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.585865E+00 | moe loss: 8.318351E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1988.05 | backward-compute: 2037.25 | backward-embedding-all-reduce: 0.01 | optimizer: 92.52 | batch-generator: 406.44
[2022-12-23 03:40:02,920] [INFO] [logging.py:68:log_dist] [Rank 0] step=99, skipped=0, lr=[9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:02,923] [INFO] [timer.py:197:stop] 0/99, RunningAvgSamplesPerSec=41.743865145449426, CurrSamplesPerSec=33.03767323410869, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration       99/     200 | consumed samples:        25344 | consumed tokens:     25952256 | elapsed time per iteration (ms): 4599.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.580017E+00 | moe loss: 8.421014E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2105.59 | backward-compute: 2292.95 | backward-embedding-all-reduce: 0.01 | optimizer: 169.08 | batch-generator: 400.58
[2022-12-23 03:40:07,417] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:07,418] [INFO] [timer.py:197:stop] 0/100, RunningAvgSamplesPerSec=41.76610288615675, CurrSamplesPerSec=44.04191238720262, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      100/     200 | consumed samples:        25600 | consumed tokens:     26214400 | elapsed time per iteration (ms): 4475.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.581981E+00 | moe loss: 8.415146E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2273.22 | backward-compute: 2095.34 | backward-embedding-all-reduce: 0.01 | optimizer: 92.38 | batch-generator: 456.37
[2022-12-23 03:40:11,665] [INFO] [logging.py:68:log_dist] [Rank 0] step=101, skipped=0, lr=[9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:11,665] [INFO] [timer.py:197:stop] 0/101, RunningAvgSamplesPerSec=41.72774079517567, CurrSamplesPerSec=38.281880572885974, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      101/     200 | consumed samples:        25856 | consumed tokens:     26476544 | elapsed time per iteration (ms): 4250.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.578728E+00 | moe loss: 8.362226E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1995.50 | backward-compute: 2151.00 | backward-embedding-all-reduce: 0.01 | optimizer: 93.95 | batch-generator: 455.06
[2022-12-23 03:40:15,947] [INFO] [logging.py:68:log_dist] [Rank 0] step=102, skipped=0, lr=[9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:15,948] [INFO] [timer.py:197:stop] 0/102, RunningAvgSamplesPerSec=41.73061162560149, CurrSamplesPerSec=42.01679260126291, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      102/     200 | consumed samples:        26112 | consumed tokens:     26738688 | elapsed time per iteration (ms): 4289.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.563707E+00 | moe loss: 8.342798E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2081.29 | backward-compute: 2100.54 | backward-embedding-all-reduce: 0.01 | optimizer: 100.40 | batch-generator: 449.18
[2022-12-23 03:40:20,051] [INFO] [logging.py:68:log_dist] [Rank 0] step=103, skipped=0, lr=[9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:20,052] [INFO] [timer.py:197:stop] 0/103, RunningAvgSamplesPerSec=41.75411940435964, CurrSamplesPerSec=44.246630183952, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      103/     200 | consumed samples:        26368 | consumed tokens:     27000832 | elapsed time per iteration (ms): 4093.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.554263E+00 | moe loss: 8.324917E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1957.89 | backward-compute: 2039.13 | backward-embedding-all-reduce: 0.01 | optimizer: 89.60 | batch-generator: 398.90
[2022-12-23 03:40:24,624] [INFO] [logging.py:68:log_dist] [Rank 0] step=104, skipped=0, lr=[9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:24,624] [INFO] [timer.py:197:stop] 0/104, RunningAvgSamplesPerSec=41.7583385443785, CurrSamplesPerSec=42.18890905153188, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      104/     200 | consumed samples:        26624 | consumed tokens:     27262976 | elapsed time per iteration (ms): 4565.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.559852E+00 | moe loss: 8.313131E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2181.78 | backward-compute: 2272.91 | backward-embedding-all-reduce: 0.01 | optimizer: 101.89 | batch-generator: 446.98
[2022-12-23 03:40:28,895] [INFO] [logging.py:68:log_dist] [Rank 0] step=105, skipped=0, lr=[9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:28,896] [INFO] [timer.py:197:stop] 0/105, RunningAvgSamplesPerSec=41.776036744179976, CurrSamplesPerSec=43.66361853318078, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      105/     200 | consumed samples:        26880 | consumed tokens:     27525120 | elapsed time per iteration (ms): 4285.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.551957E+00 | moe loss: 8.239631E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2094.48 | backward-compute: 2070.07 | backward-embedding-all-reduce: 0.01 | optimizer: 97.29 | batch-generator: 433.05
 iteration      106/     200 | consumed samples:        27136 | consumed tokens:     27787264 | elapsed time per iteration (ms): 4238.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.555638E+00 | moe loss: 8.487966E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2030.28 | backward-compute: 2104.93 | backward-embedding-all-reduce: 0.01 | optimizer: 95.14 | batch-generator: 397.57
[2022-12-23 03:40:33,145] [INFO] [logging.py:68:log_dist] [Rank 0] step=106, skipped=0, lr=[9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:33,154] [INFO] [timer.py:197:stop] 0/106, RunningAvgSamplesPerSec=41.76329994296362, CurrSamplesPerSec=40.4917401123603, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:40:37,234] [INFO] [logging.py:68:log_dist] [Rank 0] step=107, skipped=0, lr=[9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:37,234] [INFO] [timer.py:197:stop] 0/107, RunningAvgSamplesPerSec=41.78150387718113, CurrSamplesPerSec=43.76547542690973, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      107/     200 | consumed samples:        27392 | consumed tokens:     28049408 | elapsed time per iteration (ms): 4092.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.552027E+00 | moe loss: 8.389752E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1943.07 | backward-compute: 2039.21 | backward-embedding-all-reduce: 0.01 | optimizer: 91.85 | batch-generator: 388.52
 iteration      108/     200 | consumed samples:        27648 | consumed tokens:     28311552 | elapsed time per iteration (ms): 4194.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.535757E+00 | moe loss: 8.320738E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1970.97 | backward-compute: 2101.92 | backward-embedding-all-reduce: 0.01 | optimizer: 112.97 | batch-generator: 397.00
[2022-12-23 03:40:41,459] [INFO] [logging.py:68:log_dist] [Rank 0] step=108, skipped=0, lr=[9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:41,459] [INFO] [timer.py:197:stop] 0/108, RunningAvgSamplesPerSec=41.74075754543977, CurrSamplesPerSec=37.86358216087866, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:40:46,237] [INFO] [logging.py:68:log_dist] [Rank 0] step=109, skipped=0, lr=[9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:46,237] [INFO] [timer.py:197:stop] 0/109, RunningAvgSamplesPerSec=41.74727539398929, CurrSamplesPerSec=42.449905132881966, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      109/     200 | consumed samples:        27904 | consumed tokens:     28573696 | elapsed time per iteration (ms): 4815.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.524427E+00 | moe loss: 8.367269E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2359.57 | backward-compute: 2308.42 | backward-embedding-all-reduce: 0.01 | optimizer: 103.49 | batch-generator: 454.79
[2022-12-23 03:40:50,368] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:50,369] [INFO] [timer.py:197:stop] 0/110, RunningAvgSamplesPerSec=41.76679175254235, CurrSamplesPerSec=43.96602680852737, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      110/     200 | consumed samples:        28160 | consumed tokens:     28835840 | elapsed time per iteration (ms): 4127.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.522165E+00 | moe loss: 8.401510E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1995.30 | backward-compute: 2026.28 | backward-embedding-all-reduce: 0.01 | optimizer: 92.45 | batch-generator: 446.26
 iteration      111/     200 | consumed samples:        28416 | consumed tokens:     29097984 | elapsed time per iteration (ms): 4297.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.510881E+00 | moe loss: 8.443332E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2061.92 | backward-compute: 2133.10 | backward-embedding-all-reduce: 0.01 | optimizer: 95.06 | batch-generator: 392.57
[2022-12-23 03:40:54,677] [INFO] [logging.py:68:log_dist] [Rank 0] step=111, skipped=0, lr=[9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:54,681] [INFO] [timer.py:197:stop] 0/111, RunningAvgSamplesPerSec=41.75927953552008, CurrSamplesPerSec=40.96356280987574, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:40:58,825] [INFO] [logging.py:68:log_dist] [Rank 0] step=112, skipped=0, lr=[9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:40:58,826] [INFO] [timer.py:197:stop] 0/112, RunningAvgSamplesPerSec=41.76796482905936, CurrSamplesPerSec=42.73682306744903, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      112/     200 | consumed samples:        28672 | consumed tokens:     29360128 | elapsed time per iteration (ms): 4155.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.506797E+00 | moe loss: 8.359686E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1990.70 | backward-compute: 2056.35 | backward-embedding-all-reduce: 0.01 | optimizer: 92.26 | batch-generator: 412.80
[2022-12-23 03:41:03,579] [INFO] [logging.py:68:log_dist] [Rank 0] step=113, skipped=0, lr=[9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:03,579] [INFO] [timer.py:197:stop] 0/113, RunningAvgSamplesPerSec=41.62832656962143, CurrSamplesPerSec=30.435604318654516, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      113/     200 | consumed samples:        28928 | consumed tokens:     29622272 | elapsed time per iteration (ms): 4754.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.488892E+00 | moe loss: 8.396193E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2499.31 | backward-compute: 2125.24 | backward-embedding-all-reduce: 0.01 | optimizer: 107.01 | batch-generator: 387.02
[2022-12-23 03:41:07,709] [INFO] [logging.py:68:log_dist] [Rank 0] step=114, skipped=0, lr=[9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:07,710] [INFO] [timer.py:197:stop] 0/114, RunningAvgSamplesPerSec=41.649656749910164, CurrSamplesPerSec=44.161376336840334, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      114/     200 | consumed samples:        29184 | consumed tokens:     29884416 | elapsed time per iteration (ms): 4129.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.496731E+00 | moe loss: 8.562672E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1980.54 | backward-compute: 2042.10 | backward-embedding-all-reduce: 0.01 | optimizer: 92.53 | batch-generator: 440.61
 iteration      115/     200 | consumed samples:        29440 | consumed tokens:     30146560 | elapsed time per iteration (ms): 4221.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.483855E+00 | moe loss: 8.500890E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2024.55 | backward-compute: 2081.21 | backward-embedding-all-reduce: 0.01 | optimizer: 105.74 | batch-generator: 373.14
[2022-12-23 03:41:11,937] [INFO] [logging.py:68:log_dist] [Rank 0] step=115, skipped=0, lr=[9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:11,949] [INFO] [timer.py:197:stop] 0/115, RunningAvgSamplesPerSec=41.58358897294244, CurrSamplesPerSec=35.31027051145249, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:41:16,171] [INFO] [logging.py:68:log_dist] [Rank 0] step=116, skipped=0, lr=[9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:16,172] [INFO] [timer.py:197:stop] 0/116, RunningAvgSamplesPerSec=41.60398768650036, CurrSamplesPerSec=44.04551118368654, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      116/     200 | consumed samples:        29696 | consumed tokens:     30408704 | elapsed time per iteration (ms): 4240.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.478819E+00 | moe loss: 8.380611E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2073.59 | backward-compute: 2053.48 | backward-embedding-all-reduce: 0.01 | optimizer: 89.55 | batch-generator: 369.93
[2022-12-23 03:41:20,962] [INFO] [logging.py:68:log_dist] [Rank 0] step=117, skipped=0, lr=[9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:20,962] [INFO] [timer.py:197:stop] 0/117, RunningAvgSamplesPerSec=41.61125121844703, CurrSamplesPerSec=42.45625653603972, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      117/     200 | consumed samples:        29952 | consumed tokens:     30670848 | elapsed time per iteration (ms): 4807.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.477545E+00 | moe loss: 8.347472E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2546.74 | backward-compute: 2123.52 | backward-embedding-all-reduce: 0.01 | optimizer: 111.82 | batch-generator: 482.63
[2022-12-23 03:41:25,093] [INFO] [logging.py:68:log_dist] [Rank 0] step=118, skipped=0, lr=[9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:25,093] [INFO] [timer.py:197:stop] 0/118, RunningAvgSamplesPerSec=41.63373421759764, CurrSamplesPerSec=44.392067107177134, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      118/     200 | consumed samples:        30208 | consumed tokens:     30932992 | elapsed time per iteration (ms): 4114.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.477553E+00 | moe loss: 8.410691E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1967.67 | backward-compute: 2047.91 | backward-embedding-all-reduce: 0.01 | optimizer: 89.50 | batch-generator: 391.36
[2022-12-23 03:41:29,149] [INFO] [logging.py:68:log_dist] [Rank 0] step=119, skipped=0, lr=[9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:29,149] [INFO] [timer.py:197:stop] 0/119, RunningAvgSamplesPerSec=41.65518133534134, CurrSamplesPerSec=44.30252316085468, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      119/     200 | consumed samples:        30464 | consumed tokens:     31195136 | elapsed time per iteration (ms): 4055.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.462333E+00 | moe loss: 8.344910E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1918.99 | backward-compute: 2038.19 | backward-embedding-all-reduce: 0.01 | optimizer: 90.78 | batch-generator: 372.91
 iteration      120/     200 | consumed samples:        30720 | consumed tokens:     31457280 | elapsed time per iteration (ms): 5230.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.460183E+00 | moe loss: 8.286844E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2855.56 | backward-compute: 2240.23 | backward-embedding-all-reduce: 0.01 | optimizer: 114.58 | batch-generator: 428.64
[2022-12-23 03:41:34,387] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:34,397] [INFO] [timer.py:197:stop] 0/120, RunningAvgSamplesPerSec=41.610428538160896, CurrSamplesPerSec=36.96403120211422, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:41:38,452] [INFO] [logging.py:68:log_dist] [Rank 0] step=121, skipped=0, lr=[9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:38,453] [INFO] [timer.py:197:stop] 0/121, RunningAvgSamplesPerSec=41.62933822994983, CurrSamplesPerSec=43.98818833896714, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      121/     200 | consumed samples:        30976 | consumed tokens:     31719424 | elapsed time per iteration (ms): 4073.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.448993E+00 | moe loss: 8.319227E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1922.42 | backward-compute: 2037.56 | backward-embedding-all-reduce: 0.01 | optimizer: 89.83 | batch-generator: 374.67
[2022-12-23 03:41:42,516] [INFO] [logging.py:68:log_dist] [Rank 0] step=122, skipped=0, lr=[9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:42,517] [INFO] [timer.py:197:stop] 0/122, RunningAvgSamplesPerSec=41.64882650730472, CurrSamplesPerSec=44.1058966921332, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      122/     200 | consumed samples:        31232 | consumed tokens:     31981568 | elapsed time per iteration (ms): 4063.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.446171E+00 | moe loss: 8.327810E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1924.30 | backward-compute: 2039.96 | backward-embedding-all-reduce: 0.01 | optimizer: 91.19 | batch-generator: 374.19
[2022-12-23 03:41:46,586] [INFO] [logging.py:68:log_dist] [Rank 0] step=123, skipped=0, lr=[9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:46,587] [INFO] [timer.py:197:stop] 0/123, RunningAvgSamplesPerSec=41.66700438202112, CurrSamplesPerSec=43.96991570815957, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      123/     200 | consumed samples:        31488 | consumed tokens:     32243712 | elapsed time per iteration (ms): 4069.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.439860E+00 | moe loss: 8.278131E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1928.15 | backward-compute: 2044.56 | backward-embedding-all-reduce: 0.01 | optimizer: 89.49 | batch-generator: 365.35
[2022-12-23 03:41:50,635] [INFO] [logging.py:68:log_dist] [Rank 0] step=124, skipped=0, lr=[9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:41:50,635] [INFO] [timer.py:197:stop] 0/124, RunningAvgSamplesPerSec=41.68501934004878, CurrSamplesPerSec=43.98615569501236, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      124/     200 | consumed samples:        31744 | consumed tokens:     32505856 | elapsed time per iteration (ms): 4048.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.427020E+00 | moe loss: 8.223014E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1900.29 | backward-compute: 2049.45 | backward-embedding-all-reduce: 0.01 | optimizer: 90.27 | batch-generator: 346.95
[2022-12-23 03:41:56,012] [INFO] [logging.py:68:log_dist] [Rank 0] step=125, skipped=0, lr=[9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      125/     200 | consumed samples:        32000 | consumed tokens:     32768000 | elapsed time per iteration (ms): 5369.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.428580E+00 | moe loss: 8.249114E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2696.04 | backward-compute: 2546.47 | backward-embedding-all-reduce: 0.01 | optimizer: 111.03 | batch-generator: 455.34
[2022-12-23 03:41:56,017] [INFO] [timer.py:197:stop] 0/125, RunningAvgSamplesPerSec=41.66941381357841, CurrSamplesPerSec=39.849378556502934, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:42:00,067] [INFO] [logging.py:68:log_dist] [Rank 0] step=126, skipped=0, lr=[9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:00,067] [INFO] [timer.py:197:stop] 0/126, RunningAvgSamplesPerSec=41.68892436306754, CurrSamplesPerSec=44.23656780121882, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      126/     200 | consumed samples:        32256 | consumed tokens:     33030144 | elapsed time per iteration (ms): 4062.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.434927E+00 | moe loss: 8.209766E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1910.91 | backward-compute: 2043.33 | backward-embedding-all-reduce: 0.01 | optimizer: 89.69 | batch-generator: 356.89
[2022-12-23 03:42:04,122] [INFO] [logging.py:68:log_dist] [Rank 0] step=127, skipped=0, lr=[9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:04,122] [INFO] [timer.py:197:stop] 0/127, RunningAvgSamplesPerSec=41.71080456361393, CurrSamplesPerSec=44.61433739341091, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      127/     200 | consumed samples:        32512 | consumed tokens:     33292288 | elapsed time per iteration (ms): 4055.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.430586E+00 | moe loss: 8.185971E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1910.38 | backward-compute: 2046.78 | backward-embedding-all-reduce: 0.01 | optimizer: 89.57 | batch-generator: 355.55
[2022-12-23 03:42:08,201] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:08,201] [INFO] [timer.py:197:stop] 0/128, RunningAvgSamplesPerSec=41.73165663550414, CurrSamplesPerSec=44.51329319043203, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      128/     200 | consumed samples:        32768 | consumed tokens:     33554432 | elapsed time per iteration (ms): 4078.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.414253E+00 | moe loss: 8.167260E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1934.07 | backward-compute: 2047.79 | backward-embedding-all-reduce: 0.01 | optimizer: 89.45 | batch-generator: 383.21
[2022-12-23 03:42:13,109] [INFO] [logging.py:68:log_dist] [Rank 0] step=129, skipped=0, lr=[9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:13,118] [INFO] [timer.py:197:stop] 0/129, RunningAvgSamplesPerSec=41.58075681091038, CurrSamplesPerSec=28.56585528644342, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      129/     200 | consumed samples:        33024 | consumed tokens:     33816576 | elapsed time per iteration (ms): 4936.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.416340E+00 | moe loss: 8.210622E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2146.58 | backward-compute: 2603.94 | backward-embedding-all-reduce: 0.01 | optimizer: 156.99 | batch-generator: 353.89
[2022-12-23 03:42:17,550] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:17,550] [INFO] [timer.py:197:stop] 0/130, RunningAvgSamplesPerSec=41.596224269866966, CurrSamplesPerSec=43.658761094187355, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      130/     200 | consumed samples:        33280 | consumed tokens:     34078720 | elapsed time per iteration (ms): 4412.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.420138E+00 | moe loss: 8.223332E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2218.02 | backward-compute: 2092.57 | backward-embedding-all-reduce: 0.01 | optimizer: 92.16 | batch-generator: 445.99
[2022-12-23 03:42:21,611] [INFO] [logging.py:68:log_dist] [Rank 0] step=131, skipped=0, lr=[9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:21,612] [INFO] [timer.py:197:stop] 0/131, RunningAvgSamplesPerSec=41.61182469360323, CurrSamplesPerSec=43.710159804339824, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      131/     200 | consumed samples:        33536 | consumed tokens:     34340864 | elapsed time per iteration (ms): 4061.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.393735E+00 | moe loss: 8.175661E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1906.58 | backward-compute: 2050.82 | backward-embedding-all-reduce: 0.01 | optimizer: 94.50 | batch-generator: 352.11
[2022-12-23 03:42:25,727] [INFO] [logging.py:68:log_dist] [Rank 0] step=132, skipped=0, lr=[9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:25,727] [INFO] [timer.py:197:stop] 0/132, RunningAvgSamplesPerSec=41.624301808840386, CurrSamplesPerSec=43.299114130680245, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      132/     200 | consumed samples:        33792 | consumed tokens:     34603008 | elapsed time per iteration (ms): 4115.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.390085E+00 | moe loss: 8.218336E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1957.64 | backward-compute: 2059.52 | backward-embedding-all-reduce: 0.01 | optimizer: 91.01 | batch-generator: 379.48
[2022-12-23 03:42:29,781] [INFO] [logging.py:68:log_dist] [Rank 0] step=133, skipped=0, lr=[9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:29,782] [INFO] [timer.py:197:stop] 0/133, RunningAvgSamplesPerSec=41.64471719415848, CurrSamplesPerSec=44.48085338932806, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      133/     200 | consumed samples:        34048 | consumed tokens:     34865152 | elapsed time per iteration (ms): 4054.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.401710E+00 | moe loss: 8.205337E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1913.93 | backward-compute: 2040.83 | backward-embedding-all-reduce: 0.01 | optimizer: 91.35 | batch-generator: 345.86
[2022-12-23 03:42:34,950] [INFO] [logging.py:68:log_dist] [Rank 0] step=134, skipped=0, lr=[9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:34,954] [INFO] [timer.py:197:stop] 0/134, RunningAvgSamplesPerSec=41.64299952657159, CurrSamplesPerSec=41.41920356639513, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      134/     200 | consumed samples:        34304 | consumed tokens:     35127296 | elapsed time per iteration (ms): 5174.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.394153E+00 | moe loss: 8.171657E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2653.52 | backward-compute: 2390.84 | backward-embedding-all-reduce: 0.01 | optimizer: 121.39 | batch-generator: 434.08
[2022-12-23 03:42:39,058] [INFO] [logging.py:68:log_dist] [Rank 0] step=135, skipped=0, lr=[9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:39,059] [INFO] [timer.py:197:stop] 0/135, RunningAvgSamplesPerSec=41.65336540888832, CurrSamplesPerSec=43.0685006761054, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      135/     200 | consumed samples:        34560 | consumed tokens:     35389440 | elapsed time per iteration (ms): 4103.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.377326E+00 | moe loss: 8.124522E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1956.55 | backward-compute: 2042.39 | backward-embedding-all-reduce: 0.01 | optimizer: 90.86 | batch-generator: 365.56
[2022-12-23 03:42:43,152] [INFO] [logging.py:68:log_dist] [Rank 0] step=136, skipped=0, lr=[9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:43,152] [INFO] [timer.py:197:stop] 0/136, RunningAvgSamplesPerSec=41.65595267261069, CurrSamplesPerSec=42.002946702734775, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      136/     200 | consumed samples:        34816 | consumed tokens:     35651584 | elapsed time per iteration (ms): 4093.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.382463E+00 | moe loss: 8.180468E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1908.45 | backward-compute: 2083.82 | backward-embedding-all-reduce: 0.01 | optimizer: 93.45 | batch-generator: 342.73
[2022-12-23 03:42:47,203] [INFO] [logging.py:68:log_dist] [Rank 0] step=137, skipped=0, lr=[9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:47,203] [INFO] [timer.py:197:stop] 0/137, RunningAvgSamplesPerSec=41.6677931681857, CurrSamplesPerSec=43.31771412083917, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      137/     200 | consumed samples:        35072 | consumed tokens:     35913728 | elapsed time per iteration (ms): 4050.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.373429E+00 | moe loss: 8.183251E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1908.37 | backward-compute: 2044.43 | backward-embedding-all-reduce: 0.01 | optimizer: 89.45 | batch-generator: 331.58
[2022-12-23 03:42:51,237] [INFO] [logging.py:68:log_dist] [Rank 0] step=138, skipped=0, lr=[9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:51,237] [INFO] [timer.py:197:stop] 0/138, RunningAvgSamplesPerSec=41.67985175247224, CurrSamplesPerSec=43.374437087419615, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      138/     200 | consumed samples:        35328 | consumed tokens:     36175872 | elapsed time per iteration (ms): 4034.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.371444E+00 | moe loss: 8.192971E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1891.18 | backward-compute: 2043.64 | backward-embedding-all-reduce: 0.01 | optimizer: 91.00 | batch-generator: 326.01
[2022-12-23 03:42:56,603] [INFO] [logging.py:68:log_dist] [Rank 0] step=139, skipped=0, lr=[9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:42:56,606] [INFO] [timer.py:197:stop] 0/139, RunningAvgSamplesPerSec=41.67231340157798, CurrSamplesPerSec=40.671890890093735, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      139/     200 | consumed samples:        35584 | consumed tokens:     36438016 | elapsed time per iteration (ms): 5361.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.388811E+00 | moe loss: 8.177160E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2753.46 | backward-compute: 2482.56 | backward-embedding-all-reduce: 0.01 | optimizer: 109.24 | batch-generator: 447.49
[2022-12-23 03:43:00,642] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:00,643] [INFO] [timer.py:197:stop] 0/140, RunningAvgSamplesPerSec=41.68877369313624, CurrSamplesPerSec=44.07378724432699, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      140/     200 | consumed samples:        35840 | consumed tokens:     36700160 | elapsed time per iteration (ms): 4043.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.378741E+00 | moe loss: 8.176740E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1894.87 | backward-compute: 2045.23 | backward-embedding-all-reduce: 0.01 | optimizer: 89.45 | batch-generator: 345.64
 iteration      141/     200 | consumed samples:        36096 | consumed tokens:     36962304 | elapsed time per iteration (ms): 4109.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.353155E+00 | moe loss: 8.155227E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1928.44 | backward-compute: 2079.52 | backward-embedding-all-reduce: 0.01 | optimizer: 94.37 | batch-generator: 305.34
[2022-12-23 03:43:04,759] [INFO] [logging.py:68:log_dist] [Rank 0] step=141, skipped=0, lr=[9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:04,762] [INFO] [timer.py:197:stop] 0/141, RunningAvgSamplesPerSec=41.69933420365156, CurrSamplesPerSec=43.20985853055019, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:43:08,771] [INFO] [logging.py:68:log_dist] [Rank 0] step=142, skipped=0, lr=[9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:08,771] [INFO] [timer.py:197:stop] 0/142, RunningAvgSamplesPerSec=41.71612106236225, CurrSamplesPerSec=44.18879754524473, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      142/     200 | consumed samples:        36352 | consumed tokens:     37224448 | elapsed time per iteration (ms): 4019.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.352596E+00 | moe loss: 8.184959E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1875.16 | backward-compute: 2038.09 | backward-embedding-all-reduce: 0.01 | optimizer: 89.80 | batch-generator: 330.01
[2022-12-23 03:43:13,974] [INFO] [logging.py:68:log_dist] [Rank 0] step=143, skipped=0, lr=[9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:13,976] [INFO] [timer.py:197:stop] 0/143, RunningAvgSamplesPerSec=41.50706194594472, CurrSamplesPerSec=24.392876258125415, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      143/     200 | consumed samples:        36608 | consumed tokens:     37486592 | elapsed time per iteration (ms): 5206.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.369247E+00 | moe loss: 8.194559E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2623.29 | backward-compute: 2450.27 | backward-embedding-all-reduce: 0.01 | optimizer: 116.80 | batch-generator: 332.18
[2022-12-23 03:43:18,054] [INFO] [logging.py:68:log_dist] [Rank 0] step=144, skipped=0, lr=[9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:18,054] [INFO] [timer.py:197:stop] 0/144, RunningAvgSamplesPerSec=41.52438184292899, CurrSamplesPerSec=44.12023575896834, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      144/     200 | consumed samples:        36864 | consumed tokens:     37748736 | elapsed time per iteration (ms): 4076.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.350959E+00 | moe loss: 8.192296E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1942.59 | backward-compute: 2031.83 | backward-embedding-all-reduce: 0.01 | optimizer: 89.40 | batch-generator: 391.12
[2022-12-23 03:43:22,091] [INFO] [logging.py:68:log_dist] [Rank 0] step=145, skipped=0, lr=[9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:22,092] [INFO] [timer.py:197:stop] 0/145, RunningAvgSamplesPerSec=41.53788738008589, CurrSamplesPerSec=43.54918850868597, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      145/     200 | consumed samples:        37120 | consumed tokens:     38010880 | elapsed time per iteration (ms): 4037.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.346189E+00 | moe loss: 8.193926E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1893.37 | backward-compute: 2041.72 | backward-embedding-all-reduce: 0.01 | optimizer: 94.11 | batch-generator: 324.84
[2022-12-23 03:43:26,162] [INFO] [logging.py:68:log_dist] [Rank 0] step=146, skipped=0, lr=[9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:26,162] [INFO] [timer.py:197:stop] 0/146, RunningAvgSamplesPerSec=41.55107843364044, CurrSamplesPerSec=43.52776344520523, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      146/     200 | consumed samples:        37376 | consumed tokens:     38273024 | elapsed time per iteration (ms): 4070.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.360368E+00 | moe loss: 8.132260E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1908.71 | backward-compute: 2063.68 | backward-embedding-all-reduce: 0.01 | optimizer: 89.68 | batch-generator: 319.44
 iteration      147/     200 | consumed samples:        37632 | consumed tokens:     38535168 | elapsed time per iteration (ms): 4330.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.336853E+00 | moe loss: 8.105249E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2156.99 | backward-compute: 2070.40 | backward-embedding-all-reduce: 0.01 | optimizer: 92.08 | batch-generator: 324.51
[2022-12-23 03:43:30,499] [INFO] [logging.py:68:log_dist] [Rank 0] step=147, skipped=0, lr=[9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:30,500] [INFO] [timer.py:197:stop] 0/147, RunningAvgSamplesPerSec=41.55933981247958, CurrSamplesPerSec=42.78428602394583, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:43:35,523] [INFO] [logging.py:68:log_dist] [Rank 0] step=148, skipped=0, lr=[9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:35,525] [INFO] [timer.py:197:stop] 0/148, RunningAvgSamplesPerSec=41.567364709742755, CurrSamplesPerSec=42.764724081293224, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      148/     200 | consumed samples:        37888 | consumed tokens:     38797312 | elapsed time per iteration (ms): 5053.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.340657E+00 | moe loss: 8.128013E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2763.11 | backward-compute: 2145.26 | backward-embedding-all-reduce: 0.01 | optimizer: 118.69 | batch-generator: 405.54
[2022-12-23 03:43:39,621] [INFO] [logging.py:68:log_dist] [Rank 0] step=149, skipped=0, lr=[9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:39,621] [INFO] [timer.py:197:stop] 0/149, RunningAvgSamplesPerSec=41.582151302206945, CurrSamplesPerSec=43.86006761139623, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      149/     200 | consumed samples:        38144 | consumed tokens:     39059456 | elapsed time per iteration (ms): 4075.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.332023E+00 | moe loss: 8.136810E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1934.39 | backward-compute: 2045.11 | backward-embedding-all-reduce: 0.01 | optimizer: 89.59 | batch-generator: 366.00
 iteration      150/     200 | consumed samples:        38400 | consumed tokens:     39321600 | elapsed time per iteration (ms): 4105.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.337803E+00 | moe loss: 8.132873E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1936.89 | backward-compute: 2070.82 | backward-embedding-all-reduce: 0.01 | optimizer: 89.83 | batch-generator: 327.61
[2022-12-23 03:43:43,755] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:43,757] [INFO] [timer.py:197:stop] 0/150, RunningAvgSamplesPerSec=41.57956881036883, CurrSamplesPerSec=41.20340033578485, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:43:47,774] [INFO] [logging.py:68:log_dist] [Rank 0] step=151, skipped=0, lr=[9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:47,775] [INFO] [timer.py:197:stop] 0/151, RunningAvgSamplesPerSec=41.59786669515511, CurrSamplesPerSec=44.49589461899538, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      151/     200 | consumed samples:        38656 | consumed tokens:     39583744 | elapsed time per iteration (ms): 4048.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.321894E+00 | moe loss: 8.137883E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1876.04 | backward-compute: 2040.36 | backward-embedding-all-reduce: 0.01 | optimizer: 89.61 | batch-generator: 323.94
 iteration      152/     200 | consumed samples:        38912 | consumed tokens:     39845888 | elapsed time per iteration (ms): 4081.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.333094E+00 | moe loss: 8.130747E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1868.70 | backward-compute: 2105.87 | backward-embedding-all-reduce: 0.01 | optimizer: 98.40 | batch-generator: 296.53
[2022-12-23 03:43:51,861] [INFO] [logging.py:68:log_dist] [Rank 0] step=152, skipped=0, lr=[9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:51,863] [INFO] [timer.py:197:stop] 0/152, RunningAvgSamplesPerSec=41.58880286613406, CurrSamplesPerSec=40.281044119624305, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:43:56,574] [INFO] [logging.py:68:log_dist] [Rank 0] step=153, skipped=0, lr=[9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:43:56,574] [INFO] [timer.py:197:stop] 0/153, RunningAvgSamplesPerSec=41.599832001916134, CurrSamplesPerSec=43.3231951510253, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      153/     200 | consumed samples:        39168 | consumed tokens:     40108032 | elapsed time per iteration (ms): 4732.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.307112E+00 | moe loss: 8.108533E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2481.07 | backward-compute: 2116.13 | backward-embedding-all-reduce: 0.01 | optimizer: 109.11 | batch-generator: 586.09
[2022-12-23 03:44:00,669] [INFO] [logging.py:68:log_dist] [Rank 0] step=154, skipped=0, lr=[9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:00,669] [INFO] [timer.py:197:stop] 0/154, RunningAvgSamplesPerSec=41.61531919026107, CurrSamplesPerSec=44.09410186231908, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      154/     200 | consumed samples:        39424 | consumed tokens:     40370176 | elapsed time per iteration (ms): 4081.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.319602E+00 | moe loss: 8.174387E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1934.10 | backward-compute: 2049.83 | backward-embedding-all-reduce: 0.01 | optimizer: 90.15 | batch-generator: 368.34
 iteration      155/     200 | consumed samples:        39680 | consumed tokens:     40632320 | elapsed time per iteration (ms): 4403.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.321634E+00 | moe loss: 8.168578E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2046.77 | backward-compute: 2243.79 | backward-embedding-all-reduce: 0.01 | optimizer: 99.91 | batch-generator: 323.87
[2022-12-23 03:44:05,089] [INFO] [logging.py:68:log_dist] [Rank 0] step=155, skipped=0, lr=[9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:05,093] [INFO] [timer.py:197:stop] 0/155, RunningAvgSamplesPerSec=41.61619384259487, CurrSamplesPerSec=41.7495698838103, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:44:09,765] [INFO] [logging.py:68:log_dist] [Rank 0] step=156, skipped=0, lr=[9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:09,766] [INFO] [timer.py:197:stop] 0/156, RunningAvgSamplesPerSec=41.62972998903214, CurrSamplesPerSec=43.80993179022188, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      156/     200 | consumed samples:        39936 | consumed tokens:     40894464 | elapsed time per iteration (ms): 4708.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.310233E+00 | moe loss: 8.156716E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2443.79 | backward-compute: 2119.94 | backward-embedding-all-reduce: 0.01 | optimizer: 108.77 | batch-generator: 329.73
[2022-12-23 03:44:13,866] [INFO] [logging.py:68:log_dist] [Rank 0] step=157, skipped=0, lr=[9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:13,866] [INFO] [timer.py:197:stop] 0/157, RunningAvgSamplesPerSec=41.63315527605867, CurrSamplesPerSec=42.1674631452584, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      157/     200 | consumed samples:        40192 | consumed tokens:     41156608 | elapsed time per iteration (ms): 4084.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.306315E+00 | moe loss: 8.140675E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1897.84 | backward-compute: 2087.39 | backward-embedding-all-reduce: 0.01 | optimizer: 90.13 | batch-generator: 351.31
[2022-12-23 03:44:17,873] [INFO] [logging.py:68:log_dist] [Rank 0] step=158, skipped=0, lr=[9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:17,873] [INFO] [timer.py:197:stop] 0/158, RunningAvgSamplesPerSec=41.64828320264908, CurrSamplesPerSec=44.13396011500935, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      158/     200 | consumed samples:        40448 | consumed tokens:     41418752 | elapsed time per iteration (ms): 4006.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.309190E+00 | moe loss: 8.100650E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1858.86 | backward-compute: 2047.33 | backward-embedding-all-reduce: 0.01 | optimizer: 89.95 | batch-generator: 318.44
 iteration      159/     200 | consumed samples:        40704 | consumed tokens:     41680896 | elapsed time per iteration (ms): 4269.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.297820E+00 | moe loss: 8.137393E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1860.92 | backward-compute: 2280.70 | backward-embedding-all-reduce: 0.01 | optimizer: 119.50 | batch-generator: 292.50
[2022-12-23 03:44:22,165] [INFO] [logging.py:68:log_dist] [Rank 0] step=159, skipped=0, lr=[9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:22,180] [INFO] [timer.py:197:stop] 0/159, RunningAvgSamplesPerSec=41.56102043156863, CurrSamplesPerSec=31.322931541829842, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:44:26,942] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:26,942] [INFO] [timer.py:197:stop] 0/160, RunningAvgSamplesPerSec=41.57243392074258, CurrSamplesPerSec=43.44560629366645, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      160/     200 | consumed samples:        40960 | consumed tokens:     41943040 | elapsed time per iteration (ms): 4808.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.297008E+00 | moe loss: 8.144371E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2537.40 | backward-compute: 2116.14 | backward-embedding-all-reduce: 0.01 | optimizer: 103.66 | batch-generator: 371.29
[2022-12-23 03:44:30,954] [INFO] [logging.py:68:log_dist] [Rank 0] step=161, skipped=0, lr=[9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:30,954] [INFO] [timer.py:197:stop] 0/161, RunningAvgSamplesPerSec=41.58706137520991, CurrSamplesPerSec=44.0351066332324, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      161/     200 | consumed samples:        41216 | consumed tokens:     42205184 | elapsed time per iteration (ms): 4002.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.303085E+00 | moe loss: 8.128769E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1856.91 | backward-compute: 2045.25 | backward-embedding-all-reduce: 0.01 | optimizer: 91.82 | batch-generator: 306.99
 iteration      162/     200 | consumed samples:        41472 | consumed tokens:     42467328 | elapsed time per iteration (ms): 4310.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.299520E+00 | moe loss: 8.140828E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2139.88 | backward-compute: 2071.65 | backward-embedding-all-reduce: 0.01 | optimizer: 89.94 | batch-generator: 313.39
[2022-12-23 03:44:35,278] [INFO] [logging.py:68:log_dist] [Rank 0] step=162, skipped=0, lr=[9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:35,278] [INFO] [timer.py:197:stop] 0/162, RunningAvgSamplesPerSec=41.51848607123357, CurrSamplesPerSec=32.89415583349325, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:44:39,335] [INFO] [logging.py:68:log_dist] [Rank 0] step=163, skipped=0, lr=[9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:39,336] [INFO] [timer.py:197:stop] 0/163, RunningAvgSamplesPerSec=41.53064863947241, CurrSamplesPerSec=43.57295444370823, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      163/     200 | consumed samples:        41728 | consumed tokens:     42729472 | elapsed time per iteration (ms): 4071.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.303415E+00 | moe loss: 8.137452E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1912.86 | backward-compute: 2045.65 | backward-embedding-all-reduce: 0.01 | optimizer: 90.40 | batch-generator: 329.83
[2022-12-23 03:44:43,722] [INFO] [logging.py:68:log_dist] [Rank 0] step=164, skipped=0, lr=[9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:43,724] [INFO] [timer.py:197:stop] 0/164, RunningAvgSamplesPerSec=41.50734118675318, CurrSamplesPerSec=38.067732895111156, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      164/     200 | consumed samples:        41984 | consumed tokens:     42991616 | elapsed time per iteration (ms): 4402.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.294917E+00 | moe loss: 8.176213E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2061.72 | backward-compute: 2220.57 | backward-embedding-all-reduce: 0.01 | optimizer: 108.05 | batch-generator: 294.87
[2022-12-23 03:44:47,726] [INFO] [logging.py:68:log_dist] [Rank 0] step=165, skipped=0, lr=[9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:47,726] [INFO] [timer.py:197:stop] 0/165, RunningAvgSamplesPerSec=41.52347765867209, CurrSamplesPerSec=44.31437124661124, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      165/     200 | consumed samples:        42240 | consumed tokens:     43253760 | elapsed time per iteration (ms): 3988.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.295231E+00 | moe loss: 8.116753E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1848.66 | backward-compute: 2043.04 | backward-embedding-all-reduce: 0.01 | optimizer: 91.35 | batch-generator: 292.43
 iteration      166/     200 | consumed samples:        42496 | consumed tokens:     43515904 | elapsed time per iteration (ms): 4168.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.266558E+00 | moe loss: 8.092827E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1845.20 | backward-compute: 2165.92 | backward-embedding-all-reduce: 0.01 | optimizer: 146.99 | batch-generator: 286.62
[2022-12-23 03:44:51,922] [INFO] [logging.py:68:log_dist] [Rank 0] step=166, skipped=0, lr=[9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:51,936] [INFO] [timer.py:197:stop] 0/166, RunningAvgSamplesPerSec=41.46603653905278, CurrSamplesPerSec=33.83644182438534, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:44:56,657] [INFO] [logging.py:68:log_dist] [Rank 0] step=167, skipped=0, lr=[9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:44:56,658] [INFO] [timer.py:197:stop] 0/167, RunningAvgSamplesPerSec=41.48000304682198, CurrSamplesPerSec=43.90524784869803, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      167/     200 | consumed samples:        42752 | consumed tokens:     43778048 | elapsed time per iteration (ms): 4771.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.273587E+00 | moe loss: 8.131847E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2223.41 | backward-compute: 2388.91 | backward-embedding-all-reduce: 0.01 | optimizer: 103.15 | batch-generator: 373.03
[2022-12-23 03:45:00,730] [INFO] [logging.py:68:log_dist] [Rank 0] step=168, skipped=0, lr=[9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:00,731] [INFO] [timer.py:197:stop] 0/168, RunningAvgSamplesPerSec=41.49623715632878, CurrSamplesPerSec=44.360903309309336, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      168/     200 | consumed samples:        43008 | consumed tokens:     44040192 | elapsed time per iteration (ms): 4063.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.256269E+00 | moe loss: 8.099400E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1921.22 | backward-compute: 2045.13 | backward-embedding-all-reduce: 0.01 | optimizer: 89.35 | batch-generator: 354.79
 iteration      169/     200 | consumed samples:        43264 | consumed tokens:     44302336 | elapsed time per iteration (ms): 4354.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.259130E+00 | moe loss: 8.088233E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2024.45 | backward-compute: 2221.40 | backward-embedding-all-reduce: 0.01 | optimizer: 96.85 | batch-generator: 303.34
[2022-12-23 03:45:05,102] [INFO] [logging.py:68:log_dist] [Rank 0] step=169, skipped=0, lr=[9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:05,103] [INFO] [timer.py:197:stop] 0/169, RunningAvgSamplesPerSec=41.48930376624397, CurrSamplesPerSec=40.36960924853266, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:45:09,145] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:09,145] [INFO] [timer.py:197:stop] 0/170, RunningAvgSamplesPerSec=41.506827180699204, CurrSamplesPerSec=44.65664276310252, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      170/     200 | consumed samples:        43520 | consumed tokens:     44564480 | elapsed time per iteration (ms): 4060.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.273275E+00 | moe loss: 8.120339E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1901.68 | backward-compute: 2042.31 | backward-embedding-all-reduce: 0.01 | optimizer: 89.37 | batch-generator: 283.80
[2022-12-23 03:45:13,418] [INFO] [logging.py:68:log_dist] [Rank 0] step=171, skipped=0, lr=[9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:13,419] [INFO] [timer.py:197:stop] 0/171, RunningAvgSamplesPerSec=41.467947446352845, CurrSamplesPerSec=35.82956456278088, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      171/     200 | consumed samples:        43776 | consumed tokens:     44826624 | elapsed time per iteration (ms): 4306.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.261352E+00 | moe loss: 8.119183E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1958.34 | backward-compute: 2175.55 | backward-embedding-all-reduce: 0.01 | optimizer: 143.95 | batch-generator: 309.72
[2022-12-23 03:45:17,823] [INFO] [logging.py:68:log_dist] [Rank 0] step=172, skipped=0, lr=[9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:17,824] [INFO] [timer.py:197:stop] 0/172, RunningAvgSamplesPerSec=41.48148964769938, CurrSamplesPerSec=43.904601556215816, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      172/     200 | consumed samples:        44032 | consumed tokens:     45088768 | elapsed time per iteration (ms): 4372.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.261256E+00 | moe loss: 8.081827E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2166.68 | backward-compute: 2086.67 | backward-embedding-all-reduce: 0.01 | optimizer: 92.06 | batch-generator: 378.51
 iteration      173/     200 | consumed samples:        44288 | consumed tokens:     45350912 | elapsed time per iteration (ms): 4129.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.241027E+00 | moe loss: 8.087796E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1861.97 | backward-compute: 2129.00 | backward-embedding-all-reduce: 0.01 | optimizer: 127.35 | batch-generator: 316.74
[2022-12-23 03:45:21,967] [INFO] [logging.py:68:log_dist] [Rank 0] step=173, skipped=0, lr=[9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:21,977] [INFO] [timer.py:197:stop] 0/173, RunningAvgSamplesPerSec=41.45162305470183, CurrSamplesPerSec=36.931250084267525, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:45:26,306] [INFO] [logging.py:68:log_dist] [Rank 0] step=174, skipped=0, lr=[9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:26,307] [INFO] [timer.py:197:stop] 0/174, RunningAvgSamplesPerSec=41.45159117019894, CurrSamplesPerSec=41.44613964145487, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      174/     200 | consumed samples:        44544 | consumed tokens:     45613056 | elapsed time per iteration (ms): 4356.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.242166E+00 | moe loss: 8.100244E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2042.15 | backward-compute: 2190.55 | backward-embedding-all-reduce: 0.01 | optimizer: 92.78 | batch-generator: 344.44
[2022-12-23 03:45:30,294] [INFO] [logging.py:68:log_dist] [Rank 0] step=175, skipped=0, lr=[9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:30,294] [INFO] [timer.py:197:stop] 0/175, RunningAvgSamplesPerSec=41.46576699948101, CurrSamplesPerSec=44.05728002909628, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      175/     200 | consumed samples:        44800 | consumed tokens:     45875200 | elapsed time per iteration (ms): 3985.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.246202E+00 | moe loss: 8.087651E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1846.24 | backward-compute: 2038.38 | backward-embedding-all-reduce: 0.01 | optimizer: 89.65 | batch-generator: 296.95
[2022-12-23 03:45:34,496] [INFO] [logging.py:68:log_dist] [Rank 0] step=176, skipped=0, lr=[9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:34,498] [INFO] [timer.py:197:stop] 0/176, RunningAvgSamplesPerSec=41.47949900005922, CurrSamplesPerSec=44.0003448751778, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      176/     200 | consumed samples:        45056 | consumed tokens:     46137344 | elapsed time per iteration (ms): 4205.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.236970E+00 | moe loss: 8.110213E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1901.78 | backward-compute: 2193.58 | backward-embedding-all-reduce: 0.01 | optimizer: 100.36 | batch-generator: 328.28
[2022-12-23 03:45:38,482] [INFO] [logging.py:68:log_dist] [Rank 0] step=177, skipped=0, lr=[9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:38,482] [INFO] [timer.py:197:stop] 0/177, RunningAvgSamplesPerSec=41.49555460669279, CurrSamplesPerSec=44.49213335799635, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      177/     200 | consumed samples:        45312 | consumed tokens:     46399488 | elapsed time per iteration (ms): 3981.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.240032E+00 | moe loss: 8.071897E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1833.98 | backward-compute: 2050.14 | backward-embedding-all-reduce: 0.01 | optimizer: 89.83 | batch-generator: 284.47
 iteration      178/     200 | consumed samples:        45568 | consumed tokens:     46661632 | elapsed time per iteration (ms): 4318.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.234699E+00 | moe loss: 8.076146E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1959.45 | backward-compute: 2250.84 | backward-embedding-all-reduce: 0.01 | optimizer: 96.45 | batch-generator: 295.54
[2022-12-23 03:45:42,810] [INFO] [logging.py:68:log_dist] [Rank 0] step=178, skipped=0, lr=[9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:42,821] [INFO] [timer.py:197:stop] 0/178, RunningAvgSamplesPerSec=41.48906425736274, CurrSamplesPerSec=40.383687025039094, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:45:46,944] [INFO] [logging.py:68:log_dist] [Rank 0] step=179, skipped=0, lr=[9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:46,945] [INFO] [timer.py:197:stop] 0/179, RunningAvgSamplesPerSec=41.4994486276032, CurrSamplesPerSec=43.411796745774375, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      179/     200 | consumed samples:        45824 | consumed tokens:     46923776 | elapsed time per iteration (ms): 4151.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.226776E+00 | moe loss: 8.103817E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1969.71 | backward-compute: 2056.58 | backward-embedding-all-reduce: 0.01 | optimizer: 98.46 | batch-generator: 308.23
[2022-12-23 03:45:50,946] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:50,947] [INFO] [timer.py:197:stop] 0/180, RunningAvgSamplesPerSec=41.51301797253006, CurrSamplesPerSec=44.063166810569825, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      180/     200 | consumed samples:        46080 | consumed tokens:     47185920 | elapsed time per iteration (ms): 3994.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.216291E+00 | moe loss: 8.091989E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1849.50 | backward-compute: 2046.14 | backward-embedding-all-reduce: 0.01 | optimizer: 90.86 | batch-generator: 281.81
[2022-12-23 03:45:55,534] [INFO] [logging.py:68:log_dist] [Rank 0] step=181, skipped=0, lr=[9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:55,535] [INFO] [timer.py:197:stop] 0/181, RunningAvgSamplesPerSec=41.51952446876267, CurrSamplesPerSec=42.71110580740218, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      181/     200 | consumed samples:        46336 | consumed tokens:     47448064 | elapsed time per iteration (ms): 4596.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.226315E+00 | moe loss: 8.503555E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2299.04 | backward-compute: 2168.18 | backward-embedding-all-reduce: 0.01 | optimizer: 108.57 | batch-generator: 320.97
[2022-12-23 03:45:59,538] [INFO] [logging.py:68:log_dist] [Rank 0] step=182, skipped=0, lr=[9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:45:59,538] [INFO] [timer.py:197:stop] 0/182, RunningAvgSamplesPerSec=41.531681171917235, CurrSamplesPerSec=43.82875864417871, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      182/     200 | consumed samples:        46592 | consumed tokens:     47710208 | elapsed time per iteration (ms): 3994.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.235281E+00 | moe loss: 8.079680E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1841.26 | backward-compute: 2051.08 | backward-embedding-all-reduce: 0.01 | optimizer: 90.81 | batch-generator: 297.58
 iteration      183/     200 | consumed samples:        46848 | consumed tokens:     47972352 | elapsed time per iteration (ms): 4412.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.222421E+00 | moe loss: 8.099598E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1999.65 | backward-compute: 2305.22 | backward-embedding-all-reduce: 0.01 | optimizer: 97.86 | batch-generator: 318.28
[2022-12-23 03:46:03,957] [INFO] [logging.py:68:log_dist] [Rank 0] step=183, skipped=0, lr=[9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:03,966] [INFO] [timer.py:197:stop] 0/183, RunningAvgSamplesPerSec=41.515104921830485, CurrSamplesPerSec=38.73248048481351, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:46:07,949] [INFO] [logging.py:68:log_dist] [Rank 0] step=184, skipped=0, lr=[9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:07,949] [INFO] [timer.py:197:stop] 0/184, RunningAvgSamplesPerSec=41.529754198103845, CurrSamplesPerSec=44.36317602351533, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      184/     200 | consumed samples:        47104 | consumed tokens:     48234496 | elapsed time per iteration (ms): 3999.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.212231E+00 | moe loss: 8.110151E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1840.81 | backward-compute: 2045.29 | backward-embedding-all-reduce: 0.01 | optimizer: 91.00 | batch-generator: 291.07
 iteration      185/     200 | consumed samples:        47360 | consumed tokens:     48496640 | elapsed time per iteration (ms): 4079.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.209190E+00 | moe loss: 8.096346E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1828.33 | backward-compute: 2116.51 | backward-embedding-all-reduce: 0.01 | optimizer: 126.07 | batch-generator: 288.29
[2022-12-23 03:46:12,050] [INFO] [logging.py:68:log_dist] [Rank 0] step=185, skipped=0, lr=[9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:12,052] [INFO] [timer.py:197:stop] 0/185, RunningAvgSamplesPerSec=41.5040229755508, CurrSamplesPerSec=37.29811877798379, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:46:16,493] [INFO] [logging.py:68:log_dist] [Rank 0] step=186, skipped=0, lr=[9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:16,494] [INFO] [timer.py:197:stop] 0/186, RunningAvgSamplesPerSec=41.517974981115174, CurrSamplesPerSec=44.23946938106873, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      186/     200 | consumed samples:        47616 | consumed tokens:     48758784 | elapsed time per iteration (ms): 4471.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.214234E+00 | moe loss: 8.115119E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2112.31 | backward-compute: 2214.30 | backward-embedding-all-reduce: 0.01 | optimizer: 99.10 | batch-generator: 325.19
[2022-12-23 03:46:20,470] [INFO] [logging.py:68:log_dist] [Rank 0] step=187, skipped=0, lr=[9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:20,471] [INFO] [timer.py:197:stop] 0/187, RunningAvgSamplesPerSec=41.5317951113202, CurrSamplesPerSec=44.24151092162804, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      187/     200 | consumed samples:        47872 | consumed tokens:     49020928 | elapsed time per iteration (ms): 3970.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.217832E+00 | moe loss: 8.101507E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1839.17 | backward-compute: 2032.32 | backward-embedding-all-reduce: 0.01 | optimizer: 90.97 | batch-generator: 289.69
 iteration      188/     200 | consumed samples:        48128 | consumed tokens:     49283072 | elapsed time per iteration (ms): 4480.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.214196E+00 | moe loss: 8.106852E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2090.18 | backward-compute: 2283.59 | backward-embedding-all-reduce: 0.01 | optimizer: 99.08 | batch-generator: 347.46
[2022-12-23 03:46:24,966] [INFO] [logging.py:68:log_dist] [Rank 0] step=188, skipped=0, lr=[9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:24,972] [INFO] [timer.py:197:stop] 0/188, RunningAvgSamplesPerSec=41.526199355050295, CurrSamplesPerSec=40.51629659928427, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:46:28,991] [INFO] [logging.py:68:log_dist] [Rank 0] step=189, skipped=0, lr=[9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:28,991] [INFO] [timer.py:197:stop] 0/189, RunningAvgSamplesPerSec=41.540406005001884, CurrSamplesPerSec=44.36338131239429, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      189/     200 | consumed samples:        48384 | consumed tokens:     49545216 | elapsed time per iteration (ms): 4040.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.212699E+00 | moe loss: 8.095458E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1875.67 | backward-compute: 2043.82 | backward-embedding-all-reduce: 0.01 | optimizer: 89.72 | batch-generator: 340.60
 iteration      190/     200 | consumed samples:        48640 | consumed tokens:     49807360 | elapsed time per iteration (ms): 4090.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.199721E+00 | moe loss: 8.073665E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1944.02 | backward-compute: 2044.63 | backward-embedding-all-reduce: 0.01 | optimizer: 92.57 | batch-generator: 285.77
[2022-12-23 03:46:33,125] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:33,125] [INFO] [timer.py:197:stop] 0/190, RunningAvgSamplesPerSec=41.5408664106058, CurrSamplesPerSec=41.62714202595057, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:46:37,725] [INFO] [logging.py:68:log_dist] [Rank 0] step=191, skipped=0, lr=[9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:37,726] [INFO] [timer.py:197:stop] 0/191, RunningAvgSamplesPerSec=41.53778655781411, CurrSamplesPerSec=40.96677612080612, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      191/     200 | consumed samples:        48896 | consumed tokens:     50069504 | elapsed time per iteration (ms): 4657.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.205076E+00 | moe loss: 8.084971E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2305.17 | backward-compute: 2107.97 | backward-embedding-all-reduce: 0.01 | optimizer: 125.46 | batch-generator: 446.91
 iteration      192/     200 | consumed samples:        49152 | consumed tokens:     50331648 | elapsed time per iteration (ms): 4192.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.208241E+00 | moe loss: 8.071601E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2009.70 | backward-compute: 2078.98 | backward-embedding-all-reduce: 0.01 | optimizer: 92.07 | batch-generator: 280.87
[2022-12-23 03:46:41,940] [INFO] [logging.py:68:log_dist] [Rank 0] step=192, skipped=0, lr=[9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:41,941] [INFO] [timer.py:197:stop] 0/192, RunningAvgSamplesPerSec=41.485513534118596, CurrSamplesPerSec=33.51427352175256, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:46:46,070] [INFO] [logging.py:68:log_dist] [Rank 0] step=193, skipped=0, lr=[9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:46,070] [INFO] [timer.py:197:stop] 0/193, RunningAvgSamplesPerSec=41.495082189260266, CurrSamplesPerSec=43.39689006323081, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      193/     200 | consumed samples:        49408 | consumed tokens:     50593792 | elapsed time per iteration (ms): 4140.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.187655E+00 | moe loss: 8.092776E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1974.89 | backward-compute: 2053.80 | backward-embedding-all-reduce: 0.01 | optimizer: 93.82 | batch-generator: 288.58
[2022-12-23 03:46:50,052] [INFO] [logging.py:68:log_dist] [Rank 0] step=194, skipped=0, lr=[9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:50,052] [INFO] [timer.py:197:stop] 0/194, RunningAvgSamplesPerSec=41.507212044145206, CurrSamplesPerSec=43.96173541413232, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      194/     200 | consumed samples:        49664 | consumed tokens:     50855936 | elapsed time per iteration (ms): 3980.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.197723E+00 | moe loss: 8.097839E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1836.26 | backward-compute: 2043.54 | backward-embedding-all-reduce: 0.01 | optimizer: 89.48 | batch-generator: 284.58
[2022-12-23 03:46:54,860] [INFO] [logging.py:68:log_dist] [Rank 0] step=195, skipped=0, lr=[9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      195/     200 | consumed samples:        49920 | consumed tokens:     51118080 | elapsed time per iteration (ms): 4806.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.187702E+00 | moe loss: 8.078847E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2123.51 | backward-compute: 2559.42 | backward-embedding-all-reduce: 0.01 | optimizer: 111.04 | batch-generator: 324.25
[2022-12-23 03:46:54,868] [INFO] [timer.py:197:stop] 0/195, RunningAvgSamplesPerSec=41.49791326497014, CurrSamplesPerSec=39.78655878087794, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[2022-12-23 03:46:58,876] [INFO] [logging.py:68:log_dist] [Rank 0] step=196, skipped=0, lr=[9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:46:58,877] [INFO] [timer.py:197:stop] 0/196, RunningAvgSamplesPerSec=41.51012424110906, CurrSamplesPerSec=44.00947751058613, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      196/     200 | consumed samples:        50176 | consumed tokens:     51380224 | elapsed time per iteration (ms): 4018.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.192344E+00 | moe loss: 8.059312E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1879.83 | backward-compute: 2032.73 | backward-embedding-all-reduce: 0.01 | optimizer: 89.54 | batch-generator: 282.97
[2022-12-23 03:47:02,893] [INFO] [logging.py:68:log_dist] [Rank 0] step=197, skipped=0, lr=[9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:47:02,893] [INFO] [timer.py:197:stop] 0/197, RunningAvgSamplesPerSec=41.524378901006436, CurrSamplesPerSec=44.48818103171104, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      197/     200 | consumed samples:        50432 | consumed tokens:     51642368 | elapsed time per iteration (ms): 4021.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.187592E+00 | moe loss: 8.100341E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1873.63 | backward-compute: 2043.04 | backward-embedding-all-reduce: 0.01 | optimizer: 92.10 | batch-generator: 273.17
[2022-12-23 03:47:06,982] [INFO] [logging.py:68:log_dist] [Rank 0] step=198, skipped=0, lr=[9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:47:06,982] [INFO] [timer.py:197:stop] 0/198, RunningAvgSamplesPerSec=41.53196558381993, CurrSamplesPerSec=43.06630340021633, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      198/     200 | consumed samples:        50688 | consumed tokens:     51904512 | elapsed time per iteration (ms): 4086.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.187243E+00 | moe loss: 8.067082E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1861.29 | backward-compute: 2120.43 | backward-embedding-all-reduce: 0.01 | optimizer: 97.30 | batch-generator: 286.07
[2022-12-23 03:47:10,947] [INFO] [logging.py:68:log_dist] [Rank 0] step=199, skipped=0, lr=[9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:47:10,948] [INFO] [timer.py:197:stop] 0/199, RunningAvgSamplesPerSec=41.54608599693601, CurrSamplesPerSec=44.51228934074798, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
 iteration      199/     200 | consumed samples:        50944 | consumed tokens:     52166656 | elapsed time per iteration (ms): 3962.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.190081E+00 | moe loss: 8.058312E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1829.45 | backward-compute: 2032.87 | backward-embedding-all-reduce: 0.01 | optimizer: 89.52 | batch-generator: 255.98
 iteration      200/     200 | consumed samples:        51200 | consumed tokens:     52428800 | elapsed time per iteration (ms): 5033.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.185815E+00 | moe loss: 8.104232E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[2022-12-23 03:47:15,985] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
time (ms) | forward-compute: 2387.30 | backward-compute: 2530.52 | backward-embedding-all-reduce: 0.01 | optimizer: 105.70 | batch-generator: 340.53
[2022-12-23 03:47:15,993] [INFO] [timer.py:197:stop] 0/200, RunningAvgSamplesPerSec=41.5389794118103, CurrSamplesPerSec=40.18485225185089, MemAllocated=18.13GB, MaxMemAllocated=28.16GB
[after training is done] datetime: 2022-12-23 03:47:16 
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.1614060401916504 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.16517162322998047 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.1616508960723877 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.16145849227905273 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.16145086288452148 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.1616077423095703 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.16950535774230957 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20373988151550293 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20338726043701172 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20287799835205078 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2032608985900879 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20354056358337402 seconds
Loading extension module utils...
Time to load utils op: 0.19802141189575195 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20379924774169922 seconds
