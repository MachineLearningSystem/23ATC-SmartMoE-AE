--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /mnt/cache/zhaishuming/Megatron-DeepSpeed/examples/MoE/ds_config_gpt_gpt-L16_H2560_MoE16B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-2-gpus-32-mp-1-pp-1-ep-16-mlc-0.01-cap-1.2-drop-true.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2560
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 16
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.2
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [16]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /mnt/cache/zhaishuming/Auto-Megatron/deepspeed/output/tensorboard/gpt-L16_H2560_MoE16B-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-2-gpus-32-mp-1-pp-1-ep-16-mlc-0.01-cap-1.2-drop-true_SH-IDC1-10-140-1-46_2022.12.23-03.19.59
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... 200
  train_samples ................................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 16
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,157] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,157] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,157] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,157] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,158] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-23 03:20:21,158] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,158] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,158] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,636] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,637] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,638] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,638] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,638] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,638] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
[2022-12-23 03:20:21,638] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/mnt/cache/zhaishuming/.local/lib/python3.10/site-packages/deepspeed']
deepspeed info ................... 0.7.7, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3
**** Git info for Megatron: git_hash=6f9e186 git_branch=moe ****
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[2022-12-23 03:20:21,642] [INFO] [comm.py:638:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:654:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=14, local_rank=6, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=9, local_rank=1, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=10, local_rank=2, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=11, local_rank=3, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=13, local_rank=5, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=15, local_rank=7, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=12, local_rank=4, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=16, master_addr=10.140.1.46, master_port=29500
[2022-12-23 03:20:22,807] [INFO] [comm.py:690:mpi_discovery] Discovered MPI settings of world_rank=8, local_rank=0, world_size=16, master_addr=10.140.1.46, master_port=29500
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.168 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 6.987 seconds
time to initialize megatron (seconds): -0.410
[after megatron is initialized] datetime: 2022-12-23 03:20:31 
building GPT model ...
[2022-12-23 03:20:31,710] [INFO] [utils.py:827:see_memory_usage] Before Building Model
[2022-12-23 03:20:31,711] [INFO] [utils.py:828:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-12-23 03:20:31,711] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 184.38 GB, percent = 18.3%
[2022-12-23 03:20:31,762] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:20:31,778] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:20:31,790] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:20:31,797] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:20:31,805] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:20:31,812] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:20:31,819] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:20:31,826] [INFO] [logging.py:68:log_dist] [Rank 0] Creating MoE layer with num_experts: 16 | num_local_experts: 1 | expert_parallel_size: 16
[2022-12-23 03:20:31,867] [INFO] [utils.py:827:see_memory_usage] After Building Model
[2022-12-23 03:20:31,868] [INFO] [utils.py:828:see_memory_usage] MA 2.59 GB         Max_MA 2.69 GB         CA 2.7 GB         Max_CA 3 GB 
[2022-12-23 03:20:31,869] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 184.42 GB, percent = 18.3%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1390556160
> learning rate decay style: cosine
DeepSpeed is enabled.
[2022-12-23 03:20:31,873] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.7, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_16
[2022-12-23 03:20:32,063] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert and data parallel groups with size 16
[2022-12-23 03:20:32,114] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [0]
[2022-12-23 03:20:32,124] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [1]
[2022-12-23 03:20:32,134] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [2]
[2022-12-23 03:20:32,145] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [3]
[2022-12-23 03:20:32,155] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [4]
[2022-12-23 03:20:32,165] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [5]
[2022-12-23 03:20:32,175] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [6]
[2022-12-23 03:20:32,186] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [7]
[2022-12-23 03:20:32,196] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [8]
[2022-12-23 03:20:32,206] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [9]
[2022-12-23 03:20:32,216] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [10]
[2022-12-23 03:20:32,226] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [11]
[2022-12-23 03:20:32,237] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [12]
[2022-12-23 03:20:32,247] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [13]
[2022-12-23 03:20:32,257] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [14]
[2022-12-23 03:20:32,257] [INFO] [logging.py:68:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_16 with ranks: [15]
[2022-12-23 03:20:32,267] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group named ep_size_16 with ranks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[2022-12-23 03:20:34,762] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-12-23 03:20:34,762] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-12-23 03:20:34,763] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-12-23 03:20:34,768] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2022-12-23 03:20:34,769] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2022-12-23 03:20:35,057] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2022-12-23 03:20:35,057] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2022-12-23 03:20:35,057] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f32ef344cd0>
[2022-12-23 03:20:35,057] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:20:35,058] [INFO] [config.py:1020:print] DeepSpeedEngine configuration:
[2022-12-23 03:20:35,058] [INFO] [config.py:1024:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   amp_enabled .................. False
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   amp_params ................... False
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   bfloat16_enabled ............. False
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   checkpoint_parallel_write_pipeline  False
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   checkpoint_tag_validation_enabled  True
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   checkpoint_tag_validation_fail  False
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f32ef346fe0>
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   communication_data_type ...... None
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   curriculum_enabled ........... False
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 424592, 'difficulty_step': 8}}
[2022-12-23 03:20:35,059] [INFO] [config.py:1024:print]   dataloader_drop_last ......... False
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   disable_allgather ............ False
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   dump_state ................... False
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   eigenvalue_enabled ........... False
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   eigenvalue_gas_boundary_resolution  1
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   eigenvalue_layer_num ......... 0
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   eigenvalue_max_iter .......... 100
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   eigenvalue_stability ......... 1e-06
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   eigenvalue_tol ............... 0.01
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   eigenvalue_verbose ........... False
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   elasticity_enabled ........... False
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   fp16_auto_cast ............... False
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   fp16_enabled ................. True
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   fp16_master_weights_and_gradients  False
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   global_rank .................. 0
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   grad_accum_dtype ............. None
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   gradient_accumulation_steps .. 8
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   gradient_clipping ............ 1.0
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   gradient_predivide_factor .... 1.0
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   initial_dynamic_scale ........ 2048
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   load_universal_checkpoint .... False
[2022-12-23 03:20:35,060] [INFO] [config.py:1024:print]   loss_scale ................... 0
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   memory_breakdown ............. False
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7f32e8072710>
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   optimizer_legacy_fusion ...... False
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   optimizer_name ............... None
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   optimizer_params ............. None
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   pld_enabled .................. False
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   pld_params ................... False
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   prescale_gradients ........... True
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   scheduler_name ............... None
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   scheduler_params ............. None
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   sparse_attention ............. None
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   sparse_gradients_enabled ..... False
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   steps_per_print .............. 1
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   train_batch_size ............. 256
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   train_micro_batch_size_per_gpu  2
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   use_node_local_storage ....... False
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   wall_clock_breakdown ......... False
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   world_size ................... 16
[2022-12-23 03:20:35,061] [INFO] [config.py:1024:print]   zero_allow_untested_optimizer  False
[2022-12-23 03:20:35,064] [INFO] [config.py:1024:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-12-23 03:20:35,064] [INFO] [config.py:1024:print]   zero_enabled ................. False
[2022-12-23 03:20:35,064] [INFO] [config.py:1024:print]   zero_optimization_stage ...... 0
[2022-12-23 03:20:35,064] [INFO] [config.py:1009:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 4.245920e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Emitting ninja build file /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3076779842376709 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-23 03:20:35 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      51200
    validation: 25600000
    test:       25600000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.031060 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_51200ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.035 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-23 03:20:37 
done with setup ...
training ...
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20763778686523438 seconds
time (ms) | model-and-optimizer-setup: 3711.45 | train/valid/test-data-iterators-setup: 1970.98
[before the start of training step] datetime: 2022-12-23 03:20:37 
 iteration        1/     200 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 8954.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.115744E+01 | moe loss: 1.253217E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[2022-12-23 03:20:46,361] [INFO] [logging.py:68:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
time (ms) | forward-compute: 6751.41 | backward-compute: 1815.40 | backward-embedding-all-reduce: 0.01 | optimizer: 385.51 | batch-generator: 984.85
[Rank 0] (after 1 iterations) memory (MB) | allocated: 18567.935546875 | max allocated: 28628.091796875 | reserved: 32188.0 | max reserved: 32188.0
 iteration        2/     200 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 3870.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.055691E+01 | moe loss: 1.693364E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2202.85 | backward-compute: 1563.81 | backward-embedding-all-reduce: 0.01 | optimizer: 94.89 | batch-generator: 896.53
[2022-12-23 03:20:50,240] [INFO] [logging.py:68:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:20:53,987] [INFO] [logging.py:68:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:20:53,989] [INFO] [timer.py:197:stop] 0/3, RunningAvgSamplesPerSec=52.33161854899144, CurrSamplesPerSec=52.33161854899144, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration        3/     200 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 3769.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.252943E+01 | moe loss: 1.691068E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2111.39 | backward-compute: 1530.30 | backward-embedding-all-reduce: 0.01 | optimizer: 100.70 | batch-generator: 1010.89
[2022-12-23 03:20:57,613] [INFO] [logging.py:68:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:20:57,613] [INFO] [timer.py:197:stop] 0/4, RunningAvgSamplesPerSec=52.986263092579684, CurrSamplesPerSec=53.65749372646049, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration        4/     200 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 3616.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.052687E+01 | moe loss: 1.622638E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2034.84 | backward-compute: 1482.20 | backward-embedding-all-reduce: 0.01 | optimizer: 90.51 | batch-generator: 969.46
[2022-12-23 03:21:01,734] [INFO] [logging.py:68:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:01,734] [INFO] [timer.py:197:stop] 0/5, RunningAvgSamplesPerSec=43.849325478233204, CurrSamplesPerSec=32.6046503831755, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration        5/     200 | consumed samples:         1280 | consumed tokens:      1310720 | elapsed time per iteration (ms): 4124.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.008139E+01 | moe loss: 1.370422E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2496.70 | backward-compute: 1517.79 | backward-embedding-all-reduce: 0.01 | optimizer: 95.34 | batch-generator: 1100.78
[2022-12-23 03:21:05,331] [INFO] [logging.py:68:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:05,331] [INFO] [timer.py:197:stop] 0/6, RunningAvgSamplesPerSec=45.99073881884347, CurrSamplesPerSec=53.88532453188791, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration        6/     200 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 3593.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.653288E+00 | moe loss: 1.317973E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2015.42 | backward-compute: 1482.35 | backward-embedding-all-reduce: 0.01 | optimizer: 89.67 | batch-generator: 928.31
[2022-12-23 03:21:08,935] [INFO] [logging.py:68:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:08,936] [INFO] [timer.py:197:stop] 0/7, RunningAvgSamplesPerSec=47.40883411572303, CurrSamplesPerSec=54.07877293662539, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration        7/     200 | consumed samples:         1792 | consumed tokens:      1835008 | elapsed time per iteration (ms): 3604.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.208667E+00 | moe loss: 1.246933E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2022.15 | backward-compute: 1481.53 | backward-embedding-all-reduce: 0.02 | optimizer: 92.94 | batch-generator: 921.24
 iteration        8/     200 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 4250.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.786855E+00 | moe loss: 1.288223E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2384.48 | backward-compute: 1721.80 | backward-embedding-all-reduce: 0.01 | optimizer: 128.84 | batch-generator: 918.98
[2022-12-23 03:21:13,186] [INFO] [logging.py:68:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:13,197] [INFO] [timer.py:197:stop] 0/8, RunningAvgSamplesPerSec=44.13250651528856, CurrSamplesPerSec=32.79910950694201, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:21:16,861] [INFO] [logging.py:68:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:16,861] [INFO] [timer.py:197:stop] 0/9, RunningAvgSamplesPerSec=45.25772813091388, CurrSamplesPerSec=53.43162391971506, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration        9/     200 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 3675.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.457167E+00 | moe loss: 1.253927E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2087.99 | backward-compute: 1481.11 | backward-embedding-all-reduce: 0.01 | optimizer: 89.72 | batch-generator: 916.42
[2022-12-23 03:21:20,510] [INFO] [logging.py:68:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:20,511] [INFO] [timer.py:197:stop] 0/10, RunningAvgSamplesPerSec=45.95077309153592, CurrSamplesPerSec=51.46776020572114, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       10/     200 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 3649.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.126974E+00 | moe loss: 1.182690E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2071.78 | backward-compute: 1479.52 | backward-embedding-all-reduce: 0.01 | optimizer: 91.18 | batch-generator: 944.38
[2022-12-23 03:21:24,074] [INFO] [logging.py:68:log_dist] [Rank 0] step=11, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:24,075] [INFO] [timer.py:197:stop] 0/11, RunningAvgSamplesPerSec=46.65434476954067, CurrSamplesPerSec=53.16682194715393, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       11/     200 | consumed samples:         2816 | consumed tokens:      2883584 | elapsed time per iteration (ms): 3563.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.047030E+00 | moe loss: 1.241517E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1974.59 | backward-compute: 1484.23 | backward-embedding-all-reduce: 0.01 | optimizer: 97.44 | batch-generator: 874.53
[2022-12-23 03:21:27,601] [INFO] [logging.py:68:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:27,602] [INFO] [timer.py:197:stop] 0/12, RunningAvgSamplesPerSec=47.275697287832365, CurrSamplesPerSec=53.71407715236135, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       12/     200 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 3527.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.137316E+00 | moe loss: 1.255050E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1955.25 | backward-compute: 1475.72 | backward-embedding-all-reduce: 0.01 | optimizer: 89.47 | batch-generator: 875.81
 iteration       13/     200 | consumed samples:         3328 | consumed tokens:      3407872 | elapsed time per iteration (ms): 4361.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.889200E+00 | moe loss: 1.297406E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2680.19 | backward-compute: 1555.86 | backward-embedding-all-reduce: 0.01 | optimizer: 110.68 | batch-generator: 1051.87
[2022-12-23 03:21:31,973] [INFO] [logging.py:68:log_dist] [Rank 0] step=13, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:31,982] [INFO] [timer.py:197:stop] 0/13, RunningAvgSamplesPerSec=44.17639919657567, CurrSamplesPerSec=26.683344138461504, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:21:35,472] [INFO] [logging.py:68:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:35,473] [INFO] [timer.py:197:stop] 0/14, RunningAvgSamplesPerSec=44.82794346936537, CurrSamplesPerSec=53.5090132040569, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       14/     200 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 3510.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.720012E+00 | moe loss: 1.284482E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1913.78 | backward-compute: 1481.39 | backward-embedding-all-reduce: 0.01 | optimizer: 89.71 | batch-generator: 846.19
[2022-12-23 03:21:39,080] [INFO] [logging.py:68:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:39,081] [INFO] [timer.py:197:stop] 0/15, RunningAvgSamplesPerSec=45.34337689267222, CurrSamplesPerSec=52.601089114149275, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       15/     200 | consumed samples:         3840 | consumed tokens:      3932160 | elapsed time per iteration (ms): 3607.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.634949E+00 | moe loss: 1.217643E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2028.54 | backward-compute: 1481.30 | backward-embedding-all-reduce: 0.01 | optimizer: 90.88 | batch-generator: 913.54
[2022-12-23 03:21:42,714] [INFO] [logging.py:68:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:42,714] [INFO] [timer.py:197:stop] 0/16, RunningAvgSamplesPerSec=45.66790495049024, CurrSamplesPerSec=50.35286543867422, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       16/     200 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 3633.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.584828E+00 | moe loss: 1.190875E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1965.46 | backward-compute: 1571.50 | backward-embedding-all-reduce: 0.01 | optimizer: 89.94 | batch-generator: 860.82
[2022-12-23 03:21:46,184] [INFO] [logging.py:68:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:46,185] [INFO] [timer.py:197:stop] 0/17, RunningAvgSamplesPerSec=46.13811335304016, CurrSamplesPerSec=53.90895891129533, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       17/     200 | consumed samples:         4352 | consumed tokens:      4456448 | elapsed time per iteration (ms): 3469.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.527990E+00 | moe loss: 1.197850E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1891.47 | backward-compute: 1481.37 | backward-embedding-all-reduce: 0.01 | optimizer: 90.65 | batch-generator: 793.79
 iteration       18/     200 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 3597.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.484295E+00 | moe loss: 1.190329E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1883.31 | backward-compute: 1586.28 | backward-embedding-all-reduce: 0.01 | optimizer: 120.86 | batch-generator: 811.22
[2022-12-23 03:21:49,790] [INFO] [logging.py:68:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:49,791] [INFO] [timer.py:197:stop] 0/18, RunningAvgSamplesPerSec=45.918185379893394, CurrSamplesPerSec=42.85407751221196, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:21:53,978] [INFO] [logging.py:68:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:53,979] [INFO] [timer.py:197:stop] 0/19, RunningAvgSamplesPerSec=46.16478591020102, CurrSamplesPerSec=50.504480645830114, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       19/     200 | consumed samples:         4864 | consumed tokens:      4980736 | elapsed time per iteration (ms): 4205.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.433478E+00 | moe loss: 1.173972E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2536.98 | backward-compute: 1539.56 | backward-embedding-all-reduce: 0.01 | optimizer: 101.93 | batch-generator: 1067.06
[2022-12-23 03:21:57,437] [INFO] [logging.py:68:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:21:57,437] [INFO] [timer.py:197:stop] 0/20, RunningAvgSamplesPerSec=46.540919217785856, CurrSamplesPerSec=54.02372382791651, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       20/     200 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 3449.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.401438E+00 | moe loss: 1.167636E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1877.48 | backward-compute: 1475.84 | backward-embedding-all-reduce: 0.01 | optimizer: 89.38 | batch-generator: 761.65
[2022-12-23 03:22:01,009] [INFO] [logging.py:68:log_dist] [Rank 0] step=21, skipped=0, lr=[9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05, 9.99999999321766e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:01,010] [INFO] [timer.py:197:stop] 0/21, RunningAvgSamplesPerSec=46.62709351730215, CurrSamplesPerSec=48.23468140205664, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       21/     200 | consumed samples:         5376 | consumed tokens:      5505024 | elapsed time per iteration (ms): 3575.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.404937E+00 | moe loss: 1.158687E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1945.12 | backward-compute: 1510.12 | backward-embedding-all-reduce: 0.01 | optimizer: 111.86 | batch-generator: 785.62
[2022-12-23 03:22:04,430] [INFO] [logging.py:68:log_dist] [Rank 0] step=22, skipped=0, lr=[9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05, 9.99999999252247e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:04,430] [INFO] [timer.py:197:stop] 0/22, RunningAvgSamplesPerSec=46.930842620793804, CurrSamplesPerSec=53.56021775617916, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       22/     200 | consumed samples:         5632 | consumed tokens:      5767168 | elapsed time per iteration (ms): 3418.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.392437E+00 | moe loss: 1.103700E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1831.77 | backward-compute: 1481.60 | backward-embedding-all-reduce: 0.01 | optimizer: 89.63 | batch-generator: 760.11
[2022-12-23 03:22:07,853] [INFO] [logging.py:68:log_dist] [Rank 0] step=23, skipped=0, lr=[9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05, 9.999999991793369e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:07,854] [INFO] [timer.py:197:stop] 0/23, RunningAvgSamplesPerSec=47.21709461243901, CurrSamplesPerSec=53.777329559, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       23/     200 | consumed samples:         5888 | consumed tokens:      6029312 | elapsed time per iteration (ms): 3423.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.364335E+00 | moe loss: 1.096656E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1848.20 | backward-compute: 1478.64 | backward-embedding-all-reduce: 0.01 | optimizer: 89.55 | batch-generator: 776.40
[2022-12-23 03:22:12,068] [INFO] [logging.py:68:log_dist] [Rank 0] step=24, skipped=0, lr=[9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05, 9.999999991030355e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:12,069] [INFO] [timer.py:197:stop] 0/24, RunningAvgSamplesPerSec=45.460226008968945, CurrSamplesPerSec=25.519744577675493, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       24/     200 | consumed samples:         6144 | consumed tokens:      6291456 | elapsed time per iteration (ms): 4212.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.368067E+00 | moe loss: 1.090848E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2488.55 | backward-compute: 1591.30 | backward-embedding-all-reduce: 0.01 | optimizer: 115.71 | batch-generator: 1010.61
[2022-12-23 03:22:15,459] [INFO] [logging.py:68:log_dist] [Rank 0] step=25, skipped=0, lr=[9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05, 9.99999999023343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:15,459] [INFO] [timer.py:197:stop] 0/25, RunningAvgSamplesPerSec=45.745983814260924, CurrSamplesPerSec=53.08741575166915, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       25/     200 | consumed samples:         6400 | consumed tokens:      6553600 | elapsed time per iteration (ms): 3393.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.362098E+00 | moe loss: 1.080407E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1809.88 | backward-compute: 1481.04 | backward-embedding-all-reduce: 0.01 | optimizer: 92.52 | batch-generator: 733.71
[2022-12-23 03:22:18,890] [INFO] [logging.py:68:log_dist] [Rank 0] step=26, skipped=0, lr=[9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05, 9.999999989402594e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:18,890] [INFO] [timer.py:197:stop] 0/26, RunningAvgSamplesPerSec=46.00606963394615, CurrSamplesPerSec=52.927079509081395, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       26/     200 | consumed samples:         6656 | consumed tokens:      6815744 | elapsed time per iteration (ms): 3431.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.362272E+00 | moe loss: 1.064771E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1842.19 | backward-compute: 1491.13 | backward-embedding-all-reduce: 0.01 | optimizer: 89.87 | batch-generator: 766.97
[2022-12-23 03:22:22,412] [INFO] [logging.py:68:log_dist] [Rank 0] step=27, skipped=0, lr=[9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05, 9.999999988537845e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:22,412] [INFO] [timer.py:197:stop] 0/27, RunningAvgSamplesPerSec=46.254213074731766, CurrSamplesPerSec=53.13211562586472, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       27/     200 | consumed samples:         6912 | consumed tokens:      7077888 | elapsed time per iteration (ms): 3521.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.351794E+00 | moe loss: 1.016335E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1913.91 | backward-compute: 1508.53 | backward-embedding-all-reduce: 0.01 | optimizer: 91.09 | batch-generator: 711.77
[2022-12-23 03:22:25,788] [INFO] [logging.py:68:log_dist] [Rank 0] step=28, skipped=0, lr=[9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05, 9.999999987639185e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:25,789] [INFO] [timer.py:197:stop] 0/28, RunningAvgSamplesPerSec=46.46572492928235, CurrSamplesPerSec=52.463352035754795, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       28/     200 | consumed samples:         7168 | consumed tokens:      7340032 | elapsed time per iteration (ms): 3376.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.349565E+00 | moe loss: 1.034745E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1798.70 | backward-compute: 1479.81 | backward-embedding-all-reduce: 0.01 | optimizer: 89.77 | batch-generator: 721.56
[2022-12-23 03:22:29,156] [INFO] [logging.py:68:log_dist] [Rank 0] step=29, skipped=0, lr=[9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05, 9.999999986706613e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:29,157] [INFO] [timer.py:197:stop] 0/29, RunningAvgSamplesPerSec=46.708704223904896, CurrSamplesPerSec=54.058472908884035, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       29/     200 | consumed samples:         7424 | consumed tokens:      7602176 | elapsed time per iteration (ms): 3368.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.341359E+00 | moe loss: 1.010258E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1788.77 | backward-compute: 1480.01 | backward-embedding-all-reduce: 0.01 | optimizer: 90.60 | batch-generator: 694.62
[2022-12-23 03:22:32,941] [INFO] [logging.py:68:log_dist] [Rank 0] step=30, skipped=0, lr=[9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05, 9.999999985740129e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:32,948] [INFO] [timer.py:197:stop] 0/30, RunningAvgSamplesPerSec=46.83689081012981, CurrSamplesPerSec=50.5851682168106, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       30/     200 | consumed samples:         7680 | consumed tokens:      7864320 | elapsed time per iteration (ms): 3786.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.319214E+00 | moe loss: 1.013599E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1948.07 | backward-compute: 1719.54 | backward-embedding-all-reduce: 0.01 | optimizer: 110.65 | batch-generator: 761.97
[2022-12-23 03:22:36,294] [INFO] [logging.py:68:log_dist] [Rank 0] step=31, skipped=0, lr=[9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05, 9.999999984739734e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:36,295] [INFO] [timer.py:197:stop] 0/31, RunningAvgSamplesPerSec=47.02849407981152, CurrSamplesPerSec=53.11218367898519, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       31/     200 | consumed samples:         7936 | consumed tokens:      8126464 | elapsed time per iteration (ms): 3348.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.315138E+00 | moe loss: 9.762079E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1770.64 | backward-compute: 1473.84 | backward-embedding-all-reduce: 0.01 | optimizer: 93.92 | batch-generator: 703.54
[2022-12-23 03:22:39,742] [INFO] [logging.py:68:log_dist] [Rank 0] step=32, skipped=0, lr=[9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05, 9.999999983705427e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration       32/     200 | consumed samples:         8192 | consumed tokens:      8388608 | elapsed time per iteration (ms): 3445.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.289951E+00 | moe loss: 9.622438E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1815.28 | backward-compute: 1513.33 | backward-embedding-all-reduce: 0.01 | optimizer: 104.87 | batch-generator: 742.82
[2022-12-23 03:22:39,744] [INFO] [timer.py:197:stop] 0/32, RunningAvgSamplesPerSec=47.10030232154168, CurrSamplesPerSec=49.28255183806615, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:22:43,229] [INFO] [logging.py:68:log_dist] [Rank 0] step=33, skipped=0, lr=[9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05, 9.999999982637209e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:43,229] [INFO] [timer.py:197:stop] 0/33, RunningAvgSamplesPerSec=47.285111723454115, CurrSamplesPerSec=53.59375232095782, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       33/     200 | consumed samples:         8448 | consumed tokens:      8650752 | elapsed time per iteration (ms): 3498.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.290441E+00 | moe loss: 9.541299E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1894.22 | backward-compute: 1488.11 | backward-embedding-all-reduce: 0.01 | optimizer: 98.64 | batch-generator: 676.24
[2022-12-23 03:22:46,559] [INFO] [logging.py:68:log_dist] [Rank 0] step=34, skipped=0, lr=[9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05, 9.999999981535079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:46,560] [INFO] [timer.py:197:stop] 0/34, RunningAvgSamplesPerSec=47.455949450178586, CurrSamplesPerSec=53.44143162709656, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       34/     200 | consumed samples:         8704 | consumed tokens:      8912896 | elapsed time per iteration (ms): 3324.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.275356E+00 | moe loss: 9.524195E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1751.27 | backward-compute: 1473.66 | backward-embedding-all-reduce: 0.01 | optimizer: 90.71 | batch-generator: 663.91
 iteration       35/     200 | consumed samples:         8960 | consumed tokens:      9175040 | elapsed time per iteration (ms): 3388.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.260834E+00 | moe loss: 9.297343E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1734.46 | backward-compute: 1552.99 | backward-embedding-all-reduce: 0.01 | optimizer: 91.67 | batch-generator: 674.78
[2022-12-23 03:22:49,960] [INFO] [logging.py:68:log_dist] [Rank 0] step=35, skipped=0, lr=[9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05, 9.999999980399036e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:49,962] [INFO] [timer.py:197:stop] 0/35, RunningAvgSamplesPerSec=47.44185312969314, CurrSamplesPerSec=46.995150895083356, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:22:53,923] [INFO] [logging.py:68:log_dist] [Rank 0] step=36, skipped=0, lr=[9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05, 9.999999979229083e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:53,923] [INFO] [timer.py:197:stop] 0/36, RunningAvgSamplesPerSec=47.48360066072443, CurrSamplesPerSec=48.90372049130452, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       36/     200 | consumed samples:         9216 | consumed tokens:      9437184 | elapsed time per iteration (ms): 3984.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.242761E+00 | moe loss: 9.166321E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2316.24 | backward-compute: 1535.17 | backward-embedding-all-reduce: 0.01 | optimizer: 103.29 | batch-generator: 887.90
[2022-12-23 03:22:57,231] [INFO] [logging.py:68:log_dist] [Rank 0] step=37, skipped=0, lr=[9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05, 9.999999978025217e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:22:57,232] [INFO] [timer.py:197:stop] 0/37, RunningAvgSamplesPerSec=47.63664447908898, CurrSamplesPerSec=53.49937260141615, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       37/     200 | consumed samples:         9472 | consumed tokens:      9699328 | elapsed time per iteration (ms): 3299.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.246366E+00 | moe loss: 9.276178E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1729.19 | backward-compute: 1472.82 | backward-embedding-all-reduce: 0.01 | optimizer: 89.49 | batch-generator: 651.82
[2022-12-23 03:23:00,686] [INFO] [logging.py:68:log_dist] [Rank 0] step=38, skipped=0, lr=[9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05, 9.99999997678744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:00,688] [INFO] [timer.py:197:stop] 0/38, RunningAvgSamplesPerSec=47.79416374352013, CurrSamplesPerSec=54.049525699530776, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       38/     200 | consumed samples:         9728 | consumed tokens:      9961472 | elapsed time per iteration (ms): 3453.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.234341E+00 | moe loss: 8.964881E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1865.31 | backward-compute: 1487.94 | backward-embedding-all-reduce: 0.01 | optimizer: 90.78 | batch-generator: 639.95
 iteration       39/     200 | consumed samples:         9984 | consumed tokens:     10223616 | elapsed time per iteration (ms): 3990.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.235909E+00 | moe loss: 8.984073E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2332.41 | backward-compute: 1556.40 | backward-embedding-all-reduce: 0.01 | optimizer: 94.10 | batch-generator: 622.83
[2022-12-23 03:23:04,683] [INFO] [logging.py:68:log_dist] [Rank 0] step=39, skipped=0, lr=[9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05, 9.999999975515751e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:04,684] [INFO] [timer.py:197:stop] 0/39, RunningAvgSamplesPerSec=47.83857692722102, CurrSamplesPerSec=49.49432771095, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:23:07,974] [INFO] [logging.py:68:log_dist] [Rank 0] step=40, skipped=0, lr=[9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05, 9.99999997421015e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:07,974] [INFO] [timer.py:197:stop] 0/40, RunningAvgSamplesPerSec=47.986752904458776, CurrSamplesPerSec=54.19809249528252, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       40/     200 | consumed samples:        10240 | consumed tokens:     10485760 | elapsed time per iteration (ms): 3302.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.204428E+00 | moe loss: 8.928245E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1717.56 | backward-compute: 1474.41 | backward-embedding-all-reduce: 0.01 | optimizer: 90.77 | batch-generator: 645.22
[2022-12-23 03:23:11,680] [INFO] [logging.py:68:log_dist] [Rank 0] step=41, skipped=0, lr=[9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05, 9.999999972870638e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:11,680] [INFO] [timer.py:197:stop] 0/41, RunningAvgSamplesPerSec=47.36123555438093, CurrSamplesPerSec=31.672596746176723, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       41/     200 | consumed samples:        10496 | consumed tokens:     10747904 | elapsed time per iteration (ms): 3722.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.194554E+00 | moe loss: 8.866247E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1713.24 | backward-compute: 1872.60 | backward-embedding-all-reduce: 0.01 | optimizer: 119.83 | batch-generator: 647.25
[2022-12-23 03:23:15,131] [INFO] [logging.py:68:log_dist] [Rank 0] step=42, skipped=0, lr=[9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05, 9.999999971497214e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:15,131] [INFO] [timer.py:197:stop] 0/42, RunningAvgSamplesPerSec=47.498042579148795, CurrSamplesPerSec=53.52826214057648, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       42/     200 | consumed samples:        10752 | consumed tokens:     11010048 | elapsed time per iteration (ms): 3433.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.204549E+00 | moe loss: 8.848183E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1858.19 | backward-compute: 1475.85 | backward-embedding-all-reduce: 0.01 | optimizer: 89.93 | batch-generator: 641.81
[2022-12-23 03:23:18,385] [INFO] [logging.py:68:log_dist] [Rank 0] step=43, skipped=0, lr=[9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05, 9.99999997008988e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:18,385] [INFO] [timer.py:197:stop] 0/43, RunningAvgSamplesPerSec=47.62830229918278, CurrSamplesPerSec=53.49672844151372, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       43/     200 | consumed samples:        11008 | consumed tokens:     11272192 | elapsed time per iteration (ms): 3252.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.166843E+00 | moe loss: 8.765946E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1682.32 | backward-compute: 1471.89 | backward-embedding-all-reduce: 0.01 | optimizer: 90.28 | batch-generator: 612.98
 iteration       44/     200 | consumed samples:        11264 | consumed tokens:     11534336 | elapsed time per iteration (ms): 3871.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.156824E+00 | moe loss: 8.838314E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2225.61 | backward-compute: 1540.68 | backward-embedding-all-reduce: 0.01 | optimizer: 97.89 | batch-generator: 627.03
[2022-12-23 03:23:22,271] [INFO] [logging.py:68:log_dist] [Rank 0] step=44, skipped=0, lr=[9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05, 9.99999996864863e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:22,274] [INFO] [timer.py:197:stop] 0/44, RunningAvgSamplesPerSec=47.675420865613745, CurrSamplesPerSec=49.69094537309565, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:23:25,534] [INFO] [logging.py:68:log_dist] [Rank 0] step=45, skipped=0, lr=[9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05, 9.999999967173473e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:25,535] [INFO] [timer.py:197:stop] 0/45, RunningAvgSamplesPerSec=47.80366947617698, CurrSamplesPerSec=53.892529522539924, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       45/     200 | consumed samples:        11520 | consumed tokens:     11796480 | elapsed time per iteration (ms): 3276.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.144057E+00 | moe loss: 8.728500E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1685.64 | backward-compute: 1478.22 | backward-embedding-all-reduce: 0.01 | optimizer: 89.59 | batch-generator: 606.66
[2022-12-23 03:23:28,788] [INFO] [logging.py:68:log_dist] [Rank 0] step=46, skipped=0, lr=[9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05, 9.999999965664402e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:28,788] [INFO] [timer.py:197:stop] 0/46, RunningAvgSamplesPerSec=47.91169485216159, CurrSamplesPerSec=53.06835661653648, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       46/     200 | consumed samples:        11776 | consumed tokens:     12058624 | elapsed time per iteration (ms): 3257.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.127348E+00 | moe loss: 8.669656E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1686.46 | backward-compute: 1470.32 | backward-embedding-all-reduce: 0.01 | optimizer: 91.44 | batch-generator: 625.34
[2022-12-23 03:23:32,507] [INFO] [logging.py:68:log_dist] [Rank 0] step=47, skipped=0, lr=[9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05, 9.99999996412142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:32,507] [INFO] [timer.py:197:stop] 0/47, RunningAvgSamplesPerSec=48.000378039476374, CurrSamplesPerSec=52.25627264982955, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       47/     200 | consumed samples:        12032 | consumed tokens:     12320768 | elapsed time per iteration (ms): 3731.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.113606E+00 | moe loss: 8.609576E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1856.37 | backward-compute: 1753.79 | backward-embedding-all-reduce: 0.01 | optimizer: 108.63 | batch-generator: 648.29
 iteration       48/     200 | consumed samples:        12288 | consumed tokens:     12582912 | elapsed time per iteration (ms): 3978.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.107232E+00 | moe loss: 8.602588E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2278.73 | backward-compute: 1594.20 | backward-embedding-all-reduce: 0.01 | optimizer: 98.58 | batch-generator: 607.77
[2022-12-23 03:23:36,509] [INFO] [logging.py:68:log_dist] [Rank 0] step=48, skipped=0, lr=[9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05, 9.999999962544526e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:36,511] [INFO] [timer.py:197:stop] 0/48, RunningAvgSamplesPerSec=48.064403585773015, CurrSamplesPerSec=51.133621325161165, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:23:39,784] [INFO] [logging.py:68:log_dist] [Rank 0] step=49, skipped=0, lr=[9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05, 9.999999960933719e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:39,785] [INFO] [timer.py:197:stop] 0/49, RunningAvgSamplesPerSec=48.1757659455489, CurrSamplesPerSec=53.92282020078977, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       49/     200 | consumed samples:        12544 | consumed tokens:     12845056 | elapsed time per iteration (ms): 3282.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.098413E+00 | moe loss: 8.555403E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1692.63 | backward-compute: 1482.07 | backward-embedding-all-reduce: 0.01 | optimizer: 90.90 | batch-generator: 566.57
[2022-12-23 03:23:43,075] [INFO] [logging.py:68:log_dist] [Rank 0] step=50, skipped=0, lr=[9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05, 9.999999959289002e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:43,075] [INFO] [timer.py:197:stop] 0/50, RunningAvgSamplesPerSec=48.27229965322055, CurrSamplesPerSec=53.29113804433062, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       50/     200 | consumed samples:        12800 | consumed tokens:     13107200 | elapsed time per iteration (ms): 3292.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.077173E+00 | moe loss: 8.545360E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1699.41 | backward-compute: 1492.87 | backward-embedding-all-reduce: 0.01 | optimizer: 90.09 | batch-generator: 595.51
[2022-12-23 03:23:46,318] [INFO] [logging.py:68:log_dist] [Rank 0] step=51, skipped=0, lr=[9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05, 9.999999957610372e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:46,319] [INFO] [timer.py:197:stop] 0/51, RunningAvgSamplesPerSec=48.37308046395365, CurrSamplesPerSec=53.76054961499393, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       51/     200 | consumed samples:        13056 | consumed tokens:     13369344 | elapsed time per iteration (ms): 3241.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.069673E+00 | moe loss: 8.402625E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1667.57 | backward-compute: 1477.15 | backward-embedding-all-reduce: 0.01 | optimizer: 89.46 | batch-generator: 594.48
 iteration       52/     200 | consumed samples:        13312 | consumed tokens:     13631488 | elapsed time per iteration (ms): 3248.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.049038E+00 | moe loss: 8.401872E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1670.55 | backward-compute: 1477.35 | backward-embedding-all-reduce: 0.01 | optimizer: 90.68 | batch-generator: 581.44
[2022-12-23 03:23:49,584] [INFO] [logging.py:68:log_dist] [Rank 0] step=52, skipped=0, lr=[9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05, 9.999999955897831e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:49,588] [INFO] [timer.py:197:stop] 0/52, RunningAvgSamplesPerSec=48.436873202916296, CurrSamplesPerSec=51.7830691729654, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:23:53,429] [INFO] [logging.py:68:log_dist] [Rank 0] step=53, skipped=0, lr=[9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05, 9.999999954151378e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:53,430] [INFO] [timer.py:197:stop] 0/53, RunningAvgSamplesPerSec=48.49609470680582, CurrSamplesPerSec=51.653831115051986, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       53/     200 | consumed samples:        13568 | consumed tokens:     13893632 | elapsed time per iteration (ms): 3880.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.056546E+00 | moe loss: 8.462393E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2039.12 | backward-compute: 1647.68 | backward-embedding-all-reduce: 0.01 | optimizer: 115.32 | batch-generator: 653.50
[2022-12-23 03:23:56,653] [INFO] [logging.py:68:log_dist] [Rank 0] step=54, skipped=0, lr=[9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05, 9.999999952371014e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:56,653] [INFO] [timer.py:197:stop] 0/54, RunningAvgSamplesPerSec=48.59598887194123, CurrSamplesPerSec=54.300337491493934, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       54/     200 | consumed samples:        13824 | consumed tokens:     14155776 | elapsed time per iteration (ms): 3205.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.035580E+00 | moe loss: 8.414833E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1638.40 | backward-compute: 1470.72 | backward-embedding-all-reduce: 0.01 | optimizer: 89.94 | batch-generator: 546.02
[2022-12-23 03:23:59,851] [INFO] [logging.py:68:log_dist] [Rank 0] step=55, skipped=0, lr=[9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05, 9.999999950556738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:23:59,851] [INFO] [timer.py:197:stop] 0/55, RunningAvgSamplesPerSec=48.688801039864074, CurrSamplesPerSec=54.057427826554154, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       55/     200 | consumed samples:        14080 | consumed tokens:     14417920 | elapsed time per iteration (ms): 3197.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.024840E+00 | moe loss: 8.372930E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1621.76 | backward-compute: 1478.49 | backward-embedding-all-reduce: 0.01 | optimizer: 91.15 | batch-generator: 562.47
[2022-12-23 03:24:03,188] [INFO] [logging.py:68:log_dist] [Rank 0] step=56, skipped=0, lr=[9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05, 9.99999994870855e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:03,189] [INFO] [timer.py:197:stop] 0/56, RunningAvgSamplesPerSec=48.70849229144154, CurrSamplesPerSec=49.77542012396967, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       56/     200 | consumed samples:        14336 | consumed tokens:     14680064 | elapsed time per iteration (ms): 3340.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.039790E+00 | moe loss: 8.385542E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1713.55 | backward-compute: 1522.29 | backward-embedding-all-reduce: 0.01 | optimizer: 94.62 | batch-generator: 580.47
[2022-12-23 03:24:06,389] [INFO] [logging.py:68:log_dist] [Rank 0] step=57, skipped=0, lr=[9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05, 9.999999946826451e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:06,390] [INFO] [timer.py:197:stop] 0/57, RunningAvgSamplesPerSec=48.78833502522317, CurrSamplesPerSec=53.52629820020658, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       57/     200 | consumed samples:        14592 | consumed tokens:     14942208 | elapsed time per iteration (ms): 3195.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.007570E+00 | moe loss: 8.344172E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1612.53 | backward-compute: 1477.86 | backward-embedding-all-reduce: 0.01 | optimizer: 96.30 | batch-generator: 560.81
[2022-12-23 03:24:09,589] [INFO] [logging.py:68:log_dist] [Rank 0] step=58, skipped=0, lr=[9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05, 9.99999994491044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:09,590] [INFO] [timer.py:197:stop] 0/58, RunningAvgSamplesPerSec=48.86697241592571, CurrSamplesPerSec=53.620387438027095, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       58/     200 | consumed samples:        14848 | consumed tokens:     15204352 | elapsed time per iteration (ms): 3203.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.997628E+00 | moe loss: 8.256646E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1625.05 | backward-compute: 1475.33 | backward-embedding-all-reduce: 0.01 | optimizer: 91.37 | batch-generator: 526.46
[2022-12-23 03:24:13,909] [INFO] [logging.py:68:log_dist] [Rank 0] step=59, skipped=0, lr=[9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05, 9.999999942960517e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:13,912] [INFO] [timer.py:197:stop] 0/59, RunningAvgSamplesPerSec=48.864001949243324, CurrSamplesPerSec=48.69823022367387, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       59/     200 | consumed samples:        15104 | consumed tokens:     15466496 | elapsed time per iteration (ms): 4321.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.981517E+00 | moe loss: 8.290529E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2619.30 | backward-compute: 1577.24 | backward-embedding-all-reduce: 0.01 | optimizer: 110.73 | batch-generator: 724.40
[2022-12-23 03:24:17,096] [INFO] [logging.py:68:log_dist] [Rank 0] step=60, skipped=0, lr=[9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05, 9.999999940976682e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:17,097] [INFO] [timer.py:197:stop] 0/60, RunningAvgSamplesPerSec=48.945775374827484, CurrSamplesPerSec=54.10698303111466, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       60/     200 | consumed samples:        15360 | consumed tokens:     15728640 | elapsed time per iteration (ms): 3185.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.967650E+00 | moe loss: 8.225496E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1612.84 | backward-compute: 1475.45 | backward-embedding-all-reduce: 0.01 | optimizer: 90.92 | batch-generator: 546.92
[2022-12-23 03:24:20,268] [INFO] [logging.py:68:log_dist] [Rank 0] step=61, skipped=0, lr=[9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05, 9.999999938958936e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:20,268] [INFO] [timer.py:197:stop] 0/61, RunningAvgSamplesPerSec=49.023566103371074, CurrSamplesPerSec=54.00146613525299, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       61/     200 | consumed samples:        15616 | consumed tokens:     15990784 | elapsed time per iteration (ms): 3170.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.970644E+00 | moe loss: 8.131072E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1597.49 | backward-compute: 1476.83 | backward-embedding-all-reduce: 0.01 | optimizer: 90.66 | batch-generator: 532.53
[2022-12-23 03:24:23,506] [INFO] [logging.py:68:log_dist] [Rank 0] step=62, skipped=0, lr=[9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05, 9.999999936907277e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:23,506] [INFO] [timer.py:197:stop] 0/62, RunningAvgSamplesPerSec=49.09824694915764, CurrSamplesPerSec=53.94692115103183, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       62/     200 | consumed samples:        15872 | consumed tokens:     16252928 | elapsed time per iteration (ms): 3240.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.952463E+00 | moe loss: 8.186941E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1611.79 | backward-compute: 1518.54 | backward-embedding-all-reduce: 0.01 | optimizer: 95.90 | batch-generator: 533.97
[2022-12-23 03:24:26,669] [INFO] [logging.py:68:log_dist] [Rank 0] step=63, skipped=0, lr=[9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05, 9.999999934821708e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:26,670] [INFO] [timer.py:197:stop] 0/63, RunningAvgSamplesPerSec=49.17398802861459, CurrSamplesPerSec=54.18971161315433, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       63/     200 | consumed samples:        16128 | consumed tokens:     16515072 | elapsed time per iteration (ms): 3162.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.924510E+00 | moe loss: 8.252307E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1584.91 | backward-compute: 1476.48 | backward-embedding-all-reduce: 0.01 | optimizer: 90.78 | batch-generator: 523.65
 iteration       64/     200 | consumed samples:        16384 | consumed tokens:     16777216 | elapsed time per iteration (ms): 3411.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.913581E+00 | moe loss: 8.145749E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1588.38 | backward-compute: 1644.04 | backward-embedding-all-reduce: 0.01 | optimizer: 170.84 | batch-generator: 512.69
[2022-12-23 03:24:30,115] [INFO] [logging.py:68:log_dist] [Rank 0] step=64, skipped=0, lr=[9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05, 9.999999932702227e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:30,130] [INFO] [timer.py:197:stop] 0/64, RunningAvgSamplesPerSec=48.890551197178226, CurrSamplesPerSec=36.17231270094946, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:24:34,188] [INFO] [logging.py:68:log_dist] [Rank 0] step=65, skipped=0, lr=[9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05, 9.999999930548834e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:34,188] [INFO] [timer.py:197:stop] 0/65, RunningAvgSamplesPerSec=48.93308961801711, CurrSamplesPerSec=51.723282306692425, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       65/     200 | consumed samples:        16640 | consumed tokens:     17039360 | elapsed time per iteration (ms): 4122.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.914660E+00 | moe loss: 8.146490E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2374.74 | backward-compute: 1576.00 | backward-embedding-all-reduce: 0.01 | optimizer: 109.01 | batch-generator: 623.97
[2022-12-23 03:24:37,444] [INFO] [logging.py:68:log_dist] [Rank 0] step=66, skipped=0, lr=[9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05, 9.999999928361529e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:37,444] [INFO] [timer.py:197:stop] 0/66, RunningAvgSamplesPerSec=49.001729295032995, CurrSamplesPerSec=53.75187295728812, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       66/     200 | consumed samples:        16896 | consumed tokens:     17301504 | elapsed time per iteration (ms): 3241.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.888305E+00 | moe loss: 8.181302E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1664.19 | backward-compute: 1480.49 | backward-embedding-all-reduce: 0.01 | optimizer: 90.03 | batch-generator: 504.23
[2022-12-23 03:24:40,686] [INFO] [logging.py:68:log_dist] [Rank 0] step=67, skipped=0, lr=[9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05, 9.999999926140312e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:40,686] [INFO] [timer.py:197:stop] 0/67, RunningAvgSamplesPerSec=48.96476091705961, CurrSamplesPerSec=46.70946340281869, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       67/     200 | consumed samples:        17152 | consumed tokens:     17563648 | elapsed time per iteration (ms): 3239.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.887929E+00 | moe loss: 8.269243E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1588.09 | backward-compute: 1552.09 | backward-embedding-all-reduce: 0.01 | optimizer: 91.89 | batch-generator: 514.55
[2022-12-23 03:24:43,833] [INFO] [logging.py:68:log_dist] [Rank 0] step=68, skipped=0, lr=[9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05, 9.999999923885183e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:43,833] [INFO] [timer.py:197:stop] 0/68, RunningAvgSamplesPerSec=49.03934886835523, CurrSamplesPerSec=54.42856059527735, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       68/     200 | consumed samples:        17408 | consumed tokens:     17825792 | elapsed time per iteration (ms): 3148.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.871809E+00 | moe loss: 8.170711E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1567.28 | backward-compute: 1481.75 | backward-embedding-all-reduce: 0.01 | optimizer: 89.33 | batch-generator: 498.72
[2022-12-23 03:24:46,977] [INFO] [logging.py:68:log_dist] [Rank 0] step=69, skipped=0, lr=[9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05, 9.999999921596145e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:46,978] [INFO] [timer.py:197:stop] 0/69, RunningAvgSamplesPerSec=49.105631634427574, CurrSamplesPerSec=53.91526058278635, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       69/     200 | consumed samples:        17664 | consumed tokens:     18087936 | elapsed time per iteration (ms): 3144.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.856996E+00 | moe loss: 8.093354E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1568.01 | backward-compute: 1478.79 | backward-embedding-all-reduce: 0.01 | optimizer: 89.59 | batch-generator: 494.39
 iteration       70/     200 | consumed samples:        17920 | consumed tokens:     18350080 | elapsed time per iteration (ms): 3596.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.850452E+00 | moe loss: 8.206737E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1560.49 | backward-compute: 1897.09 | backward-embedding-all-reduce: 0.01 | optimizer: 131.04 | batch-generator: 508.56
[2022-12-23 03:24:50,594] [INFO] [logging.py:68:log_dist] [Rank 0] step=70, skipped=0, lr=[9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05, 9.999999919273193e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:50,608] [INFO] [timer.py:197:stop] 0/70, RunningAvgSamplesPerSec=48.62708541089751, CurrSamplesPerSec=29.418699711791813, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:24:54,596] [INFO] [logging.py:68:log_dist] [Rank 0] step=71, skipped=0, lr=[9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05, 9.999999916916329e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:54,597] [INFO] [timer.py:197:stop] 0/71, RunningAvgSamplesPerSec=48.665389752961154, CurrSamplesPerSec=51.4196689117978, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       71/     200 | consumed samples:        18176 | consumed tokens:     18612224 | elapsed time per iteration (ms): 4037.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.833724E+00 | moe loss: 8.184911E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2111.56 | backward-compute: 1763.16 | backward-embedding-all-reduce: 0.01 | optimizer: 114.56 | batch-generator: 589.31
[2022-12-23 03:24:57,751] [INFO] [logging.py:68:log_dist] [Rank 0] step=72, skipped=0, lr=[9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05, 9.999999914525555e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:24:57,752] [INFO] [timer.py:197:stop] 0/72, RunningAvgSamplesPerSec=48.73617177068532, CurrSamplesPerSec=54.172848296101485, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       72/     200 | consumed samples:        18432 | consumed tokens:     18874368 | elapsed time per iteration (ms): 3140.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.841417E+00 | moe loss: 8.104395E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1564.81 | backward-compute: 1472.13 | backward-embedding-all-reduce: 0.01 | optimizer: 90.92 | batch-generator: 501.43
[2022-12-23 03:25:00,915] [INFO] [logging.py:68:log_dist] [Rank 0] step=73, skipped=0, lr=[9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05, 9.999999912100868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:00,915] [INFO] [timer.py:197:stop] 0/73, RunningAvgSamplesPerSec=48.78540822856417, CurrSamplesPerSec=52.49799071119514, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       73/     200 | consumed samples:        18688 | consumed tokens:     19136512 | elapsed time per iteration (ms): 3163.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.836578E+00 | moe loss: 8.125966E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1577.72 | backward-compute: 1482.54 | backward-embedding-all-reduce: 0.01 | optimizer: 94.05 | batch-generator: 487.61
[2022-12-23 03:25:04,085] [INFO] [logging.py:68:log_dist] [Rank 0] step=74, skipped=0, lr=[9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05, 9.999999909642269e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:04,085] [INFO] [timer.py:197:stop] 0/74, RunningAvgSamplesPerSec=48.84775694547983, CurrSamplesPerSec=53.722505075345666, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       74/     200 | consumed samples:        18944 | consumed tokens:     19398656 | elapsed time per iteration (ms): 3171.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.814195E+00 | moe loss: 8.339324E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1593.57 | backward-compute: 1478.72 | backward-embedding-all-reduce: 0.01 | optimizer: 89.32 | batch-generator: 525.07
[2022-12-23 03:25:07,205] [INFO] [logging.py:68:log_dist] [Rank 0] step=75, skipped=0, lr=[9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05, 9.999999907149759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:07,205] [INFO] [timer.py:197:stop] 0/75, RunningAvgSamplesPerSec=48.91090000567638, CurrSamplesPerSec=53.93023025005937, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       75/     200 | consumed samples:        19200 | consumed tokens:     19660800 | elapsed time per iteration (ms): 3118.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.811455E+00 | moe loss: 8.330334E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1549.24 | backward-compute: 1473.08 | backward-embedding-all-reduce: 0.01 | optimizer: 90.99 | batch-generator: 459.14
 iteration       76/     200 | consumed samples:        19456 | consumed tokens:     19922944 | elapsed time per iteration (ms): 3702.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.789988E+00 | moe loss: 8.313761E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2073.49 | backward-compute: 1524.49 | backward-embedding-all-reduce: 0.01 | optimizer: 96.13 | batch-generator: 473.88
[2022-12-23 03:25:10,920] [INFO] [logging.py:68:log_dist] [Rank 0] step=76, skipped=0, lr=[9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05, 9.999999904623338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:10,922] [INFO] [timer.py:197:stop] 0/76, RunningAvgSamplesPerSec=48.604709295997644, CurrSamplesPerSec=33.35961186669848, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:25:14,645] [INFO] [logging.py:68:log_dist] [Rank 0] step=77, skipped=0, lr=[9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05, 9.999999902063005e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:14,645] [INFO] [timer.py:197:stop] 0/77, RunningAvgSamplesPerSec=48.64136800136462, CurrSamplesPerSec=51.51663360621004, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       77/     200 | consumed samples:        19712 | consumed tokens:     20185088 | elapsed time per iteration (ms): 3748.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.807442E+00 | moe loss: 8.322263E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2065.90 | backward-compute: 1542.84 | backward-embedding-all-reduce: 0.01 | optimizer: 110.73 | batch-generator: 571.23
[2022-12-23 03:25:17,745] [INFO] [logging.py:68:log_dist] [Rank 0] step=78, skipped=0, lr=[9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05, 9.99999989946876e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:17,746] [INFO] [timer.py:197:stop] 0/78, RunningAvgSamplesPerSec=48.706840822924576, CurrSamplesPerSec=54.17604080345938, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       78/     200 | consumed samples:        19968 | consumed tokens:     20447232 | elapsed time per iteration (ms): 3090.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.784461E+00 | moe loss: 8.235183E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1513.27 | backward-compute: 1474.94 | backward-embedding-all-reduce: 0.01 | optimizer: 90.16 | batch-generator: 472.59
[2022-12-23 03:25:20,904] [INFO] [logging.py:68:log_dist] [Rank 0] step=79, skipped=0, lr=[9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05, 9.999999896840602e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:20,904] [INFO] [timer.py:197:stop] 0/79, RunningAvgSamplesPerSec=48.754361231005454, CurrSamplesPerSec=52.65895666646134, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       79/     200 | consumed samples:        20224 | consumed tokens:     20709376 | elapsed time per iteration (ms): 3159.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.766726E+00 | moe loss: 8.143305E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1553.35 | backward-compute: 1500.15 | backward-embedding-all-reduce: 0.01 | optimizer: 97.16 | batch-generator: 483.76
[2022-12-23 03:25:24,018] [INFO] [logging.py:68:log_dist] [Rank 0] step=80, skipped=0, lr=[9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05, 9.999999894178533e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:24,018] [INFO] [timer.py:197:stop] 0/80, RunningAvgSamplesPerSec=48.822371970732156, CurrSamplesPerSec=54.69757418550298, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       80/     200 | consumed samples:        20480 | consumed tokens:     20971520 | elapsed time per iteration (ms): 3114.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.771780E+00 | moe loss: 8.229975E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1540.02 | backward-compute: 1476.50 | backward-embedding-all-reduce: 0.01 | optimizer: 89.31 | batch-generator: 457.64
[2022-12-23 03:25:27,116] [INFO] [logging.py:68:log_dist] [Rank 0] step=81, skipped=0, lr=[9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05, 9.999999891482553e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:27,117] [INFO] [timer.py:197:stop] 0/81, RunningAvgSamplesPerSec=48.87112064066371, CurrSamplesPerSec=52.998785374583214, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       81/     200 | consumed samples:        20736 | consumed tokens:     21233664 | elapsed time per iteration (ms): 3098.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.770226E+00 | moe loss: 8.224974E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1522.38 | backward-compute: 1476.50 | backward-embedding-all-reduce: 0.01 | optimizer: 89.24 | batch-generator: 432.71
 iteration       82/     200 | consumed samples:        20992 | consumed tokens:     21495808 | elapsed time per iteration (ms): 3457.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.738399E+00 | moe loss: 8.221766E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1848.53 | backward-compute: 1502.03 | backward-embedding-all-reduce: 0.01 | optimizer: 96.83 | batch-generator: 436.92
[2022-12-23 03:25:30,589] [INFO] [logging.py:68:log_dist] [Rank 0] step=82, skipped=0, lr=[9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05, 9.999999888752661e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:30,592] [INFO] [timer.py:197:stop] 0/82, RunningAvgSamplesPerSec=48.578611972058106, CurrSamplesPerSec=32.98296907775875, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:25:34,299] [INFO] [logging.py:68:log_dist] [Rank 0] step=83, skipped=0, lr=[9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05, 9.999999885988858e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:34,299] [INFO] [timer.py:197:stop] 0/83, RunningAvgSamplesPerSec=48.61923683712062, CurrSamplesPerSec=52.105157768097456, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       83/     200 | consumed samples:        21248 | consumed tokens:     21757952 | elapsed time per iteration (ms): 3732.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.763175E+00 | moe loss: 8.227207E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2047.84 | backward-compute: 1545.17 | backward-embedding-all-reduce: 0.01 | optimizer: 106.87 | batch-generator: 509.61
[2022-12-23 03:25:37,410] [INFO] [logging.py:68:log_dist] [Rank 0] step=84, skipped=0, lr=[9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05, 9.999999883191142e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:37,410] [INFO] [timer.py:197:stop] 0/84, RunningAvgSamplesPerSec=48.67247012142372, CurrSamplesPerSec=53.40917123655347, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       84/     200 | consumed samples:        21504 | consumed tokens:     22020096 | elapsed time per iteration (ms): 3102.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.719630E+00 | moe loss: 8.143313E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1520.08 | backward-compute: 1479.60 | backward-embedding-all-reduce: 0.01 | optimizer: 89.93 | batch-generator: 467.63
[2022-12-23 03:25:40,593] [INFO] [logging.py:68:log_dist] [Rank 0] step=85, skipped=0, lr=[9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05, 9.999999880359515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:40,594] [INFO] [timer.py:197:stop] 0/85, RunningAvgSamplesPerSec=48.71368097073517, CurrSamplesPerSec=52.34817154671535, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       85/     200 | consumed samples:        21760 | consumed tokens:     22282240 | elapsed time per iteration (ms): 3182.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.712655E+00 | moe loss: 8.240826E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1590.77 | backward-compute: 1492.73 | backward-embedding-all-reduce: 0.01 | optimizer: 92.12 | batch-generator: 497.04
[2022-12-23 03:25:43,675] [INFO] [logging.py:68:log_dist] [Rank 0] step=86, skipped=0, lr=[9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05, 9.999999877493975e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:43,676] [INFO] [timer.py:197:stop] 0/86, RunningAvgSamplesPerSec=48.773053683643326, CurrSamplesPerSec=54.26229321297482, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       86/     200 | consumed samples:        22016 | consumed tokens:     22544384 | elapsed time per iteration (ms): 3083.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.726451E+00 | moe loss: 8.303559E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1510.89 | backward-compute: 1472.78 | backward-embedding-all-reduce: 0.01 | optimizer: 89.46 | batch-generator: 419.56
[2022-12-23 03:25:46,746] [INFO] [logging.py:68:log_dist] [Rank 0] step=87, skipped=0, lr=[9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05, 9.999999874594524e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:46,746] [INFO] [timer.py:197:stop] 0/87, RunningAvgSamplesPerSec=48.831821126508274, CurrSamplesPerSec=54.33080254859576, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       87/     200 | consumed samples:        22272 | consumed tokens:     22806528 | elapsed time per iteration (ms): 3070.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.700832E+00 | moe loss: 8.252865E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1499.54 | backward-compute: 1470.15 | backward-embedding-all-reduce: 0.01 | optimizer: 89.62 | batch-generator: 451.53
 iteration       88/     200 | consumed samples:        22528 | consumed tokens:     23068672 | elapsed time per iteration (ms): 3169.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.691103E+00 | moe loss: 8.323869E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1505.57 | backward-compute: 1476.05 | backward-embedding-all-reduce: 0.01 | optimizer: 177.87 | batch-generator: 432.80
[2022-12-23 03:25:49,955] [INFO] [logging.py:68:log_dist] [Rank 0] step=88, skipped=0, lr=[9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05, 9.999999871661162e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:49,956] [INFO] [timer.py:197:stop] 0/88, RunningAvgSamplesPerSec=48.768725946484956, CurrSamplesPerSec=43.9426071417931, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:25:53,635] [INFO] [logging.py:68:log_dist] [Rank 0] step=89, skipped=0, lr=[9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05, 9.999999868693888e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:53,636] [INFO] [timer.py:197:stop] 0/89, RunningAvgSamplesPerSec=48.803506095965, CurrSamplesPerSec=51.99230832402347, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       89/     200 | consumed samples:        22784 | consumed tokens:     23330816 | elapsed time per iteration (ms): 3727.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.680232E+00 | moe loss: 8.362735E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2015.32 | backward-compute: 1543.65 | backward-embedding-all-reduce: 0.01 | optimizer: 101.72 | batch-generator: 465.60
[2022-12-23 03:25:56,701] [INFO] [logging.py:68:log_dist] [Rank 0] step=90, skipped=0, lr=[9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05, 9.999999865692703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:56,702] [INFO] [timer.py:197:stop] 0/90, RunningAvgSamplesPerSec=48.85729406764169, CurrSamplesPerSec=54.03884082268301, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       90/     200 | consumed samples:        23040 | consumed tokens:     23592960 | elapsed time per iteration (ms): 3058.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.675581E+00 | moe loss: 8.260293E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1487.98 | backward-compute: 1470.61 | backward-embedding-all-reduce: 0.01 | optimizer: 93.04 | batch-generator: 431.37
[2022-12-23 03:25:59,829] [INFO] [logging.py:68:log_dist] [Rank 0] step=91, skipped=0, lr=[9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05, 9.999999862657606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:25:59,829] [INFO] [timer.py:197:stop] 0/91, RunningAvgSamplesPerSec=48.89874680098407, CurrSamplesPerSec=52.84426741022091, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       91/     200 | consumed samples:        23296 | consumed tokens:     23855104 | elapsed time per iteration (ms): 3127.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.662269E+00 | moe loss: 8.269220E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1547.54 | backward-compute: 1475.49 | backward-embedding-all-reduce: 0.01 | optimizer: 95.32 | batch-generator: 450.42
[2022-12-23 03:26:03,078] [INFO] [logging.py:68:log_dist] [Rank 0] step=92, skipped=0, lr=[9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05, 9.999999859588597e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:03,078] [INFO] [timer.py:197:stop] 0/92, RunningAvgSamplesPerSec=48.909387863210014, CurrSamplesPerSec=49.8753570615032, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       92/     200 | consumed samples:        23552 | consumed tokens:     24117248 | elapsed time per iteration (ms): 3249.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.647049E+00 | moe loss: 8.366963E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1555.18 | backward-compute: 1587.69 | backward-embedding-all-reduce: 0.01 | optimizer: 97.31 | batch-generator: 429.04
[2022-12-23 03:26:06,141] [INFO] [logging.py:68:log_dist] [Rank 0] step=93, skipped=0, lr=[9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05, 9.999999856485675e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:06,142] [INFO] [timer.py:197:stop] 0/93, RunningAvgSamplesPerSec=48.95809711244785, CurrSamplesPerSec=53.778342291199706, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       93/     200 | consumed samples:        23808 | consumed tokens:     24379392 | elapsed time per iteration (ms): 3062.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.630868E+00 | moe loss: 8.297541E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1486.81 | backward-compute: 1474.10 | backward-embedding-all-reduce: 0.01 | optimizer: 89.69 | batch-generator: 427.73
[2022-12-23 03:26:09,213] [INFO] [logging.py:68:log_dist] [Rank 0] step=94, skipped=0, lr=[9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05, 9.999999853348843e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:09,214] [INFO] [timer.py:197:stop] 0/94, RunningAvgSamplesPerSec=49.00476845600653, CurrSamplesPerSec=53.65972474038241, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       94/     200 | consumed samples:        24064 | consumed tokens:     24641536 | elapsed time per iteration (ms): 3071.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.619105E+00 | moe loss: 8.245990E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1496.43 | backward-compute: 1476.18 | backward-embedding-all-reduce: 0.01 | optimizer: 91.50 | batch-generator: 415.66
 iteration       95/     200 | consumed samples:        24320 | consumed tokens:     24903680 | elapsed time per iteration (ms): 3962.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.615621E+00 | moe loss: 8.299497E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1861.53 | backward-compute: 1976.28 | backward-embedding-all-reduce: 0.01 | optimizer: 115.95 | batch-generator: 461.49
[2022-12-23 03:26:13,207] [INFO] [logging.py:68:log_dist] [Rank 0] step=95, skipped=0, lr=[9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05, 9.9999998501781e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:13,219] [INFO] [timer.py:197:stop] 0/95, RunningAvgSamplesPerSec=48.96651237841276, CurrSamplesPerSec=45.68535593109571, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:26:16,362] [INFO] [logging.py:68:log_dist] [Rank 0] step=96, skipped=0, lr=[9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05, 9.999999846973444e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:16,362] [INFO] [timer.py:197:stop] 0/96, RunningAvgSamplesPerSec=49.00663342954258, CurrSamplesPerSec=53.048975681324485, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       96/     200 | consumed samples:        24576 | consumed tokens:     25165824 | elapsed time per iteration (ms): 3186.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.611460E+00 | moe loss: 8.472280E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1565.91 | backward-compute: 1479.78 | backward-embedding-all-reduce: 0.01 | optimizer: 91.27 | batch-generator: 379.24
[2022-12-23 03:26:19,412] [INFO] [logging.py:68:log_dist] [Rank 0] step=97, skipped=0, lr=[9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05, 9.999999843734875e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:19,412] [INFO] [timer.py:197:stop] 0/97, RunningAvgSamplesPerSec=49.053550826979446, CurrSamplesPerSec=53.904563772082085, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       97/     200 | consumed samples:        24832 | consumed tokens:     25427968 | elapsed time per iteration (ms): 3049.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.610427E+00 | moe loss: 8.291330E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1487.13 | backward-compute: 1465.92 | backward-embedding-all-reduce: 0.01 | optimizer: 90.64 | batch-generator: 410.09
[2022-12-23 03:26:22,484] [INFO] [logging.py:68:log_dist] [Rank 0] step=98, skipped=0, lr=[9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05, 9.999999840462398e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:22,484] [INFO] [timer.py:197:stop] 0/98, RunningAvgSamplesPerSec=49.100142582031346, CurrSamplesPerSec=53.969980248470385, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       98/     200 | consumed samples:        25088 | consumed tokens:     25690112 | elapsed time per iteration (ms): 3071.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.592220E+00 | moe loss: 8.484222E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1499.21 | backward-compute: 1475.76 | backward-embedding-all-reduce: 0.01 | optimizer: 90.16 | batch-generator: 422.45
[2022-12-23 03:26:25,533] [INFO] [logging.py:68:log_dist] [Rank 0] step=99, skipped=0, lr=[9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05, 9.999999837156007e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:25,533] [INFO] [timer.py:197:stop] 0/99, RunningAvgSamplesPerSec=49.150177737664194, CurrSamplesPerSec=54.47983822193502, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration       99/     200 | consumed samples:        25344 | consumed tokens:     25952256 | elapsed time per iteration (ms): 3049.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.569304E+00 | moe loss: 8.298074E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1476.61 | backward-compute: 1473.27 | backward-embedding-all-reduce: 0.01 | optimizer: 89.70 | batch-generator: 404.84
[2022-12-23 03:26:28,604] [INFO] [logging.py:68:log_dist] [Rank 0] step=100, skipped=0, lr=[9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05, 9.999999833815703e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:28,604] [INFO] [timer.py:197:stop] 0/100, RunningAvgSamplesPerSec=49.19616284857994, CurrSamplesPerSec=54.10652498165781, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      100/     200 | consumed samples:        25600 | consumed tokens:     26214400 | elapsed time per iteration (ms): 3072.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.586038E+00 | moe loss: 8.254028E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1496.75 | backward-compute: 1473.58 | backward-embedding-all-reduce: 0.01 | optimizer: 89.66 | batch-generator: 408.25
[2022-12-23 03:26:32,731] [INFO] [logging.py:68:log_dist] [Rank 0] step=101, skipped=0, lr=[9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05, 9.99999983044149e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:32,734] [INFO] [timer.py:197:stop] 0/101, RunningAvgSamplesPerSec=49.20911720524334, CurrSamplesPerSec=50.512615760037306, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      101/     200 | consumed samples:        25856 | consumed tokens:     26476544 | elapsed time per iteration (ms): 4127.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.571659E+00 | moe loss: 8.381885E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2286.61 | backward-compute: 1718.88 | backward-embedding-all-reduce: 0.01 | optimizer: 105.58 | batch-generator: 631.39
[2022-12-23 03:26:35,783] [INFO] [logging.py:68:log_dist] [Rank 0] step=102, skipped=0, lr=[9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05, 9.999999827033364e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:35,783] [INFO] [timer.py:197:stop] 0/102, RunningAvgSamplesPerSec=49.25217081094628, CurrSamplesPerSec=53.92275520936702, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      102/     200 | consumed samples:        26112 | consumed tokens:     26738688 | elapsed time per iteration (ms): 3050.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.560545E+00 | moe loss: 8.295754E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1480.90 | backward-compute: 1470.22 | backward-embedding-all-reduce: 0.01 | optimizer: 91.18 | batch-generator: 420.25
[2022-12-23 03:26:38,842] [INFO] [logging.py:68:log_dist] [Rank 0] step=103, skipped=0, lr=[9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05, 9.999999823591324e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:38,842] [INFO] [timer.py:197:stop] 0/103, RunningAvgSamplesPerSec=49.29950657472804, CurrSamplesPerSec=54.54142813023196, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      103/     200 | consumed samples:        26368 | consumed tokens:     27000832 | elapsed time per iteration (ms): 3058.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.545208E+00 | moe loss: 8.166121E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1479.36 | backward-compute: 1472.05 | backward-embedding-all-reduce: 0.01 | optimizer: 89.57 | batch-generator: 401.52
[2022-12-23 03:26:41,900] [INFO] [logging.py:68:log_dist] [Rank 0] step=104, skipped=0, lr=[9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05, 9.999999820115375e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:41,901] [INFO] [timer.py:197:stop] 0/104, RunningAvgSamplesPerSec=49.33887918886704, CurrSamplesPerSec=53.667878099640326, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      104/     200 | consumed samples:        26624 | consumed tokens:     27262976 | elapsed time per iteration (ms): 3058.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.552743E+00 | moe loss: 8.185011E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1471.93 | backward-compute: 1487.88 | backward-embedding-all-reduce: 0.01 | optimizer: 90.61 | batch-generator: 409.99
 iteration      105/     200 | consumed samples:        26880 | consumed tokens:     27525120 | elapsed time per iteration (ms): 3455.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.538320E+00 | moe loss: 8.203887E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1846.95 | backward-compute: 1496.65 | backward-embedding-all-reduce: 0.01 | optimizer: 100.38 | batch-generator: 410.56
[2022-12-23 03:26:45,363] [INFO] [logging.py:68:log_dist] [Rank 0] step=105, skipped=0, lr=[9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05, 9.999999816605514e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:45,363] [INFO] [timer.py:197:stop] 0/105, RunningAvgSamplesPerSec=49.36298531908498, CurrSamplesPerSec=51.952038577228585, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:26:48,397] [INFO] [logging.py:68:log_dist] [Rank 0] step=106, skipped=0, lr=[9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05, 9.999999813061741e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:48,397] [INFO] [timer.py:197:stop] 0/106, RunningAvgSamplesPerSec=49.39860821610738, CurrSamplesPerSec=53.365256014030585, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      106/     200 | consumed samples:        27136 | consumed tokens:     27787264 | elapsed time per iteration (ms): 3042.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.532570E+00 | moe loss: 8.252152E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1463.24 | backward-compute: 1471.87 | backward-embedding-all-reduce: 0.01 | optimizer: 91.16 | batch-generator: 389.69
[2022-12-23 03:26:51,942] [INFO] [logging.py:68:log_dist] [Rank 0] step=107, skipped=0, lr=[9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05, 9.999999809484057e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:51,945] [INFO] [timer.py:197:stop] 0/107, RunningAvgSamplesPerSec=49.39869680238562, CurrSamplesPerSec=49.40791151041239, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      107/     200 | consumed samples:        27392 | consumed tokens:     28049408 | elapsed time per iteration (ms): 3543.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.534542E+00 | moe loss: 8.166717E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1902.69 | backward-compute: 1527.02 | backward-embedding-all-reduce: 0.01 | optimizer: 101.68 | batch-generator: 417.97
[2022-12-23 03:26:54,986] [INFO] [logging.py:68:log_dist] [Rank 0] step=108, skipped=0, lr=[9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05, 9.99999980587246e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:54,986] [INFO] [timer.py:197:stop] 0/108, RunningAvgSamplesPerSec=49.43790718198728, CurrSamplesPerSec=53.93289576574195, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      108/     200 | consumed samples:        27648 | consumed tokens:     28311552 | elapsed time per iteration (ms): 3045.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.513899E+00 | moe loss: 8.177193E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1472.64 | backward-compute: 1468.40 | backward-embedding-all-reduce: 0.01 | optimizer: 90.77 | batch-generator: 400.01
[2022-12-23 03:26:58,004] [INFO] [logging.py:68:log_dist] [Rank 0] step=109, skipped=0, lr=[9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05, 9.999999802226953e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:26:58,005] [INFO] [timer.py:197:stop] 0/109, RunningAvgSamplesPerSec=49.47598755617649, CurrSamplesPerSec=53.87476934649545, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      109/     200 | consumed samples:        27904 | consumed tokens:     28573696 | elapsed time per iteration (ms): 3018.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.506635E+00 | moe loss: 8.217818E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1450.52 | backward-compute: 1464.62 | backward-embedding-all-reduce: 0.01 | optimizer: 94.29 | batch-generator: 382.32
 iteration      110/     200 | consumed samples:        28160 | consumed tokens:     28835840 | elapsed time per iteration (ms): 3113.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.500962E+00 | moe loss: 8.245683E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1535.62 | backward-compute: 1476.82 | backward-embedding-all-reduce: 0.01 | optimizer: 90.70 | batch-generator: 399.80
[2022-12-23 03:27:01,126] [INFO] [logging.py:68:log_dist] [Rank 0] step=110, skipped=0, lr=[9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05, 9.999999798547534e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:01,127] [INFO] [timer.py:197:stop] 0/110, RunningAvgSamplesPerSec=49.49765682416375, CurrSamplesPerSec=51.93133433132535, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:27:04,759] [INFO] [logging.py:68:log_dist] [Rank 0] step=111, skipped=0, lr=[9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05, 9.999999794834202e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:04,759] [INFO] [timer.py:197:stop] 0/111, RunningAvgSamplesPerSec=49.51373291721272, CurrSamplesPerSec=51.3136501513591, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      111/     200 | consumed samples:        28416 | consumed tokens:     29097984 | elapsed time per iteration (ms): 3662.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.491080E+00 | moe loss: 8.214034E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1982.94 | backward-compute: 1537.36 | backward-embedding-all-reduce: 0.01 | optimizer: 116.87 | batch-generator: 440.87
[2022-12-23 03:27:07,812] [INFO] [logging.py:68:log_dist] [Rank 0] step=112, skipped=0, lr=[9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05, 9.999999791086958e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:07,813] [INFO] [timer.py:197:stop] 0/112, RunningAvgSamplesPerSec=49.54946060897023, CurrSamplesPerSec=53.77926886683776, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      112/     200 | consumed samples:        28672 | consumed tokens:     29360128 | elapsed time per iteration (ms): 3033.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.485923E+00 | moe loss: 8.131424E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1459.77 | backward-compute: 1474.24 | backward-embedding-all-reduce: 0.01 | optimizer: 89.51 | batch-generator: 375.69
[2022-12-23 03:27:11,230] [INFO] [logging.py:68:log_dist] [Rank 0] step=113, skipped=0, lr=[9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05, 9.999999787305804e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:11,232] [INFO] [timer.py:197:stop] 0/113, RunningAvgSamplesPerSec=49.57892683065788, CurrSamplesPerSec=53.04914342087789, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      113/     200 | consumed samples:        28928 | consumed tokens:     29622272 | elapsed time per iteration (ms): 3417.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.469190E+00 | moe loss: 8.152284E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1807.78 | backward-compute: 1506.25 | backward-embedding-all-reduce: 0.01 | optimizer: 95.59 | batch-generator: 392.25
[2022-12-23 03:27:14,265] [INFO] [logging.py:68:log_dist] [Rank 0] step=114, skipped=0, lr=[9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05, 9.999999783490738e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:14,266] [INFO] [timer.py:197:stop] 0/114, RunningAvgSamplesPerSec=49.618225524142275, CurrSamplesPerSec=54.40499778882218, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      114/     200 | consumed samples:        29184 | consumed tokens:     29884416 | elapsed time per iteration (ms): 3036.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.475745E+00 | moe loss: 8.238560E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1455.99 | backward-compute: 1471.75 | backward-embedding-all-reduce: 0.01 | optimizer: 90.22 | batch-generator: 390.05
[2022-12-23 03:27:17,291] [INFO] [logging.py:68:log_dist] [Rank 0] step=115, skipped=0, lr=[9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05, 9.999999779641758e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:17,291] [INFO] [timer.py:197:stop] 0/115, RunningAvgSamplesPerSec=49.65474029901324, CurrSamplesPerSec=54.11503288001516, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      115/     200 | consumed samples:        29440 | consumed tokens:     30146560 | elapsed time per iteration (ms): 3025.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.463361E+00 | moe loss: 8.179650E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1455.84 | backward-compute: 1470.41 | backward-embedding-all-reduce: 0.01 | optimizer: 91.09 | batch-generator: 393.98
 iteration      116/     200 | consumed samples:        29696 | consumed tokens:     30408704 | elapsed time per iteration (ms): 3251.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.456916E+00 | moe loss: 8.154964E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1663.77 | backward-compute: 1487.40 | backward-embedding-all-reduce: 0.01 | optimizer: 94.59 | batch-generator: 444.98
[2022-12-23 03:27:20,565] [INFO] [logging.py:68:log_dist] [Rank 0] step=116, skipped=0, lr=[9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05, 9.999999775758868e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:20,566] [INFO] [timer.py:197:stop] 0/116, RunningAvgSamplesPerSec=49.57790387451623, CurrSamplesPerSec=42.19906966800133, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:27:24,574] [INFO] [logging.py:68:log_dist] [Rank 0] step=117, skipped=0, lr=[9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05, 9.999999771842067e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:24,575] [INFO] [timer.py:197:stop] 0/117, RunningAvgSamplesPerSec=49.57587812575489, CurrSamplesPerSec=49.34602287558871, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      117/     200 | consumed samples:        29952 | consumed tokens:     30670848 | elapsed time per iteration (ms): 4044.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.457850E+00 | moe loss: 8.168496E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2102.37 | backward-compute: 1799.22 | backward-embedding-all-reduce: 0.01 | optimizer: 113.74 | batch-generator: 461.29
[2022-12-23 03:27:27,597] [INFO] [logging.py:68:log_dist] [Rank 0] step=118, skipped=0, lr=[9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05, 9.999999767891354e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:27,597] [INFO] [timer.py:197:stop] 0/118, RunningAvgSamplesPerSec=49.615996543119984, CurrSamplesPerSec=54.70713862392628, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      118/     200 | consumed samples:        30208 | consumed tokens:     30932992 | elapsed time per iteration (ms): 3011.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.457413E+00 | moe loss: 8.252356E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1435.62 | backward-compute: 1470.26 | backward-embedding-all-reduce: 0.01 | optimizer: 90.02 | batch-generator: 388.56
[2022-12-23 03:27:30,663] [INFO] [logging.py:68:log_dist] [Rank 0] step=119, skipped=0, lr=[9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05, 9.99999976390673e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:30,664] [INFO] [timer.py:197:stop] 0/119, RunningAvgSamplesPerSec=49.64975037170425, CurrSamplesPerSec=53.90354628044375, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      119/     200 | consumed samples:        30464 | consumed tokens:     31195136 | elapsed time per iteration (ms): 3064.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.440711E+00 | moe loss: 8.157623E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1473.83 | backward-compute: 1494.07 | backward-embedding-all-reduce: 0.01 | optimizer: 89.63 | batch-generator: 370.78
[2022-12-23 03:27:33,677] [INFO] [logging.py:68:log_dist] [Rank 0] step=120, skipped=0, lr=[9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05, 9.999999759888194e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:33,677] [INFO] [timer.py:197:stop] 0/120, RunningAvgSamplesPerSec=49.67503964247528, CurrSamplesPerSec=52.82299157462565, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      120/     200 | consumed samples:        30720 | consumed tokens:     31457280 | elapsed time per iteration (ms): 3016.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.439271E+00 | moe loss: 8.205629E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1437.49 | backward-compute: 1477.03 | backward-embedding-all-reduce: 0.01 | optimizer: 89.73 | batch-generator: 365.47
[2022-12-23 03:27:36,689] [INFO] [logging.py:68:log_dist] [Rank 0] step=121, skipped=0, lr=[9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05, 9.999999755835744e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:36,690] [INFO] [timer.py:197:stop] 0/121, RunningAvgSamplesPerSec=49.69946753620938, CurrSamplesPerSec=52.76102960831486, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      121/     200 | consumed samples:        30976 | consumed tokens:     31719424 | elapsed time per iteration (ms): 3009.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.427180E+00 | moe loss: 8.154985E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1436.12 | backward-compute: 1472.15 | backward-embedding-all-reduce: 0.01 | optimizer: 94.41 | batch-generator: 378.27
[2022-12-23 03:27:39,759] [INFO] [logging.py:68:log_dist] [Rank 0] step=122, skipped=0, lr=[9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05, 9.999999751749385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:39,760] [INFO] [timer.py:197:stop] 0/122, RunningAvgSamplesPerSec=49.72816885425912, CurrSamplesPerSec=53.39778201703172, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      122/     200 | consumed samples:        31232 | consumed tokens:     31981568 | elapsed time per iteration (ms): 3070.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.424368E+00 | moe loss: 8.167536E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1500.93 | backward-compute: 1469.04 | backward-embedding-all-reduce: 0.01 | optimizer: 89.65 | batch-generator: 423.94
[2022-12-23 03:27:43,915] [INFO] [logging.py:68:log_dist] [Rank 0] step=123, skipped=0, lr=[9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05, 9.999999747629113e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:43,916] [INFO] [timer.py:197:stop] 0/123, RunningAvgSamplesPerSec=49.731080724521746, CurrSamplesPerSec=50.082998433527145, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      123/     200 | consumed samples:        31488 | consumed tokens:     32243712 | elapsed time per iteration (ms): 4156.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.420577E+00 | moe loss: 8.204976E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2380.34 | backward-compute: 1646.77 | backward-embedding-all-reduce: 0.01 | optimizer: 112.73 | batch-generator: 616.45
[2022-12-23 03:27:46,924] [INFO] [logging.py:68:log_dist] [Rank 0] step=124, skipped=0, lr=[9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05, 9.99999974347493e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:46,924] [INFO] [timer.py:197:stop] 0/124, RunningAvgSamplesPerSec=49.764797586105516, CurrSamplesPerSec=54.21214668305472, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      124/     200 | consumed samples:        31744 | consumed tokens:     32505856 | elapsed time per iteration (ms): 3010.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.410094E+00 | moe loss: 8.136963E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1429.70 | backward-compute: 1476.46 | backward-embedding-all-reduce: 0.01 | optimizer: 90.52 | batch-generator: 351.30
[2022-12-23 03:27:49,911] [INFO] [logging.py:68:log_dist] [Rank 0] step=125, skipped=0, lr=[9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05, 9.999999739286833e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:49,912] [INFO] [timer.py:197:stop] 0/125, RunningAvgSamplesPerSec=49.78866181018546, CurrSamplesPerSec=52.8824946208428, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      125/     200 | consumed samples:        32000 | consumed tokens:     32768000 | elapsed time per iteration (ms): 2984.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.408581E+00 | moe loss: 8.204985E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1412.71 | backward-compute: 1474.28 | backward-embedding-all-reduce: 0.01 | optimizer: 90.44 | batch-generator: 364.41
[2022-12-23 03:27:53,001] [INFO] [logging.py:68:log_dist] [Rank 0] step=126, skipped=0, lr=[9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05, 9.999999735064827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:53,005] [INFO] [timer.py:197:stop] 0/126, RunningAvgSamplesPerSec=49.81848883897507, CurrSamplesPerSec=53.781423817375675, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      126/     200 | consumed samples:        32256 | consumed tokens:     33030144 | elapsed time per iteration (ms): 3088.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.414302E+00 | moe loss: 8.143621E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1510.26 | backward-compute: 1474.23 | backward-embedding-all-reduce: 0.01 | optimizer: 92.51 | batch-generator: 356.93
[2022-12-23 03:27:56,001] [INFO] [logging.py:68:log_dist] [Rank 0] step=127, skipped=0, lr=[9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05, 9.999999730808909e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:56,001] [INFO] [timer.py:197:stop] 0/127, RunningAvgSamplesPerSec=49.850138039159056, CurrSamplesPerSec=54.11293837987426, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      127/     200 | consumed samples:        32512 | consumed tokens:     33292288 | elapsed time per iteration (ms): 2999.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.412774E+00 | moe loss: 8.122071E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1425.19 | backward-compute: 1474.71 | backward-embedding-all-reduce: 0.01 | optimizer: 89.59 | batch-generator: 357.71
[2022-12-23 03:27:58,993] [INFO] [logging.py:68:log_dist] [Rank 0] step=128, skipped=0, lr=[9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05, 9.999999726519079e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:27:58,993] [INFO] [timer.py:197:stop] 0/128, RunningAvgSamplesPerSec=49.88406015252769, CurrSamplesPerSec=54.521687381765105, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      128/     200 | consumed samples:        32768 | consumed tokens:     33554432 | elapsed time per iteration (ms): 2992.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.397648E+00 | moe loss: 8.217946E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1418.02 | backward-compute: 1466.42 | backward-embedding-all-reduce: 0.01 | optimizer: 89.45 | batch-generator: 356.49
[2022-12-23 03:28:02,938] [INFO] [logging.py:68:log_dist] [Rank 0] step=129, skipped=0, lr=[9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05, 9.999999722195336e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:02,938] [INFO] [timer.py:197:stop] 0/129, RunningAvgSamplesPerSec=49.823513725002854, CurrSamplesPerSec=43.214630502537155, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      129/     200 | consumed samples:        33024 | consumed tokens:     33816576 | elapsed time per iteration (ms): 3950.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.395741E+00 | moe loss: 8.182056E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2124.29 | backward-compute: 1687.44 | backward-embedding-all-reduce: 0.01 | optimizer: 118.80 | batch-generator: 432.66
[2022-12-23 03:28:05,931] [INFO] [logging.py:68:log_dist] [Rank 0] step=130, skipped=0, lr=[9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05, 9.999999717837683e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:05,932] [INFO] [timer.py:197:stop] 0/130, RunningAvgSamplesPerSec=49.85233103048341, CurrSamplesPerSec=53.80455730951814, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      130/     200 | consumed samples:        33280 | consumed tokens:     34078720 | elapsed time per iteration (ms): 2989.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.401263E+00 | moe loss: 8.144359E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1411.39 | backward-compute: 1473.75 | backward-embedding-all-reduce: 0.01 | optimizer: 92.84 | batch-generator: 368.58
[2022-12-23 03:28:08,933] [INFO] [logging.py:68:log_dist] [Rank 0] step=131, skipped=0, lr=[9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05, 9.999999713446118e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:08,934] [INFO] [timer.py:197:stop] 0/131, RunningAvgSamplesPerSec=49.88598028377886, CurrSamplesPerSec=54.60357952009712, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      131/     200 | consumed samples:        33536 | consumed tokens:     34340864 | elapsed time per iteration (ms): 3001.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.376918E+00 | moe loss: 8.165395E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1422.64 | backward-compute: 1481.55 | backward-embedding-all-reduce: 0.01 | optimizer: 89.95 | batch-generator: 353.59
[2022-12-23 03:28:12,017] [INFO] [logging.py:68:log_dist] [Rank 0] step=132, skipped=0, lr=[9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05, 9.99999970902064e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:12,018] [INFO] [timer.py:197:stop] 0/132, RunningAvgSamplesPerSec=49.91409019068597, CurrSamplesPerSec=53.82671770572866, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      132/     200 | consumed samples:        33792 | consumed tokens:     34603008 | elapsed time per iteration (ms): 3082.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.372861E+00 | moe loss: 8.197555E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1504.74 | backward-compute: 1478.03 | backward-embedding-all-reduce: 0.01 | optimizer: 90.99 | batch-generator: 339.86
[2022-12-23 03:28:15,007] [INFO] [logging.py:68:log_dist] [Rank 0] step=133, skipped=0, lr=[9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05, 9.999999704561251e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:15,007] [INFO] [timer.py:197:stop] 0/133, RunningAvgSamplesPerSec=49.94621458710655, CurrSamplesPerSec=54.50663110260635, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      133/     200 | consumed samples:        34048 | consumed tokens:     34865152 | elapsed time per iteration (ms): 2989.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.383610E+00 | moe loss: 8.095554E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1416.44 | backward-compute: 1471.96 | backward-embedding-all-reduce: 0.01 | optimizer: 90.21 | batch-generator: 332.95
[2022-12-23 03:28:17,975] [INFO] [logging.py:68:log_dist] [Rank 0] step=134, skipped=0, lr=[9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05, 9.999999700067951e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:17,975] [INFO] [timer.py:197:stop] 0/134, RunningAvgSamplesPerSec=49.97714266348408, CurrSamplesPerSec=54.389124260302275, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      134/     200 | consumed samples:        34304 | consumed tokens:     35127296 | elapsed time per iteration (ms): 2967.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.375755E+00 | moe loss: 8.131066E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1394.22 | backward-compute: 1472.47 | backward-embedding-all-reduce: 0.01 | optimizer: 90.29 | batch-generator: 335.26
 iteration      135/     200 | consumed samples:        34560 | consumed tokens:     35389440 | elapsed time per iteration (ms): 3272.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.360034E+00 | moe loss: 8.116694E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1630.77 | backward-compute: 1539.46 | backward-embedding-all-reduce: 0.01 | optimizer: 93.24 | batch-generator: 347.82
[2022-12-23 03:28:21,261] [INFO] [logging.py:68:log_dist] [Rank 0] step=135, skipped=0, lr=[9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05, 9.999999695540739e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:21,262] [INFO] [timer.py:197:stop] 0/135, RunningAvgSamplesPerSec=49.94047491458783, CurrSamplesPerSec=45.53093455673925, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:28:24,634] [INFO] [logging.py:68:log_dist] [Rank 0] step=136, skipped=0, lr=[9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05, 9.999999690979616e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:24,634] [INFO] [timer.py:197:stop] 0/136, RunningAvgSamplesPerSec=49.93034722640405, CurrSamplesPerSec=48.61900701368759, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      136/     200 | consumed samples:        34816 | consumed tokens:     35651584 | elapsed time per iteration (ms): 3404.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.362685E+00 | moe loss: 8.105797E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1759.12 | backward-compute: 1507.95 | backward-embedding-all-reduce: 0.01 | optimizer: 108.99 | batch-generator: 423.71
[2022-12-23 03:28:27,628] [INFO] [logging.py:68:log_dist] [Rank 0] step=137, skipped=0, lr=[9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05, 9.999999686384579e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:27,629] [INFO] [timer.py:197:stop] 0/137, RunningAvgSamplesPerSec=49.956912457198875, CurrSamplesPerSec=53.791964013039824, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      137/     200 | consumed samples:        35072 | consumed tokens:     35913728 | elapsed time per iteration (ms): 2980.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.351614E+00 | moe loss: 8.139706E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1407.76 | backward-compute: 1466.72 | backward-embedding-all-reduce: 0.01 | optimizer: 91.94 | batch-generator: 331.03
[2022-12-23 03:28:30,633] [INFO] [logging.py:68:log_dist] [Rank 0] step=138, skipped=0, lr=[9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05, 9.999999681755632e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:30,633] [INFO] [timer.py:197:stop] 0/138, RunningAvgSamplesPerSec=49.966331703259485, CurrSamplesPerSec=51.271388461259306, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      138/     200 | consumed samples:        35328 | consumed tokens:     36175872 | elapsed time per iteration (ms): 3000.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.349397E+00 | moe loss: 8.143352E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1428.58 | backward-compute: 1474.22 | backward-embedding-all-reduce: 0.01 | optimizer: 90.87 | batch-generator: 331.20
[2022-12-23 03:28:33,617] [INFO] [logging.py:68:log_dist] [Rank 0] step=139, skipped=0, lr=[9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05, 9.999999677092774e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:33,617] [INFO] [timer.py:197:stop] 0/139, RunningAvgSamplesPerSec=49.99418932038368, CurrSamplesPerSec=54.09594837783971, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      139/     200 | consumed samples:        35584 | consumed tokens:     36438016 | elapsed time per iteration (ms): 2984.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.365591E+00 | moe loss: 8.128762E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1415.63 | backward-compute: 1468.56 | backward-embedding-all-reduce: 0.01 | optimizer: 90.01 | batch-generator: 337.29
[2022-12-23 03:28:36,598] [INFO] [logging.py:68:log_dist] [Rank 0] step=140, skipped=0, lr=[9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05, 9.999999672396003e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:36,598] [INFO] [timer.py:197:stop] 0/140, RunningAvgSamplesPerSec=50.01910011894859, CurrSamplesPerSec=53.68374133454073, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      140/     200 | consumed samples:        35840 | consumed tokens:     36700160 | elapsed time per iteration (ms): 2981.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.356884E+00 | moe loss: 8.107158E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1405.46 | backward-compute: 1471.51 | backward-embedding-all-reduce: 0.01 | optimizer: 93.37 | batch-generator: 351.35
[2022-12-23 03:28:39,569] [INFO] [logging.py:68:log_dist] [Rank 0] step=141, skipped=0, lr=[9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05, 9.999999667665321e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:39,569] [INFO] [timer.py:197:stop] 0/141, RunningAvgSamplesPerSec=50.04852484518079, CurrSamplesPerSec=54.47050783184066, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      141/     200 | consumed samples:        36096 | consumed tokens:     36962304 | elapsed time per iteration (ms): 2971.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.330838E+00 | moe loss: 8.148696E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1402.44 | backward-compute: 1470.05 | backward-embedding-all-reduce: 0.01 | optimizer: 90.97 | batch-generator: 311.83
[2022-12-23 03:28:43,499] [INFO] [logging.py:68:log_dist] [Rank 0] step=142, skipped=0, lr=[9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05, 9.999999662900726e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:43,500] [INFO] [timer.py:197:stop] 0/142, RunningAvgSamplesPerSec=50.041510672117084, CurrSamplesPerSec=49.08530462332005, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      142/     200 | consumed samples:        36352 | consumed tokens:     37224448 | elapsed time per iteration (ms): 3931.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.330937E+00 | moe loss: 8.183341E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1848.37 | backward-compute: 1951.26 | backward-embedding-all-reduce: 0.01 | optimizer: 114.96 | batch-generator: 441.37
[2022-12-23 03:28:46,470] [INFO] [logging.py:68:log_dist] [Rank 0] step=143, skipped=0, lr=[9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05, 9.99999965810222e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:46,471] [INFO] [timer.py:197:stop] 0/143, RunningAvgSamplesPerSec=50.07069074583807, CurrSamplesPerSec=54.52164308639483, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      143/     200 | consumed samples:        36608 | consumed tokens:     37486592 | elapsed time per iteration (ms): 2970.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.350008E+00 | moe loss: 8.129255E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1396.63 | backward-compute: 1469.03 | backward-embedding-all-reduce: 0.01 | optimizer: 90.63 | batch-generator: 325.05
[2022-12-23 03:28:49,447] [INFO] [logging.py:68:log_dist] [Rank 0] step=144, skipped=0, lr=[9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05, 9.999999653269802e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:49,448] [INFO] [timer.py:197:stop] 0/144, RunningAvgSamplesPerSec=50.09441737575781, CurrSamplesPerSec=53.681100389196565, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      144/     200 | consumed samples:        36864 | consumed tokens:     37748736 | elapsed time per iteration (ms): 2979.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.333549E+00 | moe loss: 8.139165E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1405.42 | backward-compute: 1470.84 | backward-embedding-all-reduce: 0.01 | optimizer: 92.66 | batch-generator: 340.55
 iteration      145/     200 | consumed samples:        37120 | consumed tokens:     38010880 | elapsed time per iteration (ms): 3091.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.329478E+00 | moe loss: 8.125686E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1438.36 | backward-compute: 1546.68 | backward-embedding-all-reduce: 0.01 | optimizer: 95.34 | batch-generator: 339.18
[2022-12-23 03:28:52,545] [INFO] [logging.py:68:log_dist] [Rank 0] step=145, skipped=0, lr=[9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05, 9.999999648403474e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:52,555] [INFO] [timer.py:197:stop] 0/145, RunningAvgSamplesPerSec=50.10868140644505, CurrSamplesPerSec=52.22012349089381, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:28:55,507] [INFO] [logging.py:68:log_dist] [Rank 0] step=146, skipped=0, lr=[9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05, 9.999999643503232e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:55,507] [INFO] [timer.py:197:stop] 0/146, RunningAvgSamplesPerSec=50.135479488702565, CurrSamplesPerSec=54.28715974900156, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      146/     200 | consumed samples:        37376 | consumed tokens:     38273024 | elapsed time per iteration (ms): 2966.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.338810E+00 | moe loss: 8.080880E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1389.02 | backward-compute: 1466.77 | backward-embedding-all-reduce: 0.01 | optimizer: 90.68 | batch-generator: 313.75
 iteration      147/     200 | consumed samples:        37632 | consumed tokens:     38535168 | elapsed time per iteration (ms): 3447.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.318076E+00 | moe loss: 8.115020E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1840.32 | backward-compute: 1499.25 | backward-embedding-all-reduce: 0.01 | optimizer: 97.58 | batch-generator: 386.97
[2022-12-23 03:28:58,996] [INFO] [logging.py:68:log_dist] [Rank 0] step=147, skipped=0, lr=[9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05, 9.999999638569081e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:28:58,997] [INFO] [timer.py:197:stop] 0/147, RunningAvgSamplesPerSec=50.12577956521487, CurrSamplesPerSec=48.76711365160754, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:29:02,767] [INFO] [logging.py:68:log_dist] [Rank 0] step=148, skipped=0, lr=[9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05, 9.999999633601016e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:02,768] [INFO] [timer.py:197:stop] 0/148, RunningAvgSamplesPerSec=49.73115810376213, CurrSamplesPerSec=23.22224944391616, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      148/     200 | consumed samples:        37888 | consumed tokens:     38797312 | elapsed time per iteration (ms): 3833.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.324211E+00 | moe loss: 8.106157E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2114.63 | backward-compute: 1540.82 | backward-embedding-all-reduce: 0.01 | optimizer: 113.35 | batch-generator: 640.67
[2022-12-23 03:29:05,785] [INFO] [logging.py:68:log_dist] [Rank 0] step=149, skipped=0, lr=[9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05, 9.999999628599041e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:05,785] [INFO] [timer.py:197:stop] 0/149, RunningAvgSamplesPerSec=49.7510619906869, CurrSamplesPerSec=52.83860881957526, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      149/     200 | consumed samples:        38144 | consumed tokens:     39059456 | elapsed time per iteration (ms): 2998.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.313186E+00 | moe loss: 8.084522E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1417.77 | backward-compute: 1474.05 | backward-embedding-all-reduce: 0.01 | optimizer: 97.34 | batch-generator: 359.93
[2022-12-23 03:29:08,753] [INFO] [logging.py:68:log_dist] [Rank 0] step=150, skipped=0, lr=[9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05, 9.999999623563152e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:08,753] [INFO] [timer.py:197:stop] 0/150, RunningAvgSamplesPerSec=49.77864638053638, CurrSamplesPerSec=54.1958164902305, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      150/     200 | consumed samples:        38400 | consumed tokens:     39321600 | elapsed time per iteration (ms): 2965.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.322367E+00 | moe loss: 8.106059E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1395.47 | backward-compute: 1474.49 | backward-embedding-all-reduce: 0.01 | optimizer: 89.68 | batch-generator: 327.93
 iteration      151/     200 | consumed samples:        38656 | consumed tokens:     39583744 | elapsed time per iteration (ms): 3239.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.305695E+00 | moe loss: 8.100525E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1635.09 | backward-compute: 1501.86 | backward-embedding-all-reduce: 0.01 | optimizer: 93.27 | batch-generator: 322.49
[2022-12-23 03:29:12,005] [INFO] [logging.py:68:log_dist] [Rank 0] step=151, skipped=0, lr=[9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05, 9.999999618493352e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:12,005] [INFO] [timer.py:197:stop] 0/151, RunningAvgSamplesPerSec=49.79113721886013, CurrSamplesPerSec=51.71156463843095, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:29:14,967] [INFO] [logging.py:68:log_dist] [Rank 0] step=152, skipped=0, lr=[9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05, 9.999999613389641e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:14,967] [INFO] [timer.py:197:stop] 0/152, RunningAvgSamplesPerSec=49.81590160443397, CurrSamplesPerSec=53.803112231215394, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      152/     200 | consumed samples:        38912 | consumed tokens:     39845888 | elapsed time per iteration (ms): 2974.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.317000E+00 | moe loss: 8.108174E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1392.94 | backward-compute: 1472.58 | backward-embedding-all-reduce: 0.01 | optimizer: 89.39 | batch-generator: 323.54
[2022-12-23 03:29:17,916] [INFO] [logging.py:68:log_dist] [Rank 0] step=153, skipped=0, lr=[9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05, 9.999999608252018e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:17,916] [INFO] [timer.py:197:stop] 0/153, RunningAvgSamplesPerSec=49.83867983782419, CurrSamplesPerSec=53.50869321681514, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      153/     200 | consumed samples:        39168 | consumed tokens:     40108032 | elapsed time per iteration (ms): 2949.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.287771E+00 | moe loss: 8.089356E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1376.79 | backward-compute: 1472.24 | backward-embedding-all-reduce: 0.01 | optimizer: 89.30 | batch-generator: 319.62
[2022-12-23 03:29:20,989] [INFO] [logging.py:68:log_dist] [Rank 0] step=154, skipped=0, lr=[9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05, 9.999999603080485e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:20,990] [INFO] [timer.py:197:stop] 0/154, RunningAvgSamplesPerSec=49.86414913023161, CurrSamplesPerSec=54.033728374266595, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      154/     200 | consumed samples:        39424 | consumed tokens:     40370176 | elapsed time per iteration (ms): 3076.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.300343E+00 | moe loss: 8.126210E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1499.97 | backward-compute: 1472.91 | backward-embedding-all-reduce: 0.01 | optimizer: 91.75 | batch-generator: 397.13
[2022-12-23 03:29:24,139] [INFO] [logging.py:68:log_dist] [Rank 0] step=155, skipped=0, lr=[9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05, 9.999999597875038e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:24,139] [INFO] [timer.py:197:stop] 0/155, RunningAvgSamplesPerSec=49.86537545145023, CurrSamplesPerSec=50.05248029117597, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      155/     200 | consumed samples:        39680 | consumed tokens:     40632320 | elapsed time per iteration (ms): 3156.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.304858E+00 | moe loss: 8.152289E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1554.74 | backward-compute: 1495.13 | backward-embedding-all-reduce: 0.01 | optimizer: 99.44 | batch-generator: 338.09
 iteration      156/     200 | consumed samples:        39936 | consumed tokens:     40894464 | elapsed time per iteration (ms): 3197.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.293145E+00 | moe loss: 8.076074E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1599.63 | backward-compute: 1498.56 | backward-embedding-all-reduce: 0.01 | optimizer: 89.52 | batch-generator: 315.48
[2022-12-23 03:29:27,358] [INFO] [logging.py:68:log_dist] [Rank 0] step=156, skipped=0, lr=[9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05, 9.999999592635681e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:27,358] [INFO] [timer.py:197:stop] 0/156, RunningAvgSamplesPerSec=49.875739171578246, CurrSamplesPerSec=51.51380614100099, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      157/     200 | consumed samples:        40192 | consumed tokens:     41156608 | elapsed time per iteration (ms): 3081.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.287088E+00 | moe loss: 8.107195E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1383.69 | backward-compute: 1556.84 | backward-embedding-all-reduce: 0.01 | optimizer: 119.18 | batch-generator: 319.40
[2022-12-23 03:29:30,438] [INFO] [logging.py:68:log_dist] [Rank 0] step=157, skipped=0, lr=[9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05, 9.99999958736241e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:30,440] [INFO] [timer.py:197:stop] 0/157, RunningAvgSamplesPerSec=49.84225274858954, CurrSamplesPerSec=45.171716682181874, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:29:33,545] [INFO] [logging.py:68:log_dist] [Rank 0] step=158, skipped=0, lr=[9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05, 9.99999958205523e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:33,546] [INFO] [timer.py:197:stop] 0/158, RunningAvgSamplesPerSec=49.86725914917332, CurrSamplesPerSec=54.07219336111788, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      158/     200 | consumed samples:        40448 | consumed tokens:     41418752 | elapsed time per iteration (ms): 3118.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.290350E+00 | moe loss: 8.125990E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1501.22 | backward-compute: 1502.90 | backward-embedding-all-reduce: 0.01 | optimizer: 89.77 | batch-generator: 341.01
[2022-12-23 03:29:36,510] [INFO] [logging.py:68:log_dist] [Rank 0] step=159, skipped=0, lr=[9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05, 9.999999576714137e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:36,510] [INFO] [timer.py:197:stop] 0/159, RunningAvgSamplesPerSec=49.88995231208867, CurrSamplesPerSec=53.70234266531695, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      159/     200 | consumed samples:        40704 | consumed tokens:     41680896 | elapsed time per iteration (ms): 2963.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.285078E+00 | moe loss: 8.111664E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1385.79 | backward-compute: 1476.78 | backward-embedding-all-reduce: 0.01 | optimizer: 92.22 | batch-generator: 304.70
[2022-12-23 03:29:39,468] [INFO] [logging.py:68:log_dist] [Rank 0] step=160, skipped=0, lr=[9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05, 9.999999571339131e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:39,468] [INFO] [timer.py:197:stop] 0/160, RunningAvgSamplesPerSec=49.9166781359949, CurrSamplesPerSec=54.500389614133795, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      160/     200 | consumed samples:        40960 | consumed tokens:     41943040 | elapsed time per iteration (ms): 2959.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.279333E+00 | moe loss: 8.144534E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1380.11 | backward-compute: 1479.44 | backward-embedding-all-reduce: 0.01 | optimizer: 89.65 | batch-generator: 290.05
[2022-12-23 03:29:42,892] [INFO] [logging.py:68:log_dist] [Rank 0] step=161, skipped=0, lr=[9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05, 9.999999565930216e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:42,894] [INFO] [timer.py:197:stop] 0/161, RunningAvgSamplesPerSec=49.925874162497564, CurrSamplesPerSec=51.42268306662457, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      161/     200 | consumed samples:        41216 | consumed tokens:     42205184 | elapsed time per iteration (ms): 3423.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.291716E+00 | moe loss: 8.179478E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1629.07 | backward-compute: 1678.83 | backward-embedding-all-reduce: 0.01 | optimizer: 107.16 | batch-generator: 361.83
[2022-12-23 03:29:46,004] [INFO] [logging.py:68:log_dist] [Rank 0] step=162, skipped=0, lr=[9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05, 9.999999560487387e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:46,005] [INFO] [timer.py:197:stop] 0/162, RunningAvgSamplesPerSec=49.8710558762932, CurrSamplesPerSec=42.45859347122434, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      162/     200 | consumed samples:        41472 | consumed tokens:     42467328 | elapsed time per iteration (ms): 3108.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.288033E+00 | moe loss: 8.146362E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1522.18 | backward-compute: 1477.86 | backward-embedding-all-reduce: 0.01 | optimizer: 95.47 | batch-generator: 309.55
[2022-12-23 03:29:48,958] [INFO] [logging.py:68:log_dist] [Rank 0] step=163, skipped=0, lr=[9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05, 9.999999555010647e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:48,958] [INFO] [timer.py:197:stop] 0/163, RunningAvgSamplesPerSec=49.892004267977924, CurrSamplesPerSec=53.48675122461309, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      163/     200 | consumed samples:        41728 | consumed tokens:     42729472 | elapsed time per iteration (ms): 2956.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.290448E+00 | moe loss: 8.138324E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1381.76 | backward-compute: 1471.73 | backward-embedding-all-reduce: 0.01 | optimizer: 89.66 | batch-generator: 303.05
[2022-12-23 03:29:51,979] [INFO] [logging.py:68:log_dist] [Rank 0] step=164, skipped=0, lr=[9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05, 9.999999549499997e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:51,980] [INFO] [timer.py:197:stop] 0/164, RunningAvgSamplesPerSec=49.9074711174194, CurrSamplesPerSec=52.529262213984246, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      164/     200 | consumed samples:        41984 | consumed tokens:     42991616 | elapsed time per iteration (ms): 3021.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.280431E+00 | moe loss: 8.153681E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1443.51 | backward-compute: 1478.86 | backward-embedding-all-reduce: 0.01 | optimizer: 90.35 | batch-generator: 305.59
[2022-12-23 03:29:54,919] [INFO] [logging.py:68:log_dist] [Rank 0] step=165, skipped=0, lr=[9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05, 9.999999543955433e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:54,919] [INFO] [timer.py:197:stop] 0/165, RunningAvgSamplesPerSec=49.931127566218386, CurrSamplesPerSec=54.084199022905786, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      165/     200 | consumed samples:        42240 | consumed tokens:     43253760 | elapsed time per iteration (ms): 2939.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.284753E+00 | moe loss: 8.133858E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1363.37 | backward-compute: 1473.56 | backward-embedding-all-reduce: 0.01 | optimizer: 89.73 | batch-generator: 294.06
[2022-12-23 03:29:57,850] [INFO] [logging.py:68:log_dist] [Rank 0] step=166, skipped=0, lr=[9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05, 9.999999538376959e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:29:57,850] [INFO] [timer.py:197:stop] 0/166, RunningAvgSamplesPerSec=49.956191391830565, CurrSamplesPerSec=54.40788688545855, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      166/     200 | consumed samples:        42496 | consumed tokens:     43515904 | elapsed time per iteration (ms): 2930.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.254634E+00 | moe loss: 8.117717E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1362.35 | backward-compute: 1470.88 | backward-embedding-all-reduce: 0.01 | optimizer: 89.66 | batch-generator: 287.88
 iteration      167/     200 | consumed samples:        42752 | consumed tokens:     43778048 | elapsed time per iteration (ms): 3565.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.258282E+00 | moe loss: 8.122385E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1945.18 | backward-compute: 1517.57 | backward-embedding-all-reduce: 0.01 | optimizer: 93.96 | batch-generator: 301.55
[2022-12-23 03:30:01,422] [INFO] [logging.py:68:log_dist] [Rank 0] step=167, skipped=0, lr=[9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05, 9.999999532764572e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:01,423] [INFO] [timer.py:197:stop] 0/167, RunningAvgSamplesPerSec=49.85968438645526, CurrSamplesPerSec=37.86368897649235, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:30:04,667] [INFO] [logging.py:68:log_dist] [Rank 0] step=168, skipped=0, lr=[9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05, 9.999999527118273e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:04,667] [INFO] [timer.py:197:stop] 0/168, RunningAvgSamplesPerSec=49.875806117025135, CurrSamplesPerSec=52.68671794895952, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      168/     200 | consumed samples:        43008 | consumed tokens:     44040192 | elapsed time per iteration (ms): 3265.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.245686E+00 | moe loss: 8.111354E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1630.58 | backward-compute: 1513.13 | backward-embedding-all-reduce: 0.01 | optimizer: 105.02 | batch-generator: 335.39
[2022-12-23 03:30:07,638] [INFO] [logging.py:68:log_dist] [Rank 0] step=169, skipped=0, lr=[9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05, 9.999999521438063e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:07,638] [INFO] [timer.py:197:stop] 0/169, RunningAvgSamplesPerSec=49.8980088683947, CurrSamplesPerSec=53.879527319775306, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      169/     200 | consumed samples:        43264 | consumed tokens:     44302336 | elapsed time per iteration (ms): 2956.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.248651E+00 | moe loss: 8.118510E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1384.07 | backward-compute: 1473.45 | backward-embedding-all-reduce: 0.01 | optimizer: 90.03 | batch-generator: 305.31
[2022-12-23 03:30:10,604] [INFO] [logging.py:68:log_dist] [Rank 0] step=170, skipped=0, lr=[9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05, 9.999999515723941e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:10,605] [INFO] [timer.py:197:stop] 0/170, RunningAvgSamplesPerSec=49.90353769185056, CurrSamplesPerSec=50.84436255197411, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      170/     200 | consumed samples:        43520 | consumed tokens:     44564480 | elapsed time per iteration (ms): 2963.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.262721E+00 | moe loss: 8.083832E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1353.18 | backward-compute: 1507.41 | backward-embedding-all-reduce: 0.01 | optimizer: 92.80 | batch-generator: 276.21
[2022-12-23 03:30:13,567] [INFO] [logging.py:68:log_dist] [Rank 0] step=171, skipped=0, lr=[9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05, 9.999999509975908e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:13,567] [INFO] [timer.py:197:stop] 0/171, RunningAvgSamplesPerSec=49.925355017741175, CurrSamplesPerSec=53.882944928225115, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      171/     200 | consumed samples:        43776 | consumed tokens:     44826624 | elapsed time per iteration (ms): 2967.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.249732E+00 | moe loss: 8.086511E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1378.94 | backward-compute: 1482.43 | backward-embedding-all-reduce: 0.01 | optimizer: 91.04 | batch-generator: 307.65
[2022-12-23 03:30:16,488] [INFO] [logging.py:68:log_dist] [Rank 0] step=172, skipped=0, lr=[9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05, 9.999999504193963e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:16,488] [INFO] [timer.py:197:stop] 0/172, RunningAvgSamplesPerSec=49.94889355846414, CurrSamplesPerSec=54.273351947739485, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      172/     200 | consumed samples:        44032 | consumed tokens:     45088768 | elapsed time per iteration (ms): 2919.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.251703E+00 | moe loss: 8.101662E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1349.89 | backward-compute: 1469.73 | backward-embedding-all-reduce: 0.01 | optimizer: 91.97 | batch-generator: 292.62
[2022-12-23 03:30:19,407] [INFO] [logging.py:68:log_dist] [Rank 0] step=173, skipped=0, lr=[9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05, 9.999999498378107e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:19,407] [INFO] [timer.py:197:stop] 0/173, RunningAvgSamplesPerSec=49.972799899291594, CurrSamplesPerSec=54.39895593739677, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      173/     200 | consumed samples:        44288 | consumed tokens:     45350912 | elapsed time per iteration (ms): 2919.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.232200E+00 | moe loss: 8.101620E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1352.55 | backward-compute: 1465.77 | backward-embedding-all-reduce: 0.01 | optimizer: 91.24 | batch-generator: 298.22
[2022-12-23 03:30:23,265] [INFO] [logging.py:68:log_dist] [Rank 0] step=174, skipped=0, lr=[9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05, 9.999999492528338e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:23,266] [INFO] [timer.py:197:stop] 0/174, RunningAvgSamplesPerSec=49.96231033520346, CurrSamplesPerSec=48.2311107789122, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      174/     200 | consumed samples:        44544 | consumed tokens:     45613056 | elapsed time per iteration (ms): 3859.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.230970E+00 | moe loss: 8.135924E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2024.20 | backward-compute: 1715.29 | backward-embedding-all-reduce: 0.01 | optimizer: 107.24 | batch-generator: 350.19
[2022-12-23 03:30:26,205] [INFO] [logging.py:68:log_dist] [Rank 0] step=175, skipped=0, lr=[9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05, 9.999999486644658e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:26,205] [INFO] [timer.py:197:stop] 0/175, RunningAvgSamplesPerSec=49.980163475405746, CurrSamplesPerSec=53.25316312558746, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      175/     200 | consumed samples:        44800 | consumed tokens:     45875200 | elapsed time per iteration (ms): 2938.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.234834E+00 | moe loss: 8.156949E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1355.97 | backward-compute: 1474.68 | backward-embedding-all-reduce: 0.01 | optimizer: 95.41 | batch-generator: 289.43
[2022-12-23 03:30:29,162] [INFO] [logging.py:68:log_dist] [Rank 0] step=176, skipped=0, lr=[9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05, 9.999999480727066e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:29,163] [INFO] [timer.py:197:stop] 0/176, RunningAvgSamplesPerSec=50.00160604136116, CurrSamplesPerSec=54.01028876798764, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      176/     200 | consumed samples:        45056 | consumed tokens:     46137344 | elapsed time per iteration (ms): 2957.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.224459E+00 | moe loss: 8.165626E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1363.08 | backward-compute: 1476.23 | backward-embedding-all-reduce: 0.01 | optimizer: 91.52 | batch-generator: 295.62
[2022-12-23 03:30:32,205] [INFO] [logging.py:68:log_dist] [Rank 0] step=177, skipped=0, lr=[9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05, 9.999999474775562e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:32,205] [INFO] [timer.py:197:stop] 0/177, RunningAvgSamplesPerSec=49.98030810406203, CurrSamplesPerSec=46.53164149713636, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      177/     200 | consumed samples:        45312 | consumed tokens:     46399488 | elapsed time per iteration (ms): 3043.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.227780E+00 | moe loss: 8.095925E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1462.23 | backward-compute: 1471.72 | backward-embedding-all-reduce: 0.01 | optimizer: 97.68 | batch-generator: 274.11
[2022-12-23 03:30:35,122] [INFO] [logging.py:68:log_dist] [Rank 0] step=178, skipped=0, lr=[9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05, 9.999999468790147e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:35,122] [INFO] [timer.py:197:stop] 0/178, RunningAvgSamplesPerSec=50.00385550283945, CurrSamplesPerSec=54.49704812809906, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      178/     200 | consumed samples:        45568 | consumed tokens:     46661632 | elapsed time per iteration (ms): 2916.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.223041E+00 | moe loss: 8.070952E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1347.02 | backward-compute: 1465.38 | backward-embedding-all-reduce: 0.01 | optimizer: 89.61 | batch-generator: 289.47
[2022-12-23 03:30:38,094] [INFO] [logging.py:68:log_dist] [Rank 0] step=179, skipped=0, lr=[9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05, 9.99999946277082e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:38,095] [INFO] [timer.py:197:stop] 0/179, RunningAvgSamplesPerSec=50.02461819636717, CurrSamplesPerSec=53.96859137259948, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      179/     200 | consumed samples:        45824 | consumed tokens:     46923776 | elapsed time per iteration (ms): 2976.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.219434E+00 | moe loss: 8.062960E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1401.97 | backward-compute: 1470.95 | backward-embedding-all-reduce: 0.01 | optimizer: 91.44 | batch-generator: 324.26
 iteration      180/     200 | consumed samples:        46080 | consumed tokens:     47185920 | elapsed time per iteration (ms): 3108.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.206745E+00 | moe loss: 8.065296E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1524.08 | backward-compute: 1485.01 | backward-embedding-all-reduce: 0.01 | optimizer: 90.25 | batch-generator: 308.91
[2022-12-23 03:30:41,211] [INFO] [logging.py:68:log_dist] [Rank 0] step=180, skipped=0, lr=[9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05, 9.999999456717581e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:41,211] [INFO] [timer.py:197:stop] 0/180, RunningAvgSamplesPerSec=50.040216450080436, CurrSamplesPerSec=52.96329481308439, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:30:44,651] [INFO] [logging.py:68:log_dist] [Rank 0] step=181, skipped=0, lr=[9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05, 9.999999450630431e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:44,651] [INFO] [timer.py:197:stop] 0/181, RunningAvgSamplesPerSec=50.04988174038313, CurrSamplesPerSec=51.831902995233406, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      181/     200 | consumed samples:        46336 | consumed tokens:     47448064 | elapsed time per iteration (ms): 3455.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.214341E+00 | moe loss: 8.593041E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1789.85 | backward-compute: 1541.72 | backward-embedding-all-reduce: 0.01 | optimizer: 102.57 | batch-generator: 280.30
[2022-12-23 03:30:47,579] [INFO] [logging.py:68:log_dist] [Rank 0] step=182, skipped=0, lr=[9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05, 9.999999444509368e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:47,580] [INFO] [timer.py:197:stop] 0/182, RunningAvgSamplesPerSec=50.07091269318874, CurrSamplesPerSec=54.143346351030864, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      182/     200 | consumed samples:        46592 | consumed tokens:     47710208 | elapsed time per iteration (ms): 2917.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.224890E+00 | moe loss: 8.061044E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1349.11 | backward-compute: 1469.88 | backward-embedding-all-reduce: 0.01 | optimizer: 91.16 | batch-generator: 288.50
 iteration      183/     200 | consumed samples:        46848 | consumed tokens:     47972352 | elapsed time per iteration (ms): 3035.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.213947E+00 | moe loss: 8.087577E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1345.81 | backward-compute: 1542.05 | backward-embedding-all-reduce: 0.01 | optimizer: 139.54 | batch-generator: 284.32
[2022-12-23 03:30:50,629] [INFO] [logging.py:68:log_dist] [Rank 0] step=183, skipped=0, lr=[9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05, 9.999999438354396e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:50,645] [INFO] [timer.py:197:stop] 0/183, RunningAvgSamplesPerSec=50.02827059000765, CurrSamplesPerSec=43.37858654258539, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:30:53,810] [INFO] [logging.py:68:log_dist] [Rank 0] step=184, skipped=0, lr=[9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05, 9.99999943216551e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:53,810] [INFO] [timer.py:197:stop] 0/184, RunningAvgSamplesPerSec=50.04658287020722, CurrSamplesPerSec=53.597583235995884, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      184/     200 | consumed samples:        47104 | consumed tokens:     48234496 | elapsed time per iteration (ms): 3196.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.203255E+00 | moe loss: 8.114653E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1480.64 | backward-compute: 1580.74 | backward-embedding-all-reduce: 0.01 | optimizer: 98.23 | batch-generator: 343.59
[2022-12-23 03:30:56,725] [INFO] [logging.py:68:log_dist] [Rank 0] step=185, skipped=0, lr=[9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05, 9.999999425942713e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:56,726] [INFO] [timer.py:197:stop] 0/185, RunningAvgSamplesPerSec=50.067386384033085, CurrSamplesPerSec=54.16521841600173, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      185/     200 | consumed samples:        47360 | consumed tokens:     48496640 | elapsed time per iteration (ms): 2914.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.198925E+00 | moe loss: 8.127109E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1339.03 | backward-compute: 1469.50 | backward-embedding-all-reduce: 0.01 | optimizer: 93.22 | batch-generator: 281.70
[2022-12-23 03:30:59,657] [INFO] [logging.py:68:log_dist] [Rank 0] step=186, skipped=0, lr=[9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05, 9.999999419686004e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:30:59,658] [INFO] [timer.py:197:stop] 0/186, RunningAvgSamplesPerSec=50.08771169080867, CurrSamplesPerSec=54.107375651107546, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      186/     200 | consumed samples:        47616 | consumed tokens:     48758784 | elapsed time per iteration (ms): 2931.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.207695E+00 | moe loss: 8.105654E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1353.16 | backward-compute: 1476.46 | backward-embedding-all-reduce: 0.01 | optimizer: 91.95 | batch-generator: 277.61
[2022-12-23 03:31:03,452] [INFO] [logging.py:68:log_dist] [Rank 0] step=187, skipped=0, lr=[9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05, 9.999999413395383e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:03,453] [INFO] [timer.py:197:stop] 0/187, RunningAvgSamplesPerSec=50.073447280272795, CurrSamplesPerSec=47.58019252189215, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      187/     200 | consumed samples:        47872 | consumed tokens:     49020928 | elapsed time per iteration (ms): 3799.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.211761E+00 | moe loss: 8.087170E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1947.85 | backward-compute: 1725.42 | backward-embedding-all-reduce: 0.01 | optimizer: 109.89 | batch-generator: 471.86
[2022-12-23 03:31:06,403] [INFO] [logging.py:68:log_dist] [Rank 0] step=188, skipped=0, lr=[9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05, 9.999999407070851e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:06,404] [INFO] [timer.py:197:stop] 0/188, RunningAvgSamplesPerSec=50.085578361342634, CurrSamplesPerSec=52.43570266867162, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      188/     200 | consumed samples:        48128 | consumed tokens:     49283072 | elapsed time per iteration (ms): 2943.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.203327E+00 | moe loss: 8.066905E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1358.98 | backward-compute: 1476.43 | backward-embedding-all-reduce: 0.01 | optimizer: 96.66 | batch-generator: 270.42
[2022-12-23 03:31:09,337] [INFO] [logging.py:68:log_dist] [Rank 0] step=189, skipped=0, lr=[9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05, 9.999999400712406e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:09,338] [INFO] [timer.py:197:stop] 0/189, RunningAvgSamplesPerSec=50.10340031876058, CurrSamplesPerSec=53.65449073143158, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      189/     200 | consumed samples:        48384 | consumed tokens:     49545216 | elapsed time per iteration (ms): 2934.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.202538E+00 | moe loss: 8.054126E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1354.46 | backward-compute: 1475.74 | backward-embedding-all-reduce: 0.01 | optimizer: 92.98 | batch-generator: 271.26
[2022-12-23 03:31:12,514] [INFO] [logging.py:68:log_dist] [Rank 0] step=190, skipped=0, lr=[9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05, 9.999999394320052e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration      190/     200 | consumed samples:        48640 | consumed tokens:     49807360 | elapsed time per iteration (ms): 3175.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.190787E+00 | moe loss: 8.080167E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1422.04 | backward-compute: 1622.51 | backward-embedding-all-reduce: 0.01 | optimizer: 99.03 | batch-generator: 301.90
[2022-12-23 03:31:12,523] [INFO] [timer.py:197:stop] 0/190, RunningAvgSamplesPerSec=50.08812592597674, CurrSamplesPerSec=47.386689629581774, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:31:15,433] [INFO] [logging.py:68:log_dist] [Rank 0] step=191, skipped=0, lr=[9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05, 9.999999387893785e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:15,433] [INFO] [timer.py:197:stop] 0/191, RunningAvgSamplesPerSec=50.10855170356914, CurrSamplesPerSec=54.26913856493666, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      191/     200 | consumed samples:        48896 | consumed tokens:     50069504 | elapsed time per iteration (ms): 2922.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.196212E+00 | moe loss: 8.059790E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1336.06 | backward-compute: 1477.12 | backward-embedding-all-reduce: 0.01 | optimizer: 90.19 | batch-generator: 271.30
 iteration      192/     200 | consumed samples:        49152 | consumed tokens:     50331648 | elapsed time per iteration (ms): 3167.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.195742E+00 | moe loss: 8.057978E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1565.12 | backward-compute: 1496.52 | backward-embedding-all-reduce: 0.01 | optimizer: 92.52 | batch-generator: 279.46
[2022-12-23 03:31:18,608] [INFO] [logging.py:68:log_dist] [Rank 0] step=192, skipped=0, lr=[9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05, 9.999999381433606e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:18,608] [INFO] [timer.py:197:stop] 0/192, RunningAvgSamplesPerSec=50.01981192171153, CurrSamplesPerSec=37.47617296515483, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:31:22,131] [INFO] [logging.py:68:log_dist] [Rank 0] step=193, skipped=0, lr=[9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05, 9.999999374939515e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:22,131] [INFO] [timer.py:197:stop] 0/193, RunningAvgSamplesPerSec=49.96879082853685, CurrSamplesPerSec=41.856801907819296, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      193/     200 | consumed samples:        49408 | consumed tokens:     50593792 | elapsed time per iteration (ms): 3562.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.172329E+00 | moe loss: 8.082732E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1748.84 | backward-compute: 1668.00 | backward-embedding-all-reduce: 0.01 | optimizer: 122.18 | batch-generator: 256.01
[2022-12-23 03:31:25,196] [INFO] [logging.py:68:log_dist] [Rank 0] step=194, skipped=0, lr=[9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05, 9.999999368411512e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:25,196] [INFO] [timer.py:197:stop] 0/194, RunningAvgSamplesPerSec=49.98233105518586, CurrSamplesPerSec=52.71040948065112, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      194/     200 | consumed samples:        49664 | consumed tokens:     50855936 | elapsed time per iteration (ms): 3034.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.183070E+00 | moe loss: 8.081527E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1462.19 | backward-compute: 1473.83 | backward-embedding-all-reduce: 0.01 | optimizer: 90.31 | batch-generator: 331.74
[2022-12-23 03:31:28,117] [INFO] [logging.py:68:log_dist] [Rank 0] step=195, skipped=0, lr=[9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05, 9.999999361849598e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:28,117] [INFO] [timer.py:197:stop] 0/195, RunningAvgSamplesPerSec=49.999938337835694, CurrSamplesPerSec=53.62705035442319, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      195/     200 | consumed samples:        49920 | consumed tokens:     51118080 | elapsed time per iteration (ms): 2921.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.175916E+00 | moe loss: 8.089488E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1341.08 | backward-compute: 1473.28 | backward-embedding-all-reduce: 0.01 | optimizer: 93.71 | batch-generator: 269.80
 iteration      196/     200 | consumed samples:        50176 | consumed tokens:     51380224 | elapsed time per iteration (ms): 3105.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.174687E+00 | moe loss: 8.083355E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1448.90 | backward-compute: 1548.57 | backward-embedding-all-reduce: 0.01 | optimizer: 93.13 | batch-generator: 286.40
[2022-12-23 03:31:31,240] [INFO] [logging.py:68:log_dist] [Rank 0] step=196, skipped=0, lr=[9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05, 9.999999355253772e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:31,241] [INFO] [timer.py:197:stop] 0/196, RunningAvgSamplesPerSec=49.97923501063037, CurrSamplesPerSec=46.28071442191747, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
[2022-12-23 03:31:34,198] [INFO] [logging.py:68:log_dist] [Rank 0] step=197, skipped=0, lr=[9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05, 9.999999348624034e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:34,198] [INFO] [timer.py:197:stop] 0/197, RunningAvgSamplesPerSec=49.99805215024434, CurrSamplesPerSec=53.937707363493736, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      197/     200 | consumed samples:        50432 | consumed tokens:     51642368 | elapsed time per iteration (ms): 2976.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.173618E+00 | moe loss: 8.093673E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1378.58 | backward-compute: 1476.92 | backward-embedding-all-reduce: 0.01 | optimizer: 91.66 | batch-generator: 269.20
[2022-12-23 03:31:37,122] [INFO] [logging.py:68:log_dist] [Rank 0] step=198, skipped=0, lr=[9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05, 9.999999341960385e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:37,122] [INFO] [timer.py:197:stop] 0/198, RunningAvgSamplesPerSec=50.0136598429633, CurrSamplesPerSec=53.25544516584182, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      198/     200 | consumed samples:        50688 | consumed tokens:     51904512 | elapsed time per iteration (ms): 2921.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.171391E+00 | moe loss: 8.072063E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1339.42 | backward-compute: 1485.72 | backward-embedding-all-reduce: 0.01 | optimizer: 89.59 | batch-generator: 265.16
[2022-12-23 03:31:40,037] [INFO] [logging.py:68:log_dist] [Rank 0] step=199, skipped=0, lr=[9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05, 9.999999335262824e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:40,037] [INFO] [timer.py:197:stop] 0/199, RunningAvgSamplesPerSec=50.02933592860016, CurrSamplesPerSec=53.30398480044226, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      199/     200 | consumed samples:        50944 | consumed tokens:     52166656 | elapsed time per iteration (ms): 2916.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.175124E+00 | moe loss: 8.062859E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1341.94 | backward-compute: 1474.61 | backward-embedding-all-reduce: 0.01 | optimizer: 89.53 | batch-generator: 262.10
[2022-12-23 03:31:43,826] [INFO] [logging.py:68:log_dist] [Rank 0] step=200, skipped=0, lr=[9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05, 9.999999328531351e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2022-12-23 03:31:43,827] [INFO] [timer.py:197:stop] 0/200, RunningAvgSamplesPerSec=50.04356963236511, CurrSamplesPerSec=53.01494650226567, MemAllocated=18.13GB, MaxMemAllocated=27.96GB
 iteration      200/     200 | consumed samples:        51200 | consumed tokens:     52428800 | elapsed time per iteration (ms): 3799.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.174340E+00 | moe loss: 8.077424E-02 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1926.93 | backward-compute: 1750.77 | backward-embedding-all-reduce: 0.01 | optimizer: 103.09 | batch-generator: 486.54
[after training is done] datetime: 2022-12-23 03:31:43 
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.21352744102478027 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.2045278549194336 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20441126823425293 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20467495918273926 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20452523231506348 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.20482635498046875 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3161122798919678 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.303225040435791 seconds
Loading extension module utils...
Time to load utils op: 0.2779829502105713 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3033573627471924 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3032190799713135 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3031764030456543 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3058013916015625 seconds
No existing process group found, creating a new group named: ep_size_16
Using /mnt/cache/zhaishuming/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.31266260147094727 seconds
