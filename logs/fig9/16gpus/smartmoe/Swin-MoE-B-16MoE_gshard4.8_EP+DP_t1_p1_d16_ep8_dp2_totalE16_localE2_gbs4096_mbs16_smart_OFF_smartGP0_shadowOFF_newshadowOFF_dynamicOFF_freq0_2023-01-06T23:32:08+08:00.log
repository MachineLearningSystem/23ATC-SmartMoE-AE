+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102392
++ scontrol show JobId=102392
++ scontrol show JobId=102392
++ scontrol show JobId=102392
++ grep BatchHost
++ scontrol show JobId=102392
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=102392
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=102392
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102392
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102392
++ scontrol show JobId=102392
++ scontrol show JobId=102392
++ scontrol show JobId=102392
++ grep BatchHost
++ scontrol show JobId=102392
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102392
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=102392
++ awk '{print $2}'
++ grep BatchHost
++ scontrol show JobId=102392
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=11
+ RANK=11
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ '[' OFF == ON ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export RANK=8
+ RANK=8
+ export NNODES=2
+ NNODES=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ false
+ true
+ EXEC=./main_moe.py
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ false
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ true
+ cd /home/zms/model_training/MoE/FastSwin
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ python_args+='
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export RANK=3
+ RANK=3
+ '[' nico == nico ']'
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ localrank=3
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NNODES=2
+ NNODES=2
+ cd /home/zms/model_training/MoE/FastSwin
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ export RANK=2
+ RANK=2
+ MAX_JOBS=64
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NNODES=2
+ true
+ NNODES=2
+ cd /home/zms/model_training/MoE/FastSwin
+ export NODE_RANK=1
+ NODE_RANK=1
+ export NNODES=2
+ NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ MASTER_ADDR=nico1
+ WORLD_SIZE=16
+ localrank=1
+ export RANK=5
+ RANK=5
+ false
+ true
+ export CUDA_VISIBLE_DEVICES=1
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=6
+ false
+ true
+ EXEC=./main_moe.py
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ CUDA_VISIBLE_DEVICES=1
+ localrank=5
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ EXEC=./main_moe.py
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ export MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ export MASTER_ADDR=nico1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ MAX_JOBS=64
+ MASTER_ADDR=nico1
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args=
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ cd /home/zms/model_training/MoE/FastSwin
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ true
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ cd /home/zms/model_training/MoE/FastSwin
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=2
+ NNODES=2
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ python_args=
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ false
+ true
+ EXEC=./main_moe.py
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ false
+ true
+ EXEC=./main_moe.py
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ TRAIN_SAMPLES=20520960
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args=
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ export NNODES=2
+ NNODES=2
+ python_args+='
+ python_args+='
+ export NODE_RANK=3
+ NODE_RANK=3
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof/table
+ python_args=
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ false
+ true
+ EXEC=./main_moe.py
+ python_args+='
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ false
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T23:32:08+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 0/16
[32m[2023-01-06 23:32:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 403)[0m: INFO Full config saved to /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default/config.json
[32m[2023-01-06 23:32:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 406)[0m: INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: part
  DATASET: imagenet22K
  DATA_PATH: /mnt/znvme/dataset/imagenet22k
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 4.8
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    GATE_NOISE: 1.0
    INIT_STD: 0.005
    IN_CHANS: 3
    IS_GSHARD_LOSS: true
    MLP_FC2_BIAS: false
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - 1
      - 3
      - 5
      - 7
      - 9
      - 11
      - 13
      - 15
      - 17
    - - 1
    MOE_DROP: 0.1
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 2
    USE_BPR: true
    WINDOW_SIZE: 12
  TYPE: swin_fastmoe
OUTPUT: /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default
PRINT_FREQ: 1
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 16
  AUTO_RESUME: true
  BASE_LR: 0.001
  CHECKPOINT_MODE: full
  CLIP_GRAD: 3.0
  EPOCHS: 90
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.1

[32m[2023-01-06 23:32:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 407)[0m: INFO {"cfg": "configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml", "opts": null, "batch_size": 16, "data_path": "/mnt/znvme/dataset/imagenet22k", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": 16, "use_checkpoint": false, "checkpoint_mode": "full", "disable_amp": false, "amp_opt_level": null, "output": "/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": true, "fused_layernorm": false, "optim": null, "num_experts": 2, "top_k": 2, "balance_strategy": "gshard", "expert_parallel_strategy": "EP+DP", "expert_ep_size": 8, "expert_dp_size": 2, "dump": false, "dynamic_placement": false, "dynamic_freq": 10, "new_shadow": false, "gshard_cap": 4.8, "init_method_std": 0.002, "num_layers": 12}
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
local rank 0 / global rank 0 successfully build train dataset
local rank 0 / global rank 0 successfully build val dataset
[32m[2023-01-06 23:32:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 120)[0m: INFO Creating model:swin_fastmoe/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 5/16
local rank 0 / global rank 5 successfully build train dataset
local rank 0 / global rank 5 successfully build val dataset
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in DP group [5, 13]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 0 in DP group [0, 8]
[32m[2023-01-06 23:33:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 122)[0m: INFO SwinTransformerFastMoE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(48, 48), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(48, 48), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(24, 24), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(24, 24), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(12, 12), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(12, 12), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(6, 6), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=4096, out_features=1024, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 1024, hidden_features = 4096, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=4096, out_features=16, bias=True)
              )
            )
          )
        )
      )
    )
  )
  (norm): TimerModule(
    (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (avgpool): TimerModule(
    (model): AdaptiveAvgPool1d(output_size=1)
  )
  (head): TimerModule(
    (model): Linear(in_features=1024, out_features=21841, bias=True)
  )
)
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 10/16
local rank 0 / global rank 10 successfully build train dataset
local rank 0 / global rank 10 successfully build val dataset
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [2, 10]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-06 23:33:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 156)[0m: INFO no checkpoint found in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default, ignoring auto resume
[32m[2023-01-06 23:33:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 176)[0m: INFO Start training
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 6/16
local rank 0 / global rank 6 successfully build train dataset
local rank 0 / global rank 6 successfully build val dataset
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in DP group [6, 14]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 3/16
local rank 0 / global rank 3 successfully build train dataset
local rank 0 / global rank 3 successfully build val dataset
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in DP group [3, 11]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 15/16
local rank 0 / global rank 15 successfully build train dataset
local rank 0 / global rank 15 successfully build val dataset
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [7, 15]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 8/16
local rank 0 / global rank 8 successfully build train dataset
local rank 0 / global rank 8 successfully build val dataset
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [0, 8]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 1/16
local rank 0 / global rank 1 successfully build train dataset
local rank 0 / global rank 1 successfully build val dataset
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in DP group [1, 9]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 7/16
local rank 0 / global rank 7 successfully build train dataset
local rank 0 / global rank 7 successfully build val dataset
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in DP group [7, 15]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 12/16
local rank 0 / global rank 12 successfully build train dataset
local rank 0 / global rank 12 successfully build val dataset
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [4, 12]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 13/16
local rank 0 / global rank 13 successfully build train dataset
local rank 0 / global rank 13 successfully build val dataset
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [5, 13]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 14/16
local rank 0 / global rank 14 successfully build train dataset
local rank 0 / global rank 14 successfully build val dataset
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [6, 14]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 4/16
local rank 0 / global rank 4 successfully build train dataset
local rank 0 / global rank 4 successfully build val dataset
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in DP group [4, 12]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 2/16
local rank 0 / global rank 2 successfully build train dataset
local rank 0 / global rank 2 successfully build val dataset
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in DP group [2, 10]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 9/16
local rank 0 / global rank 9 successfully build train dataset
local rank 0 / global rank 9 successfully build val dataset
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [1, 9]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 11/16
local rank 0 / global rank 11 successfully build train dataset
local rank 0 / global rank 11 successfully build val dataset
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [3, 11]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-06 23:33:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][15/54685]	eta 13:40:15 lr 0.0010000000	 wd 0.1000	time 0.9109 (0.9002)	loss 0.6243 (0.6248)	loss-cls 9.9879 (9.9957)	loss-aux 0.0016 (0.0016)	grad_norm 2.2042 (2.2042)	loss_scale 65536.0000 (65536.0000)	mem 8329MB	batch_time 14.4037
[32m[2023-01-06 23:33:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][31/54685]	eta 10:38:17 lr 0.0010000000	 wd 0.1000	time 0.6124 (0.7007)	loss 0.6243 (0.6248)	loss-cls 9.9864 (9.9953)	loss-aux 0.0017 (0.0017)	grad_norm 1.6409 (1.9226)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 8.0196
[32m[2023-01-06 23:33:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][47/54685]	eta 9:38:19 lr 0.0010000000	 wd 0.1000	time 0.5933 (0.6351)	loss 0.6254 (0.6251)	loss-cls 10.0038 (10.0002)	loss-aux 0.0021 (0.0018)	grad_norm 1.3282 (1.7245)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 8.0601
[32m[2023-01-06 23:33:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][63/54685]	eta 9:06:40 lr 0.0010000000	 wd 0.1000	time 0.5928 (0.6005)	loss 0.6262 (0.6252)	loss-cls 10.0172 (10.0013)	loss-aux 0.0020 (0.0019)	grad_norm 1.1945 (1.5920)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.9488
[32m[2023-01-06 23:33:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][79/54685]	eta 8:46:39 lr 0.0010000000	 wd 0.1000	time 0.5922 (0.5787)	loss 0.6257 (0.6251)	loss-cls 10.0090 (9.9992)	loss-aux 0.0022 (0.0020)	grad_norm 1.2325 (1.5201)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.8622
[32m[2023-01-06 23:33:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][95/54685]	eta 8:33:52 lr 0.0010000000	 wd 0.1000	time 0.6149 (0.5648)	loss 0.6250 (0.6250)	loss-cls 9.9968 (9.9981)	loss-aux 0.0030 (0.0022)	grad_norm 1.1702 (1.4618)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.9270
[32m[2023-01-06 23:34:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][111/54685]	eta 8:24:14 lr 0.0010000000	 wd 0.1000	time 0.5796 (0.5544)	loss 0.6236 (0.6249)	loss-cls 9.9739 (9.9967)	loss-aux 0.0035 (0.0023)	grad_norm 1.1004 (1.4101)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.8686
[32m[2023-01-06 23:34:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][127/54685]	eta 8:16:17 lr 0.0010000000	 wd 0.1000	time 0.5950 (0.5458)	loss 0.6233 (0.6251)	loss-cls 9.9681 (9.9988)	loss-aux 0.0041 (0.0025)	grad_norm 1.2412 (1.3890)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.7729
[32m[2023-01-06 23:34:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][143/54685]	eta 8:12:12 lr 0.0010000000	 wd 0.1000	time 0.6261 (0.5415)	loss 0.6271 (0.6251)	loss-cls 10.0293 (9.9984)	loss-aux 0.0037 (0.0026)	grad_norm 1.1754 (1.3653)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 8.1068
[32m[2023-01-06 23:34:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][159/54685]	eta 8:07:37 lr 0.0010000000	 wd 0.1000	time 0.5917 (0.5366)	loss 0.6306 (0.6252)	loss-cls 10.0854 (10.0001)	loss-aux 0.0039 (0.0027)	grad_norm 1.1038 (1.3391)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.8830
[32m[2023-01-06 23:34:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][175/54685]	eta 8:05:19 lr 0.0010000000	 wd 0.1000	time 0.6593 (0.5342)	loss 0.6299 (0.6252)	loss-cls 10.0745 (10.0006)	loss-aux 0.0035 (0.0028)	grad_norm 1.0771 (1.3153)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 8.1658
[32m[2023-01-06 23:34:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][191/54685]	eta 8:02:18 lr 0.0010000000	 wd 0.1000	time 0.5693 (0.5310)	loss 0.6248 (0.6251)	loss-cls 9.9935 (9.9984)	loss-aux 0.0034 (0.0029)	grad_norm 1.1690 (1.3031)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.9416
[32m[2023-01-06 23:34:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][207/54685]	eta 8:00:04 lr 0.0010000000	 wd 0.1000	time 0.5958 (0.5287)	loss 0.6229 (0.6249)	loss-cls 9.9619 (9.9950)	loss-aux 0.0039 (0.0029)	grad_norm 1.0729 (1.2854)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 8.0181
[32m[2023-01-06 23:35:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][223/54685]	eta 7:58:10 lr 0.0010000000	 wd 0.1000	time 0.5880 (0.5268)	loss 0.6307 (0.6251)	loss-cls 10.0871 (9.9980)	loss-aux 0.0040 (0.0030)	grad_norm 1.1248 (1.2740)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 8.0266
[32m[2023-01-06 23:35:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][239/54685]	eta 7:55:40 lr 0.0010000000	 wd 0.1000	time 0.5806 (0.5242)	loss 0.6285 (0.6250)	loss-cls 10.0520 (9.9969)	loss-aux 0.0043 (0.0031)	grad_norm 1.0385 (1.2583)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.8014
[32m[2023-01-06 23:35:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][255/54685]	eta 7:53:39 lr 0.0010000000	 wd 0.1000	time 0.6216 (0.5221)	loss 0.6250 (0.6250)	loss-cls 9.9958 (9.9962)	loss-aux 0.0045 (0.0032)	grad_norm 1.1331 (1.2504)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.8600
[32m[2023-01-06 23:35:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][271/54685]	eta 7:51:43 lr 0.0010000000	 wd 0.1000	time 0.5651 (0.5202)	loss 0.6263 (0.6250)	loss-cls 10.0164 (9.9968)	loss-aux 0.0042 (0.0033)	grad_norm 0.9773 (1.2344)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.8174
[32m[2023-01-06 23:35:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][287/54685]	eta 7:49:50 lr 0.0010000000	 wd 0.1000	time 0.6045 (0.5182)	loss 0.6269 (0.6250)	loss-cls 10.0252 (9.9965)	loss-aux 0.0050 (0.0034)	grad_norm 1.0961 (1.2267)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.7668
[32m[2023-01-06 23:35:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][303/54685]	eta 7:48:06 lr 0.0010000000	 wd 0.1000	time 0.5617 (0.5165)	loss 0.6259 (0.6250)	loss-cls 10.0107 (9.9964)	loss-aux 0.0043 (0.0035)	grad_norm 1.1757 (1.2240)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.7549
[32m[2023-01-06 23:35:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][319/54685]	eta 7:46:40 lr 0.0010000000	 wd 0.1000	time 0.5881 (0.5150)	loss 0.6294 (0.6249)	loss-cls 10.0653 (9.9951)	loss-aux 0.0046 (0.0035)	grad_norm 1.7012 (1.2479)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.8047
[32m[2023-01-06 23:35:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][335/54685]	eta 7:44:53 lr 0.0010000000	 wd 0.1000	time 0.5702 (0.5132)	loss 0.6235 (0.6249)	loss-cls 9.9698 (9.9944)	loss-aux 0.0062 (0.0036)	grad_norm 1.1081 (1.2412)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.6289
[32m[2023-01-06 23:36:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][351/54685]	eta 7:42:47 lr 0.0010000000	 wd 0.1000	time 0.5616 (0.5111)	loss 0.6299 (0.6249)	loss-cls 10.0713 (9.9940)	loss-aux 0.0070 (0.0037)	grad_norm 1.8943 (1.2709)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4533
[32m[2023-01-06 23:36:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][367/54685]	eta 7:40:43 lr 0.0010000000	 wd 0.1000	time 0.5833 (0.5089)	loss 0.6267 (0.6250)	loss-cls 10.0219 (9.9959)	loss-aux 0.0058 (0.0038)	grad_norm 1.0971 (1.2633)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3923
[32m[2023-01-06 23:36:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][383/54685]	eta 7:39:08 lr 0.0010000000	 wd 0.1000	time 0.5521 (0.5073)	loss 0.6314 (0.6250)	loss-cls 10.0950 (9.9967)	loss-aux 0.0080 (0.0040)	grad_norm 2.0219 (1.2949)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5298
[32m[2023-01-06 23:36:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][399/54685]	eta 7:37:40 lr 0.0010000000	 wd 0.1000	time 0.5756 (0.5058)	loss 0.6265 (0.6250)	loss-cls 10.0162 (9.9963)	loss-aux 0.0080 (0.0042)	grad_norm 1.1891 (1.2907)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5227
[32m[2023-01-06 23:36:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][415/54685]	eta 7:36:10 lr 0.0010000000	 wd 0.1000	time 0.5581 (0.5043)	loss 0.6164 (0.6250)	loss-cls 9.8560 (9.9964)	loss-aux 0.0062 (0.0043)	grad_norm 1.2163 (1.2878)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4677
[32m[2023-01-06 23:36:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][431/54685]	eta 7:35:00 lr 0.0010000000	 wd 0.1000	time 0.5873 (0.5032)	loss 0.6244 (0.6251)	loss-cls 9.9834 (9.9973)	loss-aux 0.0070 (0.0044)	grad_norm 1.2559 (1.2867)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5740
[32m[2023-01-06 23:36:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][447/54685]	eta 7:33:47 lr 0.0010000000	 wd 0.1000	time 0.5884 (0.5020)	loss 0.6294 (0.6251)	loss-cls 10.0640 (9.9975)	loss-aux 0.0058 (0.0045)	grad_norm 1.2124 (1.2840)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5205
[32m[2023-01-06 23:36:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][463/54685]	eta 7:32:50 lr 0.0010000000	 wd 0.1000	time 0.5728 (0.5011)	loss 0.6354 (0.6252)	loss-cls 10.1597 (9.9987)	loss-aux 0.0069 (0.0046)	grad_norm 1.1739 (1.2802)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.6113
[32m[2023-01-06 23:37:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][479/54685]	eta 7:31:59 lr 0.0010000000	 wd 0.1000	time 0.5601 (0.5003)	loss 0.6108 (0.6252)	loss-cls 9.7655 (9.9984)	loss-aux 0.0075 (0.0047)	grad_norm 1.1542 (1.2760)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.6327
[32m[2023-01-06 23:37:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][495/54685]	eta 7:30:50 lr 0.0010000000	 wd 0.1000	time 0.5600 (0.4992)	loss 0.6250 (0.6251)	loss-cls 9.9939 (9.9976)	loss-aux 0.0062 (0.0047)	grad_norm 0.9569 (1.2657)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4534
[32m[2023-01-06 23:37:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][511/54685]	eta 7:29:57 lr 0.0010000000	 wd 0.1000	time 0.5562 (0.4983)	loss 0.6263 (0.6251)	loss-cls 10.0141 (9.9975)	loss-aux 0.0064 (0.0048)	grad_norm 14.6884 (1.6852)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5584
[32m[2023-01-06 23:37:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][527/54685]	eta 7:29:04 lr 0.0010000000	 wd 0.1000	time 0.5837 (0.4975)	loss 0.6178 (0.6251)	loss-cls 9.8786 (9.9966)	loss-aux 0.0066 (0.0048)	grad_norm 0.9556 (1.6631)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5341
[32m[2023-01-06 23:37:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][543/54685]	eta 7:28:16 lr 0.0010000000	 wd 0.1000	time 0.5781 (0.4968)	loss 0.6227 (0.6251)	loss-cls 9.9573 (9.9969)	loss-aux 0.0064 (0.0049)	grad_norm 1.5623 (1.6601)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5619
[32m[2023-01-06 23:37:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][559/54685]	eta 7:27:08 lr 0.0010000000	 wd 0.1000	time 0.5708 (0.4957)	loss 0.6227 (0.6251)	loss-cls 9.9562 (9.9959)	loss-aux 0.0066 (0.0049)	grad_norm 1.1571 (1.6457)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3239
[32m[2023-01-06 23:37:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][575/54685]	eta 7:25:56 lr 0.0010000000	 wd 0.1000	time 0.5651 (0.4945)	loss 0.6202 (0.6251)	loss-cls 9.9163 (9.9959)	loss-aux 0.0073 (0.0050)	grad_norm 5.6419 (1.7567)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2485
[32m[2023-01-06 23:37:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][591/54685]	eta 7:24:59 lr 0.0010000000	 wd 0.1000	time 0.5643 (0.4936)	loss 0.6087 (0.6250)	loss-cls 9.7341 (9.9951)	loss-aux 0.0052 (0.0050)	grad_norm 1.6703 (1.7544)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3699
[32m[2023-01-06 23:38:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][607/54685]	eta 7:24:02 lr 0.0010000000	 wd 0.1000	time 0.5276 (0.4927)	loss 0.6474 (0.6250)	loss-cls 10.3520 (9.9952)	loss-aux 0.0064 (0.0050)	grad_norm 1.6358 (1.7513)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3469
[32m[2023-01-06 23:38:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][623/54685]	eta 7:23:03 lr 0.0010000000	 wd 0.1000	time 0.5474 (0.4917)	loss 0.6358 (0.6250)	loss-cls 10.1642 (9.9954)	loss-aux 0.0089 (0.0051)	grad_norm 1.4187 (1.7428)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2985
[32m[2023-01-06 23:38:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][639/54685]	eta 7:22:12 lr 0.0010000000	 wd 0.1000	time 0.5616 (0.4909)	loss 0.6304 (0.6250)	loss-cls 10.0794 (9.9943)	loss-aux 0.0066 (0.0052)	grad_norm 1.0779 (1.7261)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3506
[32m[2023-01-06 23:38:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][655/54685]	eta 7:21:20 lr 0.0010000000	 wd 0.1000	time 0.5794 (0.4901)	loss 0.6266 (0.6249)	loss-cls 10.0163 (9.9936)	loss-aux 0.0100 (0.0052)	grad_norm 2.7907 (1.7521)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3250
[32m[2023-01-06 23:38:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][671/54685]	eta 7:20:39 lr 0.0010000000	 wd 0.1000	time 0.5675 (0.4895)	loss 0.6198 (0.6250)	loss-cls 9.9106 (9.9944)	loss-aux 0.0068 (0.0053)	grad_norm 1.0966 (1.7365)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4226
[32m[2023-01-06 23:38:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][687/54685]	eta 7:19:56 lr 0.0010000000	 wd 0.1000	time 0.5523 (0.4888)	loss 0.6268 (0.6250)	loss-cls 10.0209 (9.9944)	loss-aux 0.0076 (0.0053)	grad_norm 2.4755 (1.7537)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3865
[32m[2023-01-06 23:38:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][703/54685]	eta 7:19:22 lr 0.0010000000	 wd 0.1000	time 0.5661 (0.4884)	loss 0.6276 (0.6250)	loss-cls 10.0357 (9.9943)	loss-aux 0.0063 (0.0053)	grad_norm 1.0250 (1.7371)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4865
[32m[2023-01-06 23:38:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][719/54685]	eta 7:18:47 lr 0.0009999999	 wd 0.1000	time 0.5476 (0.4879)	loss 0.6217 (0.6250)	loss-cls 9.9395 (9.9939)	loss-aux 0.0077 (0.0054)	grad_norm 0.8944 (1.7184)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4459
[32m[2023-01-06 23:39:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][735/54685]	eta 7:18:20 lr 0.0009999999	 wd 0.1000	time 0.5478 (0.4875)	loss 0.6362 (0.6249)	loss-cls 10.1701 (9.9937)	loss-aux 0.0091 (0.0055)	grad_norm 1.0073 (1.7029)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5363
[32m[2023-01-06 23:39:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][751/54685]	eta 7:17:56 lr 0.0009999999	 wd 0.1000	time 0.5509 (0.4872)	loss 0.6062 (0.6249)	loss-cls 9.6875 (9.9927)	loss-aux 0.0111 (0.0055)	grad_norm 1.8391 (1.7058)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5802
[32m[2023-01-06 23:39:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][767/54685]	eta 7:17:30 lr 0.0009999999	 wd 0.1000	time 0.5782 (0.4869)	loss 0.6299 (0.6248)	loss-cls 10.0680 (9.9917)	loss-aux 0.0110 (0.0057)	grad_norm 0.9647 (1.6904)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5322
[32m[2023-01-06 23:39:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][783/54685]	eta 7:17:03 lr 0.0009999999	 wd 0.1000	time 0.5741 (0.4865)	loss 0.6163 (0.6248)	loss-cls 9.8479 (9.9909)	loss-aux 0.0121 (0.0058)	grad_norm 1.3951 (1.6844)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5163
[32m[2023-01-06 23:39:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][799/54685]	eta 7:16:31 lr 0.0009999999	 wd 0.1000	time 0.5605 (0.4861)	loss 0.6300 (0.6248)	loss-cls 10.0685 (9.9903)	loss-aux 0.0107 (0.0059)	grad_norm 4.7978 (1.7466)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4288
[32m[2023-01-06 23:39:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][815/54685]	eta 7:16:05 lr 0.0009999999	 wd 0.1000	time 0.5914 (0.4857)	loss 0.6161 (0.6247)	loss-cls 9.8480 (9.9892)	loss-aux 0.0104 (0.0060)	grad_norm 4.3364 (1.7974)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4957
[32m[2023-01-06 23:39:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][831/54685]	eta 7:15:42 lr 0.0009999999	 wd 0.1000	time 0.5537 (0.4854)	loss 0.6094 (0.6246)	loss-cls 9.7401 (9.9877)	loss-aux 0.0098 (0.0061)	grad_norm 0.7811 (1.7779)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5417
[32m[2023-01-06 23:39:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][847/54685]	eta 7:15:15 lr 0.0009999999	 wd 0.1000	time 0.5640 (0.4851)	loss 0.6139 (0.6246)	loss-cls 9.8101 (9.9877)	loss-aux 0.0117 (0.0061)	grad_norm 3.6411 (1.8130)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4610
[32m[2023-01-06 23:40:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][863/54685]	eta 7:14:52 lr 0.0009999999	 wd 0.1000	time 0.5758 (0.4848)	loss 0.6205 (0.6246)	loss-cls 9.9181 (9.9869)	loss-aux 0.0101 (0.0062)	grad_norm 2.3883 (1.8237)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5093
[32m[2023-01-06 23:40:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][879/54685]	eta 7:14:33 lr 0.0009999999	 wd 0.1000	time 0.5664 (0.4846)	loss 0.6243 (0.6246)	loss-cls 9.9774 (9.9865)	loss-aux 0.0106 (0.0063)	grad_norm 0.6498 (1.8023)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5758
[32m[2023-01-06 23:40:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][895/54685]	eta 7:14:09 lr 0.0009999999	 wd 0.1000	time 0.5549 (0.4843)	loss 0.6251 (0.6245)	loss-cls 9.9906 (9.9861)	loss-aux 0.0118 (0.0064)	grad_norm 0.5800 (1.7805)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4790
[32m[2023-01-06 23:40:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][911/54685]	eta 7:13:51 lr 0.0009999999	 wd 0.1000	time 0.5654 (0.4841)	loss 0.6162 (0.6245)	loss-cls 9.8483 (9.9850)	loss-aux 0.0109 (0.0065)	grad_norm 1.0553 (1.7678)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5855
[32m[2023-01-06 23:40:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][927/54685]	eta 7:13:23 lr 0.0009999999	 wd 0.1000	time 0.5667 (0.4837)	loss 0.6177 (0.6244)	loss-cls 9.8723 (9.9843)	loss-aux 0.0116 (0.0065)	grad_norm 1.4340 (1.7620)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3892
[32m[2023-01-06 23:40:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][943/54685]	eta 7:12:54 lr 0.0009999999	 wd 0.1000	time 0.5391 (0.4833)	loss 0.6138 (0.6244)	loss-cls 9.8071 (9.9836)	loss-aux 0.0134 (0.0066)	grad_norm 0.6675 (1.7435)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3605
[32m[2023-01-06 23:40:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][959/54685]	eta 7:12:27 lr 0.0009999999	 wd 0.1000	time 0.5627 (0.4830)	loss 0.6217 (0.6243)	loss-cls 9.9330 (9.9826)	loss-aux 0.0140 (0.0067)	grad_norm 0.5116 (1.7229)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4013
[32m[2023-01-06 23:40:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][975/54685]	eta 7:11:58 lr 0.0009999999	 wd 0.1000	time 0.5594 (0.4826)	loss 0.6201 (0.6243)	loss-cls 9.9107 (9.9819)	loss-aux 0.0106 (0.0068)	grad_norm 2.4590 (1.7350)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3257
[32m[2023-01-06 23:41:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][991/54685]	eta 7:11:25 lr 0.0009999999	 wd 0.1000	time 0.5549 (0.4821)	loss 0.6129 (0.6243)	loss-cls 9.7940 (9.9815)	loss-aux 0.0131 (0.0069)	grad_norm 0.3797 (1.7132)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2664
[32m[2023-01-06 23:41:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1007/54685]	eta 7:10:52 lr 0.0009999999	 wd 0.1000	time 0.5634 (0.4816)	loss 0.6167 (0.6242)	loss-cls 9.8541 (9.9802)	loss-aux 0.0125 (0.0070)	grad_norm 0.4814 (1.6936)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2276
[32m[2023-01-06 23:41:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1023/54685]	eta 7:10:20 lr 0.0009999999	 wd 0.1000	time 0.5535 (0.4812)	loss 0.6186 (0.6242)	loss-cls 9.8849 (9.9795)	loss-aux 0.0124 (0.0071)	grad_norm 0.4542 (1.6742)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2408
[32m[2023-01-06 23:41:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1039/54685]	eta 7:09:53 lr 0.0009999999	 wd 0.1000	time 0.5691 (0.4808)	loss 0.6281 (0.6241)	loss-cls 10.0365 (9.9791)	loss-aux 0.0124 (0.0072)	grad_norm 0.7838 (1.6605)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3243
[32m[2023-01-06 23:41:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1055/54685]	eta 7:09:21 lr 0.0009999999	 wd 0.1000	time 0.5661 (0.4804)	loss 0.6195 (0.6241)	loss-cls 9.8993 (9.9779)	loss-aux 0.0119 (0.0073)	grad_norm 11.8941 (1.8156)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2296
[32m[2023-01-06 23:41:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1071/54685]	eta 7:09:02 lr 0.0009999999	 wd 0.1000	time 0.6438 (0.4801)	loss 0.6362 (0.6240)	loss-cls 10.1663 (9.9772)	loss-aux 0.0129 (0.0074)	grad_norm 0.7684 (1.8000)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4455
[32m[2023-01-06 23:41:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1087/54685]	eta 7:08:34 lr 0.0009999999	 wd 0.1000	time 0.5489 (0.4798)	loss 0.6150 (0.6240)	loss-cls 9.8240 (9.9772)	loss-aux 0.0158 (0.0075)	grad_norm 0.4294 (1.7798)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2763
[32m[2023-01-06 23:41:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1103/54685]	eta 7:08:04 lr 0.0009999999	 wd 0.1000	time 0.5778 (0.4794)	loss 0.6233 (0.6240)	loss-cls 9.9576 (9.9765)	loss-aux 0.0158 (0.0076)	grad_norm 0.4956 (1.7612)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2242
[32m[2023-01-06 23:42:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1119/54685]	eta 7:07:33 lr 0.0009999999	 wd 0.1000	time 0.5665 (0.4789)	loss 0.6245 (0.6239)	loss-cls 9.9740 (9.9754)	loss-aux 0.0173 (0.0078)	grad_norm 0.5527 (1.7439)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1828
[32m[2023-01-06 23:42:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1135/54685]	eta 7:07:04 lr 0.0009999999	 wd 0.1000	time 0.5319 (0.4785)	loss 0.6234 (0.6239)	loss-cls 9.9557 (9.9748)	loss-aux 0.0179 (0.0079)	grad_norm 0.5772 (1.7275)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1931
[32m[2023-01-06 23:42:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1151/54685]	eta 7:06:32 lr 0.0009999999	 wd 0.1000	time 0.5494 (0.4781)	loss 0.6157 (0.6238)	loss-cls 9.8337 (9.9735)	loss-aux 0.0181 (0.0080)	grad_norm 0.3505 (1.7084)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1509
[32m[2023-01-06 23:42:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1167/54685]	eta 7:06:02 lr 0.0009999999	 wd 0.1000	time 0.5540 (0.4776)	loss 0.6098 (0.6238)	loss-cls 9.7395 (9.9731)	loss-aux 0.0171 (0.0081)	grad_norm 0.3468 (1.6897)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1456
[32m[2023-01-06 23:42:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1183/54685]	eta 7:05:35 lr 0.0009999999	 wd 0.1000	time 0.5666 (0.4773)	loss 0.6157 (0.6238)	loss-cls 9.8355 (9.9722)	loss-aux 0.0163 (0.0082)	grad_norm 0.3667 (1.6718)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2189
[32m[2023-01-06 23:42:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1199/54685]	eta 7:05:02 lr 0.0009999999	 wd 0.1000	time 0.5409 (0.4768)	loss 0.6160 (0.6237)	loss-cls 9.8395 (9.9708)	loss-aux 0.0164 (0.0084)	grad_norm 0.2988 (1.6535)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0695
[32m[2023-01-06 23:42:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1215/54685]	eta 7:04:34 lr 0.0009999999	 wd 0.1000	time 0.5357 (0.4764)	loss 0.6186 (0.6237)	loss-cls 9.8812 (9.9700)	loss-aux 0.0157 (0.0085)	grad_norm 0.2975 (1.6357)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1733
[32m[2023-01-06 23:42:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1231/54685]	eta 7:04:08 lr 0.0009999999	 wd 0.1000	time 0.5495 (0.4761)	loss 0.6218 (0.6236)	loss-cls 9.9328 (9.9695)	loss-aux 0.0159 (0.0086)	grad_norm 0.4004 (1.6196)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1872
[32m[2023-01-06 23:42:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1247/54685]	eta 7:03:42 lr 0.0009999998	 wd 0.1000	time 0.5625 (0.4757)	loss 0.6146 (0.6236)	loss-cls 9.8169 (9.9687)	loss-aux 0.0165 (0.0087)	grad_norm 0.7233 (1.6082)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1827
[32m[2023-01-06 23:43:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1263/54685]	eta 7:03:11 lr 0.0009999998	 wd 0.1000	time 0.5313 (0.4753)	loss 0.6152 (0.6235)	loss-cls 9.8278 (9.9679)	loss-aux 0.0159 (0.0087)	grad_norm 0.3023 (1.5916)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0785
[32m[2023-01-06 23:43:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1279/54685]	eta 7:02:56 lr 0.0009999998	 wd 0.1000	time 0.5805 (0.4752)	loss 0.6133 (0.6235)	loss-cls 9.7965 (9.9673)	loss-aux 0.0157 (0.0088)	grad_norm 2.3714 (1.6014)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4154
[32m[2023-01-06 23:43:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1295/54685]	eta 7:02:31 lr 0.0009999998	 wd 0.1000	time 0.5462 (0.4748)	loss 0.6168 (0.6234)	loss-cls 9.8536 (9.9661)	loss-aux 0.0153 (0.0089)	grad_norm 0.3058 (1.5854)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1761
[32m[2023-01-06 23:43:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1311/54685]	eta 7:02:07 lr 0.0009999998	 wd 0.1000	time 0.5579 (0.4745)	loss 0.6119 (0.6234)	loss-cls 9.7757 (9.9652)	loss-aux 0.0152 (0.0090)	grad_norm 0.3253 (1.5700)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1916
[32m[2023-01-06 23:43:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1327/54685]	eta 7:01:45 lr 0.0009999998	 wd 0.1000	time 0.5713 (0.4743)	loss 0.6138 (0.6233)	loss-cls 9.8046 (9.9641)	loss-aux 0.0155 (0.0091)	grad_norm 0.3451 (1.5553)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2311
[32m[2023-01-06 23:43:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1343/54685]	eta 7:01:25 lr 0.0009999998	 wd 0.1000	time 0.5536 (0.4740)	loss 0.6138 (0.6233)	loss-cls 9.8051 (9.9631)	loss-aux 0.0150 (0.0091)	grad_norm 0.3016 (1.5403)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2930
[32m[2023-01-06 23:43:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1359/54685]	eta 7:01:03 lr 0.0009999998	 wd 0.1000	time 0.5421 (0.4737)	loss 0.6208 (0.6232)	loss-cls 9.9176 (9.9623)	loss-aux 0.0154 (0.0092)	grad_norm 0.3496 (1.5263)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1998
[32m[2023-01-06 23:43:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1375/54685]	eta 7:00:43 lr 0.0009999998	 wd 0.1000	time 0.5589 (0.4735)	loss 0.6132 (0.6231)	loss-cls 9.7959 (9.9610)	loss-aux 0.0154 (0.0093)	grad_norm 0.3263 (1.5124)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2637
[32m[2023-01-06 23:44:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1391/54685]	eta 7:00:24 lr 0.0009999998	 wd 0.1000	time 0.5599 (0.4733)	loss 0.6395 (0.6231)	loss-cls 10.2163 (9.9603)	loss-aux 0.0162 (0.0094)	grad_norm 0.3383 (1.4989)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2784
[32m[2023-01-06 23:44:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1407/54685]	eta 7:00:09 lr 0.0009999998	 wd 0.1000	time 0.5593 (0.4732)	loss 0.6269 (0.6231)	loss-cls 10.0159 (9.9598)	loss-aux 0.0148 (0.0094)	grad_norm 0.3598 (1.4859)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3714
[32m[2023-01-06 23:44:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1423/54685]	eta 6:59:46 lr 0.0009999998	 wd 0.1000	time 0.5332 (0.4729)	loss 0.6269 (0.6231)	loss-cls 10.0161 (9.9595)	loss-aux 0.0150 (0.0095)	grad_norm 0.4300 (1.4741)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1810
[32m[2023-01-06 23:44:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1439/54685]	eta 6:59:39 lr 0.0009999998	 wd 0.1000	time 0.5724 (0.4729)	loss 0.6204 (0.6230)	loss-cls 9.9123 (9.9592)	loss-aux 0.0139 (0.0095)	grad_norm 0.4293 (1.4625)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.5753
[32m[2023-01-06 23:44:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1455/54685]	eta 6:59:19 lr 0.0009999998	 wd 0.1000	time 0.5508 (0.4727)	loss 0.6197 (0.6230)	loss-cls 9.9019 (9.9585)	loss-aux 0.0135 (0.0096)	grad_norm 0.4131 (1.4509)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2239
[32m[2023-01-06 23:44:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1471/54685]	eta 6:59:00 lr 0.0009999998	 wd 0.1000	time 0.5539 (0.4724)	loss 0.6172 (0.6230)	loss-cls 9.8605 (9.9579)	loss-aux 0.0143 (0.0096)	grad_norm 0.4032 (1.4395)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2471
[32m[2023-01-06 23:44:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1487/54685]	eta 6:58:41 lr 0.0009999998	 wd 0.1000	time 0.5653 (0.4722)	loss 0.6223 (0.6229)	loss-cls 9.9416 (9.9574)	loss-aux 0.0148 (0.0097)	grad_norm 0.3794 (1.4281)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2395
[32m[2023-01-06 23:44:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1503/54685]	eta 6:58:22 lr 0.0009999998	 wd 0.1000	time 0.5507 (0.4720)	loss 0.6232 (0.6229)	loss-cls 9.9553 (9.9565)	loss-aux 0.0154 (0.0097)	grad_norm 0.3296 (1.4165)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2257
[32m[2023-01-06 23:45:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1519/54685]	eta 6:58:01 lr 0.0009999998	 wd 0.1000	time 0.5270 (0.4718)	loss 0.6281 (0.6228)	loss-cls 10.0348 (9.9557)	loss-aux 0.0154 (0.0098)	grad_norm 0.4591 (1.4064)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1766
[32m[2023-01-06 23:45:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1535/54685]	eta 6:57:39 lr 0.0009999998	 wd 0.1000	time 0.5464 (0.4715)	loss 0.6171 (0.6228)	loss-cls 9.8597 (9.9549)	loss-aux 0.0132 (0.0098)	grad_norm 0.4178 (1.3961)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1102
[32m[2023-01-06 23:45:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1551/54685]	eta 6:57:17 lr 0.0009999998	 wd 0.1000	time 0.5614 (0.4712)	loss 0.6163 (0.6228)	loss-cls 9.8464 (9.9544)	loss-aux 0.0144 (0.0099)	grad_norm 0.3572 (1.3854)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1419
[32m[2023-01-06 23:45:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1567/54685]	eta 6:56:55 lr 0.0009999998	 wd 0.1000	time 0.5387 (0.4709)	loss 0.5992 (0.6227)	loss-cls 9.5720 (9.9539)	loss-aux 0.0146 (0.0099)	grad_norm 0.2821 (1.3741)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1147
[32m[2023-01-06 23:45:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1583/54685]	eta 6:56:37 lr 0.0009999998	 wd 0.1000	time 0.5438 (0.4708)	loss 0.6199 (0.6227)	loss-cls 9.9033 (9.9534)	loss-aux 0.0148 (0.0100)	grad_norm 0.2910 (1.3632)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2271
[32m[2023-01-06 23:45:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1599/54685]	eta 6:56:21 lr 0.0009999997	 wd 0.1000	time 0.5653 (0.4706)	loss 0.6131 (0.6227)	loss-cls 9.7946 (9.9531)	loss-aux 0.0150 (0.0100)	grad_norm 0.2878 (1.3524)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2702
[32m[2023-01-06 23:45:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1615/54685]	eta 6:55:59 lr 0.0009999997	 wd 0.1000	time 0.5293 (0.4703)	loss 0.6192 (0.6227)	loss-cls 9.8924 (9.9525)	loss-aux 0.0151 (0.0101)	grad_norm 0.2554 (1.3416)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0827
[32m[2023-01-06 23:45:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1631/54685]	eta 6:55:38 lr 0.0009999997	 wd 0.1000	time 0.5529 (0.4701)	loss 0.6184 (0.6226)	loss-cls 9.8802 (9.9516)	loss-aux 0.0149 (0.0101)	grad_norm 0.8850 (1.3371)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1040
[32m[2023-01-06 23:45:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1647/54685]	eta 6:55:17 lr 0.0009999997	 wd 0.1000	time 0.5176 (0.4698)	loss 0.6166 (0.6226)	loss-cls 9.8513 (9.9509)	loss-aux 0.0142 (0.0102)	grad_norm 0.2268 (1.3263)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0937
[32m[2023-01-06 23:46:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1663/54685]	eta 6:54:59 lr 0.0009999997	 wd 0.1000	time 0.5590 (0.4696)	loss 0.6110 (0.6225)	loss-cls 9.7618 (9.9504)	loss-aux 0.0142 (0.0102)	grad_norm 0.2794 (1.3162)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2096
[32m[2023-01-06 23:46:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1679/54685]	eta 6:54:45 lr 0.0009999997	 wd 0.1000	time 0.6107 (0.4695)	loss 0.6167 (0.6225)	loss-cls 9.8530 (9.9495)	loss-aux 0.0141 (0.0102)	grad_norm 0.2274 (1.3059)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2940
[32m[2023-01-06 23:46:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1695/54685]	eta 6:54:22 lr 0.0009999997	 wd 0.1000	time 0.5485 (0.4692)	loss 0.6201 (0.6224)	loss-cls 9.9075 (9.9482)	loss-aux 0.0139 (0.0103)	grad_norm 0.2347 (1.2958)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0360
[32m[2023-01-06 23:46:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1711/54685]	eta 6:54:01 lr 0.0009999997	 wd 0.1000	time 0.5355 (0.4689)	loss 0.6127 (0.6224)	loss-cls 9.7891 (9.9477)	loss-aux 0.0140 (0.0103)	grad_norm 0.2573 (1.2861)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0427
[32m[2023-01-06 23:46:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1727/54685]	eta 6:53:42 lr 0.0009999997	 wd 0.1000	time 0.5250 (0.4687)	loss 0.6101 (0.6223)	loss-cls 9.7476 (9.9466)	loss-aux 0.0141 (0.0103)	grad_norm 0.2335 (1.2763)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1447
[32m[2023-01-06 23:46:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1743/54685]	eta 6:53:26 lr 0.0009999997	 wd 0.1000	time 0.5442 (0.4686)	loss 0.6162 (0.6223)	loss-cls 9.8460 (9.9464)	loss-aux 0.0135 (0.0104)	grad_norm 0.3260 (1.2676)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2035
[32m[2023-01-06 23:46:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1759/54685]	eta 6:53:11 lr 0.0009999997	 wd 0.1000	time 0.5425 (0.4684)	loss 0.6186 (0.6223)	loss-cls 9.8828 (9.9458)	loss-aux 0.0142 (0.0104)	grad_norm 0.8515 (1.2638)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2502
[32m[2023-01-06 23:46:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1775/54685]	eta 6:52:56 lr 0.0009999997	 wd 0.1000	time 0.5364 (0.4683)	loss 0.6074 (0.6222)	loss-cls 9.7031 (9.9450)	loss-aux 0.0146 (0.0104)	grad_norm 0.3487 (1.2556)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2640
[32m[2023-01-06 23:47:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1791/54685]	eta 6:52:40 lr 0.0009999997	 wd 0.1000	time 0.5418 (0.4681)	loss 0.6146 (0.6222)	loss-cls 9.8191 (9.9446)	loss-aux 0.0148 (0.0105)	grad_norm 0.2795 (1.2468)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1843
[32m[2023-01-06 23:47:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1807/54685]	eta 6:52:24 lr 0.0009999997	 wd 0.1000	time 0.5534 (0.4680)	loss 0.6155 (0.6222)	loss-cls 9.8334 (9.9441)	loss-aux 0.0148 (0.0105)	grad_norm 0.2660 (1.2382)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2100
[32m[2023-01-06 23:47:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1823/54685]	eta 6:52:06 lr 0.0009999997	 wd 0.1000	time 0.5398 (0.4678)	loss 0.6099 (0.6221)	loss-cls 9.7441 (9.9433)	loss-aux 0.0150 (0.0105)	grad_norm 0.3035 (1.2300)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1351
[32m[2023-01-06 23:47:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1839/54685]	eta 6:51:50 lr 0.0009999997	 wd 0.1000	time 0.5437 (0.4676)	loss 0.6185 (0.6221)	loss-cls 9.8806 (9.9426)	loss-aux 0.0150 (0.0106)	grad_norm 0.4731 (1.2234)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1646
[32m[2023-01-06 23:47:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1855/54685]	eta 6:51:35 lr 0.0009999997	 wd 0.1000	time 0.5486 (0.4674)	loss 0.6149 (0.6221)	loss-cls 9.8234 (9.9422)	loss-aux 0.0153 (0.0106)	grad_norm 0.2690 (1.2152)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2168
[32m[2023-01-06 23:47:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1871/54685]	eta 6:51:16 lr 0.0009999997	 wd 0.1000	time 0.5296 (0.4672)	loss 0.6123 (0.6220)	loss-cls 9.7811 (9.9417)	loss-aux 0.0152 (0.0106)	grad_norm 0.3242 (1.2075)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0814
[32m[2023-01-06 23:47:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1887/54685]	eta 6:50:59 lr 0.0009999996	 wd 0.1000	time 0.5388 (0.4671)	loss 0.6116 (0.6220)	loss-cls 9.7696 (9.9407)	loss-aux 0.0154 (0.0107)	grad_norm 0.2271 (1.1992)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1519
[32m[2023-01-06 23:47:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1903/54685]	eta 6:50:42 lr 0.0009999996	 wd 0.1000	time 0.5526 (0.4669)	loss 0.6172 (0.6219)	loss-cls 9.8612 (9.9398)	loss-aux 0.0147 (0.0107)	grad_norm 0.2294 (1.1911)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1139
[32m[2023-01-06 23:48:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1919/54685]	eta 6:50:26 lr 0.0009999996	 wd 0.1000	time 0.5592 (0.4667)	loss 0.6157 (0.6219)	loss-cls 9.8353 (9.9394)	loss-aux 0.0154 (0.0108)	grad_norm 0.6437 (1.1865)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1661
[32m[2023-01-06 23:48:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1935/54685]	eta 6:50:14 lr 0.0009999996	 wd 0.1000	time 0.5449 (0.4666)	loss 0.6060 (0.6218)	loss-cls 9.6811 (9.9388)	loss-aux 0.0148 (0.0108)	grad_norm 0.2213 (1.1785)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2797
[32m[2023-01-06 23:48:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1951/54685]	eta 6:49:59 lr 0.0009999996	 wd 0.1000	time 0.5667 (0.4665)	loss 0.6227 (0.6218)	loss-cls 9.9485 (9.9383)	loss-aux 0.0146 (0.0108)	grad_norm 0.2158 (1.1707)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2089
[32m[2023-01-06 23:48:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1967/54685]	eta 6:49:46 lr 0.0009999996	 wd 0.1000	time 0.5334 (0.4664)	loss 0.6141 (0.6218)	loss-cls 9.8113 (9.9376)	loss-aux 0.0136 (0.0109)	grad_norm 0.2305 (1.1630)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2593
[32m[2023-01-06 23:48:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1983/54685]	eta 6:49:30 lr 0.0009999996	 wd 0.1000	time 0.5477 (0.4662)	loss 0.6089 (0.6217)	loss-cls 9.7278 (9.9369)	loss-aux 0.0146 (0.0109)	grad_norm 0.2342 (1.1555)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1256
[32m[2023-01-06 23:48:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1999/54685]	eta 6:49:12 lr 0.0009999996	 wd 0.1000	time 0.5273 (0.4660)	loss 0.6152 (0.6217)	loss-cls 9.8285 (9.9364)	loss-aux 0.0139 (0.0109)	grad_norm 0.2272 (1.1481)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0677
[32m[2023-01-06 23:48:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2015/54685]	eta 6:48:54 lr 0.0009999996	 wd 0.1000	time 0.5382 (0.4658)	loss 0.6138 (0.6217)	loss-cls 9.8069 (9.9356)	loss-aux 0.0147 (0.0109)	grad_norm 0.2450 (1.1409)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0502
[32m[2023-01-06 23:48:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2031/54685]	eta 6:48:38 lr 0.0009999996	 wd 0.1000	time 0.5410 (0.4656)	loss 0.6075 (0.6216)	loss-cls 9.7059 (9.9346)	loss-aux 0.0146 (0.0110)	grad_norm 0.2197 (1.1337)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1095
[32m[2023-01-06 23:48:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2047/54685]	eta 6:48:20 lr 0.0009999996	 wd 0.1000	time 0.5397 (0.4654)	loss 0.6182 (0.6216)	loss-cls 9.8781 (9.9346)	loss-aux 0.0138 (0.0110)	grad_norm 0.2704 (1.1269)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0456
[32m[2023-01-06 23:49:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2063/54685]	eta 6:48:04 lr 0.0009999996	 wd 0.1000	time 0.5528 (0.4653)	loss 0.6239 (0.6216)	loss-cls 9.9681 (9.9340)	loss-aux 0.0139 (0.0110)	grad_norm 0.3552 (1.1209)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1253
[32m[2023-01-06 23:49:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2079/54685]	eta 6:47:51 lr 0.0009999996	 wd 0.1000	time 0.5665 (0.4652)	loss 0.6064 (0.6215)	loss-cls 9.6877 (9.9330)	loss-aux 0.0146 (0.0110)	grad_norm 0.2633 (1.1143)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.2013
[32m[2023-01-06 23:49:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2095/54685]	eta 6:47:40 lr 0.0009999996	 wd 0.1000	time 0.5458 (0.4651)	loss 0.6115 (0.6215)	loss-cls 9.7681 (9.9322)	loss-aux 0.0154 (0.0111)	grad_norm 0.2852 (1.1080)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3282
[32m[2023-01-06 23:49:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2111/54685]	eta 6:47:25 lr 0.0009999996	 wd 0.1000	time 0.5526 (0.4650)	loss 0.6049 (0.6214)	loss-cls 9.6629 (9.9315)	loss-aux 0.0155 (0.0111)	grad_norm 0.2884 (1.1018)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1256
[32m[2023-01-06 23:49:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2127/54685]	eta 6:47:09 lr 0.0009999995	 wd 0.1000	time 0.5042 (0.4648)	loss 0.6154 (0.6214)	loss-cls 9.8318 (9.9306)	loss-aux 0.0144 (0.0111)	grad_norm 0.2677 (1.0955)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1010
[32m[2023-01-06 23:49:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2143/54685]	eta 6:46:52 lr 0.0009999995	 wd 0.1000	time 0.5418 (0.4646)	loss 0.6198 (0.6214)	loss-cls 9.9027 (9.9305)	loss-aux 0.0140 (0.0111)	grad_norm 0.2881 (1.0895)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0417
[32m[2023-01-06 23:49:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2159/54685]	eta 6:46:37 lr 0.0009999995	 wd 0.1000	time 0.5458 (0.4645)	loss 0.6228 (0.6213)	loss-cls 9.9502 (9.9297)	loss-aux 0.0148 (0.0112)	grad_norm 0.4816 (1.0850)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1355
[32m[2023-01-06 23:49:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2175/54685]	eta 6:46:21 lr 0.0009999995	 wd 0.1000	time 0.5217 (0.4643)	loss 0.6234 (0.6213)	loss-cls 9.9603 (9.9293)	loss-aux 0.0145 (0.0112)	grad_norm 0.2738 (1.0790)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0580
[32m[2023-01-06 23:50:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2191/54685]	eta 6:46:07 lr 0.0009999995	 wd 0.1000	time 0.5714 (0.4642)	loss 0.6235 (0.6213)	loss-cls 9.9610 (9.9288)	loss-aux 0.0149 (0.0112)	grad_norm 0.2769 (1.0732)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1432
[32m[2023-01-06 23:50:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2207/54685]	eta 6:45:53 lr 0.0009999995	 wd 0.1000	time 0.5504 (0.4641)	loss 0.6098 (0.6212)	loss-cls 9.7415 (9.9282)	loss-aux 0.0153 (0.0112)	grad_norm 0.1979 (1.0668)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1619
[32m[2023-01-06 23:50:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2223/54685]	eta 6:45:35 lr 0.0009999995	 wd 0.1000	time 0.5425 (0.4639)	loss 0.6113 (0.6212)	loss-cls 9.7668 (9.9277)	loss-aux 0.0144 (0.0113)	grad_norm 0.2325 (1.0608)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9955
[32m[2023-01-06 23:50:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2239/54685]	eta 6:45:22 lr 0.0009999995	 wd 0.1000	time 0.5269 (0.4638)	loss 0.6237 (0.6212)	loss-cls 9.9646 (9.9273)	loss-aux 0.0145 (0.0113)	grad_norm 0.2383 (1.0550)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1883
[32m[2023-01-06 23:50:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2255/54685]	eta 6:45:08 lr 0.0009999995	 wd 0.1000	time 0.5683 (0.4636)	loss 0.6142 (0.6211)	loss-cls 9.8114 (9.9266)	loss-aux 0.0152 (0.0113)	grad_norm 0.6249 (1.0519)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1392
[32m[2023-01-06 23:50:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2271/54685]	eta 6:44:50 lr 0.0009999995	 wd 0.1000	time 0.5209 (0.4634)	loss 0.6083 (0.6211)	loss-cls 9.7186 (9.9259)	loss-aux 0.0144 (0.0113)	grad_norm 0.2271 (1.0461)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9519
[32m[2023-01-06 23:50:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2287/54685]	eta 6:44:34 lr 0.0009999995	 wd 0.1000	time 0.5497 (0.4633)	loss 0.6165 (0.6210)	loss-cls 9.8495 (9.9254)	loss-aux 0.0148 (0.0114)	grad_norm 0.5574 (1.0427)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0072
[32m[2023-01-06 23:50:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2303/54685]	eta 6:44:16 lr 0.0009999995	 wd 0.1000	time 0.5235 (0.4631)	loss 0.6074 (0.6210)	loss-cls 9.7033 (9.9245)	loss-aux 0.0146 (0.0114)	grad_norm 0.2394 (1.0371)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9594
[32m[2023-01-06 23:50:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2319/54685]	eta 6:44:01 lr 0.0009999995	 wd 0.1000	time 0.5421 (0.4629)	loss 0.6151 (0.6210)	loss-cls 9.8287 (9.9238)	loss-aux 0.0134 (0.0114)	grad_norm 0.2113 (1.0314)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0670
[32m[2023-01-06 23:51:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2335/54685]	eta 6:43:48 lr 0.0009999995	 wd 0.1000	time 0.5526 (0.4628)	loss 0.6136 (0.6209)	loss-cls 9.8031 (9.9228)	loss-aux 0.0139 (0.0114)	grad_norm 0.2215 (1.0259)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1622
[32m[2023-01-06 23:51:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2351/54685]	eta 6:43:31 lr 0.0009999994	 wd 0.1000	time 0.5418 (0.4626)	loss 0.6082 (0.6208)	loss-cls 9.7157 (9.9220)	loss-aux 0.0149 (0.0114)	grad_norm 0.2535 (1.0206)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9743
[32m[2023-01-06 23:51:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2367/54685]	eta 6:43:24 lr 0.0009999994	 wd 0.1000	time 0.5512 (0.4626)	loss 0.6255 (0.6208)	loss-cls 9.9925 (9.9216)	loss-aux 0.0148 (0.0115)	grad_norm 0.2758 (1.0156)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.4388
[32m[2023-01-06 23:51:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2383/54685]	eta 6:43:10 lr 0.0009999994	 wd 0.1000	time 0.5508 (0.4625)	loss 0.6098 (0.6208)	loss-cls 9.7421 (9.9211)	loss-aux 0.0148 (0.0115)	grad_norm 0.2393 (1.0104)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0806
[32m[2023-01-06 23:51:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2399/54685]	eta 6:42:56 lr 0.0009999994	 wd 0.1000	time 0.5583 (0.4624)	loss 0.6116 (0.6208)	loss-cls 9.7711 (9.9205)	loss-aux 0.0138 (0.0115)	grad_norm 0.2946 (1.0056)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1062
[32m[2023-01-06 23:51:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2415/54685]	eta 6:42:42 lr 0.0009999994	 wd 0.1000	time 0.5329 (0.4623)	loss 0.6252 (0.6207)	loss-cls 9.9890 (9.9198)	loss-aux 0.0143 (0.0115)	grad_norm 0.2661 (1.0007)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0828
[32m[2023-01-06 23:51:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2431/54685]	eta 6:42:26 lr 0.0009999994	 wd 0.1000	time 0.5372 (0.4621)	loss 0.6068 (0.6207)	loss-cls 9.6942 (9.9191)	loss-aux 0.0151 (0.0116)	grad_norm 0.2325 (0.9957)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9952
[32m[2023-01-06 23:51:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2447/54685]	eta 6:42:10 lr 0.0009999994	 wd 0.1000	time 0.5634 (0.4619)	loss 0.6159 (0.6206)	loss-cls 9.8397 (9.9186)	loss-aux 0.0145 (0.0116)	grad_norm 0.4114 (0.9918)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9817
[32m[2023-01-06 23:52:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2463/54685]	eta 6:41:55 lr 0.0009999994	 wd 0.1000	time 0.5486 (0.4618)	loss 0.6186 (0.6206)	loss-cls 9.8821 (9.9181)	loss-aux 0.0149 (0.0116)	grad_norm 1.4463 (0.9948)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0479
[32m[2023-01-06 23:52:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2479/54685]	eta 6:41:40 lr 0.0009999994	 wd 0.1000	time 0.5371 (0.4616)	loss 0.6216 (0.6206)	loss-cls 9.9318 (9.9180)	loss-aux 0.0133 (0.0116)	grad_norm 0.5788 (0.9921)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0165
[32m[2023-01-06 23:52:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2495/54685]	eta 6:41:27 lr 0.0009999994	 wd 0.1000	time 0.5880 (0.4615)	loss 0.6129 (0.6206)	loss-cls 9.7940 (9.9175)	loss-aux 0.0127 (0.0116)	grad_norm 0.4794 (0.9888)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1500
[32m[2023-01-06 23:52:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2511/54685]	eta 6:41:13 lr 0.0009999994	 wd 0.1000	time 0.5345 (0.4614)	loss 0.6137 (0.6205)	loss-cls 9.8077 (9.9169)	loss-aux 0.0109 (0.0116)	grad_norm 0.5039 (0.9857)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0702
[32m[2023-01-06 23:52:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2527/54685]	eta 6:41:00 lr 0.0009999994	 wd 0.1000	time 0.5761 (0.4613)	loss 0.6282 (0.6205)	loss-cls 10.0413 (9.9164)	loss-aux 0.0102 (0.0116)	grad_norm 3.0587 (0.9988)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0935
[32m[2023-01-06 23:52:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2543/54685]	eta 6:40:47 lr 0.0009999994	 wd 0.1000	time 0.5229 (0.4612)	loss 0.6220 (0.6205)	loss-cls 9.9352 (9.9164)	loss-aux 0.0164 (0.0116)	grad_norm 3.6989 (1.0158)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0787
[32m[2023-01-06 23:52:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2559/54685]	eta 6:40:35 lr 0.0009999993	 wd 0.1000	time 0.5452 (0.4611)	loss 0.6205 (0.6205)	loss-cls 9.9129 (9.9160)	loss-aux 0.0157 (0.0116)	grad_norm 0.4182 (1.0121)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1802
[32m[2023-01-06 23:52:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2575/54685]	eta 6:40:22 lr 0.0009999993	 wd 0.1000	time 0.5518 (0.4610)	loss 0.6208 (0.6205)	loss-cls 9.9193 (9.9156)	loss-aux 0.0138 (0.0117)	grad_norm 1.5599 (1.0155)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0940
[32m[2023-01-06 23:52:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2591/54685]	eta 6:40:10 lr 0.0009999993	 wd 0.1000	time 0.5236 (0.4609)	loss 0.6101 (0.6204)	loss-cls 9.7469 (9.9151)	loss-aux 0.0150 (0.0117)	grad_norm 0.8799 (1.0147)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1318
[32m[2023-01-06 23:53:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2607/54685]	eta 6:39:58 lr 0.0009999993	 wd 0.1000	time 0.5227 (0.4608)	loss 0.6144 (0.6204)	loss-cls 9.8183 (9.9149)	loss-aux 0.0117 (0.0117)	grad_norm 0.7104 (1.0128)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1529
[32m[2023-01-06 23:53:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2623/54685]	eta 6:39:47 lr 0.0009999993	 wd 0.1000	time 0.5459 (0.4607)	loss 0.6054 (0.6204)	loss-cls 9.6738 (9.9142)	loss-aux 0.0120 (0.0117)	grad_norm 4.9496 (1.0368)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1710
[32m[2023-01-06 23:53:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2639/54685]	eta 6:39:39 lr 0.0009999993	 wd 0.1000	time 0.5277 (0.4607)	loss 0.6108 (0.6203)	loss-cls 9.7598 (9.9136)	loss-aux 0.0130 (0.0117)	grad_norm 0.8202 (1.0355)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.3770
[32m[2023-01-06 23:53:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2655/54685]	eta 6:39:26 lr 0.0009999993	 wd 0.1000	time 0.5406 (0.4606)	loss 0.6225 (0.6203)	loss-cls 9.9479 (9.9130)	loss-aux 0.0124 (0.0117)	grad_norm 0.5854 (1.0328)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0607
[32m[2023-01-06 23:53:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2671/54685]	eta 6:39:12 lr 0.0009999993	 wd 0.1000	time 0.5348 (0.4605)	loss 0.6226 (0.6203)	loss-cls 9.9503 (9.9124)	loss-aux 0.0120 (0.0117)	grad_norm 0.4721 (1.0294)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0587
[32m[2023-01-06 23:53:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2687/54685]	eta 6:39:00 lr 0.0009999993	 wd 0.1000	time 0.5361 (0.4604)	loss 0.6189 (0.6202)	loss-cls 9.8902 (9.9123)	loss-aux 0.0120 (0.0117)	grad_norm 0.2804 (1.0250)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1237
[32m[2023-01-06 23:53:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2703/54685]	eta 6:38:46 lr 0.0009999993	 wd 0.1000	time 0.5456 (0.4603)	loss 0.6077 (0.6202)	loss-cls 9.7101 (9.9119)	loss-aux 0.0124 (0.0117)	grad_norm 0.9459 (1.0245)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0164
[32m[2023-01-06 23:53:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2719/54685]	eta 6:38:31 lr 0.0009999993	 wd 0.1000	time 0.5586 (0.4601)	loss 0.6191 (0.6202)	loss-cls 9.8916 (9.9115)	loss-aux 0.0134 (0.0117)	grad_norm 0.2891 (1.0202)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9628
[32m[2023-01-06 23:54:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2735/54685]	eta 6:38:17 lr 0.0009999993	 wd 0.1000	time 0.5378 (0.4600)	loss 0.6135 (0.6202)	loss-cls 9.8023 (9.9109)	loss-aux 0.0141 (0.0117)	grad_norm 2.9143 (1.0312)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9843
[32m[2023-01-06 23:54:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2751/54685]	eta 6:38:02 lr 0.0009999992	 wd 0.1000	time 0.5361 (0.4599)	loss 0.6176 (0.6201)	loss-cls 9.8665 (9.9106)	loss-aux 0.0147 (0.0117)	grad_norm 0.2201 (1.0265)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9854
[32m[2023-01-06 23:54:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2767/54685]	eta 6:37:47 lr 0.0009999992	 wd 0.1000	time 0.5702 (0.4597)	loss 0.6075 (0.6201)	loss-cls 9.7068 (9.9100)	loss-aux 0.0137 (0.0118)	grad_norm 0.3340 (1.0225)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9474
[32m[2023-01-06 23:54:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2783/54685]	eta 6:37:33 lr 0.0009999992	 wd 0.1000	time 0.5337 (0.4596)	loss 0.6224 (0.6201)	loss-cls 9.9441 (9.9093)	loss-aux 0.0141 (0.0118)	grad_norm 0.2236 (1.0179)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0118
[32m[2023-01-06 23:54:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2799/54685]	eta 6:37:18 lr 0.0009999992	 wd 0.1000	time 0.5456 (0.4594)	loss 0.6155 (0.6200)	loss-cls 9.8348 (9.9086)	loss-aux 0.0131 (0.0118)	grad_norm 0.1994 (1.0133)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9368
[32m[2023-01-06 23:54:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2815/54685]	eta 6:37:02 lr 0.0009999992	 wd 0.1000	time 0.5336 (0.4593)	loss 0.6010 (0.6200)	loss-cls 9.6017 (9.9081)	loss-aux 0.0136 (0.0118)	grad_norm 0.2124 (1.0087)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.8417
[32m[2023-01-06 23:54:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2831/54685]	eta 6:36:47 lr 0.0009999992	 wd 0.1000	time 0.5437 (0.4591)	loss 0.6114 (0.6200)	loss-cls 9.7694 (9.9080)	loss-aux 0.0137 (0.0118)	grad_norm 0.2564 (1.0045)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9783
[32m[2023-01-06 23:54:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2847/54685]	eta 6:36:35 lr 0.0009999992	 wd 0.1000	time 0.5393 (0.4590)	loss 0.6236 (0.6200)	loss-cls 9.9628 (9.9076)	loss-aux 0.0141 (0.0118)	grad_norm 0.4027 (1.0011)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0434
[32m[2023-01-06 23:54:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2863/54685]	eta 6:36:20 lr 0.0009999992	 wd 0.1000	time 0.5456 (0.4589)	loss 0.6171 (0.6199)	loss-cls 9.8591 (9.9072)	loss-aux 0.0146 (0.0118)	grad_norm 0.2094 (0.9966)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9660
[32m[2023-01-06 23:55:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2879/54685]	eta 6:36:09 lr 0.0009999992	 wd 0.1000	time 0.5235 (0.4588)	loss 0.6195 (0.6199)	loss-cls 9.8964 (9.9068)	loss-aux 0.0148 (0.0118)	grad_norm 0.2104 (0.9923)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.1388
[32m[2023-01-06 23:55:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2895/54685]	eta 6:35:55 lr 0.0009999992	 wd 0.1000	time 0.5267 (0.4587)	loss 0.6206 (0.6199)	loss-cls 9.9151 (9.9063)	loss-aux 0.0149 (0.0119)	grad_norm 0.2333 (0.9881)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9627
[32m[2023-01-06 23:55:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2911/54685]	eta 6:35:41 lr 0.0009999992	 wd 0.1000	time 0.5193 (0.4586)	loss 0.6096 (0.6199)	loss-cls 9.7393 (9.9060)	loss-aux 0.0136 (0.0119)	grad_norm 0.2107 (0.9838)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9747
[32m[2023-01-06 23:55:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2927/54685]	eta 6:35:27 lr 0.0009999991	 wd 0.1000	time 0.5358 (0.4584)	loss 0.6150 (0.6198)	loss-cls 9.8250 (9.9055)	loss-aux 0.0152 (0.0119)	grad_norm 0.2193 (0.9796)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9627
[32m[2023-01-06 23:55:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2943/54685]	eta 6:35:12 lr 0.0009999991	 wd 0.1000	time 0.5086 (0.4583)	loss 0.6194 (0.6198)	loss-cls 9.8961 (9.9052)	loss-aux 0.0148 (0.0119)	grad_norm 0.2237 (0.9755)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.8568
[32m[2023-01-06 23:55:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2959/54685]	eta 6:34:56 lr 0.0009999991	 wd 0.1000	time 0.5325 (0.4581)	loss 0.6142 (0.6198)	loss-cls 9.8131 (9.9047)	loss-aux 0.0147 (0.0119)	grad_norm 0.2300 (0.9715)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.8577
[32m[2023-01-06 23:55:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2975/54685]	eta 6:34:40 lr 0.0009999991	 wd 0.1000	time 0.5431 (0.4580)	loss 0.6206 (0.6198)	loss-cls 9.9148 (9.9045)	loss-aux 0.0147 (0.0119)	grad_norm 0.2284 (0.9675)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.8350
[32m[2023-01-06 23:55:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2991/54685]	eta 6:34:26 lr 0.0009999991	 wd 0.1000	time 0.5460 (0.4578)	loss 0.6144 (0.6198)	loss-cls 9.8153 (9.9041)	loss-aux 0.0147 (0.0120)	grad_norm 0.1992 (0.9634)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9120
[32m[2023-01-06 23:56:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3007/54685]	eta 6:34:10 lr 0.0009999991	 wd 0.1000	time 0.5176 (0.4577)	loss 0.6062 (0.6197)	loss-cls 9.6851 (9.9037)	loss-aux 0.0146 (0.0120)	grad_norm 0.2248 (0.9595)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.8626
[32m[2023-01-06 23:56:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3023/54685]	eta 6:33:53 lr 0.0009999991	 wd 0.1000	time 0.5194 (0.4575)	loss 0.6244 (0.6197)	loss-cls 9.9770 (9.9033)	loss-aux 0.0141 (0.0120)	grad_norm 0.2203 (0.9556)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.7324
[32m[2023-01-06 23:56:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3039/54685]	eta 6:33:39 lr 0.0009999991	 wd 0.1000	time 0.5279 (0.4573)	loss 0.6234 (0.6197)	loss-cls 9.9601 (9.9029)	loss-aux 0.0146 (0.0120)	grad_norm 0.2552 (0.9519)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9037
[32m[2023-01-06 23:56:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3055/54685]	eta 6:33:23 lr 0.0009999991	 wd 0.1000	time 0.5299 (0.4572)	loss 0.6032 (0.6196)	loss-cls 9.6358 (9.9024)	loss-aux 0.0148 (0.0120)	grad_norm 0.2060 (0.9480)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.8150
[32m[2023-01-06 23:56:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3071/54685]	eta 6:33:10 lr 0.0009999991	 wd 0.1000	time 0.5244 (0.4571)	loss 0.5995 (0.6196)	loss-cls 9.5774 (9.9019)	loss-aux 0.0151 (0.0120)	grad_norm 0.2158 (0.9442)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0164
[32m[2023-01-06 23:56:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3087/54685]	eta 6:32:55 lr 0.0009999990	 wd 0.1000	time 0.5216 (0.4569)	loss 0.6101 (0.6196)	loss-cls 9.7468 (9.9018)	loss-aux 0.0145 (0.0120)	grad_norm 0.2372 (0.9405)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.8272
[32m[2023-01-06 23:56:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3103/54685]	eta 6:32:42 lr 0.0009999990	 wd 0.1000	time 0.5261 (0.4568)	loss 0.6056 (0.6196)	loss-cls 9.6749 (9.9011)	loss-aux 0.0148 (0.0120)	grad_norm 0.2183 (0.9368)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9565
[32m[2023-01-06 23:56:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3119/54685]	eta 6:32:25 lr 0.0009999990	 wd 0.1000	time 0.5422 (0.4566)	loss 0.6138 (0.6195)	loss-cls 9.8066 (9.9006)	loss-aux 0.0149 (0.0121)	grad_norm 0.2148 (0.9331)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.7614
[32m[2023-01-06 23:56:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3135/54685]	eta 6:32:09 lr 0.0009999990	 wd 0.1000	time 0.5010 (0.4564)	loss 0.6114 (0.6195)	loss-cls 9.7667 (9.9002)	loss-aux 0.0150 (0.0121)	grad_norm 0.1906 (0.9293)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.7646
[32m[2023-01-06 23:57:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3151/54685]	eta 6:31:58 lr 0.0009999990	 wd 0.1000	time 0.5269 (0.4564)	loss 0.6201 (0.6195)	loss-cls 9.9075 (9.9000)	loss-aux 0.0145 (0.0121)	grad_norm 0.2031 (0.9256)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0295
[32m[2023-01-06 23:57:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3167/54685]	eta 6:31:43 lr 0.0009999990	 wd 0.1000	time 0.5441 (0.4562)	loss 0.6320 (0.6195)	loss-cls 10.0977 (9.8997)	loss-aux 0.0144 (0.0121)	grad_norm 0.2158 (0.9220)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.8275
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13261.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12231.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9401.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17877.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n02368116/n02368116_318.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_11746.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8806.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24544.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24434.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12740.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12142.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_14701.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_10353.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18729.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_1914.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29025.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27221.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6850.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6669.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7365.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11816.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13641.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9031.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27412.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33623.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18350.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_135.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27317.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9819.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9068.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8783.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15697.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13396.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12206.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13871.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25530.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20118.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_16430.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_7026.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_9566.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27296.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9282.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14020.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19281.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_26924.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_4539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8645.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30926.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25717.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33259.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27578.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14718.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10175248/n10175248_583.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7728.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_34297.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24638.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12108.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_23316.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12364.JPEG
[32m[2023-01-06 23:57:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3183/54685]	eta 6:31:29 lr 0.0009999990	 wd 0.1000	time 0.5467 (0.4561)	loss 0.6125 (0.6195)	loss-cls 9.7861 (9.8993)	loss-aux 0.0147 (0.0121)	grad_norm 0.1985 (0.9184)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 6.9336
[32m[2023-01-06 23:57:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3199/54685]	eta 6:31:18 lr 0.0009999990	 wd 0.1000	time 0.5312 (0.4560)	loss 0.6028 (0.6194)	loss-cls 9.6307 (9.8989)	loss-aux 0.0149 (0.0121)	grad_norm 0.2087 (0.9148)	loss_scale 65536.0000 (65536.0000)	mem 10863MB	batch_time 7.0583
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n06470073/n06470073_47249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25750.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15830.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13103.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9041.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2040.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6236.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2322.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_15341.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27611.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22980.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28288.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29695.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11827.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9215.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7996.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6567.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_16320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12757.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7072.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28595.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7411.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22698.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19005.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30043.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22995.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7953.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4524.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15811.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_3493.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33630.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15455.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13306.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8873.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_32625.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13419.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12064.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22581.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25408.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11766.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2852.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15480.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4456.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5664.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_16737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22719.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8812.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6710.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10242.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_21756.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10586.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_1796.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6247.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22166.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03957420/n03957420_33553.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10675.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17471.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5051.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7013.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13244.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8925.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27627.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13516.JPEG
[32m[2023-01-06 23:57:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 282)[0m: INFO EPOCH 0 training takes 0:24:20
