+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102393
++ grep BatchHost
++ scontrol show JobId=102393
++ scontrol show JobId=102393
++ grep BatchHost
++ scontrol show JobId=102393
++ grep BatchHost
++ scontrol show JobId=102393
++ scontrol show JobId=102393
++ scontrol show JobId=102393
++ scontrol show JobId=102393
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102393
++ scontrol show JobId=102393
++ grep BatchHost
++ scontrol show JobId=102393
++ scontrol show JobId=102393
++ scontrol show JobId=102393
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102393
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=102393
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=102393
++ grep BatchHost
++ awk '{print $2}'
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export NNODES=2
+ NNODES=2
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NNODES=2
+ NNODES=2
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ export NNODES=2
+ NNODES=2
+ TRAIN_SAMPLES=20520960
+ export NODE_RANK=2
+ NODE_RANK=2
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ '[' nico == nico ']'
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ MASTER_ADDR=nico1
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export NNODES=2
+ NNODES=2
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export NODE_RANK=0
+ NODE_RANK=0
+ false
+ true
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ EXEC=./main_moe.py
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ false
+ true
+ EXEC=./main_moe.py
+ '[' EP+DP == EP+DP ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ false
+ true
+ EXEC=./main_moe.py
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ python_args+='
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ export NNODES=2
+ NNODES=2
+ false
+ true
+ export NODE_RANK=1
+ NODE_RANK=1
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ USE_MEGATRON=0
+ cd /home/zms/model_training/MoE/FastSwin
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 8
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep8_dp2_totalE16_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T00:19:18+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 8 --expert-dp-size 2
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 0/16
[32m[2023-01-07 00:19:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 403)[0m: INFO Full config saved to /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default/config.json
[32m[2023-01-07 00:19:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 406)[0m: INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: part
  DATASET: imagenet22K
  DATA_PATH: /mnt/znvme/dataset/imagenet22k
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 4.8
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    GATE_NOISE: 1.0
    INIT_STD: 0.005
    IN_CHANS: 3
    IS_GSHARD_LOSS: true
    MLP_FC2_BIAS: false
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - 1
      - 3
      - 5
      - 7
      - 9
      - 11
      - 13
      - 15
      - 17
    - - 1
    MOE_DROP: 0.1
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 2
    USE_BPR: true
    WINDOW_SIZE: 12
  TYPE: swin_fastmoe
OUTPUT: /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default
PRINT_FREQ: 1
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 16
  AUTO_RESUME: true
  BASE_LR: 0.001
  CHECKPOINT_MODE: full
  CLIP_GRAD: 3.0
  EPOCHS: 90
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.1

[32m[2023-01-07 00:19:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 407)[0m: INFO {"cfg": "configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml", "opts": null, "batch_size": 16, "data_path": "/mnt/znvme/dataset/imagenet22k", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": 16, "use_checkpoint": false, "checkpoint_mode": "full", "disable_amp": false, "amp_opt_level": null, "output": "/home/zms/model_training/Auto-Megatron/logs/swin_naive", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": true, "fused_layernorm": false, "optim": null, "num_experts": 2, "top_k": 2, "balance_strategy": "naive", "expert_parallel_strategy": "EP+DP", "expert_ep_size": 8, "expert_dp_size": 2, "dump": false, "dynamic_placement": false, "dynamic_freq": 10, "new_shadow": false, "gshard_cap": 4.8, "init_method_std": 0.002, "num_layers": 12}
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
local rank 0 / global rank 0 successfully build train dataset
local rank 0 / global rank 0 successfully build val dataset
[32m[2023-01-07 00:19:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 120)[0m: INFO Creating model:swin_fastmoe/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 0 in DP group [0, 8]
[32m[2023-01-07 00:20:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 122)[0m: INFO SwinTransformerFastMoE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(48, 48), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(48, 48), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(24, 24), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(24, 24), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(12, 12), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(12, 12), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(6, 6), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=4096, out_features=1024, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 1024, hidden_features = 4096, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=4096, out_features=16, bias=True)
              )
            )
          )
        )
      )
    )
  )
  (norm): TimerModule(
    (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (avgpool): TimerModule(
    (model): AdaptiveAvgPool1d(output_size=1)
  )
  (head): TimerModule(
    (model): Linear(in_features=1024, out_features=21841, bias=True)
  )
)
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 4/16
local rank 0 / global rank 4 successfully build train dataset
local rank 0 / global rank 4 successfully build val dataset
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in DP group [4, 12]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-07 00:20:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 156)[0m: INFO no checkpoint found in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default, ignoring auto resume
[32m[2023-01-07 00:20:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 176)[0m: INFO Start training
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 14/16
local rank 0 / global rank 14 successfully build train dataset
local rank 0 / global rank 14 successfully build val dataset
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [6, 14]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 7/16
local rank 0 / global rank 7 successfully build train dataset
local rank 0 / global rank 7 successfully build val dataset
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in DP group [7, 15]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 3/16
local rank 0 / global rank 3 successfully build train dataset
local rank 0 / global rank 3 successfully build val dataset
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in DP group [3, 11]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 5/16
local rank 0 / global rank 5 successfully build train dataset
local rank 0 / global rank 5 successfully build val dataset
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in DP group [5, 13]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 2/16
local rank 0 / global rank 2 successfully build train dataset
local rank 0 / global rank 2 successfully build val dataset
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in DP group [2, 10]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 1/16
local rank 0 / global rank 1 successfully build train dataset
local rank 0 / global rank 1 successfully build val dataset
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in DP group [1, 9]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 8/16
local rank 0 / global rank 8 successfully build train dataset
local rank 0 / global rank 8 successfully build val dataset
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [0, 8]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 6/16
local rank 0 / global rank 6 successfully build train dataset
local rank 0 / global rank 6 successfully build val dataset
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in DP group [6, 14]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 15/16
local rank 0 / global rank 15 successfully build train dataset
local rank 0 / global rank 15 successfully build val dataset
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [7, 15]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 13/16
local rank 0 / global rank 13 successfully build train dataset
local rank 0 / global rank 13 successfully build val dataset
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [5, 13]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 9/16
local rank 0 / global rank 9 successfully build train dataset
local rank 0 / global rank 9 successfully build val dataset
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [1, 9]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 12/16
local rank 0 / global rank 12 successfully build train dataset
local rank 0 / global rank 12 successfully build val dataset
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [4, 12]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 10/16
local rank 0 / global rank 10 successfully build train dataset
local rank 0 / global rank 10 successfully build val dataset
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [2, 10]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 11/16
local rank 0 / global rank 11 successfully build train dataset
local rank 0 / global rank 11 successfully build val dataset
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [3, 11]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-07 00:20:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][15/54685]	eta 13:52:11 lr 0.0010000000	 wd 0.1000	time 0.8862 (0.9133)	loss 0.6243 (0.6246)	loss-cls 9.9893 (9.9942)	loss-aux 0.0000 (0.0000)	grad_norm 2.1953 (2.1953)	loss_scale 65536.0000 (65536.0000)	mem 8335MB	batch_time 14.6133
[32m[2023-01-07 00:20:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][31/54685]	eta 10:50:04 lr 0.0010000000	 wd 0.1000	time 0.6634 (0.7137)	loss 0.6246 (0.6247)	loss-cls 9.9939 (9.9956)	loss-aux 0.0000 (0.0000)	grad_norm 1.6382 (1.9167)	loss_scale 65536.0000 (65536.0000)	mem 10889MB	batch_time 8.2238
[32m[2023-01-07 00:20:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][47/54685]	eta 9:50:50 lr 0.0010000000	 wd 0.1000	time 0.6287 (0.6488)	loss 0.6247 (0.6252)	loss-cls 9.9956 (10.0030)	loss-aux 0.0000 (0.0000)	grad_norm 1.4398 (1.7578)	loss_scale 65536.0000 (65536.0000)	mem 10889MB	batch_time 8.3063
[32m[2023-01-07 00:20:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][63/54685]	eta 9:27:47 lr 0.0010000000	 wd 0.1000	time 0.7045 (0.6237)	loss 0.6253 (0.6253)	loss-cls 10.0045 (10.0042)	loss-aux 0.0000 (0.0000)	grad_norm 3.3908 (2.1660)	loss_scale 65536.0000 (65536.0000)	mem 10889MB	batch_time 8.7737
[32m[2023-01-07 00:21:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][79/54685]	eta 9:13:41 lr 0.0010000000	 wd 0.1000	time 0.6354 (0.6084)	loss 0.6217 (0.6251)	loss-cls 9.9465 (10.0015)	loss-aux 0.0000 (0.0000)	grad_norm 1.4950 (2.0318)	loss_scale 65536.0000 (65536.0000)	mem 10889MB	batch_time 8.7531
[32m[2023-01-07 00:21:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][95/54685]	eta 9:05:01 lr 0.0010000000	 wd 0.1000	time 0.6491 (0.5990)	loss 0.6225 (0.6249)	loss-cls 9.9601 (9.9991)	loss-aux 0.0000 (0.0000)	grad_norm 1.3682 (1.9212)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.8381
[32m[2023-01-07 00:21:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][111/54685]	eta 9:00:47 lr 0.0010000000	 wd 0.1000	time 0.6567 (0.5946)	loss 0.6257 (0.6248)	loss-cls 10.0110 (9.9972)	loss-aux 0.0000 (0.0000)	grad_norm 1.2955 (1.8318)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.0821
[32m[2023-01-07 00:21:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][127/54685]	eta 8:57:41 lr 0.0010000000	 wd 0.1000	time 0.6725 (0.5913)	loss 0.6251 (0.6248)	loss-cls 10.0015 (9.9968)	loss-aux 0.0000 (0.0000)	grad_norm 1.2366 (1.7574)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.0992
[32m[2023-01-07 00:21:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][143/54685]	eta 8:56:31 lr 0.0010000000	 wd 0.1000	time 0.6589 (0.5902)	loss 0.6268 (0.6249)	loss-cls 10.0286 (9.9977)	loss-aux 0.0000 (0.0000)	grad_norm 1.2934 (1.7059)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3025
[32m[2023-01-07 00:21:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][159/54685]	eta 8:55:38 lr 0.0010000000	 wd 0.1000	time 0.6820 (0.5894)	loss 0.6237 (0.6249)	loss-cls 9.9797 (9.9978)	loss-aux 0.0000 (0.0000)	grad_norm 1.2715 (1.6624)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3153
[32m[2023-01-07 00:21:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][175/54685]	eta 8:53:40 lr 0.0010000000	 wd 0.1000	time 0.6867 (0.5874)	loss 0.6298 (0.6249)	loss-cls 10.0768 (9.9982)	loss-aux 0.0000 (0.0000)	grad_norm 1.2342 (1.6235)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.0780
[32m[2023-01-07 00:22:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][191/54685]	eta 8:50:10 lr 0.0010000000	 wd 0.1000	time 0.6514 (0.5837)	loss 0.6239 (0.6247)	loss-cls 9.9821 (9.9946)	loss-aux 0.0000 (0.0000)	grad_norm 1.1664 (1.5854)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.6928
[32m[2023-01-07 00:22:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][207/54685]	eta 8:46:42 lr 0.0010000000	 wd 0.1000	time 0.6465 (0.5801)	loss 0.6233 (0.6245)	loss-cls 9.9724 (9.9921)	loss-aux 0.0000 (0.0000)	grad_norm 1.1304 (1.5504)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.5808
[32m[2023-01-07 00:22:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][223/54685]	eta 8:44:06 lr 0.0010000000	 wd 0.1000	time 0.6709 (0.5774)	loss 0.6246 (0.6247)	loss-cls 9.9943 (9.9954)	loss-aux 0.0000 (0.0000)	grad_norm 1.1421 (1.5212)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.6813
[32m[2023-01-07 00:22:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][239/54685]	eta 8:42:11 lr 0.0010000000	 wd 0.1000	time 0.6558 (0.5755)	loss 0.6304 (0.6248)	loss-cls 10.0862 (9.9964)	loss-aux 0.0000 (0.0000)	grad_norm 1.1727 (1.4980)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.7719
[32m[2023-01-07 00:22:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][255/54685]	eta 8:40:49 lr 0.0010000000	 wd 0.1000	time 0.6481 (0.5741)	loss 0.6278 (0.6248)	loss-cls 10.0441 (9.9972)	loss-aux 0.0000 (0.0000)	grad_norm 1.1284 (1.4749)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.8631
[32m[2023-01-07 00:22:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][271/54685]	eta 8:39:25 lr 0.0010000000	 wd 0.1000	time 0.6457 (0.5727)	loss 0.6282 (0.6248)	loss-cls 10.0520 (9.9969)	loss-aux 0.0000 (0.0000)	grad_norm 1.2123 (1.4595)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.8119
[32m[2023-01-07 00:22:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][287/54685]	eta 8:38:00 lr 0.0010000000	 wd 0.1000	time 0.6768 (0.5714)	loss 0.6310 (0.6250)	loss-cls 10.0959 (9.9994)	loss-aux 0.0000 (0.0000)	grad_norm 1.3732 (1.4547)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.7620
[32m[2023-01-07 00:23:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][303/54685]	eta 8:36:44 lr 0.0010000000	 wd 0.1000	time 0.6754 (0.5701)	loss 0.6263 (0.6249)	loss-cls 10.0208 (9.9985)	loss-aux 0.0000 (0.0000)	grad_norm 1.2477 (1.4438)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.7689
[32m[2023-01-07 00:23:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][319/54685]	eta 8:35:43 lr 0.0010000000	 wd 0.1000	time 0.6446 (0.5692)	loss 0.6345 (0.6249)	loss-cls 10.1516 (9.9980)	loss-aux 0.0000 (0.0000)	grad_norm 1.3588 (1.4395)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.8138
[32m[2023-01-07 00:23:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][335/54685]	eta 8:34:35 lr 0.0010000000	 wd 0.1000	time 0.6409 (0.5681)	loss 0.6285 (0.6248)	loss-cls 10.0554 (9.9974)	loss-aux 0.0000 (0.0000)	grad_norm 1.3306 (1.4343)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.7481
[32m[2023-01-07 00:23:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][351/54685]	eta 8:33:36 lr 0.0010000000	 wd 0.1000	time 0.6750 (0.5672)	loss 0.6241 (0.6249)	loss-cls 9.9859 (9.9977)	loss-aux 0.0000 (0.0000)	grad_norm 1.3171 (1.4290)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.7620
[32m[2023-01-07 00:23:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][367/54685]	eta 8:32:29 lr 0.0010000000	 wd 0.1000	time 0.6694 (0.5661)	loss 0.6279 (0.6249)	loss-cls 10.0469 (9.9989)	loss-aux 0.0000 (0.0000)	grad_norm 1.1907 (1.4186)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.6803
[32m[2023-01-07 00:23:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][383/54685]	eta 8:31:18 lr 0.0010000000	 wd 0.1000	time 0.6564 (0.5650)	loss 0.6337 (0.6249)	loss-cls 10.1398 (9.9989)	loss-aux 0.0000 (0.0000)	grad_norm 1.2996 (1.4137)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.6207
[32m[2023-01-07 00:24:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][399/54685]	eta 8:30:41 lr 0.0010000000	 wd 0.1000	time 0.6771 (0.5644)	loss 0.6218 (0.6249)	loss-cls 9.9487 (9.9989)	loss-aux 0.0000 (0.0000)	grad_norm 1.2452 (1.4069)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.8320
[32m[2023-01-07 00:24:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][415/54685]	eta 8:29:49 lr 0.0010000000	 wd 0.1000	time 0.6442 (0.5637)	loss 0.6190 (0.6250)	loss-cls 9.9047 (9.9999)	loss-aux 0.0000 (0.0000)	grad_norm 1.2206 (1.3998)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.7042
[32m[2023-01-07 00:24:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][431/54685]	eta 8:29:27 lr 0.0010000000	 wd 0.1000	time 0.6273 (0.5634)	loss 0.6234 (0.6251)	loss-cls 9.9746 (10.0008)	loss-aux 0.0000 (0.0000)	grad_norm 1.2292 (1.3935)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.9167
[32m[2023-01-07 00:24:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][447/54685]	eta 8:29:30 lr 0.0010000000	 wd 0.1000	time 0.6251 (0.5636)	loss 0.6343 (0.6251)	loss-cls 10.1490 (10.0010)	loss-aux 0.0000 (0.0000)	grad_norm 1.2197 (1.3873)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.1173
[32m[2023-01-07 00:24:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][463/54685]	eta 8:29:16 lr 0.0010000000	 wd 0.1000	time 0.6583 (0.5636)	loss 0.6266 (0.6252)	loss-cls 10.0250 (10.0027)	loss-aux 0.0000 (0.0000)	grad_norm 1.2723 (1.3833)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.9744
[32m[2023-01-07 00:24:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][479/54685]	eta 8:29:24 lr 0.0010000000	 wd 0.1000	time 0.6739 (0.5639)	loss 0.6026 (0.6252)	loss-cls 9.6414 (10.0025)	loss-aux 0.0000 (0.0000)	grad_norm 1.2003 (1.3772)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.1660
[32m[2023-01-07 00:24:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][495/54685]	eta 8:29:12 lr 0.0010000000	 wd 0.1000	time 0.6801 (0.5638)	loss 0.6318 (0.6251)	loss-cls 10.1091 (10.0020)	loss-aux 0.0000 (0.0000)	grad_norm 1.2509 (1.3731)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.9905
[32m[2023-01-07 00:25:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][511/54685]	eta 8:28:59 lr 0.0010000000	 wd 0.1000	time 0.6689 (0.5637)	loss 0.6298 (0.6253)	loss-cls 10.0762 (10.0046)	loss-aux 0.0000 (0.0000)	grad_norm 1.5247 (1.3779)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.9858
[32m[2023-01-07 00:25:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][527/54685]	eta 8:28:59 lr 0.0010000000	 wd 0.1000	time 0.6932 (0.5639)	loss 0.6158 (0.6252)	loss-cls 9.8525 (10.0031)	loss-aux 0.0000 (0.0000)	grad_norm 1.2606 (1.3743)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.1031
[32m[2023-01-07 00:25:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][543/54685]	eta 8:28:50 lr 0.0010000000	 wd 0.1000	time 0.6674 (0.5639)	loss 0.6283 (0.6252)	loss-cls 10.0534 (10.0031)	loss-aux 0.0000 (0.0000)	grad_norm 1.3098 (1.3724)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.0222
[32m[2023-01-07 00:25:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][559/54685]	eta 8:28:27 lr 0.0010000000	 wd 0.1000	time 0.6351 (0.5636)	loss 0.6280 (0.6252)	loss-cls 10.0473 (10.0032)	loss-aux 0.0000 (0.0000)	grad_norm 1.4505 (1.3746)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.8863
[32m[2023-01-07 00:25:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][575/54685]	eta 8:28:23 lr 0.0010000000	 wd 0.1000	time 0.6573 (0.5637)	loss 0.6300 (0.6253)	loss-cls 10.0804 (10.0049)	loss-aux 0.0000 (0.0000)	grad_norm 3.0274 (1.4205)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.0655
[32m[2023-01-07 00:25:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][591/54685]	eta 8:28:13 lr 0.0010000000	 wd 0.1000	time 0.6243 (0.5637)	loss 0.6268 (0.6255)	loss-cls 10.0291 (10.0078)	loss-aux 0.0000 (0.0000)	grad_norm 1.4403 (1.4211)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.0119
[32m[2023-01-07 00:25:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][607/54685]	eta 8:28:01 lr 0.0010000000	 wd 0.1000	time 0.6784 (0.5637)	loss 0.6526 (0.6256)	loss-cls 10.4408 (10.0099)	loss-aux 0.0000 (0.0000)	grad_norm 1.6217 (1.4264)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 8.9901
[32m[2023-01-07 00:26:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][623/54685]	eta 8:27:51 lr 0.0010000000	 wd 0.1000	time 0.6550 (0.5636)	loss 0.6302 (0.6257)	loss-cls 10.0830 (10.0106)	loss-aux 0.0000 (0.0000)	grad_norm 1.2072 (1.4207)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.0000
[32m[2023-01-07 00:26:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][639/54685]	eta 8:27:47 lr 0.0010000000	 wd 0.1000	time 0.6493 (0.5637)	loss 0.6334 (0.6257)	loss-cls 10.1348 (10.0111)	loss-aux 0.0000 (0.0000)	grad_norm 1.4227 (1.4208)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.0773
[32m[2023-01-07 00:26:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][655/54685]	eta 8:27:48 lr 0.0010000000	 wd 0.1000	time 0.6582 (0.5639)	loss 0.6323 (0.6258)	loss-cls 10.1160 (10.0123)	loss-aux 0.0000 (0.0000)	grad_norm 1.1696 (1.4147)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.1408
[32m[2023-01-07 00:26:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][671/54685]	eta 8:27:56 lr 0.0010000000	 wd 0.1000	time 0.6909 (0.5642)	loss 0.6280 (0.6258)	loss-cls 10.0480 (10.0130)	loss-aux 0.0000 (0.0000)	grad_norm 1.2575 (1.4109)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2419
[32m[2023-01-07 00:26:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][687/54685]	eta 8:28:03 lr 0.0010000000	 wd 0.1000	time 0.6986 (0.5645)	loss 0.6316 (0.6259)	loss-cls 10.1056 (10.0149)	loss-aux 0.0000 (0.0000)	grad_norm 1.1699 (1.4053)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2286
[32m[2023-01-07 00:26:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][703/54685]	eta 8:28:22 lr 0.0010000000	 wd 0.1000	time 0.7229 (0.5650)	loss 0.6337 (0.6260)	loss-cls 10.1398 (10.0153)	loss-aux 0.0000 (0.0000)	grad_norm 1.0852 (1.3980)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3958
[32m[2023-01-07 00:27:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][719/54685]	eta 8:28:28 lr 0.0009999999	 wd 0.1000	time 0.6649 (0.5653)	loss 0.6085 (0.6260)	loss-cls 9.7366 (10.0155)	loss-aux 0.0000 (0.0000)	grad_norm 1.0911 (1.3912)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2471
[32m[2023-01-07 00:27:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][735/54685]	eta 8:28:38 lr 0.0009999999	 wd 0.1000	time 0.6837 (0.5657)	loss 0.6363 (0.6260)	loss-cls 10.1806 (10.0163)	loss-aux 0.0000 (0.0000)	grad_norm 1.0945 (1.3848)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2986
[32m[2023-01-07 00:27:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][751/54685]	eta 8:28:47 lr 0.0009999999	 wd 0.1000	time 0.6815 (0.5660)	loss 0.5961 (0.6260)	loss-cls 9.5383 (10.0157)	loss-aux 0.0000 (0.0000)	grad_norm 1.0778 (1.3782)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3099
[32m[2023-01-07 00:27:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][767/54685]	eta 8:29:03 lr 0.0009999999	 wd 0.1000	time 0.6912 (0.5665)	loss 0.6288 (0.6260)	loss-cls 10.0612 (10.0155)	loss-aux 0.0000 (0.0000)	grad_norm 3.9822 (1.4325)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4103
[32m[2023-01-07 00:27:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][783/54685]	eta 8:29:03 lr 0.0009999999	 wd 0.1000	time 0.6691 (0.5667)	loss 0.6286 (0.6260)	loss-cls 10.0574 (10.0155)	loss-aux 0.0000 (0.0000)	grad_norm 3.7208 (1.4792)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.1993
[32m[2023-01-07 00:27:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][799/54685]	eta 8:29:08 lr 0.0009999999	 wd 0.1000	time 0.6814 (0.5669)	loss 0.6256 (0.6259)	loss-cls 10.0090 (10.0143)	loss-aux 0.0000 (0.0000)	grad_norm 1.1776 (1.4732)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2645
[32m[2023-01-07 00:27:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][815/54685]	eta 8:29:04 lr 0.0009999999	 wd 0.1000	time 0.6639 (0.5670)	loss 0.6170 (0.6259)	loss-cls 9.8718 (10.0141)	loss-aux 0.0000 (0.0000)	grad_norm 3.8550 (1.5199)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.1520
[32m[2023-01-07 00:28:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][831/54685]	eta 8:29:14 lr 0.0009999999	 wd 0.1000	time 0.6763 (0.5674)	loss 0.6058 (0.6258)	loss-cls 9.6934 (10.0131)	loss-aux 0.0000 (0.0000)	grad_norm 2.7097 (1.5427)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3720
[32m[2023-01-07 00:28:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][847/54685]	eta 8:29:33 lr 0.0009999999	 wd 0.1000	time 0.6821 (0.5679)	loss 0.6080 (0.6260)	loss-cls 9.7276 (10.0155)	loss-aux 0.0000 (0.0000)	grad_norm 1.3026 (1.5382)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5146
[32m[2023-01-07 00:28:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][863/54685]	eta 8:29:42 lr 0.0009999999	 wd 0.1000	time 0.7028 (0.5682)	loss 0.6350 (0.6260)	loss-cls 10.1602 (10.0158)	loss-aux 0.0000 (0.0000)	grad_norm 3.4254 (1.5732)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3778
[32m[2023-01-07 00:28:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][879/54685]	eta 8:29:54 lr 0.0009999999	 wd 0.1000	time 0.6945 (0.5686)	loss 0.6180 (0.6260)	loss-cls 9.8881 (10.0166)	loss-aux 0.0000 (0.0000)	grad_norm 1.1331 (1.5652)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4326
[32m[2023-01-07 00:28:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][895/54685]	eta 8:30:16 lr 0.0009999999	 wd 0.1000	time 0.6838 (0.5692)	loss 0.6318 (0.6262)	loss-cls 10.1082 (10.0192)	loss-aux 0.0000 (0.0000)	grad_norm 2.2087 (1.5767)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.6198
[32m[2023-01-07 00:28:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][911/54685]	eta 8:30:36 lr 0.0009999999	 wd 0.1000	time 0.6909 (0.5697)	loss 0.6151 (0.6262)	loss-cls 9.8410 (10.0199)	loss-aux 0.0000 (0.0000)	grad_norm 1.1244 (1.5687)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.6075
[32m[2023-01-07 00:29:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][927/54685]	eta 8:30:49 lr 0.0009999999	 wd 0.1000	time 0.6930 (0.5701)	loss 0.6234 (0.6263)	loss-cls 9.9742 (10.0208)	loss-aux 0.0000 (0.0000)	grad_norm 2.4921 (1.5846)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4824
[32m[2023-01-07 00:29:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][943/54685]	eta 8:31:05 lr 0.0009999999	 wd 0.1000	time 0.7482 (0.5706)	loss 0.6192 (0.6262)	loss-cls 9.9071 (10.0196)	loss-aux 0.0000 (0.0000)	grad_norm 2.0356 (1.5923)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5722
[32m[2023-01-07 00:29:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][959/54685]	eta 8:31:17 lr 0.0009999999	 wd 0.1000	time 0.6800 (0.5710)	loss 0.6286 (0.6262)	loss-cls 10.0582 (10.0184)	loss-aux 0.0000 (0.0000)	grad_norm 1.3360 (1.5880)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4982
[32m[2023-01-07 00:29:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][975/54685]	eta 8:31:33 lr 0.0009999999	 wd 0.1000	time 0.7016 (0.5715)	loss 0.6171 (0.6260)	loss-cls 9.8733 (10.0167)	loss-aux 0.0000 (0.0000)	grad_norm 0.4287 (1.5690)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.6029
[32m[2023-01-07 00:29:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][991/54685]	eta 8:31:46 lr 0.0009999999	 wd 0.1000	time 0.7181 (0.5719)	loss 0.6105 (0.6260)	loss-cls 9.7687 (10.0158)	loss-aux 0.0000 (0.0000)	grad_norm 0.5195 (1.5521)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5581
[32m[2023-01-07 00:29:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1007/54685]	eta 8:31:57 lr 0.0009999999	 wd 0.1000	time 0.6882 (0.5723)	loss 0.6228 (0.6259)	loss-cls 9.9650 (10.0142)	loss-aux 0.0000 (0.0000)	grad_norm 0.3949 (1.5337)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5232
[32m[2023-01-07 00:30:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1023/54685]	eta 8:32:05 lr 0.0009999999	 wd 0.1000	time 0.7079 (0.5726)	loss 0.6146 (0.6258)	loss-cls 9.8340 (10.0132)	loss-aux 0.0000 (0.0000)	grad_norm 0.4106 (1.5162)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4865
[32m[2023-01-07 00:30:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1039/54685]	eta 8:32:12 lr 0.0009999999	 wd 0.1000	time 0.6917 (0.5729)	loss 0.6291 (0.6258)	loss-cls 10.0651 (10.0124)	loss-aux 0.0000 (0.0000)	grad_norm 0.4310 (1.4995)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4597
[32m[2023-01-07 00:30:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1055/54685]	eta 8:32:10 lr 0.0009999999	 wd 0.1000	time 0.6926 (0.5730)	loss 0.6166 (0.6257)	loss-cls 9.8659 (10.0109)	loss-aux 0.0000 (0.0000)	grad_norm 0.4431 (1.4835)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3240
[32m[2023-01-07 00:30:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1071/54685]	eta 8:32:16 lr 0.0009999999	 wd 0.1000	time 0.6872 (0.5733)	loss 0.6256 (0.6256)	loss-cls 10.0103 (10.0093)	loss-aux 0.0000 (0.0000)	grad_norm 0.4803 (1.4685)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4545
[32m[2023-01-07 00:30:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1087/54685]	eta 8:32:18 lr 0.0009999999	 wd 0.1000	time 0.6694 (0.5735)	loss 0.6147 (0.6256)	loss-cls 9.8351 (10.0088)	loss-aux 0.0000 (0.0000)	grad_norm 0.6262 (1.4561)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4041
[32m[2023-01-07 00:30:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1103/54685]	eta 8:32:15 lr 0.0009999999	 wd 0.1000	time 0.6856 (0.5736)	loss 0.6205 (0.6255)	loss-cls 9.9286 (10.0076)	loss-aux 0.0000 (0.0000)	grad_norm 0.4347 (1.4413)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3171
[32m[2023-01-07 00:30:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1119/54685]	eta 8:32:21 lr 0.0009999999	 wd 0.1000	time 0.7027 (0.5739)	loss 0.6214 (0.6254)	loss-cls 9.9425 (10.0061)	loss-aux 0.0000 (0.0000)	grad_norm 0.4738 (1.4275)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4891
[32m[2023-01-07 00:31:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1135/54685]	eta 8:32:40 lr 0.0009999999	 wd 0.1000	time 0.6946 (0.5744)	loss 0.6184 (0.6253)	loss-cls 9.8950 (10.0047)	loss-aux 0.0000 (0.0000)	grad_norm 0.3980 (1.4130)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.7871
[32m[2023-01-07 00:31:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1151/54685]	eta 8:32:43 lr 0.0009999999	 wd 0.1000	time 0.6988 (0.5747)	loss 0.6085 (0.6252)	loss-cls 9.7367 (10.0027)	loss-aux 0.0000 (0.0000)	grad_norm 0.7703 (1.4041)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4400
[32m[2023-01-07 00:31:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1167/54685]	eta 8:32:46 lr 0.0009999999	 wd 0.1000	time 0.6990 (0.5749)	loss 0.6126 (0.6251)	loss-cls 9.8011 (10.0021)	loss-aux 0.0000 (0.0000)	grad_norm 0.5366 (1.3922)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4709
[32m[2023-01-07 00:31:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1183/54685]	eta 8:33:01 lr 0.0009999999	 wd 0.1000	time 0.6870 (0.5753)	loss 0.6094 (0.6251)	loss-cls 9.7500 (10.0008)	loss-aux 0.0000 (0.0000)	grad_norm 0.4263 (1.3791)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.7210
[32m[2023-01-07 00:31:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1199/54685]	eta 8:33:07 lr 0.0009999999	 wd 0.1000	time 0.6913 (0.5756)	loss 0.6142 (0.6249)	loss-cls 9.8271 (9.9989)	loss-aux 0.0000 (0.0000)	grad_norm 0.3534 (1.3654)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5621
[32m[2023-01-07 00:31:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1215/54685]	eta 8:33:13 lr 0.0009999999	 wd 0.1000	time 0.7139 (0.5759)	loss 0.6215 (0.6249)	loss-cls 9.9434 (9.9978)	loss-aux 0.0000 (0.0000)	grad_norm 0.3447 (1.3520)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5478
[32m[2023-01-07 00:32:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1231/54685]	eta 8:33:21 lr 0.0009999999	 wd 0.1000	time 0.6824 (0.5762)	loss 0.6189 (0.6248)	loss-cls 9.9019 (9.9971)	loss-aux 0.0000 (0.0000)	grad_norm 0.3754 (1.3393)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.6007
[32m[2023-01-07 00:32:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1247/54685]	eta 8:33:28 lr 0.0009999998	 wd 0.1000	time 0.7118 (0.5765)	loss 0.6158 (0.6248)	loss-cls 9.8527 (9.9961)	loss-aux 0.0000 (0.0000)	grad_norm 0.3636 (1.3268)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.6068
[32m[2023-01-07 00:32:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1263/54685]	eta 8:33:36 lr 0.0009999998	 wd 0.1000	time 0.7009 (0.5768)	loss 0.6206 (0.6247)	loss-cls 9.9301 (9.9951)	loss-aux 0.0000 (0.0000)	grad_norm 0.3417 (1.3143)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.6303
[32m[2023-01-07 00:32:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1279/54685]	eta 8:33:48 lr 0.0009999998	 wd 0.1000	time 0.7010 (0.5772)	loss 0.6138 (0.6246)	loss-cls 9.8204 (9.9940)	loss-aux 0.0000 (0.0000)	grad_norm 0.3346 (1.3021)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.7369
[32m[2023-01-07 00:32:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1295/54685]	eta 8:34:10 lr 0.0009999998	 wd 0.1000	time 0.6825 (0.5778)	loss 0.6107 (0.6245)	loss-cls 9.7717 (9.9923)	loss-aux 0.0000 (0.0000)	grad_norm 0.3383 (1.2902)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 10.0006
[32m[2023-01-07 00:32:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1311/54685]	eta 8:34:10 lr 0.0009999998	 wd 0.1000	time 0.7032 (0.5780)	loss 0.6110 (0.6245)	loss-cls 9.7765 (9.9914)	loss-aux 0.0000 (0.0000)	grad_norm 0.3560 (1.2788)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4762
[32m[2023-01-07 00:33:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1327/54685]	eta 8:34:11 lr 0.0009999998	 wd 0.1000	time 0.6906 (0.5782)	loss 0.6147 (0.6244)	loss-cls 9.8351 (9.9898)	loss-aux 0.0000 (0.0000)	grad_norm 0.3213 (1.2673)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4901
[32m[2023-01-07 00:33:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1343/54685]	eta 8:34:16 lr 0.0009999998	 wd 0.1000	time 0.6955 (0.5785)	loss 0.6137 (0.6243)	loss-cls 9.8193 (9.9886)	loss-aux 0.0000 (0.0000)	grad_norm 0.3196 (1.2560)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.6199
[32m[2023-01-07 00:33:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1359/54685]	eta 8:34:18 lr 0.0009999998	 wd 0.1000	time 0.6938 (0.5787)	loss 0.6184 (0.6242)	loss-cls 9.8940 (9.9874)	loss-aux 0.0000 (0.0000)	grad_norm 0.3417 (1.2452)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5300
[32m[2023-01-07 00:33:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1375/54685]	eta 8:34:18 lr 0.0009999998	 wd 0.1000	time 0.6734 (0.5789)	loss 0.6121 (0.6241)	loss-cls 9.7928 (9.9858)	loss-aux 0.0000 (0.0000)	grad_norm 0.3355 (1.2347)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5084
[32m[2023-01-07 00:33:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1391/54685]	eta 8:34:20 lr 0.0009999998	 wd 0.1000	time 0.6790 (0.5791)	loss 0.6353 (0.6240)	loss-cls 10.1649 (9.9847)	loss-aux 0.0000 (0.0000)	grad_norm 0.3222 (1.2242)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5412
[32m[2023-01-07 00:33:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1407/54685]	eta 8:34:24 lr 0.0009999998	 wd 0.1000	time 0.7354 (0.5793)	loss 0.6201 (0.6240)	loss-cls 9.9215 (9.9840)	loss-aux 0.0000 (0.0000)	grad_norm 0.3670 (1.2144)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.6334
[32m[2023-01-07 00:34:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1423/54685]	eta 8:34:27 lr 0.0009999998	 wd 0.1000	time 0.6892 (0.5795)	loss 0.6192 (0.6239)	loss-cls 9.9074 (9.9830)	loss-aux 0.0000 (0.0000)	grad_norm 0.3094 (1.2043)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5937
[32m[2023-01-07 00:34:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1439/54685]	eta 8:34:27 lr 0.0009999998	 wd 0.1000	time 0.7009 (0.5797)	loss 0.6186 (0.6239)	loss-cls 9.8983 (9.9822)	loss-aux 0.0000 (0.0000)	grad_norm 0.2564 (1.1937)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5317
[32m[2023-01-07 00:34:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1455/54685]	eta 8:34:27 lr 0.0009999998	 wd 0.1000	time 0.6847 (0.5799)	loss 0.6155 (0.6238)	loss-cls 9.8478 (9.9812)	loss-aux 0.0000 (0.0000)	grad_norm 0.2903 (1.1838)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5181
[32m[2023-01-07 00:34:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1471/54685]	eta 8:34:25 lr 0.0009999998	 wd 0.1000	time 0.6849 (0.5800)	loss 0.6130 (0.6238)	loss-cls 9.8080 (9.9800)	loss-aux 0.0000 (0.0000)	grad_norm 0.3131 (1.1743)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4849
[32m[2023-01-07 00:34:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1487/54685]	eta 8:34:23 lr 0.0009999998	 wd 0.1000	time 0.6948 (0.5802)	loss 0.6203 (0.6237)	loss-cls 9.9244 (9.9790)	loss-aux 0.0000 (0.0000)	grad_norm 0.3409 (1.1654)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4908
[32m[2023-01-07 00:34:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1503/54685]	eta 8:34:21 lr 0.0009999998	 wd 0.1000	time 0.6986 (0.5803)	loss 0.6220 (0.6236)	loss-cls 9.9514 (9.9780)	loss-aux 0.0000 (0.0000)	grad_norm 0.2775 (1.1559)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4843
[32m[2023-01-07 00:34:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1519/54685]	eta 8:34:32 lr 0.0009999998	 wd 0.1000	time 0.8381 (0.5807)	loss 0.6212 (0.6235)	loss-cls 9.9387 (9.9768)	loss-aux 0.0000 (0.0000)	grad_norm 0.3079 (1.1470)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.8644
[32m[2023-01-07 00:35:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1535/54685]	eta 8:34:29 lr 0.0009999998	 wd 0.1000	time 0.6811 (0.5808)	loss 0.6138 (0.6235)	loss-cls 9.8214 (9.9758)	loss-aux 0.0000 (0.0000)	grad_norm 0.2879 (1.1381)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4771
[32m[2023-01-07 00:35:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1551/54685]	eta 8:34:22 lr 0.0009999998	 wd 0.1000	time 0.6947 (0.5808)	loss 0.6247 (0.6234)	loss-cls 9.9951 (9.9749)	loss-aux 0.0000 (0.0000)	grad_norm 0.3875 (1.1303)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3528
[32m[2023-01-07 00:35:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1567/54685]	eta 8:34:17 lr 0.0009999998	 wd 0.1000	time 0.6889 (0.5809)	loss 0.6006 (0.6234)	loss-cls 9.6090 (9.9742)	loss-aux 0.0000 (0.0000)	grad_norm 0.3037 (1.1219)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4288
[32m[2023-01-07 00:35:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1583/54685]	eta 8:34:14 lr 0.0009999998	 wd 0.1000	time 0.6889 (0.5810)	loss 0.6204 (0.6233)	loss-cls 9.9267 (9.9736)	loss-aux 0.0000 (0.0000)	grad_norm 0.3543 (1.1141)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4674
[32m[2023-01-07 00:35:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1599/54685]	eta 8:34:10 lr 0.0009999997	 wd 0.1000	time 0.6846 (0.5811)	loss 0.6125 (0.6233)	loss-cls 9.8002 (9.9729)	loss-aux 0.0000 (0.0000)	grad_norm 0.2797 (1.1058)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4642
[32m[2023-01-07 00:35:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1615/54685]	eta 8:34:02 lr 0.0009999997	 wd 0.1000	time 0.6946 (0.5812)	loss 0.6201 (0.6233)	loss-cls 9.9209 (9.9722)	loss-aux 0.0000 (0.0000)	grad_norm 0.3072 (1.0979)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3450
[32m[2023-01-07 00:36:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1631/54685]	eta 8:34:00 lr 0.0009999997	 wd 0.1000	time 0.6955 (0.5813)	loss 0.6153 (0.6232)	loss-cls 9.8447 (9.9710)	loss-aux 0.0000 (0.0000)	grad_norm 0.2743 (1.0898)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5016
[32m[2023-01-07 00:36:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1647/54685]	eta 8:33:52 lr 0.0009999997	 wd 0.1000	time 0.6928 (0.5813)	loss 0.6137 (0.6231)	loss-cls 9.8195 (9.9702)	loss-aux 0.0000 (0.0000)	grad_norm 0.3267 (1.0824)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3631
[32m[2023-01-07 00:36:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1663/54685]	eta 8:33:47 lr 0.0009999997	 wd 0.1000	time 0.7053 (0.5814)	loss 0.6095 (0.6231)	loss-cls 9.7519 (9.9693)	loss-aux 0.0000 (0.0000)	grad_norm 0.2774 (1.0747)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4269
[32m[2023-01-07 00:36:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1679/54685]	eta 8:33:49 lr 0.0009999997	 wd 0.1000	time 0.6974 (0.5816)	loss 0.6175 (0.6230)	loss-cls 9.8803 (9.9682)	loss-aux 0.0000 (0.0000)	grad_norm 0.2856 (1.0671)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.6426
[32m[2023-01-07 00:36:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1695/54685]	eta 8:33:44 lr 0.0009999997	 wd 0.1000	time 0.6881 (0.5817)	loss 0.6186 (0.6229)	loss-cls 9.8972 (9.9669)	loss-aux 0.0000 (0.0000)	grad_norm 0.2784 (1.0597)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4435
[32m[2023-01-07 00:36:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1711/54685]	eta 8:33:39 lr 0.0009999997	 wd 0.1000	time 0.6961 (0.5818)	loss 0.6115 (0.6229)	loss-cls 9.7844 (9.9662)	loss-aux 0.0000 (0.0000)	grad_norm 0.3323 (1.0529)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4758
[32m[2023-01-07 00:37:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1727/54685]	eta 8:33:38 lr 0.0009999997	 wd 0.1000	time 0.7123 (0.5819)	loss 0.6051 (0.6228)	loss-cls 9.6820 (9.9650)	loss-aux 0.0000 (0.0000)	grad_norm 0.3082 (1.0460)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5573
[32m[2023-01-07 00:37:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1743/54685]	eta 8:33:31 lr 0.0009999997	 wd 0.1000	time 0.6789 (0.5820)	loss 0.6161 (0.6228)	loss-cls 9.8576 (9.9643)	loss-aux 0.0000 (0.0000)	grad_norm 0.3167 (1.0393)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4004
[32m[2023-01-07 00:37:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1759/54685]	eta 8:33:26 lr 0.0009999997	 wd 0.1000	time 0.6929 (0.5821)	loss 0.6169 (0.6227)	loss-cls 9.8711 (9.9635)	loss-aux 0.0000 (0.0000)	grad_norm 0.2751 (1.0324)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4630
[32m[2023-01-07 00:37:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1775/54685]	eta 8:33:18 lr 0.0009999997	 wd 0.1000	time 0.6810 (0.5821)	loss 0.6051 (0.6227)	loss-cls 9.6811 (9.9624)	loss-aux 0.0000 (0.0000)	grad_norm 0.2763 (1.0256)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3411
[32m[2023-01-07 00:37:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1791/54685]	eta 8:33:12 lr 0.0009999997	 wd 0.1000	time 0.7105 (0.5822)	loss 0.6120 (0.6226)	loss-cls 9.7927 (9.9618)	loss-aux 0.0000 (0.0000)	grad_norm 0.2750 (1.0189)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4297
[32m[2023-01-07 00:37:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1807/54685]	eta 8:33:04 lr 0.0009999997	 wd 0.1000	time 0.7000 (0.5822)	loss 0.6139 (0.6226)	loss-cls 9.8218 (9.9610)	loss-aux 0.0000 (0.0000)	grad_norm 0.3028 (1.0125)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3611
[32m[2023-01-07 00:37:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1823/54685]	eta 8:32:58 lr 0.0009999997	 wd 0.1000	time 0.6943 (0.5822)	loss 0.6015 (0.6225)	loss-cls 9.6240 (9.9598)	loss-aux 0.0000 (0.0000)	grad_norm 0.2735 (1.0060)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4336
[32m[2023-01-07 00:38:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1839/54685]	eta 8:32:51 lr 0.0009999997	 wd 0.1000	time 0.7023 (0.5823)	loss 0.6169 (0.6224)	loss-cls 9.8708 (9.9589)	loss-aux 0.0000 (0.0000)	grad_norm 0.2818 (0.9997)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3815
[32m[2023-01-07 00:38:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1855/54685]	eta 8:32:44 lr 0.0009999997	 wd 0.1000	time 0.6851 (0.5823)	loss 0.6102 (0.6224)	loss-cls 9.7638 (9.9582)	loss-aux 0.0000 (0.0000)	grad_norm 0.2513 (0.9933)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4204
[32m[2023-01-07 00:38:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1871/54685]	eta 8:32:35 lr 0.0009999997	 wd 0.1000	time 0.6748 (0.5823)	loss 0.6099 (0.6223)	loss-cls 9.7590 (9.9574)	loss-aux 0.0000 (0.0000)	grad_norm 0.3325 (0.9876)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3306
[32m[2023-01-07 00:38:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1887/54685]	eta 8:32:27 lr 0.0009999996	 wd 0.1000	time 0.6998 (0.5824)	loss 0.6049 (0.6223)	loss-cls 9.6788 (9.9563)	loss-aux 0.0000 (0.0000)	grad_norm 0.3098 (0.9819)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3448
[32m[2023-01-07 00:38:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1903/54685]	eta 8:32:19 lr 0.0009999996	 wd 0.1000	time 0.7014 (0.5824)	loss 0.6141 (0.6222)	loss-cls 9.8250 (9.9552)	loss-aux 0.0000 (0.0000)	grad_norm 0.3000 (0.9762)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3713
[32m[2023-01-07 00:38:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1919/54685]	eta 8:32:14 lr 0.0009999996	 wd 0.1000	time 0.7071 (0.5825)	loss 0.6131 (0.6222)	loss-cls 9.8090 (9.9547)	loss-aux 0.0000 (0.0000)	grad_norm 0.2584 (0.9702)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4620
[32m[2023-01-07 00:39:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1935/54685]	eta 8:32:04 lr 0.0009999996	 wd 0.1000	time 0.6827 (0.5825)	loss 0.6093 (0.6221)	loss-cls 9.7493 (9.9541)	loss-aux 0.0000 (0.0000)	grad_norm 0.3279 (0.9649)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3232
[32m[2023-01-07 00:39:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1951/54685]	eta 8:31:57 lr 0.0009999996	 wd 0.1000	time 0.6849 (0.5825)	loss 0.6243 (0.6221)	loss-cls 9.9881 (9.9535)	loss-aux 0.0000 (0.0000)	grad_norm 0.2586 (0.9591)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3954
[32m[2023-01-07 00:39:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1967/54685]	eta 8:31:51 lr 0.0009999996	 wd 0.1000	time 0.6845 (0.5826)	loss 0.6159 (0.6220)	loss-cls 9.8549 (9.9527)	loss-aux 0.0000 (0.0000)	grad_norm 0.3383 (0.9540)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4262
[32m[2023-01-07 00:39:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1983/54685]	eta 8:31:45 lr 0.0009999996	 wd 0.1000	time 0.6894 (0.5826)	loss 0.6109 (0.6220)	loss-cls 9.7741 (9.9520)	loss-aux 0.0000 (0.0000)	grad_norm 0.7239 (0.9522)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4402
[32m[2023-01-07 00:39:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1999/54685]	eta 8:31:37 lr 0.0009999996	 wd 0.1000	time 0.6660 (0.5826)	loss 0.6137 (0.6220)	loss-cls 9.8187 (9.9514)	loss-aux 0.0000 (0.0000)	grad_norm 0.3011 (0.9470)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3771
[32m[2023-01-07 00:39:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2015/54685]	eta 8:31:29 lr 0.0009999996	 wd 0.1000	time 0.6895 (0.5827)	loss 0.6135 (0.6219)	loss-cls 9.8165 (9.9505)	loss-aux 0.0000 (0.0000)	grad_norm 0.2851 (0.9417)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3905
[32m[2023-01-07 00:39:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2031/54685]	eta 8:31:19 lr 0.0009999996	 wd 0.1000	time 0.6763 (0.5827)	loss 0.6090 (0.6218)	loss-cls 9.7441 (9.9496)	loss-aux 0.0000 (0.0000)	grad_norm 0.2900 (0.9366)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2816
[32m[2023-01-07 00:40:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2047/54685]	eta 8:31:13 lr 0.0009999996	 wd 0.1000	time 0.7019 (0.5827)	loss 0.6160 (0.6218)	loss-cls 9.8560 (9.9495)	loss-aux 0.0000 (0.0000)	grad_norm 0.3427 (0.9319)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4722
[32m[2023-01-07 00:40:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2063/54685]	eta 8:31:06 lr 0.0009999996	 wd 0.1000	time 0.6866 (0.5828)	loss 0.6187 (0.6218)	loss-cls 9.8993 (9.9489)	loss-aux 0.0000 (0.0000)	grad_norm 0.2961 (0.9270)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3974
[32m[2023-01-07 00:40:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2079/54685]	eta 8:31:02 lr 0.0009999996	 wd 0.1000	time 0.6882 (0.5829)	loss 0.6110 (0.6217)	loss-cls 9.7756 (9.9477)	loss-aux 0.0000 (0.0000)	grad_norm 0.2974 (0.9222)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5220
[32m[2023-01-07 00:40:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2095/54685]	eta 8:30:56 lr 0.0009999996	 wd 0.1000	time 0.6997 (0.5829)	loss 0.6130 (0.6217)	loss-cls 9.8080 (9.9467)	loss-aux 0.0000 (0.0000)	grad_norm 0.2747 (0.9172)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4741
[32m[2023-01-07 00:40:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2111/54685]	eta 8:30:50 lr 0.0009999996	 wd 0.1000	time 0.6837 (0.5830)	loss 0.6050 (0.6216)	loss-cls 9.6794 (9.9459)	loss-aux 0.0000 (0.0000)	grad_norm 0.3277 (0.9128)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4439
[32m[2023-01-07 00:40:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2127/54685]	eta 8:30:44 lr 0.0009999995	 wd 0.1000	time 0.6951 (0.5831)	loss 0.6118 (0.6216)	loss-cls 9.7891 (9.9449)	loss-aux 0.0000 (0.0000)	grad_norm 0.2636 (0.9079)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4966
[32m[2023-01-07 00:41:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2143/54685]	eta 8:30:38 lr 0.0009999995	 wd 0.1000	time 0.7187 (0.5831)	loss 0.6212 (0.6215)	loss-cls 9.9397 (9.9446)	loss-aux 0.0000 (0.0000)	grad_norm 0.3083 (0.9034)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4414
[32m[2023-01-07 00:41:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2159/54685]	eta 8:30:32 lr 0.0009999995	 wd 0.1000	time 0.6827 (0.5832)	loss 0.6183 (0.6215)	loss-cls 9.8928 (9.9437)	loss-aux 0.0000 (0.0000)	grad_norm 0.2696 (0.8987)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4690
[32m[2023-01-07 00:41:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2175/54685]	eta 8:30:24 lr 0.0009999995	 wd 0.1000	time 0.7087 (0.5832)	loss 0.6208 (0.6215)	loss-cls 9.9329 (9.9433)	loss-aux 0.0000 (0.0000)	grad_norm 0.3147 (0.8944)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3804
[32m[2023-01-07 00:41:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2191/54685]	eta 8:30:14 lr 0.0009999995	 wd 0.1000	time 0.6780 (0.5832)	loss 0.6245 (0.6214)	loss-cls 9.9914 (9.9428)	loss-aux 0.0000 (0.0000)	grad_norm 0.4342 (0.8911)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3319
[32m[2023-01-07 00:41:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2207/54685]	eta 8:30:06 lr 0.0009999995	 wd 0.1000	time 0.7060 (0.5832)	loss 0.6052 (0.6214)	loss-cls 9.6832 (9.9420)	loss-aux 0.0000 (0.0000)	grad_norm 0.3680 (0.8873)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3564
[32m[2023-01-07 00:41:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2223/54685]	eta 8:29:59 lr 0.0009999995	 wd 0.1000	time 0.6844 (0.5833)	loss 0.6108 (0.6213)	loss-cls 9.7732 (9.9414)	loss-aux 0.0000 (0.0000)	grad_norm 0.3585 (0.8835)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4330
[32m[2023-01-07 00:42:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2239/54685]	eta 8:29:49 lr 0.0009999995	 wd 0.1000	time 0.6869 (0.5833)	loss 0.6222 (0.6213)	loss-cls 9.9557 (9.9409)	loss-aux 0.0000 (0.0000)	grad_norm 0.3742 (0.8798)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3206
[32m[2023-01-07 00:42:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2255/54685]	eta 8:29:40 lr 0.0009999995	 wd 0.1000	time 0.7086 (0.5833)	loss 0.6131 (0.6213)	loss-cls 9.8096 (9.9402)	loss-aux 0.0000 (0.0000)	grad_norm 0.4063 (0.8765)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3617
[32m[2023-01-07 00:42:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2271/54685]	eta 8:29:32 lr 0.0009999995	 wd 0.1000	time 0.6812 (0.5833)	loss 0.6057 (0.6212)	loss-cls 9.6904 (9.9395)	loss-aux 0.0000 (0.0000)	grad_norm 0.3587 (0.8728)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3710
[32m[2023-01-07 00:42:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2287/54685]	eta 8:29:22 lr 0.0009999995	 wd 0.1000	time 0.6860 (0.5833)	loss 0.6183 (0.6212)	loss-cls 9.8924 (9.9389)	loss-aux 0.0000 (0.0000)	grad_norm 0.3707 (0.8693)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3129
[32m[2023-01-07 00:42:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2303/54685]	eta 8:29:14 lr 0.0009999995	 wd 0.1000	time 0.6983 (0.5833)	loss 0.6098 (0.6211)	loss-cls 9.7566 (9.9381)	loss-aux 0.0000 (0.0000)	grad_norm 0.3148 (0.8655)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3750
[32m[2023-01-07 00:42:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2319/54685]	eta 8:29:04 lr 0.0009999995	 wd 0.1000	time 0.7031 (0.5833)	loss 0.6170 (0.6211)	loss-cls 9.8724 (9.9374)	loss-aux 0.0000 (0.0000)	grad_norm 0.2993 (0.8616)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3135
[32m[2023-01-07 00:42:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2335/54685]	eta 8:28:53 lr 0.0009999995	 wd 0.1000	time 0.6808 (0.5833)	loss 0.6172 (0.6210)	loss-cls 9.8755 (9.9365)	loss-aux 0.0000 (0.0000)	grad_norm 0.3245 (0.8579)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2523
[32m[2023-01-07 00:43:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2351/54685]	eta 8:28:46 lr 0.0009999994	 wd 0.1000	time 0.6907 (0.5833)	loss 0.6045 (0.6210)	loss-cls 9.6714 (9.9357)	loss-aux 0.0000 (0.0000)	grad_norm 0.3091 (0.8541)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4155
[32m[2023-01-07 00:43:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2367/54685]	eta 8:28:35 lr 0.0009999994	 wd 0.1000	time 0.6850 (0.5833)	loss 0.6258 (0.6209)	loss-cls 10.0129 (9.9351)	loss-aux 0.0000 (0.0000)	grad_norm 0.2805 (0.8503)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2972
[32m[2023-01-07 00:43:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2383/54685]	eta 8:28:24 lr 0.0009999994	 wd 0.1000	time 0.6734 (0.5832)	loss 0.6086 (0.6209)	loss-cls 9.7369 (9.9346)	loss-aux 0.0000 (0.0000)	grad_norm 0.3539 (0.8469)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2433
[32m[2023-01-07 00:43:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2399/54685]	eta 8:28:13 lr 0.0009999994	 wd 0.1000	time 0.6730 (0.5832)	loss 0.6147 (0.6209)	loss-cls 9.8349 (9.9340)	loss-aux 0.0000 (0.0000)	grad_norm 0.2530 (0.8430)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2635
[32m[2023-01-07 00:43:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2415/54685]	eta 8:28:04 lr 0.0009999994	 wd 0.1000	time 0.6908 (0.5832)	loss 0.6260 (0.6208)	loss-cls 10.0158 (9.9332)	loss-aux 0.0000 (0.0000)	grad_norm 0.2436 (0.8390)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3293
[32m[2023-01-07 00:43:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2431/54685]	eta 8:27:53 lr 0.0009999994	 wd 0.1000	time 0.6698 (0.5832)	loss 0.6096 (0.6208)	loss-cls 9.7539 (9.9325)	loss-aux 0.0000 (0.0000)	grad_norm 0.3124 (0.8355)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2358
[32m[2023-01-07 00:44:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2447/54685]	eta 8:27:44 lr 0.0009999994	 wd 0.1000	time 0.6896 (0.5832)	loss 0.6146 (0.6207)	loss-cls 9.8336 (9.9319)	loss-aux 0.0000 (0.0000)	grad_norm 0.2552 (0.8318)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3672
[32m[2023-01-07 00:44:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2463/54685]	eta 8:27:33 lr 0.0009999994	 wd 0.1000	time 0.7089 (0.5832)	loss 0.6123 (0.6207)	loss-cls 9.7960 (9.9312)	loss-aux 0.0000 (0.0000)	grad_norm 0.2202 (0.8278)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2671
[32m[2023-01-07 00:44:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2479/54685]	eta 8:27:24 lr 0.0009999994	 wd 0.1000	time 0.6842 (0.5832)	loss 0.6194 (0.6207)	loss-cls 9.9099 (9.9311)	loss-aux 0.0000 (0.0000)	grad_norm 0.2879 (0.8243)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3540
[32m[2023-01-07 00:44:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2495/54685]	eta 8:27:15 lr 0.0009999994	 wd 0.1000	time 0.6573 (0.5832)	loss 0.6055 (0.6206)	loss-cls 9.6878 (9.9303)	loss-aux 0.0000 (0.0000)	grad_norm 0.2767 (0.8208)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3288
[32m[2023-01-07 00:44:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2511/54685]	eta 8:27:06 lr 0.0009999994	 wd 0.1000	time 0.7051 (0.5832)	loss 0.6127 (0.6206)	loss-cls 9.8034 (9.9296)	loss-aux 0.0000 (0.0000)	grad_norm 0.2334 (0.8170)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3194
[32m[2023-01-07 00:44:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2527/54685]	eta 8:26:56 lr 0.0009999994	 wd 0.1000	time 0.6789 (0.5832)	loss 0.6251 (0.6206)	loss-cls 10.0019 (9.9292)	loss-aux 0.0000 (0.0000)	grad_norm 0.2786 (0.8136)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3055
[32m[2023-01-07 00:44:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2543/54685]	eta 8:26:46 lr 0.0009999994	 wd 0.1000	time 0.6834 (0.5831)	loss 0.6176 (0.6206)	loss-cls 9.8809 (9.9289)	loss-aux 0.0000 (0.0000)	grad_norm 0.3454 (0.8107)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2953
[32m[2023-01-07 00:45:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2559/54685]	eta 8:26:36 lr 0.0009999993	 wd 0.1000	time 0.6925 (0.5831)	loss 0.6195 (0.6205)	loss-cls 9.9123 (9.9285)	loss-aux 0.0000 (0.0000)	grad_norm 0.3366 (0.8077)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2932
[32m[2023-01-07 00:45:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2575/54685]	eta 8:26:25 lr 0.0009999993	 wd 0.1000	time 0.6896 (0.5831)	loss 0.6196 (0.6205)	loss-cls 9.9133 (9.9280)	loss-aux 0.0000 (0.0000)	grad_norm 0.2548 (0.8043)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2573
[32m[2023-01-07 00:45:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2591/54685]	eta 8:26:14 lr 0.0009999993	 wd 0.1000	time 0.6934 (0.5831)	loss 0.6148 (0.6205)	loss-cls 9.8374 (9.9273)	loss-aux 0.0000 (0.0000)	grad_norm 0.3136 (0.8013)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2687
[32m[2023-01-07 00:45:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2607/54685]	eta 8:26:04 lr 0.0009999993	 wd 0.1000	time 0.6738 (0.5831)	loss 0.6212 (0.6204)	loss-cls 9.9399 (9.9271)	loss-aux 0.0000 (0.0000)	grad_norm 0.2563 (0.7979)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2852
[32m[2023-01-07 00:45:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2623/54685]	eta 8:25:54 lr 0.0009999993	 wd 0.1000	time 0.6745 (0.5830)	loss 0.6063 (0.6204)	loss-cls 9.7014 (9.9263)	loss-aux 0.0000 (0.0000)	grad_norm 0.2320 (0.7945)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2840
[32m[2023-01-07 00:45:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2639/54685]	eta 8:25:43 lr 0.0009999993	 wd 0.1000	time 0.6762 (0.5830)	loss 0.6066 (0.6204)	loss-cls 9.7063 (9.9257)	loss-aux 0.0000 (0.0000)	grad_norm 0.2887 (0.7914)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2613
[32m[2023-01-07 00:46:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2655/54685]	eta 8:25:33 lr 0.0009999993	 wd 0.1000	time 0.6571 (0.5830)	loss 0.6266 (0.6203)	loss-cls 10.0254 (9.9250)	loss-aux 0.0000 (0.0000)	grad_norm 0.3014 (0.7885)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2670
[32m[2023-01-07 00:46:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2671/54685]	eta 8:25:24 lr 0.0009999993	 wd 0.1000	time 0.6801 (0.5830)	loss 0.6261 (0.6203)	loss-cls 10.0169 (9.9245)	loss-aux 0.0000 (0.0000)	grad_norm 0.2908 (0.7855)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3691
[32m[2023-01-07 00:46:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2687/54685]	eta 8:25:16 lr 0.0009999993	 wd 0.1000	time 0.7024 (0.5830)	loss 0.6168 (0.6203)	loss-cls 9.8682 (9.9243)	loss-aux 0.0000 (0.0000)	grad_norm 0.2816 (0.7825)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3806
[32m[2023-01-07 00:46:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2703/54685]	eta 8:25:06 lr 0.0009999993	 wd 0.1000	time 0.6946 (0.5830)	loss 0.6096 (0.6202)	loss-cls 9.7538 (9.9237)	loss-aux 0.0000 (0.0000)	grad_norm 0.2737 (0.7795)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3158
[32m[2023-01-07 00:46:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2719/54685]	eta 8:24:56 lr 0.0009999993	 wd 0.1000	time 0.6837 (0.5830)	loss 0.6218 (0.6202)	loss-cls 9.9481 (9.9233)	loss-aux 0.0000 (0.0000)	grad_norm 0.3988 (0.7772)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3062
[32m[2023-01-07 00:46:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2735/54685]	eta 8:24:47 lr 0.0009999993	 wd 0.1000	time 0.6882 (0.5830)	loss 0.6127 (0.6202)	loss-cls 9.8030 (9.9225)	loss-aux 0.0000 (0.0000)	grad_norm 0.2564 (0.7742)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3358
[32m[2023-01-07 00:46:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2751/54685]	eta 8:24:38 lr 0.0009999992	 wd 0.1000	time 0.6883 (0.5830)	loss 0.6180 (0.6201)	loss-cls 9.8874 (9.9222)	loss-aux 0.0000 (0.0000)	grad_norm 0.2825 (0.7713)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3171
[32m[2023-01-07 00:47:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2767/54685]	eta 8:24:29 lr 0.0009999992	 wd 0.1000	time 0.6795 (0.5830)	loss 0.6050 (0.6201)	loss-cls 9.6793 (9.9214)	loss-aux 0.0000 (0.0000)	grad_norm 0.2622 (0.7684)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3725
[32m[2023-01-07 00:47:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2783/54685]	eta 8:24:23 lr 0.0009999992	 wd 0.1000	time 0.6947 (0.5831)	loss 0.6222 (0.6200)	loss-cls 9.9547 (9.9207)	loss-aux 0.0000 (0.0000)	grad_norm 0.2849 (0.7656)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4715
[32m[2023-01-07 00:47:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2799/54685]	eta 8:24:16 lr 0.0009999992	 wd 0.1000	time 0.6961 (0.5831)	loss 0.6071 (0.6200)	loss-cls 9.7131 (9.9198)	loss-aux 0.0000 (0.0000)	grad_norm 0.3020 (0.7630)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4879
[32m[2023-01-07 00:47:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2815/54685]	eta 8:24:08 lr 0.0009999992	 wd 0.1000	time 0.6694 (0.5832)	loss 0.6006 (0.6200)	loss-cls 9.6101 (9.9192)	loss-aux 0.0000 (0.0000)	grad_norm 0.2656 (0.7601)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3976
[32m[2023-01-07 00:47:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2831/54685]	eta 8:24:00 lr 0.0009999992	 wd 0.1000	time 0.6945 (0.5832)	loss 0.6129 (0.6199)	loss-cls 9.8071 (9.9191)	loss-aux 0.0000 (0.0000)	grad_norm 0.2584 (0.7573)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4166
[32m[2023-01-07 00:47:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2847/54685]	eta 8:23:53 lr 0.0009999992	 wd 0.1000	time 0.6783 (0.5832)	loss 0.6221 (0.6199)	loss-cls 9.9534 (9.9187)	loss-aux 0.0000 (0.0000)	grad_norm 0.3112 (0.7548)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4213
[32m[2023-01-07 00:48:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2863/54685]	eta 8:23:44 lr 0.0009999992	 wd 0.1000	time 0.6956 (0.5832)	loss 0.6177 (0.6199)	loss-cls 9.8838 (9.9183)	loss-aux 0.0000 (0.0000)	grad_norm 0.2997 (0.7522)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3930
[32m[2023-01-07 00:48:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2879/54685]	eta 8:23:38 lr 0.0009999992	 wd 0.1000	time 0.6839 (0.5833)	loss 0.6179 (0.6199)	loss-cls 9.8865 (9.9179)	loss-aux 0.0000 (0.0000)	grad_norm 0.2507 (0.7495)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.5123
[32m[2023-01-07 00:48:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2895/54685]	eta 8:23:28 lr 0.0009999992	 wd 0.1000	time 0.6697 (0.5833)	loss 0.6187 (0.6198)	loss-cls 9.8996 (9.9174)	loss-aux 0.0000 (0.0000)	grad_norm 0.2858 (0.7469)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2939
[32m[2023-01-07 00:48:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2911/54685]	eta 8:23:19 lr 0.0009999992	 wd 0.1000	time 0.6962 (0.5833)	loss 0.6114 (0.6198)	loss-cls 9.7816 (9.9171)	loss-aux 0.0000 (0.0000)	grad_norm 0.2371 (0.7441)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3222
[32m[2023-01-07 00:48:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2927/54685]	eta 8:23:08 lr 0.0009999991	 wd 0.1000	time 0.6906 (0.5833)	loss 0.6158 (0.6198)	loss-cls 9.8521 (9.9166)	loss-aux 0.0000 (0.0000)	grad_norm 0.2994 (0.7417)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2621
[32m[2023-01-07 00:48:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2943/54685]	eta 8:22:59 lr 0.0009999991	 wd 0.1000	time 0.6835 (0.5833)	loss 0.6181 (0.6198)	loss-cls 9.8895 (9.9162)	loss-aux 0.0000 (0.0000)	grad_norm 0.2532 (0.7390)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3459
[32m[2023-01-07 00:49:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2959/54685]	eta 8:22:51 lr 0.0009999991	 wd 0.1000	time 0.7003 (0.5833)	loss 0.6100 (0.6197)	loss-cls 9.7605 (9.9156)	loss-aux 0.0000 (0.0000)	grad_norm 0.2747 (0.7365)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3903
[32m[2023-01-07 00:49:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2975/54685]	eta 8:22:43 lr 0.0009999991	 wd 0.1000	time 0.6887 (0.5833)	loss 0.6255 (0.6197)	loss-cls 10.0082 (9.9154)	loss-aux 0.0000 (0.0000)	grad_norm 0.3027 (0.7342)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4030
[32m[2023-01-07 00:49:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2991/54685]	eta 8:22:34 lr 0.0009999991	 wd 0.1000	time 0.6938 (0.5833)	loss 0.6127 (0.6197)	loss-cls 9.8029 (9.9150)	loss-aux 0.0000 (0.0000)	grad_norm 0.2759 (0.7317)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3906
[32m[2023-01-07 00:49:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3007/54685]	eta 8:22:27 lr 0.0009999991	 wd 0.1000	time 0.7008 (0.5834)	loss 0.6030 (0.6197)	loss-cls 9.6479 (9.9147)	loss-aux 0.0000 (0.0000)	grad_norm 0.2647 (0.7292)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4522
[32m[2023-01-07 00:49:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3023/54685]	eta 8:22:20 lr 0.0009999991	 wd 0.1000	time 0.7105 (0.5834)	loss 0.6260 (0.6196)	loss-cls 10.0159 (9.9142)	loss-aux 0.0000 (0.0000)	grad_norm 0.3440 (0.7272)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4495
[32m[2023-01-07 00:49:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3039/54685]	eta 8:22:10 lr 0.0009999991	 wd 0.1000	time 0.6747 (0.5834)	loss 0.6220 (0.6196)	loss-cls 9.9527 (9.9138)	loss-aux 0.0000 (0.0000)	grad_norm 0.2342 (0.7246)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3449
[32m[2023-01-07 00:49:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3055/54685]	eta 8:22:03 lr 0.0009999991	 wd 0.1000	time 0.6948 (0.5834)	loss 0.6063 (0.6196)	loss-cls 9.7010 (9.9133)	loss-aux 0.0000 (0.0000)	grad_norm 0.1948 (0.7218)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4454
[32m[2023-01-07 00:50:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3071/54685]	eta 8:21:55 lr 0.0009999991	 wd 0.1000	time 0.6767 (0.5835)	loss 0.6053 (0.6196)	loss-cls 9.6855 (9.9128)	loss-aux 0.0000 (0.0000)	grad_norm 0.2250 (0.7192)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3846
[32m[2023-01-07 00:50:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3087/54685]	eta 8:21:45 lr 0.0009999990	 wd 0.1000	time 0.6657 (0.5835)	loss 0.6105 (0.6195)	loss-cls 9.7679 (9.9126)	loss-aux 0.0000 (0.0000)	grad_norm 0.1944 (0.7165)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2961
[32m[2023-01-07 00:50:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3103/54685]	eta 8:21:35 lr 0.0009999990	 wd 0.1000	time 0.6948 (0.5834)	loss 0.6044 (0.6195)	loss-cls 9.6698 (9.9119)	loss-aux 0.0000 (0.0000)	grad_norm 0.2439 (0.7141)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.3213
[32m[2023-01-07 00:50:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3119/54685]	eta 8:21:24 lr 0.0009999990	 wd 0.1000	time 0.6679 (0.5834)	loss 0.6109 (0.6195)	loss-cls 9.7746 (9.9114)	loss-aux 0.0000 (0.0000)	grad_norm 0.2645 (0.7118)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2405
[32m[2023-01-07 00:50:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3135/54685]	eta 8:21:16 lr 0.0009999990	 wd 0.1000	time 0.6828 (0.5834)	loss 0.6129 (0.6194)	loss-cls 9.8068 (9.9110)	loss-aux 0.0000 (0.0000)	grad_norm 0.2428 (0.7094)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4269
[32m[2023-01-07 00:50:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3151/54685]	eta 8:21:06 lr 0.0009999990	 wd 0.1000	time 0.6846 (0.5834)	loss 0.6276 (0.6194)	loss-cls 10.0423 (9.9108)	loss-aux 0.0000 (0.0000)	grad_norm 0.2788 (0.7072)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2740
[32m[2023-01-07 00:51:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3167/54685]	eta 8:20:55 lr 0.0009999990	 wd 0.1000	time 0.6885 (0.5834)	loss 0.6331 (0.6194)	loss-cls 10.1291 (9.9105)	loss-aux 0.0000 (0.0000)	grad_norm 0.2534 (0.7049)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2663
[32m[2023-01-07 00:51:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3183/54685]	eta 8:20:45 lr 0.0009999990	 wd 0.1000	time 0.6711 (0.5834)	loss 0.6107 (0.6194)	loss-cls 9.7707 (9.9101)	loss-aux 0.0000 (0.0000)	grad_norm 0.2643 (0.7027)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.2615
[32m[2023-01-07 00:51:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3199/54685]	eta 8:20:37 lr 0.0009999990	 wd 0.1000	time 0.6918 (0.5834)	loss 0.6033 (0.6193)	loss-cls 9.6522 (9.9095)	loss-aux 0.0000 (0.0000)	grad_norm 0.2659 (0.7005)	loss_scale 65536.0000 (65536.0000)	mem 10910MB	batch_time 9.4516
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15455.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11766.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2852.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12064.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22581.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25408.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33630.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7411.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22698.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19005.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28595.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_3493.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30043.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22995.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7953.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13103.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13306.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28288.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29695.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03957420/n03957420_33553.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10675.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17471.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5051.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7013.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15811.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_16320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9041.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2040.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6236.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2322.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13419.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10242.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11827.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9215.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7072.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7996.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6567.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6710.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_21756.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8812.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10586.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_1796.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6247.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22166.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22980.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13244.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5664.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_16737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22719.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n06470073/n06470073_47249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25750.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15830.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12757.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_15341.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27611.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4456.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15480.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4524.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8873.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_32625.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8925.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27627.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13516.JPEG
[32m[2023-01-07 00:51:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 282)[0m: INFO EPOCH 0 training takes 0:31:08
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33623.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18350.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33259.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27578.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27317.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9819.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25530.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_26924.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7365.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11816.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_9566.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27296.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25717.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_135.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14020.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19281.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9068.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8783.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_4539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9031.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27412.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13641.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15697.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12206.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8645.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30926.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9282.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13871.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_11746.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8806.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9401.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12231.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13261.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12142.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_14701.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6850.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6669.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20118.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_16430.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_7026.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29025.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13396.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17877.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n02368116/n02368116_318.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18729.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_10353.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27221.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_1914.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24544.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24434.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12740.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7728.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_34297.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14718.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10175248/n10175248_583.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24638.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12108.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_23316.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12364.JPEG
