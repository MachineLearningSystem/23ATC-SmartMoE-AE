+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102395
++ tr = ' '
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ scontrol show JobId=102395
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ tr = ' '
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
++ awk '{print $2}'
+ '[' nico == sh-lab ']'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102395
++ scontrol show JobId=102395
++ tr = ' '
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ MASTER_ADDR=nico1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ '[' nico == nico ']'
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ export RANK=10
+ RANK=10
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ export NODE_RANK=5
+ NODE_RANK=5
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MASTER_ADDR=nico1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ true
+ MASTER_ADDR=nico1
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=2
+ NNODES=2
+ export RANK=12
+ RANK=12
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args=
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ DATASET_PREFIX=/mnt/znvme/zms
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ true
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' EP+DP == EP+DP ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ export MASTER_ADDR=nico1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ MASTER_ADDR=nico1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ python_args+='
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ localrank=6
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ export RANK=0
+ RANK=0
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ export MASTER_ADDR=nico1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ MASTER_ADDR=nico1
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ export RANK=2
+ RANK=2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ export RANK=4
+ RANK=4
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NNODES=2
+ NNODES=2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ export NNODES=2
+ NNODES=2
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ export NODE_RANK=1
+ NODE_RANK=1
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ export NNODES=2
+ NNODES=2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MASTER_ADDR=nico1
+ true
+ MASTER_ADDR=nico1
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ export NNODES=2
+ NNODES=2
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ python_args=
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof/table
+ python_args=
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_naive/Swin-MoE-B-16MoE_naive4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-07T03:37:48+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_naive --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 0/16
[32m[2023-01-07 03:38:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 403)[0m: INFO Full config saved to /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default/config.json
[32m[2023-01-07 03:38:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 406)[0m: INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: part
  DATASET: imagenet22K
  DATA_PATH: /mnt/znvme/dataset/imagenet22k
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 0.0
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    GATE_NOISE: 1.0
    INIT_STD: 0.005
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: false
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - 1
      - 3
      - 5
      - 7
      - 9
      - 11
      - 13
      - 15
      - 17
    - - 1
    MOE_DROP: 0.1
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 2
    USE_BPR: true
    WINDOW_SIZE: 12
  TYPE: swin_fastmoe
OUTPUT: /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default
PRINT_FREQ: 1
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 16
  AUTO_RESUME: true
  BASE_LR: 0.001
  CHECKPOINT_MODE: full
  CLIP_GRAD: 3.0
  EPOCHS: 90
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.1

[32m[2023-01-07 03:38:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 407)[0m: INFO {"cfg": "configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml", "opts": null, "batch_size": 16, "data_path": "/mnt/znvme/dataset/imagenet22k", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": 16, "use_checkpoint": false, "checkpoint_mode": "full", "disable_amp": false, "amp_opt_level": null, "output": "/home/zms/model_training/Auto-Megatron/logs/swin_naive", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": true, "fused_layernorm": false, "optim": null, "num_experts": 1, "top_k": 2, "balance_strategy": "naive", "expert_parallel_strategy": "EP+DP", "expert_ep_size": 16, "expert_dp_size": 1, "dump": false, "dynamic_placement": false, "dynamic_freq": 10, "new_shadow": false, "gshard_cap": 4.8, "init_method_std": 0.002, "num_layers": 12}
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
local rank 0 / global rank 0 successfully build train dataset
local rank 0 / global rank 0 successfully build val dataset
[32m[2023-01-07 03:38:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 120)[0m: INFO Creating model:swin_fastmoe/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 14/16
local rank 0 / global rank 14 successfully build train dataset
local rank 0 / global rank 14 successfully build val dataset
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 6/16
local rank 0 / global rank 6 successfully build train dataset
local rank 0 / global rank 6 successfully build val dataset
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 13/16
local rank 0 / global rank 13 successfully build train dataset
local rank 0 / global rank 13 successfully build val dataset
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0]
[32m[2023-01-07 03:38:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 122)[0m: INFO SwinTransformerFastMoE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(48, 48), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(48, 48), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(24, 24), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(24, 24), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(12, 12), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(12, 12), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(6, 6), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=4096, out_features=1024, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 1024, hidden_features = 4096, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=4096, out_features=16, bias=True)
              )
            )
          )
        )
      )
    )
  )
  (norm): TimerModule(
    (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (avgpool): TimerModule(
    (model): AdaptiveAvgPool1d(output_size=1)
  )
  (head): TimerModule(
    (model): Linear(in_features=1024, out_features=21841, bias=True)
  )
)
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 2/16
local rank 0 / global rank 2 successfully build train dataset
local rank 0 / global rank 2 successfully build val dataset
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 9/16
local rank 0 / global rank 9 successfully build train dataset
local rank 0 / global rank 9 successfully build val dataset
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 12/16
local rank 0 / global rank 12 successfully build train dataset
local rank 0 / global rank 12 successfully build val dataset
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 7/16
local rank 0 / global rank 7 successfully build train dataset
local rank 0 / global rank 7 successfully build val dataset
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 10/16
local rank 0 / global rank 10 successfully build train dataset
local rank 0 / global rank 10 successfully build val dataset
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 15/16
local rank 0 / global rank 15 successfully build train dataset
local rank 0 / global rank 15 successfully build val dataset
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 5/16
local rank 0 / global rank 5 successfully build train dataset
local rank 0 / global rank 5 successfully build val dataset
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 1/16
local rank 0 / global rank 1 successfully build train dataset
local rank 0 / global rank 1 successfully build val dataset
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 4/16
local rank 0 / global rank 4 successfully build train dataset
local rank 0 / global rank 4 successfully build val dataset
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 8/16
local rank 0 / global rank 8 successfully build train dataset
local rank 0 / global rank 8 successfully build val dataset
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-07 03:38:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 156)[0m: INFO no checkpoint found in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default, ignoring auto resume
[32m[2023-01-07 03:38:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 176)[0m: INFO Start training
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 3/16
local rank 0 / global rank 3 successfully build train dataset
local rank 0 / global rank 3 successfully build val dataset
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 11/16
local rank 0 / global rank 11 successfully build train dataset
local rank 0 / global rank 11 successfully build val dataset
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_naive/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-07 03:38:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][15/54685]	eta 15:44:42 lr 0.0010000000	 wd 0.1000	time 0.8425 (1.0368)	loss 0.6247 (0.6245)	loss-cls 9.9954 (9.9918)	loss-aux 0.0000 (0.0000)	grad_norm 2.0798 (2.0798)	loss_scale 65536.0000 (65536.0000)	mem 5176MB	batch_time 16.5889
[32m[2023-01-07 03:39:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][31/54685]	eta 12:18:42 lr 0.0010000000	 wd 0.1000	time 0.6818 (0.8110)	loss 0.6247 (0.6248)	loss-cls 9.9955 (9.9971)	loss-aux 0.0000 (0.0000)	grad_norm 1.7026 (1.8912)	loss_scale 65536.0000 (65536.0000)	mem 7494MB	batch_time 9.3620
[32m[2023-01-07 03:39:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][47/54685]	eta 11:10:21 lr 0.0010000000	 wd 0.1000	time 0.6660 (0.7362)	loss 0.6254 (0.6249)	loss-cls 10.0060 (9.9978)	loss-aux 0.0000 (0.0000)	grad_norm 1.3446 (1.7090)	loss_scale 65536.0000 (65536.0000)	mem 7500MB	batch_time 9.3843
[32m[2023-01-07 03:39:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][63/54685]	eta 10:36:30 lr 0.0010000000	 wd 0.1000	time 0.6587 (0.6992)	loss 0.6286 (0.6250)	loss-cls 10.0575 (10.0000)	loss-aux 0.0000 (0.0000)	grad_norm 1.3868 (1.6284)	loss_scale 65536.0000 (65536.0000)	mem 7500MB	batch_time 9.4126
[32m[2023-01-07 03:39:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][79/54685]	eta 10:16:30 lr 0.0010000000	 wd 0.1000	time 0.6836 (0.6774)	loss 0.6242 (0.6250)	loss-cls 9.9873 (9.9993)	loss-aux 0.0000 (0.0000)	grad_norm 1.3216 (1.5671)	loss_scale 65536.0000 (65536.0000)	mem 7511MB	batch_time 9.4441
[32m[2023-01-07 03:39:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][95/54685]	eta 10:03:08 lr 0.0010000000	 wd 0.1000	time 0.6774 (0.6629)	loss 0.6233 (0.6249)	loss-cls 9.9733 (9.9981)	loss-aux 0.0000 (0.0000)	grad_norm 1.2677 (1.5172)	loss_scale 65536.0000 (65536.0000)	mem 7514MB	batch_time 9.4472
[32m[2023-01-07 03:39:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][111/54685]	eta 9:53:53 lr 0.0010000000	 wd 0.1000	time 0.6746 (0.6529)	loss 0.6255 (0.6249)	loss-cls 10.0087 (9.9985)	loss-aux 0.0000 (0.0000)	grad_norm 1.1996 (1.4718)	loss_scale 65536.0000 (65536.0000)	mem 7514MB	batch_time 9.4898
[32m[2023-01-07 03:40:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][127/54685]	eta 9:45:29 lr 0.0010000000	 wd 0.1000	time 0.6370 (0.6439)	loss 0.6276 (0.6248)	loss-cls 10.0418 (9.9960)	loss-aux 0.0000 (0.0000)	grad_norm 1.3003 (1.4504)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 9.2893
[32m[2023-01-07 03:40:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][143/54685]	eta 9:40:26 lr 0.0010000000	 wd 0.1000	time 0.6718 (0.6385)	loss 0.6251 (0.6248)	loss-cls 10.0011 (9.9967)	loss-aux 0.0000 (0.0000)	grad_norm 1.3220 (1.4361)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 9.5299
[32m[2023-01-07 03:40:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][159/54685]	eta 9:36:55 lr 0.0010000000	 wd 0.1000	time 0.6775 (0.6348)	loss 0.6260 (0.6247)	loss-cls 10.0160 (9.9959)	loss-aux 0.0000 (0.0000)	grad_norm 1.3154 (1.4240)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 9.6259
[32m[2023-01-07 03:40:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][175/54685]	eta 9:33:36 lr 0.0010000000	 wd 0.1000	time 0.6658 (0.6314)	loss 0.6284 (0.6247)	loss-cls 10.0550 (9.9956)	loss-aux 0.0000 (0.0000)	grad_norm 1.2912 (1.4120)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 9.5481
[32m[2023-01-07 03:40:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][191/54685]	eta 9:32:17 lr 0.0010000000	 wd 0.1000	time 0.6560 (0.6301)	loss 0.6314 (0.6247)	loss-cls 10.1030 (9.9952)	loss-aux 0.0000 (0.0000)	grad_norm 1.3690 (1.4084)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 9.8606
[32m[2023-01-07 03:40:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][207/54685]	eta 9:29:57 lr 0.0010000000	 wd 0.1000	time 0.6781 (0.6277)	loss 0.6284 (0.6245)	loss-cls 10.0541 (9.9927)	loss-aux 0.0000 (0.0000)	grad_norm 1.2995 (1.4000)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 9.5849
[32m[2023-01-07 03:40:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][223/54685]	eta 9:28:00 lr 0.0010000000	 wd 0.1000	time 0.6498 (0.6258)	loss 0.6260 (0.6248)	loss-cls 10.0163 (9.9970)	loss-aux 0.0000 (0.0000)	grad_norm 1.2867 (1.3919)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 9.6026
[32m[2023-01-07 03:41:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][239/54685]	eta 9:29:29 lr 0.0010000000	 wd 0.1000	time 0.6701 (0.6276)	loss 0.6255 (0.6248)	loss-cls 10.0077 (9.9966)	loss-aux 0.0000 (0.0000)	grad_norm 1.2334 (1.3814)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 10.4483
[32m[2023-01-07 03:41:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][255/54685]	eta 9:28:05 lr 0.0010000000	 wd 0.1000	time 0.6269 (0.6262)	loss 0.6210 (0.6248)	loss-cls 9.9368 (9.9965)	loss-aux 0.0000 (0.0000)	grad_norm 1.2830 (1.3752)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 9.6935
[32m[2023-01-07 03:41:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][271/54685]	eta 9:26:58 lr 0.0010000000	 wd 0.1000	time 0.6939 (0.6252)	loss 0.6293 (0.6248)	loss-cls 10.0684 (9.9965)	loss-aux 0.0000 (0.0000)	grad_norm 1.2002 (1.3649)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 9.7346
[32m[2023-01-07 03:41:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][287/54685]	eta 9:26:46 lr 0.0010000000	 wd 0.1000	time 0.6779 (0.6251)	loss 0.6314 (0.6249)	loss-cls 10.1022 (9.9981)	loss-aux 0.0000 (0.0000)	grad_norm 1.2309 (1.3575)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 9.9934
[32m[2023-01-07 03:41:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][303/54685]	eta 9:25:30 lr 0.0010000000	 wd 0.1000	time 0.6861 (0.6239)	loss 0.6279 (0.6247)	loss-cls 10.0469 (9.9958)	loss-aux 0.0000 (0.0000)	grad_norm 1.1995 (1.3491)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 9.6348
[32m[2023-01-07 03:41:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][319/54685]	eta 9:23:48 lr 0.0010000000	 wd 0.1000	time 0.6595 (0.6222)	loss 0.6301 (0.6248)	loss-cls 10.0814 (9.9973)	loss-aux 0.0000 (0.0000)	grad_norm 1.2236 (1.3429)	loss_scale 65536.0000 (65536.0000)	mem 7558MB	batch_time 9.4402
[32m[2023-01-07 03:42:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][335/54685]	eta 9:22:54 lr 0.0010000000	 wd 0.1000	time 0.6557 (0.6214)	loss 0.6275 (0.6249)	loss-cls 10.0396 (9.9982)	loss-aux 0.0000 (0.0000)	grad_norm 1.2254 (1.3373)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.6811
[32m[2023-01-07 03:42:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][351/54685]	eta 9:21:49 lr 0.0010000000	 wd 0.1000	time 0.6880 (0.6204)	loss 0.6233 (0.6249)	loss-cls 9.9729 (9.9979)	loss-aux 0.0000 (0.0000)	grad_norm 1.1715 (1.3297)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.5892
[32m[2023-01-07 03:42:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][367/54685]	eta 9:20:55 lr 0.0010000000	 wd 0.1000	time 0.6585 (0.6196)	loss 0.6341 (0.6249)	loss-cls 10.1455 (9.9990)	loss-aux 0.0000 (0.0000)	grad_norm 1.1657 (1.3226)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.6259
[32m[2023-01-07 03:42:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][383/54685]	eta 9:20:26 lr 0.0010000000	 wd 0.1000	time 0.6767 (0.6192)	loss 0.6182 (0.6250)	loss-cls 9.8911 (10.0006)	loss-aux 0.0000 (0.0000)	grad_norm 1.2218 (1.3184)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.7779
[32m[2023-01-07 03:42:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][399/54685]	eta 9:20:22 lr 0.0010000000	 wd 0.1000	time 0.6777 (0.6194)	loss 0.6251 (0.6251)	loss-cls 10.0010 (10.0014)	loss-aux 0.0000 (0.0000)	grad_norm 1.1303 (1.3109)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.9543
[32m[2023-01-07 03:42:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][415/54685]	eta 9:19:30 lr 0.0010000000	 wd 0.1000	time 0.6808 (0.6186)	loss 0.6203 (0.6252)	loss-cls 9.9245 (10.0031)	loss-aux 0.0000 (0.0000)	grad_norm 1.0983 (1.3027)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.5836
[32m[2023-01-07 03:43:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][431/54685]	eta 9:18:54 lr 0.0010000000	 wd 0.1000	time 0.6688 (0.6181)	loss 0.6264 (0.6253)	loss-cls 10.0222 (10.0051)	loss-aux 0.0000 (0.0000)	grad_norm 1.2059 (1.2991)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.6928
[32m[2023-01-07 03:43:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][447/54685]	eta 9:18:14 lr 0.0010000000	 wd 0.1000	time 0.6779 (0.6176)	loss 0.6238 (0.6253)	loss-cls 9.9802 (10.0044)	loss-aux 0.0000 (0.0000)	grad_norm 1.1619 (1.2942)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.6432
[32m[2023-01-07 03:43:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][463/54685]	eta 9:17:51 lr 0.0010000000	 wd 0.1000	time 0.6833 (0.6173)	loss 0.6314 (0.6253)	loss-cls 10.1030 (10.0048)	loss-aux 0.0000 (0.0000)	grad_norm 1.1848 (1.2904)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.7681
[32m[2023-01-07 03:43:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][479/54685]	eta 9:17:37 lr 0.0010000000	 wd 0.1000	time 0.6266 (0.6172)	loss 0.6204 (0.6253)	loss-cls 9.9269 (10.0054)	loss-aux 0.0000 (0.0000)	grad_norm 1.1503 (1.2858)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.8408
[32m[2023-01-07 03:43:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][495/54685]	eta 9:17:23 lr 0.0010000000	 wd 0.1000	time 0.6822 (0.6171)	loss 0.6234 (0.6253)	loss-cls 9.9744 (10.0051)	loss-aux 0.0000 (0.0000)	grad_norm 1.0834 (1.2792)	loss_scale 65536.0000 (65536.0000)	mem 7602MB	batch_time 9.8317
[32m[2023-01-07 03:43:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][511/54685]	eta 9:17:44 lr 0.0010000000	 wd 0.1000	time 0.6829 (0.6177)	loss 0.6274 (0.6254)	loss-cls 10.0380 (10.0067)	loss-aux 0.0000 (0.0000)	grad_norm 1.1505 (1.2752)	loss_scale 65536.0000 (65536.0000)	mem 7612MB	batch_time 10.1720
[32m[2023-01-07 03:44:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][527/54685]	eta 9:17:23 lr 0.0010000000	 wd 0.1000	time 0.6801 (0.6175)	loss 0.6227 (0.6254)	loss-cls 9.9638 (10.0068)	loss-aux 0.0000 (0.0000)	grad_norm 1.1175 (1.2704)	loss_scale 65536.0000 (65536.0000)	mem 7612MB	batch_time 9.7753
[32m[2023-01-07 03:44:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][543/54685]	eta 9:17:12 lr 0.0010000000	 wd 0.1000	time 0.7004 (0.6175)	loss 0.6304 (0.6255)	loss-cls 10.0861 (10.0074)	loss-aux 0.0000 (0.0000)	grad_norm 1.5198 (1.2778)	loss_scale 65536.0000 (65536.0000)	mem 7612MB	batch_time 9.8695
[32m[2023-01-07 03:44:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][559/54685]	eta 9:16:50 lr 0.0010000000	 wd 0.1000	time 0.6883 (0.6173)	loss 0.6223 (0.6255)	loss-cls 9.9571 (10.0073)	loss-aux 0.0000 (0.0000)	grad_norm 1.2281 (1.2764)	loss_scale 65536.0000 (65536.0000)	mem 7612MB	batch_time 9.7484
[32m[2023-01-07 03:44:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][575/54685]	eta 9:16:35 lr 0.0010000000	 wd 0.1000	time 0.6945 (0.6172)	loss 0.6302 (0.6254)	loss-cls 10.0828 (10.0069)	loss-aux 0.0000 (0.0000)	grad_norm 1.1940 (1.2741)	loss_scale 65536.0000 (65536.0000)	mem 7612MB	batch_time 9.8222
[32m[2023-01-07 03:44:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][591/54685]	eta 9:16:18 lr 0.0010000000	 wd 0.1000	time 0.7535 (0.6171)	loss 0.6174 (0.6254)	loss-cls 9.8790 (10.0069)	loss-aux 0.0000 (0.0000)	grad_norm 1.2041 (1.2722)	loss_scale 65536.0000 (65536.0000)	mem 7612MB	batch_time 9.8032
[32m[2023-01-07 03:44:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][607/54685]	eta 9:16:06 lr 0.0010000000	 wd 0.1000	time 0.6749 (0.6170)	loss 0.6383 (0.6254)	loss-cls 10.2124 (10.0066)	loss-aux 0.0000 (0.0000)	grad_norm 1.1949 (1.2701)	loss_scale 65536.0000 (65536.0000)	mem 7616MB	batch_time 9.8512
[32m[2023-01-07 03:45:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][623/54685]	eta 9:15:58 lr 0.0010000000	 wd 0.1000	time 0.7030 (0.6170)	loss 0.6262 (0.6254)	loss-cls 10.0192 (10.0062)	loss-aux 0.0000 (0.0000)	grad_norm 2.1857 (1.2936)	loss_scale 65536.0000 (65536.0000)	mem 7651MB	batch_time 9.8882
[32m[2023-01-07 03:45:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][639/54685]	eta 9:15:42 lr 0.0010000000	 wd 0.1000	time 0.6929 (0.6169)	loss 0.6221 (0.6254)	loss-cls 9.9533 (10.0063)	loss-aux 0.0000 (0.0000)	grad_norm 1.2375 (1.2922)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 9.7980
[32m[2023-01-07 03:45:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][655/54685]	eta 9:15:31 lr 0.0010000000	 wd 0.1000	time 0.6848 (0.6169)	loss 0.6350 (0.6254)	loss-cls 10.1594 (10.0068)	loss-aux 0.0000 (0.0000)	grad_norm 1.2131 (1.2903)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 9.8537
[32m[2023-01-07 03:45:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][671/54685]	eta 9:15:25 lr 0.0010000000	 wd 0.1000	time 0.6935 (0.6170)	loss 0.6209 (0.6255)	loss-cls 9.9346 (10.0078)	loss-aux 0.0000 (0.0000)	grad_norm 1.2458 (1.2892)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 9.9190
[32m[2023-01-07 03:45:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][687/54685]	eta 9:16:38 lr 0.0010000000	 wd 0.1000	time 0.7117 (0.6185)	loss 0.6247 (0.6256)	loss-cls 9.9957 (10.0092)	loss-aux 0.0000 (0.0000)	grad_norm 1.1975 (1.2871)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.9345
[32m[2023-01-07 03:45:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][703/54685]	eta 9:16:49 lr 0.0010000000	 wd 0.1000	time 0.6960 (0.6189)	loss 0.6280 (0.6256)	loss-cls 10.0485 (10.0098)	loss-aux 0.0000 (0.0000)	grad_norm 1.9059 (1.3012)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.1635
[32m[2023-01-07 03:46:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][719/54685]	eta 9:17:29 lr 0.0009999999	 wd 0.1000	time 0.6703 (0.6198)	loss 0.6367 (0.6256)	loss-cls 10.1874 (10.0096)	loss-aux 0.0000 (0.0000)	grad_norm 1.2513 (1.3001)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5764
[32m[2023-01-07 03:46:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][735/54685]	eta 9:17:41 lr 0.0009999999	 wd 0.1000	time 0.6844 (0.6202)	loss 0.6300 (0.6256)	loss-cls 10.0803 (10.0093)	loss-aux 0.0000 (0.0000)	grad_norm 1.1639 (1.2971)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.2064
[32m[2023-01-07 03:46:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][751/54685]	eta 9:17:45 lr 0.0009999999	 wd 0.1000	time 0.7062 (0.6205)	loss 0.6206 (0.6256)	loss-cls 9.9291 (10.0091)	loss-aux 0.0000 (0.0000)	grad_norm 1.2841 (1.2968)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.1223
[32m[2023-01-07 03:46:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][767/54685]	eta 9:17:59 lr 0.0009999999	 wd 0.1000	time 0.6818 (0.6209)	loss 0.6294 (0.6256)	loss-cls 10.0712 (10.0088)	loss-aux 0.0000 (0.0000)	grad_norm 1.3998 (1.2990)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.2625
[32m[2023-01-07 03:46:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][783/54685]	eta 9:18:20 lr 0.0009999999	 wd 0.1000	time 0.7088 (0.6215)	loss 0.6286 (0.6255)	loss-cls 10.0571 (10.0082)	loss-aux 0.0000 (0.0000)	grad_norm 1.4864 (1.3028)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3920
[32m[2023-01-07 03:46:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][799/54685]	eta 9:18:44 lr 0.0009999999	 wd 0.1000	time 0.7470 (0.6221)	loss 0.6226 (0.6255)	loss-cls 9.9623 (10.0080)	loss-aux 0.0000 (0.0000)	grad_norm 1.2640 (1.3020)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4488
[32m[2023-01-07 03:47:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][815/54685]	eta 9:19:10 lr 0.0009999999	 wd 0.1000	time 0.8104 (0.6228)	loss 0.6326 (0.6255)	loss-cls 10.1213 (10.0076)	loss-aux 0.0000 (0.0000)	grad_norm 1.2849 (1.3017)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5026
[32m[2023-01-07 03:47:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][831/54685]	eta 9:19:22 lr 0.0009999999	 wd 0.1000	time 0.6955 (0.6232)	loss 0.6077 (0.6255)	loss-cls 9.7237 (10.0080)	loss-aux 0.0000 (0.0000)	grad_norm 1.3155 (1.3019)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.2967
[32m[2023-01-07 03:47:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][847/54685]	eta 9:19:36 lr 0.0009999999	 wd 0.1000	time 0.7035 (0.6237)	loss 0.6138 (0.6256)	loss-cls 9.8202 (10.0095)	loss-aux 0.0000 (0.0000)	grad_norm 1.2383 (1.3007)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3459
[32m[2023-01-07 03:47:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][863/54685]	eta 9:20:00 lr 0.0009999999	 wd 0.1000	time 0.7876 (0.6243)	loss 0.6304 (0.6256)	loss-cls 10.0858 (10.0098)	loss-aux 0.0000 (0.0000)	grad_norm 1.1780 (1.2985)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5278
[32m[2023-01-07 03:47:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][879/54685]	eta 9:21:06 lr 0.0009999999	 wd 0.1000	time 0.7054 (0.6257)	loss 0.6244 (0.6257)	loss-cls 9.9909 (10.0110)	loss-aux 0.0000 (0.0000)	grad_norm 1.2088 (1.2968)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 11.2399
[32m[2023-01-07 03:47:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][895/54685]	eta 9:21:22 lr 0.0009999999	 wd 0.1000	time 0.7254 (0.6262)	loss 0.6589 (0.6259)	loss-cls 10.5419 (10.0140)	loss-aux 0.0000 (0.0000)	grad_norm 1.1852 (1.2948)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4358
[32m[2023-01-07 03:48:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][911/54685]	eta 9:21:53 lr 0.0009999999	 wd 0.1000	time 0.7305 (0.6269)	loss 0.6285 (0.6260)	loss-cls 10.0561 (10.0163)	loss-aux 0.0000 (0.0000)	grad_norm 1.6036 (1.3003)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.7095
[32m[2023-01-07 03:48:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][927/54685]	eta 9:22:00 lr 0.0009999999	 wd 0.1000	time 0.7210 (0.6273)	loss 0.6210 (0.6260)	loss-cls 9.9359 (10.0162)	loss-aux 0.0000 (0.0000)	grad_norm 1.1367 (1.2974)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3298
[32m[2023-01-07 03:48:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][943/54685]	eta 9:22:09 lr 0.0009999999	 wd 0.1000	time 0.7017 (0.6276)	loss 0.6300 (0.6261)	loss-cls 10.0793 (10.0172)	loss-aux 0.0000 (0.0000)	grad_norm 1.3126 (1.2977)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3646
[32m[2023-01-07 03:48:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][959/54685]	eta 9:22:20 lr 0.0009999999	 wd 0.1000	time 0.7330 (0.6280)	loss 0.6432 (0.6261)	loss-cls 10.2917 (10.0181)	loss-aux 0.0000 (0.0000)	grad_norm 1.3278 (1.2982)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4265
[32m[2023-01-07 03:48:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][975/54685]	eta 9:22:31 lr 0.0009999999	 wd 0.1000	time 0.6937 (0.6284)	loss 0.6172 (0.6262)	loss-cls 9.8755 (10.0186)	loss-aux 0.0000 (0.0000)	grad_norm 1.2460 (1.2973)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4393
[32m[2023-01-07 03:49:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][991/54685]	eta 9:22:39 lr 0.0009999999	 wd 0.1000	time 0.7486 (0.6287)	loss 0.6073 (0.6262)	loss-cls 9.7171 (10.0193)	loss-aux 0.0000 (0.0000)	grad_norm 1.2057 (1.2959)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3803
[32m[2023-01-07 03:49:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1007/54685]	eta 9:22:44 lr 0.0009999999	 wd 0.1000	time 0.7236 (0.6290)	loss 0.6244 (0.6262)	loss-cls 9.9910 (10.0191)	loss-aux 0.0000 (0.0000)	grad_norm 1.1006 (1.2928)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3378
[32m[2023-01-07 03:49:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1023/54685]	eta 9:22:50 lr 0.0009999999	 wd 0.1000	time 0.7208 (0.6293)	loss 0.6125 (0.6262)	loss-cls 9.7999 (10.0191)	loss-aux 0.0000 (0.0000)	grad_norm 1.1193 (1.2901)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3656
[32m[2023-01-07 03:49:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1039/54685]	eta 9:22:48 lr 0.0009999999	 wd 0.1000	time 0.6897 (0.6295)	loss 0.6397 (0.6262)	loss-cls 10.2351 (10.0199)	loss-aux 0.0000 (0.0000)	grad_norm 1.1582 (1.2880)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.2365
[32m[2023-01-07 03:49:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1055/54685]	eta 9:22:45 lr 0.0009999999	 wd 0.1000	time 0.7220 (0.6296)	loss 0.6244 (0.6262)	loss-cls 9.9906 (10.0198)	loss-aux 0.0000 (0.0000)	grad_norm 1.1314 (1.2857)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.2139
[32m[2023-01-07 03:49:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1071/54685]	eta 9:23:21 lr 0.0009999999	 wd 0.1000	time 0.7101 (0.6305)	loss 0.6377 (0.6262)	loss-cls 10.2028 (10.0190)	loss-aux 0.0000 (0.0000)	grad_norm 1.5877 (1.2902)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.9809
[32m[2023-01-07 03:50:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1087/54685]	eta 9:23:32 lr 0.0009999999	 wd 0.1000	time 0.7039 (0.6309)	loss 0.6079 (0.6262)	loss-cls 9.7264 (10.0197)	loss-aux 0.0000 (0.0000)	grad_norm 1.0445 (1.2865)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5315
[32m[2023-01-07 03:50:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1103/54685]	eta 9:23:51 lr 0.0009999999	 wd 0.1000	time 0.7456 (0.6314)	loss 0.6287 (0.6262)	loss-cls 10.0599 (10.0195)	loss-aux 0.0000 (0.0000)	grad_norm 0.9957 (1.2823)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6935
[32m[2023-01-07 03:50:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1119/54685]	eta 9:24:00 lr 0.0009999999	 wd 0.1000	time 0.7181 (0.6318)	loss 0.6290 (0.6263)	loss-cls 10.0639 (10.0203)	loss-aux 0.0000 (0.0000)	grad_norm 1.0514 (1.2790)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5033
[32m[2023-01-07 03:50:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1135/54685]	eta 9:24:12 lr 0.0009999999	 wd 0.1000	time 0.7231 (0.6322)	loss 0.6277 (0.6263)	loss-cls 10.0432 (10.0202)	loss-aux 0.0000 (0.0000)	grad_norm 0.9966 (1.2751)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5614
[32m[2023-01-07 03:50:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1151/54685]	eta 9:24:19 lr 0.0009999999	 wd 0.1000	time 0.7333 (0.6325)	loss 0.6119 (0.6262)	loss-cls 9.7903 (10.0197)	loss-aux 0.0000 (0.0000)	grad_norm 1.0230 (1.2716)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4904
[32m[2023-01-07 03:50:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1167/54685]	eta 9:24:31 lr 0.0009999999	 wd 0.1000	time 0.7227 (0.6329)	loss 0.6236 (0.6263)	loss-cls 9.9783 (10.0204)	loss-aux 0.0000 (0.0000)	grad_norm 0.9653 (1.2674)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6038
[32m[2023-01-07 03:51:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1183/54685]	eta 9:25:14 lr 0.0009999999	 wd 0.1000	time 0.7404 (0.6339)	loss 0.6067 (0.6263)	loss-cls 9.7077 (10.0202)	loss-aux 0.0000 (0.0000)	grad_norm 0.9733 (1.2634)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 11.3104
[32m[2023-01-07 03:51:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1199/54685]	eta 9:25:26 lr 0.0009999999	 wd 0.1000	time 0.7298 (0.6343)	loss 0.6243 (0.6262)	loss-cls 9.9893 (10.0194)	loss-aux 0.0000 (0.0000)	grad_norm 0.9220 (1.2588)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6189
[32m[2023-01-07 03:51:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1215/54685]	eta 9:25:34 lr 0.0009999999	 wd 0.1000	time 0.7362 (0.6346)	loss 0.6350 (0.6262)	loss-cls 10.1597 (10.0199)	loss-aux 0.0000 (0.0000)	grad_norm 0.9361 (1.2546)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5738
[32m[2023-01-07 03:51:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1231/54685]	eta 9:25:43 lr 0.0009999999	 wd 0.1000	time 0.7518 (0.6350)	loss 0.6198 (0.6263)	loss-cls 9.9172 (10.0200)	loss-aux 0.0000 (0.0000)	grad_norm 0.9408 (1.2505)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6044
[32m[2023-01-07 03:51:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1247/54685]	eta 9:25:52 lr 0.0009999998	 wd 0.1000	time 0.6923 (0.6354)	loss 0.6315 (0.6263)	loss-cls 10.1047 (10.0204)	loss-aux 0.0000 (0.0000)	grad_norm 0.9307 (1.2464)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5944
[32m[2023-01-07 03:52:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1263/54685]	eta 9:25:57 lr 0.0009999998	 wd 0.1000	time 0.7196 (0.6356)	loss 0.6197 (0.6262)	loss-cls 9.9157 (10.0196)	loss-aux 0.0000 (0.0000)	grad_norm 0.9214 (1.2423)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5217
[32m[2023-01-07 03:52:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1279/54685]	eta 9:26:05 lr 0.0009999998	 wd 0.1000	time 0.7454 (0.6360)	loss 0.6293 (0.6262)	loss-cls 10.0682 (10.0191)	loss-aux 0.0000 (0.0000)	grad_norm 1.5896 (1.2466)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6207
[32m[2023-01-07 03:52:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1295/54685]	eta 9:26:11 lr 0.0009999998	 wd 0.1000	time 0.7317 (0.6363)	loss 0.6254 (0.6261)	loss-cls 10.0065 (10.0183)	loss-aux 0.0000 (0.0000)	grad_norm 0.9045 (1.2424)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5532
[32m[2023-01-07 03:52:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1311/54685]	eta 9:26:11 lr 0.0009999998	 wd 0.1000	time 0.7355 (0.6365)	loss 0.6058 (0.6261)	loss-cls 9.6921 (10.0173)	loss-aux 0.0000 (0.0000)	grad_norm 0.8392 (1.2375)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4387
[32m[2023-01-07 03:52:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1327/54685]	eta 9:26:12 lr 0.0009999998	 wd 0.1000	time 0.7186 (0.6367)	loss 0.6146 (0.6260)	loss-cls 9.8332 (10.0165)	loss-aux 0.0000 (0.0000)	grad_norm 1.2691 (1.2379)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4697
[32m[2023-01-07 03:52:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1343/54685]	eta 9:26:16 lr 0.0009999998	 wd 0.1000	time 0.7231 (0.6370)	loss 0.6244 (0.6260)	loss-cls 9.9897 (10.0159)	loss-aux 0.0000 (0.0000)	grad_norm 0.8008 (1.2327)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5320
[32m[2023-01-07 03:53:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1359/54685]	eta 9:26:20 lr 0.0009999998	 wd 0.1000	time 0.7093 (0.6372)	loss 0.6250 (0.6259)	loss-cls 9.9997 (10.0149)	loss-aux 0.0000 (0.0000)	grad_norm 0.7738 (1.2273)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5645
[32m[2023-01-07 03:53:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1375/54685]	eta 9:26:18 lr 0.0009999998	 wd 0.1000	time 0.7234 (0.6374)	loss 0.6096 (0.6258)	loss-cls 9.7528 (10.0134)	loss-aux 0.0000 (0.0000)	grad_norm 0.7816 (1.2221)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3986
[32m[2023-01-07 03:53:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1391/54685]	eta 9:26:16 lr 0.0009999998	 wd 0.1000	time 0.7144 (0.6375)	loss 0.6449 (0.6258)	loss-cls 10.3186 (10.0124)	loss-aux 0.0000 (0.0000)	grad_norm 0.8010 (1.2173)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4163
[32m[2023-01-07 03:53:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1407/54685]	eta 9:26:29 lr 0.0009999998	 wd 0.1000	time 0.7461 (0.6380)	loss 0.6218 (0.6257)	loss-cls 9.9491 (10.0118)	loss-aux 0.0000 (0.0000)	grad_norm 0.8319 (1.2129)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.7999
[32m[2023-01-07 03:53:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1423/54685]	eta 9:26:25 lr 0.0009999998	 wd 0.1000	time 0.7214 (0.6381)	loss 0.6230 (0.6257)	loss-cls 9.9683 (10.0113)	loss-aux 0.0000 (0.0000)	grad_norm 0.8065 (1.2083)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3939
[32m[2023-01-07 03:53:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1439/54685]	eta 9:26:40 lr 0.0009999998	 wd 0.1000	time 0.7273 (0.6386)	loss 0.6312 (0.6257)	loss-cls 10.0988 (10.0113)	loss-aux 0.0000 (0.0000)	grad_norm 0.6842 (1.2025)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.8747
[32m[2023-01-07 03:54:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1455/54685]	eta 9:26:40 lr 0.0009999998	 wd 0.1000	time 0.7060 (0.6387)	loss 0.6173 (0.6256)	loss-cls 9.8763 (10.0101)	loss-aux 0.0000 (0.0000)	grad_norm 0.6891 (1.1968)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4987
[32m[2023-01-07 03:54:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1471/54685]	eta 9:26:45 lr 0.0009999998	 wd 0.1000	time 0.7186 (0.6390)	loss 0.6257 (0.6256)	loss-cls 10.0110 (10.0094)	loss-aux 0.0000 (0.0000)	grad_norm 0.7242 (1.1917)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6460
[32m[2023-01-07 03:54:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1487/54685]	eta 9:26:52 lr 0.0009999998	 wd 0.1000	time 0.7668 (0.6394)	loss 0.6400 (0.6256)	loss-cls 10.2396 (10.0089)	loss-aux 0.0000 (0.0000)	grad_norm 0.7364 (1.1868)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.7098
[32m[2023-01-07 03:54:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1503/54685]	eta 9:26:47 lr 0.0009999998	 wd 0.1000	time 0.7264 (0.6394)	loss 0.6294 (0.6255)	loss-cls 10.0700 (10.0083)	loss-aux 0.0000 (0.0000)	grad_norm 0.6997 (1.1816)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3632
[32m[2023-01-07 03:54:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1519/54685]	eta 9:26:40 lr 0.0009999998	 wd 0.1000	time 0.7265 (0.6395)	loss 0.6242 (0.6255)	loss-cls 9.9872 (10.0075)	loss-aux 0.0000 (0.0000)	grad_norm 0.6340 (1.1759)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3397
[32m[2023-01-07 03:55:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1535/54685]	eta 9:26:44 lr 0.0009999998	 wd 0.1000	time 0.7276 (0.6398)	loss 0.6151 (0.6254)	loss-cls 9.8412 (10.0064)	loss-aux 0.0000 (0.0000)	grad_norm 0.6427 (1.1703)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6455
[32m[2023-01-07 03:55:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1551/54685]	eta 9:27:12 lr 0.0009999998	 wd 0.1000	time 0.7083 (0.6405)	loss 0.6256 (0.6253)	loss-cls 10.0091 (10.0055)	loss-aux 0.0000 (0.0000)	grad_norm 0.6642 (1.1651)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 11.3495
[32m[2023-01-07 03:55:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1567/54685]	eta 9:27:02 lr 0.0009999998	 wd 0.1000	time 0.7062 (0.6405)	loss 0.6009 (0.6253)	loss-cls 9.6139 (10.0049)	loss-aux 0.0000 (0.0000)	grad_norm 0.6246 (1.1596)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.2641
[32m[2023-01-07 03:55:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1583/54685]	eta 9:26:55 lr 0.0009999998	 wd 0.1000	time 0.7226 (0.6406)	loss 0.6287 (0.6253)	loss-cls 10.0589 (10.0049)	loss-aux 0.0000 (0.0000)	grad_norm 0.6305 (1.1542)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3336
[32m[2023-01-07 03:55:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1599/54685]	eta 9:26:45 lr 0.0009999997	 wd 0.1000	time 0.7068 (0.6406)	loss 0.6110 (0.6253)	loss-cls 9.7753 (10.0045)	loss-aux 0.0000 (0.0000)	grad_norm 0.6271 (1.1490)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.2556
[32m[2023-01-07 03:55:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1615/54685]	eta 9:26:35 lr 0.0009999997	 wd 0.1000	time 0.7133 (0.6406)	loss 0.6204 (0.6252)	loss-cls 9.9258 (10.0040)	loss-aux 0.0000 (0.0000)	grad_norm 0.6176 (1.1437)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.2657
[32m[2023-01-07 03:56:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1631/54685]	eta 9:26:29 lr 0.0009999997	 wd 0.1000	time 0.7112 (0.6407)	loss 0.6196 (0.6252)	loss-cls 9.9132 (10.0033)	loss-aux 0.0000 (0.0000)	grad_norm 0.7862 (1.1402)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3774
[32m[2023-01-07 03:56:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1647/54685]	eta 9:26:22 lr 0.0009999997	 wd 0.1000	time 0.7299 (0.6407)	loss 0.6154 (0.6252)	loss-cls 9.8463 (10.0027)	loss-aux 0.0000 (0.0000)	grad_norm 1.2389 (1.1412)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3584
[32m[2023-01-07 03:56:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1663/54685]	eta 9:26:19 lr 0.0009999997	 wd 0.1000	time 0.7374 (0.6409)	loss 0.6074 (0.6251)	loss-cls 9.7188 (10.0019)	loss-aux 0.0000 (0.0000)	grad_norm 0.5188 (1.1352)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.4796
[32m[2023-01-07 03:56:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1679/54685]	eta 9:26:13 lr 0.0009999997	 wd 0.1000	time 0.7364 (0.6409)	loss 0.6151 (0.6251)	loss-cls 9.8422 (10.0009)	loss-aux 0.0000 (0.0000)	grad_norm 2.4582 (1.1478)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3715
[32m[2023-01-07 03:56:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1695/54685]	eta 9:26:06 lr 0.0009999997	 wd 0.1000	time 0.7362 (0.6410)	loss 0.6227 (0.6250)	loss-cls 9.9638 (9.9999)	loss-aux 0.0000 (0.0000)	grad_norm 0.5181 (1.1418)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.3762
[32m[2023-01-07 03:56:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1711/54685]	eta 9:26:05 lr 0.0009999997	 wd 0.1000	time 0.7374 (0.6412)	loss 0.6183 (0.6250)	loss-cls 9.8927 (9.9994)	loss-aux 0.0000 (0.0000)	grad_norm 0.5344 (1.1362)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5253
[32m[2023-01-07 03:57:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1727/54685]	eta 9:26:02 lr 0.0009999997	 wd 0.1000	time 0.7294 (0.6413)	loss 0.6090 (0.6249)	loss-cls 9.7439 (9.9982)	loss-aux 0.0000 (0.0000)	grad_norm 0.3803 (1.1292)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5231
[32m[2023-01-07 03:57:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1743/54685]	eta 9:26:04 lr 0.0009999997	 wd 0.1000	time 0.7395 (0.6415)	loss 0.6160 (0.6249)	loss-cls 9.8560 (9.9976)	loss-aux 0.0000 (0.0000)	grad_norm 0.4048 (1.1225)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6426
[32m[2023-01-07 03:57:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1759/54685]	eta 9:26:02 lr 0.0009999997	 wd 0.1000	time 0.7492 (0.6417)	loss 0.6155 (0.6248)	loss-cls 9.8472 (9.9966)	loss-aux 0.0000 (0.0000)	grad_norm 0.6119 (1.1179)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5467
[32m[2023-01-07 03:57:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1775/54685]	eta 9:26:00 lr 0.0009999997	 wd 0.1000	time 0.7430 (0.6419)	loss 0.6066 (0.6247)	loss-cls 9.7060 (9.9954)	loss-aux 0.0000 (0.0000)	grad_norm 0.9095 (1.1160)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5482
[32m[2023-01-07 03:57:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1791/54685]	eta 9:26:02 lr 0.0009999997	 wd 0.1000	time 0.7114 (0.6421)	loss 0.6108 (0.6247)	loss-cls 9.7725 (9.9948)	loss-aux 0.0000 (0.0000)	grad_norm 0.3182 (1.1089)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6798
[32m[2023-01-07 03:57:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1807/54685]	eta 9:26:04 lr 0.0009999997	 wd 0.1000	time 0.7123 (0.6423)	loss 0.6167 (0.6246)	loss-cls 9.8665 (9.9938)	loss-aux 0.0000 (0.0000)	grad_norm 0.4059 (1.1026)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6858
[32m[2023-01-07 03:58:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1823/54685]	eta 9:26:02 lr 0.0009999997	 wd 0.1000	time 0.7256 (0.6425)	loss 0.6083 (0.6245)	loss-cls 9.7322 (9.9925)	loss-aux 0.0000 (0.0000)	grad_norm 0.3891 (1.0964)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5639
[32m[2023-01-07 03:58:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1839/54685]	eta 9:26:02 lr 0.0009999997	 wd 0.1000	time 0.7389 (0.6427)	loss 0.6148 (0.6245)	loss-cls 9.8366 (9.9914)	loss-aux 0.0000 (0.0000)	grad_norm 0.8280 (1.0941)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.6634
[32m[2023-01-07 03:58:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1855/54685]	eta 9:26:09 lr 0.0009999997	 wd 0.1000	time 0.7147 (0.6430)	loss 0.6156 (0.6244)	loss-cls 9.8495 (9.9905)	loss-aux 0.0000 (0.0000)	grad_norm 6.4921 (1.1406)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.8885
[32m[2023-01-07 03:58:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1871/54685]	eta 9:26:06 lr 0.0009999997	 wd 0.1000	time 0.6952 (0.6431)	loss 0.6105 (0.6243)	loss-cls 9.7687 (9.9893)	loss-aux 0.0000 (0.0000)	grad_norm 0.3229 (1.1336)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.5442
[32m[2023-01-07 03:58:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1887/54685]	eta 9:26:18 lr 0.0009999996	 wd 0.1000	time 0.7642 (0.6436)	loss 0.6069 (0.6242)	loss-cls 9.7104 (9.9878)	loss-aux 0.0000 (0.0000)	grad_norm 4.4822 (1.1620)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 11.0672
[32m[2023-01-07 03:59:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1903/54685]	eta 9:26:39 lr 0.0009999996	 wd 0.1000	time 0.7212 (0.6442)	loss 0.6193 (0.6242)	loss-cls 9.9084 (9.9866)	loss-aux 0.0000 (0.0000)	grad_norm 0.3096 (1.1548)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 11.4370
[32m[2023-01-07 03:59:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1919/54685]	eta 9:26:46 lr 0.0009999996	 wd 0.1000	time 0.7283 (0.6445)	loss 0.6125 (0.6241)	loss-cls 9.7995 (9.9858)	loss-aux 0.0000 (0.0000)	grad_norm 0.2818 (1.1475)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.9376
[32m[2023-01-07 03:59:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1935/54685]	eta 9:27:09 lr 0.0009999996	 wd 0.1000	time 0.7180 (0.6451)	loss 0.6059 (0.6241)	loss-cls 9.6942 (9.9849)	loss-aux 0.0000 (0.0000)	grad_norm 0.3415 (1.1409)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 11.5390
[32m[2023-01-07 03:59:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1951/54685]	eta 9:27:13 lr 0.0009999996	 wd 0.1000	time 0.7343 (0.6454)	loss 0.6230 (0.6240)	loss-cls 9.9675 (9.9842)	loss-aux 0.0000 (0.0000)	grad_norm 0.3559 (1.1344)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 10.8506
[32m[2023-01-07 03:59:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1967/54685]	eta 9:27:37 lr 0.0009999996	 wd 0.1000	time 0.7305 (0.6460)	loss 0.6156 (0.6239)	loss-cls 9.8496 (9.9831)	loss-aux 0.0000 (0.0000)	grad_norm 0.3058 (1.1277)	loss_scale 65536.0000 (65536.0000)	mem 7652MB	batch_time 11.5804
[32m[2023-01-07 04:00:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1983/54685]	eta 9:27:46 lr 0.0009999996	 wd 0.1000	time 0.6901 (0.6464)	loss 0.6109 (0.6239)	loss-cls 9.7736 (9.9821)	loss-aux 0.0000 (0.0000)	grad_norm inf (inf)	loss_scale 32768.0000 (65519.4839)	mem 7652MB	batch_time 11.0807
[32m[2023-01-07 04:00:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1999/54685]	eta 9:27:49 lr 0.0009999996	 wd 0.1000	time 0.7506 (0.6466)	loss 0.6115 (0.6238)	loss-cls 9.7837 (9.9812)	loss-aux 0.0000 (0.0000)	grad_norm 0.3481 (inf)	loss_scale 32768.0000 (65257.4720)	mem 7652MB	batch_time 10.8399
[32m[2023-01-07 04:00:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2015/54685]	eta 9:27:54 lr 0.0009999996	 wd 0.1000	time 0.7238 (0.6469)	loss 0.6168 (0.6238)	loss-cls 9.8684 (9.9801)	loss-aux 0.0000 (0.0000)	grad_norm 0.3339 (inf)	loss_scale 32768.0000 (64999.6190)	mem 7652MB	batch_time 10.9383
[32m[2023-01-07 04:00:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2031/54685]	eta 9:27:59 lr 0.0009999996	 wd 0.1000	time 0.7014 (0.6472)	loss 0.6091 (0.6237)	loss-cls 9.7453 (9.9789)	loss-aux 0.0000 (0.0000)	grad_norm 0.2942 (inf)	loss_scale 32768.0000 (64745.8268)	mem 7652MB	batch_time 10.9516
[32m[2023-01-07 04:00:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2047/54685]	eta 9:28:04 lr 0.0009999996	 wd 0.1000	time 0.7305 (0.6475)	loss 0.6190 (0.6237)	loss-cls 9.9035 (9.9785)	loss-aux 0.0000 (0.0000)	grad_norm 2.1522 (inf)	loss_scale 32768.0000 (64496.0000)	mem 7652MB	batch_time 10.9586
[32m[2023-01-07 04:00:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2063/54685]	eta 9:28:02 lr 0.0009999996	 wd 0.1000	time 0.7468 (0.6477)	loss 0.6189 (0.6236)	loss-cls 9.9021 (9.9777)	loss-aux 0.0000 (0.0000)	grad_norm 1.3061 (inf)	loss_scale 32768.0000 (64250.0465)	mem 7652MB	batch_time 10.7008
[32m[2023-01-07 04:01:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2079/54685]	eta 9:28:04 lr 0.0009999996	 wd 0.1000	time 0.8679 (0.6479)	loss 0.6111 (0.6235)	loss-cls 9.7769 (9.9763)	loss-aux 0.0000 (0.0000)	grad_norm 0.2889 (inf)	loss_scale 32768.0000 (64007.8769)	mem 7652MB	batch_time 10.8151
[32m[2023-01-07 04:01:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2095/54685]	eta 9:28:03 lr 0.0009999996	 wd 0.1000	time 0.7448 (0.6481)	loss 0.6116 (0.6235)	loss-cls 9.7859 (9.9753)	loss-aux 0.0000 (0.0000)	grad_norm 0.9364 (inf)	loss_scale 32768.0000 (63769.4046)	mem 7652MB	batch_time 10.7637
[32m[2023-01-07 04:01:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2111/54685]	eta 9:28:01 lr 0.0009999996	 wd 0.1000	time 0.7853 (0.6483)	loss 0.6057 (0.6234)	loss-cls 9.6908 (9.9744)	loss-aux 0.0000 (0.0000)	grad_norm 0.3971 (inf)	loss_scale 32768.0000 (63534.5455)	mem 7652MB	batch_time 10.7003
[32m[2023-01-07 04:01:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2127/54685]	eta 9:27:58 lr 0.0009999995	 wd 0.1000	time 0.7489 (0.6484)	loss 0.6168 (0.6233)	loss-cls 9.8691 (9.9733)	loss-aux 0.0000 (0.0000)	grad_norm 0.3091 (inf)	loss_scale 32768.0000 (63303.2180)	mem 7652MB	batch_time 10.6609
[32m[2023-01-07 04:01:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2143/54685]	eta 9:27:52 lr 0.0009999995	 wd 0.1000	time 0.7074 (0.6485)	loss 0.6177 (0.6233)	loss-cls 9.8833 (9.9728)	loss-aux 0.0000 (0.0000)	grad_norm 0.2937 (inf)	loss_scale 32768.0000 (63075.3433)	mem 7652MB	batch_time 10.5764
[32m[2023-01-07 04:01:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2159/54685]	eta 9:27:46 lr 0.0009999995	 wd 0.1000	time 0.7235 (0.6486)	loss 0.6178 (0.6232)	loss-cls 9.8850 (9.9718)	loss-aux 0.0000 (0.0000)	grad_norm 0.5636 (inf)	loss_scale 32768.0000 (62850.8444)	mem 7652MB	batch_time 10.5306
[32m[2023-01-07 04:02:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2175/54685]	eta 9:27:38 lr 0.0009999995	 wd 0.1000	time 0.7318 (0.6486)	loss 0.6191 (0.6232)	loss-cls 9.9056 (9.9713)	loss-aux 0.0000 (0.0000)	grad_norm 0.5918 (inf)	loss_scale 32768.0000 (62629.6471)	mem 7652MB	batch_time 10.5018
[32m[2023-01-07 04:02:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2191/54685]	eta 9:27:35 lr 0.0009999995	 wd 0.1000	time 0.7379 (0.6488)	loss 0.6241 (0.6232)	loss-cls 9.9853 (9.9706)	loss-aux 0.0000 (0.0000)	grad_norm 0.2968 (inf)	loss_scale 32768.0000 (62411.6788)	mem 7652MB	batch_time 10.6901
[32m[2023-01-07 04:02:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2207/54685]	eta 9:27:31 lr 0.0009999995	 wd 0.1000	time 0.7342 (0.6489)	loss 0.6051 (0.6231)	loss-cls 9.6811 (9.9696)	loss-aux 0.0000 (0.0000)	grad_norm 0.2715 (inf)	loss_scale 32768.0000 (62196.8696)	mem 7652MB	batch_time 10.6297
[32m[2023-01-07 04:02:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2223/54685]	eta 9:27:27 lr 0.0009999995	 wd 0.1000	time 0.7212 (0.6490)	loss 0.6107 (0.6230)	loss-cls 9.7717 (9.9687)	loss-aux 0.0000 (0.0000)	grad_norm 0.8516 (inf)	loss_scale 32768.0000 (61985.1511)	mem 7652MB	batch_time 10.6482
[32m[2023-01-07 04:02:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2239/54685]	eta 9:27:23 lr 0.0009999995	 wd 0.1000	time 0.7285 (0.6491)	loss 0.6236 (0.6230)	loss-cls 9.9778 (9.9681)	loss-aux 0.0000 (0.0000)	grad_norm 0.5851 (inf)	loss_scale 32768.0000 (61776.4571)	mem 7652MB	batch_time 10.6723
[32m[2023-01-07 04:03:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2255/54685]	eta 9:27:19 lr 0.0009999995	 wd 0.1000	time 0.7505 (0.6492)	loss 0.6215 (0.6229)	loss-cls 9.9447 (9.9672)	loss-aux 0.0000 (0.0000)	grad_norm 0.2684 (inf)	loss_scale 32768.0000 (61570.7234)	mem 7652MB	batch_time 10.6667
[32m[2023-01-07 04:03:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2271/54685]	eta 9:27:15 lr 0.0009999995	 wd 0.1000	time 0.7490 (0.6494)	loss 0.6057 (0.6229)	loss-cls 9.6914 (9.9661)	loss-aux 0.0000 (0.0000)	grad_norm 0.5518 (inf)	loss_scale 32768.0000 (61367.8873)	mem 7652MB	batch_time 10.6425
[32m[2023-01-07 04:03:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2287/54685]	eta 9:27:12 lr 0.0009999995	 wd 0.1000	time 0.7480 (0.6495)	loss 0.6255 (0.6228)	loss-cls 10.0076 (9.9654)	loss-aux 0.0000 (0.0000)	grad_norm 0.3800 (inf)	loss_scale 32768.0000 (61167.8881)	mem 7652MB	batch_time 10.7360
[32m[2023-01-07 04:03:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2303/54685]	eta 9:27:08 lr 0.0009999995	 wd 0.1000	time 0.7222 (0.6496)	loss 0.6053 (0.6228)	loss-cls 9.6849 (9.9644)	loss-aux 0.0000 (0.0000)	grad_norm 0.2757 (inf)	loss_scale 32768.0000 (60970.6667)	mem 7652MB	batch_time 10.6720
[32m[2023-01-07 04:03:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2319/54685]	eta 9:27:03 lr 0.0009999995	 wd 0.1000	time 0.7220 (0.6497)	loss 0.6156 (0.6227)	loss-cls 9.8498 (9.9635)	loss-aux 0.0000 (0.0000)	grad_norm 0.7085 (inf)	loss_scale 32768.0000 (60776.1655)	mem 7652MB	batch_time 10.6009
[32m[2023-01-07 04:03:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2335/54685]	eta 9:26:59 lr 0.0009999995	 wd 0.1000	time 0.7321 (0.6499)	loss 0.6124 (0.6226)	loss-cls 9.7985 (9.9622)	loss-aux 0.0000 (0.0000)	grad_norm 0.3045 (inf)	loss_scale 32768.0000 (60584.3288)	mem 7652MB	batch_time 10.7030
[32m[2023-01-07 04:04:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2351/54685]	eta 9:26:51 lr 0.0009999994	 wd 0.1000	time 0.7263 (0.6499)	loss 0.6099 (0.6226)	loss-cls 9.7591 (9.9612)	loss-aux 0.0000 (0.0000)	grad_norm 0.9382 (inf)	loss_scale 32768.0000 (60395.1020)	mem 7652MB	batch_time 10.5156
[32m[2023-01-07 04:04:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2367/54685]	eta 9:26:46 lr 0.0009999994	 wd 0.1000	time 0.7148 (0.6500)	loss 0.6278 (0.6225)	loss-cls 10.0444 (9.9605)	loss-aux 0.0000 (0.0000)	grad_norm 0.2713 (inf)	loss_scale 32768.0000 (60208.4324)	mem 7652MB	batch_time 10.6426
[32m[2023-01-07 04:04:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2383/54685]	eta 9:26:45 lr 0.0009999994	 wd 0.1000	time 0.7672 (0.6502)	loss 0.6108 (0.6225)	loss-cls 9.7722 (9.9598)	loss-aux 0.0000 (0.0000)	grad_norm 0.3197 (inf)	loss_scale 32768.0000 (60024.2685)	mem 7652MB	batch_time 10.7967
[32m[2023-01-07 04:04:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2399/54685]	eta 9:26:41 lr 0.0009999994	 wd 0.1000	time 0.7553 (0.6503)	loss 0.6166 (0.6224)	loss-cls 9.8655 (9.9591)	loss-aux 0.0000 (0.0000)	grad_norm 0.5860 (inf)	loss_scale 32768.0000 (59842.5600)	mem 7652MB	batch_time 10.6965
[32m[2023-01-07 04:04:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2415/54685]	eta 9:26:38 lr 0.0009999994	 wd 0.1000	time 0.8814 (0.6504)	loss 0.6221 (0.6224)	loss-cls 9.9530 (9.9580)	loss-aux 0.0000 (0.0000)	grad_norm 0.2407 (inf)	loss_scale 32768.0000 (59663.2583)	mem 7652MB	batch_time 10.7547
[32m[2023-01-07 04:05:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2431/54685]	eta 9:26:35 lr 0.0009999994	 wd 0.1000	time 0.7477 (0.6506)	loss 0.6047 (0.6223)	loss-cls 9.6752 (9.9572)	loss-aux 0.0000 (0.0000)	grad_norm 0.2907 (inf)	loss_scale 32768.0000 (59486.3158)	mem 7652MB	batch_time 10.7384
[32m[2023-01-07 04:05:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2447/54685]	eta 9:26:28 lr 0.0009999994	 wd 0.1000	time 0.7528 (0.6506)	loss 0.6163 (0.6223)	loss-cls 9.8611 (9.9564)	loss-aux 0.0000 (0.0000)	grad_norm 0.4498 (inf)	loss_scale 32768.0000 (59311.6863)	mem 7652MB	batch_time 10.5749
[32m[2023-01-07 04:05:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2463/54685]	eta 9:26:18 lr 0.0009999994	 wd 0.1000	time 0.6921 (0.6507)	loss 0.6100 (0.6222)	loss-cls 9.7604 (9.9556)	loss-aux 0.0000 (0.0000)	grad_norm 0.2883 (inf)	loss_scale 32768.0000 (59139.3247)	mem 7652MB	batch_time 10.4443
[32m[2023-01-07 04:05:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2479/54685]	eta 9:26:12 lr 0.0009999994	 wd 0.1000	time 0.7297 (0.6507)	loss 0.6186 (0.6222)	loss-cls 9.8977 (9.9551)	loss-aux 0.0000 (0.0000)	grad_norm 0.3030 (inf)	loss_scale 32768.0000 (58969.1871)	mem 7652MB	batch_time 10.6129
[32m[2023-01-07 04:05:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2495/54685]	eta 9:26:05 lr 0.0009999994	 wd 0.1000	time 0.7066 (0.6508)	loss 0.6064 (0.6221)	loss-cls 9.7023 (9.9542)	loss-aux 0.0000 (0.0000)	grad_norm 0.5006 (inf)	loss_scale 32768.0000 (58801.2308)	mem 7652MB	batch_time 10.5709
[32m[2023-01-07 04:05:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2511/54685]	eta 9:25:58 lr 0.0009999994	 wd 0.1000	time 0.7268 (0.6509)	loss 0.6159 (0.6221)	loss-cls 9.8537 (9.9534)	loss-aux 0.0000 (0.0000)	grad_norm 0.2434 (inf)	loss_scale 32768.0000 (58635.4140)	mem 7652MB	batch_time 10.5720
[32m[2023-01-07 04:06:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2527/54685]	eta 9:25:51 lr 0.0009999994	 wd 0.1000	time 0.7344 (0.6509)	loss 0.6224 (0.6220)	loss-cls 9.9586 (9.9527)	loss-aux 0.0000 (0.0000)	grad_norm 0.2730 (inf)	loss_scale 32768.0000 (58471.6962)	mem 7652MB	batch_time 10.6049
[32m[2023-01-07 04:06:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2543/54685]	eta 9:25:46 lr 0.0009999994	 wd 0.1000	time 0.7294 (0.6510)	loss 0.6174 (0.6220)	loss-cls 9.8778 (9.9523)	loss-aux 0.0000 (0.0000)	grad_norm 0.3147 (inf)	loss_scale 32768.0000 (58310.0377)	mem 7652MB	batch_time 10.6475
[32m[2023-01-07 04:06:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2559/54685]	eta 9:25:38 lr 0.0009999993	 wd 0.1000	time 0.7046 (0.6511)	loss 0.6215 (0.6220)	loss-cls 9.9436 (9.9517)	loss-aux 0.0000 (0.0000)	grad_norm 0.2995 (inf)	loss_scale 32768.0000 (58150.4000)	mem 7652MB	batch_time 10.5316
[32m[2023-01-07 04:06:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2575/54685]	eta 9:25:31 lr 0.0009999993	 wd 0.1000	time 0.7309 (0.6512)	loss 0.6186 (0.6219)	loss-cls 9.8972 (9.9511)	loss-aux 0.0000 (0.0000)	grad_norm 0.2823 (inf)	loss_scale 32768.0000 (57992.7453)	mem 7652MB	batch_time 10.6090
[32m[2023-01-07 04:06:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2591/54685]	eta 9:25:24 lr 0.0009999993	 wd 0.1000	time 0.7390 (0.6512)	loss 0.6140 (0.6219)	loss-cls 9.8237 (9.9502)	loss-aux 0.0000 (0.0000)	grad_norm 0.2830 (inf)	loss_scale 32768.0000 (57837.0370)	mem 7652MB	batch_time 10.5959
[32m[2023-01-07 04:06:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2607/54685]	eta 9:25:18 lr 0.0009999993	 wd 0.1000	time 0.7203 (0.6513)	loss 0.6167 (0.6219)	loss-cls 9.8669 (9.9498)	loss-aux 0.0000 (0.0000)	grad_norm 0.2487 (inf)	loss_scale 32768.0000 (57683.2393)	mem 7652MB	batch_time 10.6368
[32m[2023-01-07 04:07:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2623/54685]	eta 9:25:11 lr 0.0009999993	 wd 0.1000	time 0.7200 (0.6514)	loss 0.6060 (0.6218)	loss-cls 9.6952 (9.9489)	loss-aux 0.0000 (0.0000)	grad_norm 0.2309 (inf)	loss_scale 32768.0000 (57531.3171)	mem 7652MB	batch_time 10.5835
[32m[2023-01-07 04:07:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2639/54685]	eta 9:25:02 lr 0.0009999993	 wd 0.1000	time 0.7491 (0.6514)	loss 0.6061 (0.6218)	loss-cls 9.6973 (9.9481)	loss-aux 0.0000 (0.0000)	grad_norm 0.2352 (inf)	loss_scale 32768.0000 (57381.2364)	mem 7652MB	batch_time 10.5193
[32m[2023-01-07 04:07:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2655/54685]	eta 9:24:55 lr 0.0009999993	 wd 0.1000	time 0.7371 (0.6515)	loss 0.6248 (0.6217)	loss-cls 9.9960 (9.9473)	loss-aux 0.0000 (0.0000)	grad_norm 0.7877 (inf)	loss_scale 32768.0000 (57232.9639)	mem 7652MB	batch_time 10.5921
[32m[2023-01-07 04:07:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2671/54685]	eta 9:24:48 lr 0.0009999993	 wd 0.1000	time 0.7130 (0.6515)	loss 0.6235 (0.6217)	loss-cls 9.9764 (9.9466)	loss-aux 0.0000 (0.0000)	grad_norm 0.3151 (inf)	loss_scale 32768.0000 (57086.4671)	mem 7652MB	batch_time 10.5786
[32m[2023-01-07 04:07:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2687/54685]	eta 9:24:41 lr 0.0009999993	 wd 0.1000	time 0.7570 (0.6516)	loss 0.6170 (0.6216)	loss-cls 9.8712 (9.9461)	loss-aux 0.0000 (0.0000)	grad_norm 0.2182 (inf)	loss_scale 32768.0000 (56941.7143)	mem 7652MB	batch_time 10.6016
[32m[2023-01-07 04:08:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2703/54685]	eta 9:24:32 lr 0.0009999993	 wd 0.1000	time 0.7321 (0.6516)	loss 0.6101 (0.6216)	loss-cls 9.7613 (9.9454)	loss-aux 0.0000 (0.0000)	grad_norm 0.2253 (inf)	loss_scale 32768.0000 (56798.6746)	mem 7652MB	batch_time 10.4865
[32m[2023-01-07 04:08:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2719/54685]	eta 9:24:24 lr 0.0009999993	 wd 0.1000	time 0.7206 (0.6517)	loss 0.6228 (0.6216)	loss-cls 9.9644 (9.9449)	loss-aux 0.0000 (0.0000)	grad_norm 0.2091 (inf)	loss_scale 32768.0000 (56657.3176)	mem 7652MB	batch_time 10.5654
[32m[2023-01-07 04:08:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2735/54685]	eta 9:24:16 lr 0.0009999993	 wd 0.1000	time 0.7499 (0.6517)	loss 0.6116 (0.6215)	loss-cls 9.7861 (9.9439)	loss-aux 0.0000 (0.0000)	grad_norm 0.2059 (inf)	loss_scale 32768.0000 (56517.6140)	mem 7652MB	batch_time 10.5713
[32m[2023-01-07 04:08:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2751/54685]	eta 9:24:06 lr 0.0009999992	 wd 0.1000	time 0.7297 (0.6517)	loss 0.6127 (0.6215)	loss-cls 9.8034 (9.9434)	loss-aux 0.0000 (0.0000)	grad_norm 0.2258 (inf)	loss_scale 32768.0000 (56379.5349)	mem 7652MB	batch_time 10.4550
[32m[2023-01-07 04:08:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2767/54685]	eta 9:24:00 lr 0.0009999992	 wd 0.1000	time 0.7559 (0.6518)	loss 0.6051 (0.6214)	loss-cls 9.6823 (9.9425)	loss-aux 0.0000 (0.0000)	grad_norm 0.4289 (inf)	loss_scale 32768.0000 (56243.0520)	mem 7652MB	batch_time 10.6713
[32m[2023-01-07 04:08:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2783/54685]	eta 9:23:52 lr 0.0009999992	 wd 0.1000	time 0.7390 (0.6518)	loss 0.6246 (0.6214)	loss-cls 9.9935 (9.9417)	loss-aux 0.0000 (0.0000)	grad_norm 0.2759 (inf)	loss_scale 32768.0000 (56108.1379)	mem 7652MB	batch_time 10.5122
[32m[2023-01-07 04:09:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2799/54685]	eta 9:23:44 lr 0.0009999992	 wd 0.1000	time 0.7346 (0.6519)	loss 0.6084 (0.6213)	loss-cls 9.7343 (9.9408)	loss-aux 0.0000 (0.0000)	grad_norm 0.2532 (inf)	loss_scale 32768.0000 (55974.7657)	mem 7652MB	batch_time 10.5961
[32m[2023-01-07 04:09:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2815/54685]	eta 9:23:37 lr 0.0009999992	 wd 0.1000	time 0.7321 (0.6520)	loss 0.6030 (0.6213)	loss-cls 9.6479 (9.9401)	loss-aux 0.0000 (0.0000)	grad_norm 0.2160 (inf)	loss_scale 32768.0000 (55842.9091)	mem 7652MB	batch_time 10.6055
[32m[2023-01-07 04:09:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2831/54685]	eta 9:23:31 lr 0.0009999992	 wd 0.1000	time 0.7438 (0.6520)	loss 0.6094 (0.6212)	loss-cls 9.7501 (9.9397)	loss-aux 0.0000 (0.0000)	grad_norm 0.2284 (inf)	loss_scale 32768.0000 (55712.5424)	mem 7652MB	batch_time 10.6547
[32m[2023-01-07 04:09:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2847/54685]	eta 9:23:25 lr 0.0009999992	 wd 0.1000	time 0.7406 (0.6521)	loss 0.6196 (0.6212)	loss-cls 9.9133 (9.9391)	loss-aux 0.0000 (0.0000)	grad_norm 0.2258 (inf)	loss_scale 32768.0000 (55583.6404)	mem 7652MB	batch_time 10.7126
[32m[2023-01-07 04:09:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2863/54685]	eta 9:23:18 lr 0.0009999992	 wd 0.1000	time 0.7237 (0.6522)	loss 0.6145 (0.6212)	loss-cls 9.8318 (9.9385)	loss-aux 0.0000 (0.0000)	grad_norm 0.2226 (inf)	loss_scale 32768.0000 (55456.1788)	mem 7652MB	batch_time 10.6073
[32m[2023-01-07 04:09:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2879/54685]	eta 9:23:10 lr 0.0009999992	 wd 0.1000	time 0.7433 (0.6523)	loss 0.6145 (0.6211)	loss-cls 9.8319 (9.9379)	loss-aux 0.0000 (0.0000)	grad_norm 0.2091 (inf)	loss_scale 32768.0000 (55330.1333)	mem 7652MB	batch_time 10.5774
[32m[2023-01-07 04:10:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2895/54685]	eta 9:23:04 lr 0.0009999992	 wd 0.1000	time 0.7437 (0.6523)	loss 0.6196 (0.6211)	loss-cls 9.9129 (9.9373)	loss-aux 0.0000 (0.0000)	grad_norm 0.2833 (inf)	loss_scale 32768.0000 (55205.4807)	mem 7652MB	batch_time 10.6549
[32m[2023-01-07 04:10:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2911/54685]	eta 9:22:58 lr 0.0009999992	 wd 0.1000	time 0.7687 (0.6524)	loss 0.6074 (0.6210)	loss-cls 9.7187 (9.9368)	loss-aux 0.0000 (0.0000)	grad_norm 0.1971 (inf)	loss_scale 32768.0000 (55082.1978)	mem 7652MB	batch_time 10.7153
[32m[2023-01-07 04:10:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2927/54685]	eta 9:22:51 lr 0.0009999991	 wd 0.1000	time 0.7598 (0.6525)	loss 0.6150 (0.6210)	loss-cls 9.8398 (9.9361)	loss-aux 0.0000 (0.0000)	grad_norm 0.2246 (inf)	loss_scale 32768.0000 (54960.2623)	mem 7652MB	batch_time 10.6447
[32m[2023-01-07 04:10:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2943/54685]	eta 9:22:45 lr 0.0009999991	 wd 0.1000	time 0.7415 (0.6526)	loss 0.6156 (0.6210)	loss-cls 9.8489 (9.9356)	loss-aux 0.0000 (0.0000)	grad_norm 0.2098 (inf)	loss_scale 32768.0000 (54839.6522)	mem 7652MB	batch_time 10.6863
[32m[2023-01-07 04:10:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2959/54685]	eta 9:22:38 lr 0.0009999991	 wd 0.1000	time 0.7276 (0.6526)	loss 0.6096 (0.6209)	loss-cls 9.7528 (9.9349)	loss-aux 0.0000 (0.0000)	grad_norm 0.2119 (inf)	loss_scale 32768.0000 (54720.3459)	mem 7652MB	batch_time 10.6089
[32m[2023-01-07 04:11:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2975/54685]	eta 9:22:31 lr 0.0009999991	 wd 0.1000	time 0.7276 (0.6527)	loss 0.6207 (0.6209)	loss-cls 9.9319 (9.9345)	loss-aux 0.0000 (0.0000)	grad_norm 0.2256 (inf)	loss_scale 32768.0000 (54602.3226)	mem 7652MB	batch_time 10.6606
[32m[2023-01-07 04:11:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2991/54685]	eta 9:22:27 lr 0.0009999991	 wd 0.1000	time 0.7459 (0.6528)	loss 0.6121 (0.6209)	loss-cls 9.7941 (9.9340)	loss-aux 0.0000 (0.0000)	grad_norm 0.2198 (inf)	loss_scale 32768.0000 (54485.5615)	mem 7652MB	batch_time 10.8211
[32m[2023-01-07 04:11:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3007/54685]	eta 9:22:20 lr 0.0009999991	 wd 0.1000	time 0.7302 (0.6529)	loss 0.6017 (0.6208)	loss-cls 9.6273 (9.9334)	loss-aux 0.0000 (0.0000)	grad_norm 0.3045 (inf)	loss_scale 32768.0000 (54370.0426)	mem 7652MB	batch_time 10.6652
[32m[2023-01-07 04:11:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3023/54685]	eta 9:22:15 lr 0.0009999991	 wd 0.1000	time 0.7430 (0.6530)	loss 0.6236 (0.6208)	loss-cls 9.9771 (9.9328)	loss-aux 0.0000 (0.0000)	grad_norm 0.2258 (inf)	loss_scale 32768.0000 (54255.7460)	mem 7652MB	batch_time 10.7466
[32m[2023-01-07 04:11:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3039/54685]	eta 9:22:09 lr 0.0009999991	 wd 0.1000	time 0.7380 (0.6531)	loss 0.6275 (0.6208)	loss-cls 10.0401 (9.9323)	loss-aux 0.0000 (0.0000)	grad_norm 0.2068 (inf)	loss_scale 32768.0000 (54142.6526)	mem 7652MB	batch_time 10.7131
[32m[2023-01-07 04:11:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3055/54685]	eta 9:22:03 lr 0.0009999991	 wd 0.1000	time 0.7474 (0.6532)	loss 0.6051 (0.6207)	loss-cls 9.6821 (9.9316)	loss-aux 0.0000 (0.0000)	grad_norm 0.2038 (inf)	loss_scale 32768.0000 (54030.7435)	mem 7652MB	batch_time 10.7120
[32m[2023-01-07 04:12:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3071/54685]	eta 9:21:56 lr 0.0009999991	 wd 0.1000	time 0.7277 (0.6533)	loss 0.6035 (0.6207)	loss-cls 9.6563 (9.9310)	loss-aux 0.0000 (0.0000)	grad_norm 0.1924 (inf)	loss_scale 32768.0000 (53920.0000)	mem 7652MB	batch_time 10.6696
[32m[2023-01-07 04:12:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3087/54685]	eta 9:21:49 lr 0.0009999990	 wd 0.1000	time 0.7192 (0.6533)	loss 0.6102 (0.6207)	loss-cls 9.7632 (9.9307)	loss-aux 0.0000 (0.0000)	grad_norm 0.1981 (inf)	loss_scale 32768.0000 (53810.4041)	mem 7652MB	batch_time 10.6230
[32m[2023-01-07 04:12:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3103/54685]	eta 9:21:43 lr 0.0009999990	 wd 0.1000	time 0.7394 (0.6534)	loss 0.6055 (0.6206)	loss-cls 9.6877 (9.9299)	loss-aux 0.0000 (0.0000)	grad_norm 0.2098 (inf)	loss_scale 32768.0000 (53701.9381)	mem 7652MB	batch_time 10.7195
[32m[2023-01-07 04:12:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3119/54685]	eta 9:21:35 lr 0.0009999990	 wd 0.1000	time 0.7442 (0.6534)	loss 0.6115 (0.6206)	loss-cls 9.7839 (9.9292)	loss-aux 0.0000 (0.0000)	grad_norm 0.2159 (inf)	loss_scale 32768.0000 (53594.5846)	mem 7652MB	batch_time 10.6073
[32m[2023-01-07 04:12:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3135/54685]	eta 9:21:27 lr 0.0009999990	 wd 0.1000	time 0.7467 (0.6535)	loss 0.6143 (0.6205)	loss-cls 9.8291 (9.9286)	loss-aux 0.0000 (0.0000)	grad_norm 0.2026 (inf)	loss_scale 32768.0000 (53488.3265)	mem 7652MB	batch_time 10.6041
[32m[2023-01-07 04:12:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3151/54685]	eta 9:21:20 lr 0.0009999990	 wd 0.1000	time 0.7931 (0.6536)	loss 0.6220 (0.6205)	loss-cls 9.9522 (9.9282)	loss-aux 0.0000 (0.0000)	grad_norm 0.2228 (inf)	loss_scale 32768.0000 (53383.1472)	mem 7652MB	batch_time 10.6951
[32m[2023-01-07 04:13:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3167/54685]	eta 9:21:13 lr 0.0009999990	 wd 0.1000	time 0.7360 (0.6536)	loss 0.6332 (0.6205)	loss-cls 10.1316 (9.9278)	loss-aux 0.0000 (0.0000)	grad_norm 0.2060 (inf)	loss_scale 32768.0000 (53279.0303)	mem 7652MB	batch_time 10.6392
[32m[2023-01-07 04:13:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3183/54685]	eta 9:21:05 lr 0.0009999990	 wd 0.1000	time 0.7309 (0.6537)	loss 0.6108 (0.6205)	loss-cls 9.7736 (9.9272)	loss-aux 0.0000 (0.0000)	grad_norm 0.2087 (inf)	loss_scale 32768.0000 (53175.9598)	mem 7652MB	batch_time 10.6074
[32m[2023-01-07 04:13:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3199/54685]	eta 9:20:57 lr 0.0009999990	 wd 0.1000	time 0.6893 (0.6537)	loss 0.6010 (0.6204)	loss-cls 9.6156 (9.9265)	loss-aux 0.0000 (0.0000)	grad_norm nan (nan)	loss_scale 16384.0000 (53068.8000)	mem 7652MB	batch_time 10.6555
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15830.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_15341.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27611.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n06470073/n06470073_47249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25750.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22980.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27221.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_10353.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24544.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18729.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_1914.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24434.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12740.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28288.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29695.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13103.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11827.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9215.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7996.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6567.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12757.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_16320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14020.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19281.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2040.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6236.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2322.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9031.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27412.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13871.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13641.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7072.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12206.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9041.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9282.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03957420/n03957420_33553.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10675.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17471.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5051.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12231.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_11746.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8806.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_21756.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7013.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9401.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17877.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n02368116/n02368116_318.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5664.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_16737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22719.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8812.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_9566.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27296.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13261.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8645.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30926.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25717.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_26924.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13244.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10586.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_1796.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6247.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22166.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6710.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_4539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33623.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18350.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25530.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10242.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29025.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13306.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13419.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20118.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_16430.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_7026.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15811.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7365.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11816.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30043.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22995.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7953.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12142.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_14701.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6850.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6669.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28595.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7411.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22698.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19005.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11766.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2852.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33259.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27578.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_135.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33630.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15455.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_3493.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12064.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22581.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25408.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15480.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4524.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4456.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8873.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_32625.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27317.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9819.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9068.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8783.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15697.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13396.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14718.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10175248/n10175248_583.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7728.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_34297.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27627.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8925.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13516.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24638.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12108.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_23316.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12364.JPEG
[32m[2023-01-07 04:13:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 282)[0m: INFO EPOCH 0 training takes 0:34:53
