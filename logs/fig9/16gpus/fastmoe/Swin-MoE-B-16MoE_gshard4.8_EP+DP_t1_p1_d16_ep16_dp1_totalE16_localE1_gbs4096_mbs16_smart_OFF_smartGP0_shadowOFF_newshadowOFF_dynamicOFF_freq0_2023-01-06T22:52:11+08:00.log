+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102390
++ scontrol show JobId=102390
++ scontrol show JobId=102390
++ grep BatchHost
++ scontrol show JobId=102390
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102390
++ scontrol show JobId=102390
++ tr = ' '
++ scontrol show JobId=102390
++ scontrol show JobId=102390
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=102390
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=102390
++ grep BatchHost
++ scontrol show JobId=102390
++ scontrol show JobId=102390
++ scontrol show JobId=102390
++ grep BatchHost
++ scontrol show JobId=102390
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=102390
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=102390
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=2
+ NNODES=2
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ MASTER_ADDR=nico1
+ DATASET_PREFIX=/mnt/znvme/zms
+ export RANK=6
+ RANK=6
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MASTER_ADDR=nico1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ localrank=1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ TRAIN_SAMPLES=20520960
+ localrank=7
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=2
+ NNODES=2
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=4
+ NODE_RANK=4
+ export NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ NNODES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NNODES=2
+ NNODES=2
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=6
+ NODE_RANK=6
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=7
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ NODE_RANK=7
+ export MAX_JOBS=64
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=2
+ NNODES=2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ '[' OFF == ON ']'
+ false
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ true
+ EXEC=./main_moe.py
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ export NNODES=2
+ NNODES=2
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export NNODES=2
+ NNODES=2
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MASTER_ADDR=nico1
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ export RANK=12
+ RANK=12
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ localrank=4
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ true
+ cd /home/zms/model_training/MoE/FastSwin
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ export NNODES=2
+ NNODES=2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ export NODE_RANK=6
+ NODE_RANK=6
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ '[' nico == nico ']'
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ cd /home/zms/model_training/MoE/FastSwin
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ TRAIN_SAMPLES=20520960
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ python_args=
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof/table
+ python_args=
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy gshard         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 16
                --balance-strategy gshard
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 16
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-16MoE_gshard4.8_EP+DP_t1_p1_d16_ep16_dp1_totalE16_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T22:52:11+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 16 --balance-strategy gshard --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 16 --expert-dp-size 1
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 0/16
[32m[2023-01-06 22:52:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 403)[0m: INFO Full config saved to /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default/config.json
[32m[2023-01-06 22:52:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 406)[0m: INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: part
  DATASET: imagenet22K
  DATA_PATH: /mnt/znvme/dataset/imagenet22k
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 4.8
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    GATE_NOISE: 1.0
    INIT_STD: 0.005
    IN_CHANS: 3
    IS_GSHARD_LOSS: true
    MLP_FC2_BIAS: false
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - 1
      - 3
      - 5
      - 7
      - 9
      - 11
      - 13
      - 15
      - 17
    - - 1
    MOE_DROP: 0.1
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 2
    USE_BPR: true
    WINDOW_SIZE: 12
  TYPE: swin_fastmoe
OUTPUT: /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default
PRINT_FREQ: 1
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 16
  AUTO_RESUME: true
  BASE_LR: 0.001
  CHECKPOINT_MODE: full
  CLIP_GRAD: 3.0
  EPOCHS: 90
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.1

[32m[2023-01-06 22:52:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 407)[0m: INFO {"cfg": "configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml", "opts": null, "batch_size": 16, "data_path": "/mnt/znvme/dataset/imagenet22k", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": 16, "use_checkpoint": false, "checkpoint_mode": "full", "disable_amp": false, "amp_opt_level": null, "output": "/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": true, "fused_layernorm": false, "optim": null, "num_experts": 1, "top_k": 2, "balance_strategy": "gshard", "expert_parallel_strategy": "EP+DP", "expert_ep_size": 16, "expert_dp_size": 1, "dump": false, "dynamic_placement": false, "dynamic_freq": 10, "new_shadow": false, "gshard_cap": 4.8, "init_method_std": 0.002, "num_layers": 12}
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
local rank 0 / global rank 0 successfully build train dataset
local rank 0 / global rank 0 successfully build val dataset
[32m[2023-01-06 22:52:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 120)[0m: INFO Creating model:swin_fastmoe/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 6/16
local rank 0 / global rank 6 successfully build train dataset
local rank 0 / global rank 6 successfully build val dataset
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 2/16
local rank 0 / global rank 2 successfully build train dataset
local rank 0 / global rank 2 successfully build val dataset
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 15/16
local rank 0 / global rank 15 successfully build train dataset
local rank 0 / global rank 15 successfully build val dataset
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 1/16
local rank 0 / global rank 1 successfully build train dataset
local rank 0 / global rank 1 successfully build val dataset
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 5/16
local rank 0 / global rank 5 successfully build train dataset
local rank 0 / global rank 5 successfully build val dataset
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0]
[32m[2023-01-06 22:53:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 122)[0m: INFO SwinTransformerFastMoE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(48, 48), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(48, 48), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(24, 24), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(24, 24), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(12, 12), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=2048, out_features=16, bias=True)
              )
            )
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(12, 12), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(6, 6), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=4096, out_features=1024, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 1024, hidden_features = 4096, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): GShardGate(
                (gate): Linear(in_features=4096, out_features=16, bias=True)
              )
            )
          )
        )
      )
    )
  )
  (norm): TimerModule(
    (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (avgpool): TimerModule(
    (model): AdaptiveAvgPool1d(output_size=1)
  )
  (head): TimerModule(
    (model): Linear(in_features=1024, out_features=21841, bias=True)
  )
)
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 7/16
local rank 0 / global rank 7 successfully build train dataset
local rank 0 / global rank 7 successfully build val dataset
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 8/16
local rank 0 / global rank 8 successfully build train dataset
local rank 0 / global rank 8 successfully build val dataset
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 10/16
local rank 0 / global rank 10 successfully build train dataset
local rank 0 / global rank 10 successfully build val dataset
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 4/16
local rank 0 / global rank 4 successfully build train dataset
local rank 0 / global rank 4 successfully build val dataset
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 3/16
local rank 0 / global rank 3 successfully build train dataset
local rank 0 / global rank 3 successfully build val dataset
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-06 22:53:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 156)[0m: INFO no checkpoint found in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default, ignoring auto resume
[32m[2023-01-06 22:53:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 176)[0m: INFO Start training
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 13/16
local rank 0 / global rank 13 successfully build train dataset
local rank 0 / global rank 13 successfully build val dataset
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 14/16
local rank 0 / global rank 14 successfully build train dataset
local rank 0 / global rank 14 successfully build val dataset
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 9/16
local rank 0 / global rank 9 successfully build train dataset
local rank 0 / global rank 9 successfully build val dataset
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 12/16
local rank 0 / global rank 12 successfully build train dataset
local rank 0 / global rank 12 successfully build val dataset
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 11/16
local rank 0 / global rank 11 successfully build train dataset
local rank 0 / global rank 11 successfully build val dataset
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-06 22:53:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][15/54685]	eta 15:24:23 lr 0.0010000000	 wd 0.1000	time 0.8025 (1.0145)	loss 0.6245 (0.6244)	loss-cls 9.9915 (9.9900)	loss-aux 0.0004 (0.0004)	grad_norm 2.0708 (2.0708)	loss_scale 65536.0000 (65536.0000)	mem 5170MB	batch_time 16.2322
[32m[2023-01-06 22:53:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][31/54685]	eta 12:03:57 lr 0.0010000000	 wd 0.1000	time 0.5785 (0.7948)	loss 0.6245 (0.6247)	loss-cls 9.9919 (9.9955)	loss-aux 0.0004 (0.0004)	grad_norm 1.6972 (1.8840)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 9.2006
[32m[2023-01-06 22:53:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][47/54685]	eta 10:53:59 lr 0.0010000000	 wd 0.1000	time 0.6270 (0.7182)	loss 0.6257 (0.6249)	loss-cls 10.0102 (9.9984)	loss-aux 0.0005 (0.0004)	grad_norm 1.4161 (1.7280)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 9.0395
[32m[2023-01-06 22:53:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][63/54685]	eta 10:15:42 lr 0.0010000000	 wd 0.1000	time 0.5994 (0.6763)	loss 0.6263 (0.6251)	loss-cls 10.0198 (10.0006)	loss-aux 0.0007 (0.0005)	grad_norm 1.3407 (1.6312)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.8129
[32m[2023-01-06 22:53:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][79/54685]	eta 9:53:39 lr 0.0010000000	 wd 0.1000	time 0.6257 (0.6523)	loss 0.6248 (0.6250)	loss-cls 9.9965 (9.9998)	loss-aux 0.0008 (0.0006)	grad_norm 1.2793 (1.5608)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.8996
[32m[2023-01-06 22:54:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][95/54685]	eta 9:37:25 lr 0.0010000000	 wd 0.1000	time 0.6219 (0.6347)	loss 0.6242 (0.6251)	loss-cls 9.9863 (10.0003)	loss-aux 0.0010 (0.0006)	grad_norm 1.3157 (1.5200)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.7418
[32m[2023-01-06 22:54:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][111/54685]	eta 9:26:17 lr 0.0010000000	 wd 0.1000	time 0.6334 (0.6226)	loss 0.6268 (0.6251)	loss-cls 10.0282 (10.0012)	loss-aux 0.0012 (0.0007)	grad_norm 1.2153 (1.4764)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.8048
[32m[2023-01-06 22:54:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][127/54685]	eta 9:16:22 lr 0.0010000000	 wd 0.1000	time 0.6064 (0.6119)	loss 0.6243 (0.6250)	loss-cls 9.9881 (9.9988)	loss-aux 0.0011 (0.0007)	grad_norm 1.2694 (1.4506)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.5880
[32m[2023-01-06 22:54:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][143/54685]	eta 9:08:29 lr 0.0010000000	 wd 0.1000	time 0.6098 (0.6034)	loss 0.6240 (0.6249)	loss-cls 9.9825 (9.9976)	loss-aux 0.0010 (0.0008)	grad_norm 1.2053 (1.4233)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.5662
[32m[2023-01-06 22:54:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][159/54685]	eta 9:01:18 lr 0.0010000000	 wd 0.1000	time 0.5897 (0.5957)	loss 0.6226 (0.6249)	loss-cls 9.9609 (9.9974)	loss-aux 0.0010 (0.0008)	grad_norm 1.1174 (1.3927)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.4186
[32m[2023-01-06 22:54:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][175/54685]	eta 8:55:31 lr 0.0010000000	 wd 0.1000	time 0.6096 (0.5895)	loss 0.6276 (0.6249)	loss-cls 10.0409 (9.9974)	loss-aux 0.0010 (0.0008)	grad_norm 1.0969 (1.3658)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.4415
[32m[2023-01-06 22:54:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][191/54685]	eta 8:50:57 lr 0.0010000000	 wd 0.1000	time 0.5875 (0.5846)	loss 0.6233 (0.6248)	loss-cls 9.9719 (9.9954)	loss-aux 0.0010 (0.0008)	grad_norm 1.1310 (1.3463)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.5001
[32m[2023-01-06 22:55:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][207/54685]	eta 8:47:21 lr 0.0010000000	 wd 0.1000	time 0.5898 (0.5808)	loss 0.6275 (0.6246)	loss-cls 10.0391 (9.9932)	loss-aux 0.0012 (0.0008)	grad_norm 1.1637 (1.3322)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.5623
[32m[2023-01-06 22:55:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][223/54685]	eta 8:43:26 lr 0.0010000000	 wd 0.1000	time 0.6125 (0.5767)	loss 0.6210 (0.6247)	loss-cls 9.9349 (9.9938)	loss-aux 0.0012 (0.0009)	grad_norm 1.0499 (1.3120)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.3646
[32m[2023-01-06 22:55:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][239/54685]	eta 8:41:10 lr 0.0010000000	 wd 0.1000	time 0.6195 (0.5743)	loss 0.6306 (0.6247)	loss-cls 10.0887 (9.9946)	loss-aux 0.0014 (0.0009)	grad_norm 1.0578 (1.2951)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.6679
[32m[2023-01-06 22:55:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][255/54685]	eta 8:38:29 lr 0.0010000000	 wd 0.1000	time 0.5947 (0.5716)	loss 0.6226 (0.6247)	loss-cls 9.9599 (9.9943)	loss-aux 0.0012 (0.0009)	grad_norm 1.0416 (1.2793)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.4768
[32m[2023-01-06 22:55:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][271/54685]	eta 8:35:46 lr 0.0010000000	 wd 0.1000	time 0.5836 (0.5687)	loss 0.6255 (0.6248)	loss-cls 10.0074 (9.9956)	loss-aux 0.0014 (0.0009)	grad_norm 1.1162 (1.2697)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.3741
[32m[2023-01-06 22:55:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][287/54685]	eta 8:32:35 lr 0.0010000000	 wd 0.1000	time 0.5855 (0.5654)	loss 0.6237 (0.6249)	loss-cls 9.9774 (9.9969)	loss-aux 0.0020 (0.0010)	grad_norm 1.3269 (1.2728)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.1400
[32m[2023-01-06 22:55:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][303/54685]	eta 8:30:03 lr 0.0010000000	 wd 0.1000	time 0.5816 (0.5627)	loss 0.6297 (0.6250)	loss-cls 10.0741 (9.9996)	loss-aux 0.0015 (0.0010)	grad_norm 1.2758 (1.2730)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.2438
[32m[2023-01-06 22:56:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][319/54685]	eta 8:28:08 lr 0.0010000000	 wd 0.1000	time 0.6035 (0.5608)	loss 0.6324 (0.6252)	loss-cls 10.1174 (10.0016)	loss-aux 0.0017 (0.0011)	grad_norm 1.2509 (1.2719)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.3790
[32m[2023-01-06 22:56:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][335/54685]	eta 8:25:42 lr 0.0010000000	 wd 0.1000	time 0.5793 (0.5583)	loss 0.6262 (0.6252)	loss-cls 10.0179 (10.0023)	loss-aux 0.0018 (0.0011)	grad_norm 1.2403 (1.2704)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.1301
[32m[2023-01-06 22:56:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][351/54685]	eta 8:23:14 lr 0.0010000000	 wd 0.1000	time 0.5875 (0.5557)	loss 0.6298 (0.6252)	loss-cls 10.0745 (10.0028)	loss-aux 0.0016 (0.0011)	grad_norm 1.2208 (1.2681)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.0319
[32m[2023-01-06 22:56:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][367/54685]	eta 8:21:01 lr 0.0010000000	 wd 0.1000	time 0.5792 (0.5534)	loss 0.6295 (0.6254)	loss-cls 10.0706 (10.0057)	loss-aux 0.0017 (0.0012)	grad_norm 1.1673 (1.2638)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.0454
[32m[2023-01-06 22:56:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][383/54685]	eta 8:18:49 lr 0.0010000000	 wd 0.1000	time 0.5618 (0.5512)	loss 0.6316 (0.6255)	loss-cls 10.1031 (10.0066)	loss-aux 0.0021 (0.0012)	grad_norm 1.2289 (1.2623)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9884
[32m[2023-01-06 22:56:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][399/54685]	eta 8:17:02 lr 0.0010000000	 wd 0.1000	time 0.5773 (0.5494)	loss 0.6210 (0.6253)	loss-cls 9.9345 (10.0041)	loss-aux 0.0020 (0.0012)	grad_norm 1.3109 (1.2642)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.0948
[32m[2023-01-06 22:56:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][415/54685]	eta 8:15:29 lr 0.0010000000	 wd 0.1000	time 0.5936 (0.5478)	loss 0.6200 (0.6254)	loss-cls 9.9184 (10.0052)	loss-aux 0.0018 (0.0013)	grad_norm 1.2335 (1.2631)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.1400
[32m[2023-01-06 22:56:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][431/54685]	eta 8:14:15 lr 0.0010000000	 wd 0.1000	time 0.5893 (0.5466)	loss 0.6282 (0.6254)	loss-cls 10.0488 (10.0051)	loss-aux 0.0020 (0.0013)	grad_norm 1.1453 (1.2587)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.2468
[32m[2023-01-06 22:57:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][447/54685]	eta 8:12:57 lr 0.0010000000	 wd 0.1000	time 0.5792 (0.5453)	loss 0.6253 (0.6254)	loss-cls 10.0023 (10.0048)	loss-aux 0.0023 (0.0013)	grad_norm 1.4747 (1.2664)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.1743
[32m[2023-01-06 22:57:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][463/54685]	eta 8:11:50 lr 0.0010000000	 wd 0.1000	time 0.6106 (0.5443)	loss 0.6369 (0.6256)	loss-cls 10.1875 (10.0075)	loss-aux 0.0022 (0.0013)	grad_norm 2.9920 (1.3259)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.2264
[32m[2023-01-06 22:57:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][479/54685]	eta 8:10:51 lr 0.0010000000	 wd 0.1000	time 0.5717 (0.5433)	loss 0.6243 (0.6256)	loss-cls 9.9862 (10.0077)	loss-aux 0.0025 (0.0014)	grad_norm 1.0304 (1.3161)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.2669
[32m[2023-01-06 22:57:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][495/54685]	eta 8:09:50 lr 0.0010000000	 wd 0.1000	time 0.5843 (0.5424)	loss 0.6242 (0.6256)	loss-cls 9.9845 (10.0075)	loss-aux 0.0024 (0.0014)	grad_norm 0.9435 (1.3041)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.2137
[32m[2023-01-06 22:57:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][511/54685]	eta 8:08:44 lr 0.0010000000	 wd 0.1000	time 0.5958 (0.5413)	loss 0.6265 (0.6256)	loss-cls 10.0229 (10.0086)	loss-aux 0.0018 (0.0014)	grad_norm 1.0787 (1.2970)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.1298
[32m[2023-01-06 22:57:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][527/54685]	eta 8:07:59 lr 0.0010000000	 wd 0.1000	time 0.5787 (0.5406)	loss 0.6241 (0.6256)	loss-cls 9.9840 (10.0079)	loss-aux 0.0018 (0.0014)	grad_norm 1.0097 (1.2883)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.3154
[32m[2023-01-06 22:57:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][543/54685]	eta 8:06:36 lr 0.0010000000	 wd 0.1000	time 0.5740 (0.5393)	loss 0.6351 (0.6256)	loss-cls 10.1596 (10.0082)	loss-aux 0.0022 (0.0014)	grad_norm 1.0828 (1.2823)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9007
[32m[2023-01-06 22:58:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][559/54685]	eta 8:05:23 lr 0.0010000000	 wd 0.1000	time 0.5953 (0.5381)	loss 0.6256 (0.6256)	loss-cls 10.0072 (10.0078)	loss-aux 0.0020 (0.0015)	grad_norm 1.1201 (1.2776)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9581
[32m[2023-01-06 22:58:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][575/54685]	eta 8:04:03 lr 0.0010000000	 wd 0.1000	time 0.5481 (0.5368)	loss 0.6307 (0.6256)	loss-cls 10.0894 (10.0080)	loss-aux 0.0019 (0.0015)	grad_norm 1.0849 (1.2723)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.8538
[32m[2023-01-06 22:58:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][591/54685]	eta 8:02:50 lr 0.0010000000	 wd 0.1000	time 0.5607 (0.5356)	loss 0.6136 (0.6256)	loss-cls 9.8163 (10.0081)	loss-aux 0.0019 (0.0015)	grad_norm 1.4203 (1.2763)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.8794
[32m[2023-01-06 22:58:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][607/54685]	eta 8:01:35 lr 0.0010000000	 wd 0.1000	time 0.5547 (0.5343)	loss 0.6406 (0.6256)	loss-cls 10.2485 (10.0078)	loss-aux 0.0017 (0.0015)	grad_norm 1.0216 (1.2696)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.8210
[32m[2023-01-06 22:58:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][623/54685]	eta 8:00:34 lr 0.0010000000	 wd 0.1000	time 0.5584 (0.5334)	loss 0.6247 (0.6255)	loss-cls 9.9933 (10.0067)	loss-aux 0.0020 (0.0015)	grad_norm 1.0160 (1.2631)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9411
[32m[2023-01-06 22:58:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][639/54685]	eta 7:59:28 lr 0.0010000000	 wd 0.1000	time 0.5348 (0.5323)	loss 0.6324 (0.6255)	loss-cls 10.1166 (10.0061)	loss-aux 0.0018 (0.0015)	grad_norm 0.9620 (1.2555)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.8602
[32m[2023-01-06 22:58:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][655/54685]	eta 7:58:32 lr 0.0010000000	 wd 0.1000	time 0.5854 (0.5314)	loss 0.6249 (0.6254)	loss-cls 9.9955 (10.0055)	loss-aux 0.0021 (0.0015)	grad_norm 1.4200 (1.2596)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9436
[32m[2023-01-06 22:58:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][671/54685]	eta 7:57:44 lr 0.0010000000	 wd 0.1000	time 0.5667 (0.5307)	loss 0.6146 (0.6254)	loss-cls 9.8311 (10.0052)	loss-aux 0.0018 (0.0015)	grad_norm 0.9974 (1.2533)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.0073
[32m[2023-01-06 22:59:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][687/54685]	eta 7:56:54 lr 0.0010000000	 wd 0.1000	time 0.5986 (0.5299)	loss 0.6171 (0.6254)	loss-cls 9.8712 (10.0047)	loss-aux 0.0021 (0.0016)	grad_norm 0.8317 (1.2435)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9604
[32m[2023-01-06 22:59:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][703/54685]	eta 7:56:05 lr 0.0010000000	 wd 0.1000	time 0.5677 (0.5292)	loss 0.6241 (0.6253)	loss-cls 9.9828 (10.0036)	loss-aux 0.0020 (0.0016)	grad_norm 0.7920 (1.2332)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9471
[32m[2023-01-06 22:59:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][719/54685]	eta 7:55:12 lr 0.0009999999	 wd 0.1000	time 0.5719 (0.5283)	loss 0.6175 (0.6253)	loss-cls 9.8780 (10.0027)	loss-aux 0.0022 (0.0016)	grad_norm 1.4369 (1.2378)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.8757
[32m[2023-01-06 22:59:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][735/54685]	eta 7:54:29 lr 0.0009999999	 wd 0.1000	time 0.5736 (0.5277)	loss 0.6342 (0.6252)	loss-cls 10.1451 (10.0011)	loss-aux 0.0021 (0.0016)	grad_norm 0.8039 (1.2283)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9790
[32m[2023-01-06 22:59:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][751/54685]	eta 7:53:34 lr 0.0009999999	 wd 0.1000	time 0.5662 (0.5268)	loss 0.6122 (0.6251)	loss-cls 9.7936 (10.0004)	loss-aux 0.0023 (0.0016)	grad_norm 0.9543 (1.2225)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7959
[32m[2023-01-06 22:59:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][767/54685]	eta 7:52:38 lr 0.0009999999	 wd 0.1000	time 0.5403 (0.5260)	loss 0.6360 (0.6251)	loss-cls 10.1743 (10.0004)	loss-aux 0.0022 (0.0016)	grad_norm 0.9935 (1.2177)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7487
[32m[2023-01-06 22:59:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][783/54685]	eta 7:51:52 lr 0.0009999999	 wd 0.1000	time 0.5727 (0.5253)	loss 0.6268 (0.6251)	loss-cls 10.0272 (10.0007)	loss-aux 0.0023 (0.0016)	grad_norm 1.0799 (1.2149)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.8775
[32m[2023-01-06 23:00:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][799/54685]	eta 7:51:13 lr 0.0009999999	 wd 0.1000	time 0.5633 (0.5247)	loss 0.6256 (0.6251)	loss-cls 10.0070 (9.9999)	loss-aux 0.0026 (0.0017)	grad_norm 0.8964 (1.2086)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9390
[32m[2023-01-06 23:00:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][815/54685]	eta 7:50:39 lr 0.0009999999	 wd 0.1000	time 0.5511 (0.5242)	loss 0.6225 (0.6251)	loss-cls 9.9570 (9.9992)	loss-aux 0.0025 (0.0017)	grad_norm 3.0102 (1.2439)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 8.0078
[32m[2023-01-06 23:00:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][831/54685]	eta 7:49:55 lr 0.0009999999	 wd 0.1000	time 0.5701 (0.5235)	loss 0.6138 (0.6250)	loss-cls 9.8183 (9.9981)	loss-aux 0.0026 (0.0017)	grad_norm 1.1136 (1.2414)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.8377
[32m[2023-01-06 23:00:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][847/54685]	eta 7:49:02 lr 0.0009999999	 wd 0.1000	time 0.5278 (0.5227)	loss 0.6163 (0.6250)	loss-cls 9.8568 (9.9985)	loss-aux 0.0034 (0.0017)	grad_norm 0.9923 (1.2367)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6838
[32m[2023-01-06 23:00:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][863/54685]	eta 7:48:12 lr 0.0009999999	 wd 0.1000	time 0.5314 (0.5219)	loss 0.6269 (0.6250)	loss-cls 10.0274 (9.9986)	loss-aux 0.0034 (0.0018)	grad_norm 1.0102 (1.2325)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6837
[32m[2023-01-06 23:00:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][879/54685]	eta 7:47:27 lr 0.0009999999	 wd 0.1000	time 0.5583 (0.5213)	loss 0.6234 (0.6250)	loss-cls 9.9700 (9.9989)	loss-aux 0.0036 (0.0018)	grad_norm 0.9561 (1.2275)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7575
[32m[2023-01-06 23:00:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][895/54685]	eta 7:46:41 lr 0.0009999999	 wd 0.1000	time 0.5688 (0.5206)	loss 0.6220 (0.6251)	loss-cls 9.9478 (9.9996)	loss-aux 0.0034 (0.0018)	grad_norm 0.9425 (1.2224)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7224
[32m[2023-01-06 23:00:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][911/54685]	eta 7:45:56 lr 0.0009999999	 wd 0.1000	time 0.5471 (0.5199)	loss 0.6182 (0.6251)	loss-cls 9.8873 (10.0005)	loss-aux 0.0032 (0.0019)	grad_norm 0.8999 (1.2167)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6919
[32m[2023-01-06 23:01:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][927/54685]	eta 7:45:14 lr 0.0009999999	 wd 0.1000	time 0.5612 (0.5193)	loss 0.6238 (0.6252)	loss-cls 9.9771 (10.0013)	loss-aux 0.0035 (0.0019)	grad_norm 1.0131 (1.2132)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7403
[32m[2023-01-06 23:01:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][943/54685]	eta 7:44:34 lr 0.0009999999	 wd 0.1000	time 0.5629 (0.5187)	loss 0.6227 (0.6253)	loss-cls 9.9606 (10.0025)	loss-aux 0.0033 (0.0019)	grad_norm 0.9051 (1.2080)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7529
[32m[2023-01-06 23:01:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][959/54685]	eta 7:43:56 lr 0.0009999999	 wd 0.1000	time 0.5678 (0.5181)	loss 0.6168 (0.6253)	loss-cls 9.8662 (10.0027)	loss-aux 0.0031 (0.0019)	grad_norm 0.8643 (1.2022)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7641
[32m[2023-01-06 23:01:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][975/54685]	eta 7:43:19 lr 0.0009999999	 wd 0.1000	time 0.5597 (0.5176)	loss 0.6260 (0.6253)	loss-cls 10.0138 (10.0025)	loss-aux 0.0024 (0.0019)	grad_norm 1.3518 (1.2047)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7755
[32m[2023-01-06 23:01:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][991/54685]	eta 7:42:47 lr 0.0009999999	 wd 0.1000	time 0.5640 (0.5171)	loss 0.6169 (0.6253)	loss-cls 9.8674 (10.0035)	loss-aux 0.0030 (0.0020)	grad_norm 4.9425 (1.2650)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.8374
[32m[2023-01-06 23:01:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1007/54685]	eta 7:42:03 lr 0.0009999999	 wd 0.1000	time 0.5551 (0.5165)	loss 0.6288 (0.6254)	loss-cls 10.0580 (10.0041)	loss-aux 0.0033 (0.0020)	grad_norm 1.0499 (1.2616)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6146
[32m[2023-01-06 23:01:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1023/54685]	eta 7:41:21 lr 0.0009999999	 wd 0.1000	time 0.5131 (0.5159)	loss 0.6174 (0.6254)	loss-cls 9.8763 (10.0046)	loss-aux 0.0027 (0.0020)	grad_norm 0.9471 (1.2567)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6194
[32m[2023-01-06 23:01:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1039/54685]	eta 7:40:47 lr 0.0009999999	 wd 0.1000	time 0.5602 (0.5154)	loss 0.6341 (0.6255)	loss-cls 10.1424 (10.0054)	loss-aux 0.0026 (0.0020)	grad_norm 0.9743 (1.2523)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7496
[32m[2023-01-06 23:02:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1055/54685]	eta 7:40:12 lr 0.0009999999	 wd 0.1000	time 0.5309 (0.5149)	loss 0.6186 (0.6255)	loss-cls 9.8942 (10.0053)	loss-aux 0.0029 (0.0020)	grad_norm 0.9826 (1.2482)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7173
[32m[2023-01-06 23:02:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1071/54685]	eta 7:39:48 lr 0.0009999999	 wd 0.1000	time 0.5251 (0.5146)	loss 0.6439 (0.6255)	loss-cls 10.2998 (10.0056)	loss-aux 0.0033 (0.0020)	grad_norm 2.0913 (1.2608)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.9307
[32m[2023-01-06 23:02:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1087/54685]	eta 7:39:19 lr 0.0009999999	 wd 0.1000	time 0.5611 (0.5142)	loss 0.6202 (0.6255)	loss-cls 9.9198 (10.0064)	loss-aux 0.0033 (0.0021)	grad_norm 1.1557 (1.2593)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.8051
[32m[2023-01-06 23:02:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1103/54685]	eta 7:38:41 lr 0.0009999999	 wd 0.1000	time 0.5562 (0.5136)	loss 0.6335 (0.6255)	loss-cls 10.1326 (10.0060)	loss-aux 0.0039 (0.0021)	grad_norm 0.5959 (1.2497)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6149
[32m[2023-01-06 23:02:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1119/54685]	eta 7:38:05 lr 0.0009999999	 wd 0.1000	time 0.5486 (0.5131)	loss 0.6207 (0.6254)	loss-cls 9.9287 (10.0049)	loss-aux 0.0033 (0.0021)	grad_norm 2.1136 (1.2620)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6408
[32m[2023-01-06 23:02:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1135/54685]	eta 7:37:28 lr 0.0009999999	 wd 0.1000	time 0.5379 (0.5126)	loss 0.6232 (0.6254)	loss-cls 9.9676 (10.0038)	loss-aux 0.0029 (0.0021)	grad_norm 0.7631 (1.2550)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5835
[32m[2023-01-06 23:02:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1151/54685]	eta 7:36:49 lr 0.0009999999	 wd 0.1000	time 0.5203 (0.5120)	loss 0.6148 (0.6253)	loss-cls 9.8334 (10.0022)	loss-aux 0.0029 (0.0021)	grad_norm 0.4994 (1.2445)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5555
[32m[2023-01-06 23:02:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1167/54685]	eta 7:36:17 lr 0.0009999999	 wd 0.1000	time 0.5869 (0.5116)	loss 0.6089 (0.6252)	loss-cls 9.7403 (10.0014)	loss-aux 0.0026 (0.0021)	grad_norm 7.4795 (1.3299)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6579
[32m[2023-01-06 23:03:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1183/54685]	eta 7:35:49 lr 0.0009999999	 wd 0.1000	time 0.5553 (0.5112)	loss 0.6157 (0.6252)	loss-cls 9.8492 (10.0003)	loss-aux 0.0026 (0.0021)	grad_norm 0.4654 (1.3182)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7473
[32m[2023-01-06 23:03:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1199/54685]	eta 7:35:22 lr 0.0009999999	 wd 0.1000	time 0.5666 (0.5108)	loss 0.6152 (0.6251)	loss-cls 9.8413 (9.9990)	loss-aux 0.0026 (0.0021)	grad_norm 0.8376 (1.3118)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7605
[32m[2023-01-06 23:03:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1215/54685]	eta 7:34:56 lr 0.0009999999	 wd 0.1000	time 0.5589 (0.5105)	loss 0.6191 (0.6250)	loss-cls 9.9022 (9.9982)	loss-aux 0.0030 (0.0021)	grad_norm 0.9494 (1.3070)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7765
[32m[2023-01-06 23:03:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1231/54685]	eta 7:34:31 lr 0.0009999999	 wd 0.1000	time 0.5646 (0.5102)	loss 0.6218 (0.6250)	loss-cls 9.9459 (9.9974)	loss-aux 0.0028 (0.0022)	grad_norm 0.4230 (1.2955)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7665
[32m[2023-01-06 23:03:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1247/54685]	eta 7:34:03 lr 0.0009999998	 wd 0.1000	time 0.5414 (0.5098)	loss 0.6142 (0.6249)	loss-cls 9.8240 (9.9965)	loss-aux 0.0026 (0.0022)	grad_norm 0.4391 (1.2846)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7159
[32m[2023-01-06 23:03:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1263/54685]	eta 7:33:34 lr 0.0009999998	 wd 0.1000	time 0.5381 (0.5094)	loss 0.6200 (0.6248)	loss-cls 9.9178 (9.9952)	loss-aux 0.0026 (0.0022)	grad_norm 0.8835 (1.2795)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6521
[32m[2023-01-06 23:03:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1279/54685]	eta 7:33:07 lr 0.0009999998	 wd 0.1000	time 0.5369 (0.5091)	loss 0.6166 (0.6248)	loss-cls 9.8633 (9.9944)	loss-aux 0.0031 (0.0022)	grad_norm 0.5922 (1.2709)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6972
[32m[2023-01-06 23:04:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1295/54685]	eta 7:32:42 lr 0.0009999998	 wd 0.1000	time 0.5542 (0.5088)	loss 0.6135 (0.6247)	loss-cls 9.8138 (9.9929)	loss-aux 0.0029 (0.0022)	grad_norm 0.8170 (1.2653)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.7361
[32m[2023-01-06 23:04:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1311/54685]	eta 7:32:15 lr 0.0009999998	 wd 0.1000	time 0.5565 (0.5084)	loss 0.6090 (0.6246)	loss-cls 9.7414 (9.9916)	loss-aux 0.0029 (0.0022)	grad_norm 0.5760 (1.2569)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6929
[32m[2023-01-06 23:04:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1327/54685]	eta 7:31:49 lr 0.0009999998	 wd 0.1000	time 0.5575 (0.5081)	loss 0.6195 (0.6245)	loss-cls 9.9086 (9.9905)	loss-aux 0.0028 (0.0022)	grad_norm 0.4806 (1.2475)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6705
[32m[2023-01-06 23:04:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1343/54685]	eta 7:31:23 lr 0.0009999998	 wd 0.1000	time 0.5558 (0.5077)	loss 0.6228 (0.6245)	loss-cls 9.9619 (9.9898)	loss-aux 0.0028 (0.0022)	grad_norm 0.5285 (1.2390)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6775
[32m[2023-01-06 23:04:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1359/54685]	eta 7:30:55 lr 0.0009999998	 wd 0.1000	time 0.5460 (0.5074)	loss 0.6173 (0.6244)	loss-cls 9.8745 (9.9886)	loss-aux 0.0027 (0.0022)	grad_norm 0.4092 (1.2292)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6286
[32m[2023-01-06 23:04:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1375/54685]	eta 7:30:20 lr 0.0009999998	 wd 0.1000	time 0.5161 (0.5069)	loss 0.6095 (0.6243)	loss-cls 9.7480 (9.9870)	loss-aux 0.0033 (0.0022)	grad_norm 0.4363 (1.2200)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4160
[32m[2023-01-06 23:04:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1391/54685]	eta 7:29:45 lr 0.0009999998	 wd 0.1000	time 0.5299 (0.5064)	loss 0.6361 (0.6243)	loss-cls 10.1747 (9.9859)	loss-aux 0.0030 (0.0022)	grad_norm 0.4437 (1.2111)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4215
[32m[2023-01-06 23:04:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1407/54685]	eta 7:29:10 lr 0.0009999998	 wd 0.1000	time 0.5413 (0.5059)	loss 0.6231 (0.6242)	loss-cls 9.9658 (9.9849)	loss-aux 0.0032 (0.0022)	grad_norm 0.4520 (1.2024)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3932
[32m[2023-01-06 23:05:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1423/54685]	eta 7:28:40 lr 0.0009999998	 wd 0.1000	time 0.5200 (0.5054)	loss 0.6199 (0.6242)	loss-cls 9.9160 (9.9842)	loss-aux 0.0029 (0.0023)	grad_norm 0.3635 (1.1930)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4863
[32m[2023-01-06 23:05:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1439/54685]	eta 7:28:15 lr 0.0009999998	 wd 0.1000	time 0.5327 (0.5051)	loss 0.6190 (0.6241)	loss-cls 9.9009 (9.9835)	loss-aux 0.0031 (0.0023)	grad_norm 0.3520 (1.1837)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6483
[32m[2023-01-06 23:05:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1455/54685]	eta 7:27:51 lr 0.0009999998	 wd 0.1000	time 0.5776 (0.5048)	loss 0.6192 (0.6240)	loss-cls 9.9039 (9.9825)	loss-aux 0.0031 (0.0023)	grad_norm 0.3428 (1.1744)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6508
[32m[2023-01-06 23:05:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1471/54685]	eta 7:27:24 lr 0.0009999998	 wd 0.1000	time 0.5342 (0.5045)	loss 0.6149 (0.6240)	loss-cls 9.8357 (9.9814)	loss-aux 0.0032 (0.0023)	grad_norm 0.3167 (1.1651)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5365
[32m[2023-01-06 23:05:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1487/54685]	eta 7:27:01 lr 0.0009999998	 wd 0.1000	time 0.5473 (0.5042)	loss 0.6236 (0.6239)	loss-cls 9.9743 (9.9804)	loss-aux 0.0034 (0.0023)	grad_norm 0.5638 (1.1586)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6706
[32m[2023-01-06 23:05:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1503/54685]	eta 7:26:32 lr 0.0009999998	 wd 0.1000	time 0.5595 (0.5038)	loss 0.6245 (0.6239)	loss-cls 9.9896 (9.9796)	loss-aux 0.0032 (0.0023)	grad_norm 0.3333 (1.1499)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4499
[32m[2023-01-06 23:05:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1519/54685]	eta 7:26:03 lr 0.0009999998	 wd 0.1000	time 0.5483 (0.5034)	loss 0.6228 (0.6238)	loss-cls 9.9610 (9.9784)	loss-aux 0.0033 (0.0023)	grad_norm 0.3296 (1.1412)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4828
[32m[2023-01-06 23:05:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1535/54685]	eta 7:25:34 lr 0.0009999998	 wd 0.1000	time 0.5484 (0.5030)	loss 0.6161 (0.6237)	loss-cls 9.8548 (9.9774)	loss-aux 0.0032 (0.0023)	grad_norm 11.0074 (1.2440)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4406
[32m[2023-01-06 23:06:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1551/54685]	eta 7:25:07 lr 0.0009999998	 wd 0.1000	time 0.5233 (0.5027)	loss 0.6251 (0.6237)	loss-cls 9.9980 (9.9767)	loss-aux 0.0033 (0.0023)	grad_norm 0.3491 (1.2348)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5051
[32m[2023-01-06 23:06:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1567/54685]	eta 7:24:41 lr 0.0009999998	 wd 0.1000	time 0.5305 (0.5023)	loss 0.5994 (0.6237)	loss-cls 9.5871 (9.9761)	loss-aux 0.0031 (0.0023)	grad_norm 0.4447 (1.2267)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4925
[32m[2023-01-06 23:06:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1583/54685]	eta 7:24:17 lr 0.0009999998	 wd 0.1000	time 0.5460 (0.5020)	loss 0.6215 (0.6236)	loss-cls 9.9412 (9.9755)	loss-aux 0.0032 (0.0023)	grad_norm 1.3944 (1.2284)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5578
[32m[2023-01-06 23:06:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1599/54685]	eta 7:23:53 lr 0.0009999997	 wd 0.1000	time 0.5155 (0.5017)	loss 0.6119 (0.6236)	loss-cls 9.7872 (9.9749)	loss-aux 0.0033 (0.0024)	grad_norm 0.3438 (1.2196)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5618
[32m[2023-01-06 23:06:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1615/54685]	eta 7:23:32 lr 0.0009999997	 wd 0.1000	time 0.5453 (0.5015)	loss 0.6252 (0.6236)	loss-cls 9.9989 (9.9746)	loss-aux 0.0042 (0.0024)	grad_norm 0.4272 (1.2117)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6234
[32m[2023-01-06 23:06:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1631/54685]	eta 7:23:07 lr 0.0009999997	 wd 0.1000	time 0.5379 (0.5011)	loss 0.6155 (0.6235)	loss-cls 9.8445 (9.9738)	loss-aux 0.0037 (0.0024)	grad_norm 0.4656 (1.2044)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5249
[32m[2023-01-06 23:06:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1647/54685]	eta 7:22:41 lr 0.0009999997	 wd 0.1000	time 0.5466 (0.5008)	loss 0.6162 (0.6235)	loss-cls 9.8565 (9.9734)	loss-aux 0.0033 (0.0024)	grad_norm 0.7460 (1.2000)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4321
[32m[2023-01-06 23:06:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1663/54685]	eta 7:22:08 lr 0.0009999997	 wd 0.1000	time 0.5073 (0.5003)	loss 0.6130 (0.6234)	loss-cls 9.8028 (9.9727)	loss-aux 0.0044 (0.0024)	grad_norm 0.3284 (1.1916)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.2454
[32m[2023-01-06 23:07:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1679/54685]	eta 7:21:38 lr 0.0009999997	 wd 0.1000	time 0.5304 (0.4999)	loss 0.6150 (0.6234)	loss-cls 9.8358 (9.9718)	loss-aux 0.0039 (0.0024)	grad_norm 0.3164 (1.1832)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3154
[32m[2023-01-06 23:07:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1695/54685]	eta 7:21:15 lr 0.0009999997	 wd 0.1000	time 0.5457 (0.4996)	loss 0.6205 (0.6233)	loss-cls 9.9232 (9.9707)	loss-aux 0.0043 (0.0024)	grad_norm 0.2579 (1.1745)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5206
[32m[2023-01-06 23:07:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1711/54685]	eta 7:20:52 lr 0.0009999997	 wd 0.1000	time 0.5312 (0.4993)	loss 0.6117 (0.6233)	loss-cls 9.7826 (9.9698)	loss-aux 0.0040 (0.0025)	grad_norm 0.2923 (1.1663)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4863
[32m[2023-01-06 23:07:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1727/54685]	eta 7:20:26 lr 0.0009999997	 wd 0.1000	time 0.5633 (0.4990)	loss 0.6119 (0.6232)	loss-cls 9.7864 (9.9686)	loss-aux 0.0038 (0.0025)	grad_norm 0.2688 (1.1580)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4229
[32m[2023-01-06 23:07:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1743/54685]	eta 7:20:03 lr 0.0009999997	 wd 0.1000	time 0.5341 (0.4987)	loss 0.6141 (0.6232)	loss-cls 9.8211 (9.9683)	loss-aux 0.0042 (0.0025)	grad_norm 1.8835 (1.1646)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4832
[32m[2023-01-06 23:07:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1759/54685]	eta 7:19:42 lr 0.0009999997	 wd 0.1000	time 0.5518 (0.4985)	loss 0.6189 (0.6231)	loss-cls 9.8981 (9.9675)	loss-aux 0.0044 (0.0025)	grad_norm 0.2811 (1.1566)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5280
[32m[2023-01-06 23:07:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1775/54685]	eta 7:19:21 lr 0.0009999997	 wd 0.1000	time 0.5411 (0.4982)	loss 0.6037 (0.6231)	loss-cls 9.6550 (9.9664)	loss-aux 0.0043 (0.0025)	grad_norm 1.4786 (1.1595)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5391
[32m[2023-01-06 23:07:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1791/54685]	eta 7:19:01 lr 0.0009999997	 wd 0.1000	time 0.5359 (0.4980)	loss 0.6092 (0.6230)	loss-cls 9.7431 (9.9658)	loss-aux 0.0045 (0.0025)	grad_norm 0.2794 (1.1516)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5901
[32m[2023-01-06 23:08:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1807/54685]	eta 7:18:44 lr 0.0009999997	 wd 0.1000	time 0.5337 (0.4978)	loss 0.6166 (0.6230)	loss-cls 9.8607 (9.9650)	loss-aux 0.0046 (0.0026)	grad_norm 0.2489 (1.1436)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6477
[32m[2023-01-06 23:08:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1823/54685]	eta 7:18:24 lr 0.0009999997	 wd 0.1000	time 0.5433 (0.4976)	loss 0.6233 (0.6229)	loss-cls 9.9674 (9.9641)	loss-aux 0.0047 (0.0026)	grad_norm 0.3004 (1.1362)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5610
[32m[2023-01-06 23:08:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1839/54685]	eta 7:18:06 lr 0.0009999997	 wd 0.1000	time 0.5400 (0.4974)	loss 0.6176 (0.6229)	loss-cls 9.8773 (9.9633)	loss-aux 0.0046 (0.0026)	grad_norm 0.2487 (1.1285)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6183
[32m[2023-01-06 23:08:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1855/54685]	eta 7:17:50 lr 0.0009999997	 wd 0.1000	time 0.5567 (0.4973)	loss 0.6161 (0.6228)	loss-cls 9.8528 (9.9627)	loss-aux 0.0047 (0.0026)	grad_norm 0.2567 (1.1210)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6428
[32m[2023-01-06 23:08:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1871/54685]	eta 7:17:32 lr 0.0009999997	 wd 0.1000	time 0.5399 (0.4971)	loss 0.6117 (0.6228)	loss-cls 9.7825 (9.9619)	loss-aux 0.0045 (0.0026)	grad_norm 1.0635 (1.1205)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6045
[32m[2023-01-06 23:08:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1887/54685]	eta 7:17:15 lr 0.0009999996	 wd 0.1000	time 0.5603 (0.4969)	loss 0.6096 (0.6227)	loss-cls 9.7490 (9.9608)	loss-aux 0.0043 (0.0026)	grad_norm 0.2374 (1.1130)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6372
[32m[2023-01-06 23:08:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1903/54685]	eta 7:16:55 lr 0.0009999996	 wd 0.1000	time 0.5349 (0.4967)	loss 0.6152 (0.6226)	loss-cls 9.8386 (9.9596)	loss-aux 0.0047 (0.0027)	grad_norm 0.2713 (1.1060)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5101
[32m[2023-01-06 23:08:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1919/54685]	eta 7:16:35 lr 0.0009999996	 wd 0.1000	time 0.5426 (0.4964)	loss 0.6112 (0.6226)	loss-cls 9.7740 (9.9590)	loss-aux 0.0045 (0.0027)	grad_norm 0.3110 (1.0993)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5074
[32m[2023-01-06 23:09:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1935/54685]	eta 7:16:10 lr 0.0009999996	 wd 0.1000	time 0.5269 (0.4961)	loss 0.6040 (0.6226)	loss-cls 9.6592 (9.9583)	loss-aux 0.0045 (0.0027)	grad_norm 0.3596 (1.0932)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3246
[32m[2023-01-06 23:09:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1951/54685]	eta 7:15:50 lr 0.0009999996	 wd 0.1000	time 0.5057 (0.4959)	loss 0.6219 (0.6225)	loss-cls 9.9453 (9.9576)	loss-aux 0.0049 (0.0027)	grad_norm 0.2573 (1.0864)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4938
[32m[2023-01-06 23:09:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1967/54685]	eta 7:15:33 lr 0.0009999996	 wd 0.1000	time 0.5298 (0.4957)	loss 0.6144 (0.6225)	loss-cls 9.8257 (9.9570)	loss-aux 0.0048 (0.0027)	grad_norm 0.3012 (1.0800)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5840
[32m[2023-01-06 23:09:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1983/54685]	eta 7:15:17 lr 0.0009999996	 wd 0.1000	time 0.4851 (0.4956)	loss 0.6074 (0.6224)	loss-cls 9.7139 (9.9561)	loss-aux 0.0048 (0.0027)	grad_norm 0.3331 (1.0740)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6451
[32m[2023-01-06 23:09:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1999/54685]	eta 7:14:58 lr 0.0009999996	 wd 0.1000	time 0.5190 (0.4954)	loss 0.6089 (0.6224)	loss-cls 9.7375 (9.9554)	loss-aux 0.0048 (0.0028)	grad_norm 0.2970 (1.0677)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5186
[32m[2023-01-06 23:09:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2015/54685]	eta 7:14:42 lr 0.0009999996	 wd 0.1000	time 0.5574 (0.4952)	loss 0.6113 (0.6223)	loss-cls 9.7761 (9.9544)	loss-aux 0.0048 (0.0028)	grad_norm 0.4914 (1.0632)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5857
[32m[2023-01-06 23:09:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2031/54685]	eta 7:14:24 lr 0.0009999996	 wd 0.1000	time 0.5395 (0.4950)	loss 0.6058 (0.6223)	loss-cls 9.6887 (9.9533)	loss-aux 0.0045 (0.0028)	grad_norm 0.3098 (1.0572)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5550
[32m[2023-01-06 23:09:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2047/54685]	eta 7:14:08 lr 0.0009999996	 wd 0.1000	time 0.5024 (0.4949)	loss 0.6170 (0.6222)	loss-cls 9.8680 (9.9532)	loss-aux 0.0047 (0.0028)	grad_norm 0.3033 (1.0513)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5867
[32m[2023-01-06 23:10:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2063/54685]	eta 7:13:50 lr 0.0009999996	 wd 0.1000	time 0.5206 (0.4947)	loss 0.6223 (0.6222)	loss-cls 9.9526 (9.9525)	loss-aux 0.0044 (0.0028)	grad_norm 0.4908 (1.0470)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5291
[32m[2023-01-06 23:10:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2079/54685]	eta 7:13:34 lr 0.0009999996	 wd 0.1000	time 0.5537 (0.4945)	loss 0.6074 (0.6222)	loss-cls 9.7139 (9.9516)	loss-aux 0.0044 (0.0028)	grad_norm 1.2163 (1.0483)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5872
[32m[2023-01-06 23:10:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2095/54685]	eta 7:13:19 lr 0.0009999996	 wd 0.1000	time 0.5267 (0.4944)	loss 0.6115 (0.6221)	loss-cls 9.7797 (9.9506)	loss-aux 0.0042 (0.0028)	grad_norm 3.3450 (1.0658)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6366
[32m[2023-01-06 23:10:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2111/54685]	eta 7:13:02 lr 0.0009999996	 wd 0.1000	time 0.5765 (0.4942)	loss 0.6064 (0.6220)	loss-cls 9.6981 (9.9497)	loss-aux 0.0046 (0.0028)	grad_norm 0.3523 (1.0604)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5533
[32m[2023-01-06 23:10:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2127/54685]	eta 7:12:43 lr 0.0009999995	 wd 0.1000	time 0.5293 (0.4940)	loss 0.6178 (0.6220)	loss-cls 9.8791 (9.9488)	loss-aux 0.0051 (0.0029)	grad_norm 0.3014 (1.0547)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4757
[32m[2023-01-06 23:10:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2143/54685]	eta 7:12:25 lr 0.0009999995	 wd 0.1000	time 0.5274 (0.4938)	loss 0.6173 (0.6220)	loss-cls 9.8713 (9.9486)	loss-aux 0.0052 (0.0029)	grad_norm 0.3274 (1.0493)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4626
[32m[2023-01-06 23:10:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2159/54685]	eta 7:12:07 lr 0.0009999995	 wd 0.1000	time 0.5025 (0.4936)	loss 0.6230 (0.6219)	loss-cls 9.9622 (9.9478)	loss-aux 0.0055 (0.0029)	grad_norm 1.5718 (1.0532)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4910
[32m[2023-01-06 23:10:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2175/54685]	eta 7:11:49 lr 0.0009999995	 wd 0.1000	time 0.5397 (0.4934)	loss 0.6211 (0.6219)	loss-cls 9.9325 (9.9474)	loss-aux 0.0053 (0.0029)	grad_norm 0.3520 (1.0480)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4971
[32m[2023-01-06 23:11:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2191/54685]	eta 7:11:29 lr 0.0009999995	 wd 0.1000	time 0.4963 (0.4932)	loss 0.6258 (0.6219)	loss-cls 10.0083 (9.9470)	loss-aux 0.0053 (0.0029)	grad_norm 0.3409 (1.0428)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3711
[32m[2023-01-06 23:11:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2207/54685]	eta 7:11:09 lr 0.0009999995	 wd 0.1000	time 0.5415 (0.4929)	loss 0.6075 (0.6218)	loss-cls 9.7146 (9.9463)	loss-aux 0.0051 (0.0029)	grad_norm 0.2528 (1.0371)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3693
[32m[2023-01-06 23:11:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2223/54685]	eta 7:10:49 lr 0.0009999995	 wd 0.1000	time 0.5429 (0.4927)	loss 0.6112 (0.6218)	loss-cls 9.7740 (9.9458)	loss-aux 0.0045 (0.0030)	grad_norm 0.3197 (1.0320)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3879
[32m[2023-01-06 23:11:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2239/54685]	eta 7:10:32 lr 0.0009999995	 wd 0.1000	time 0.5322 (0.4926)	loss 0.6410 (0.6218)	loss-cls 10.2515 (9.9458)	loss-aux 0.0046 (0.0030)	grad_norm 0.6253 (1.0291)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5097
[32m[2023-01-06 23:11:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2255/54685]	eta 7:10:15 lr 0.0009999995	 wd 0.1000	time 0.5753 (0.4924)	loss 0.6163 (0.6218)	loss-cls 9.8567 (9.9453)	loss-aux 0.0047 (0.0030)	grad_norm 0.3628 (1.0243)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4866
[32m[2023-01-06 23:11:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2271/54685]	eta 7:09:59 lr 0.0009999995	 wd 0.1000	time 0.5681 (0.4922)	loss 0.6120 (0.6218)	loss-cls 9.7876 (9.9451)	loss-aux 0.0046 (0.0030)	grad_norm 0.4116 (1.0200)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5140
[32m[2023-01-06 23:11:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2287/54685]	eta 7:09:42 lr 0.0009999995	 wd 0.1000	time 0.5343 (0.4921)	loss 0.6304 (0.6217)	loss-cls 10.0823 (9.9448)	loss-aux 0.0046 (0.0030)	grad_norm 0.4349 (1.0159)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4887
[32m[2023-01-06 23:11:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2303/54685]	eta 7:09:24 lr 0.0009999995	 wd 0.1000	time 0.5331 (0.4918)	loss 0.6172 (0.6217)	loss-cls 9.8700 (9.9440)	loss-aux 0.0049 (0.0030)	grad_norm 0.3932 (1.0116)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4007
[32m[2023-01-06 23:12:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2319/54685]	eta 7:09:09 lr 0.0009999995	 wd 0.1000	time 0.5425 (0.4917)	loss 0.6176 (0.6217)	loss-cls 9.8767 (9.9436)	loss-aux 0.0049 (0.0030)	grad_norm 0.4665 (1.0078)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5871
[32m[2023-01-06 23:12:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2335/54685]	eta 7:08:54 lr 0.0009999995	 wd 0.1000	time 0.5636 (0.4916)	loss 0.6265 (0.6216)	loss-cls 10.0195 (9.9427)	loss-aux 0.0048 (0.0031)	grad_norm 0.3614 (1.0034)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5212
[32m[2023-01-06 23:12:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2351/54685]	eta 7:08:37 lr 0.0009999994	 wd 0.1000	time 0.5576 (0.4914)	loss 0.6089 (0.6216)	loss-cls 9.7374 (9.9419)	loss-aux 0.0048 (0.0031)	grad_norm 0.3382 (0.9989)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4598
[32m[2023-01-06 23:12:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2367/54685]	eta 7:08:21 lr 0.0009999994	 wd 0.1000	time 0.5676 (0.4913)	loss 0.6267 (0.6215)	loss-cls 10.0229 (9.9414)	loss-aux 0.0050 (0.0031)	grad_norm 0.5951 (0.9962)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5137
[32m[2023-01-06 23:12:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2383/54685]	eta 7:08:08 lr 0.0009999994	 wd 0.1000	time 0.5498 (0.4912)	loss 0.6134 (0.6215)	loss-cls 9.8099 (9.9409)	loss-aux 0.0052 (0.0031)	grad_norm 0.3221 (0.9916)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6390
[32m[2023-01-06 23:12:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2399/54685]	eta 7:07:53 lr 0.0009999994	 wd 0.1000	time 0.5261 (0.4910)	loss 0.6149 (0.6215)	loss-cls 9.8344 (9.9402)	loss-aux 0.0048 (0.0031)	grad_norm 0.3433 (0.9873)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5211
[32m[2023-01-06 23:12:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2415/54685]	eta 7:07:39 lr 0.0009999994	 wd 0.1000	time 0.5237 (0.4909)	loss 0.6280 (0.6215)	loss-cls 10.0430 (9.9403)	loss-aux 0.0050 (0.0031)	grad_norm 0.8486 (0.9864)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5453
[32m[2023-01-06 23:12:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2431/54685]	eta 7:07:25 lr 0.0009999994	 wd 0.1000	time 0.5376 (0.4908)	loss 0.6245 (0.6215)	loss-cls 9.9871 (9.9408)	loss-aux 0.0050 (0.0031)	grad_norm 1.2188 (0.9879)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5674
[32m[2023-01-06 23:13:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2447/54685]	eta 7:07:07 lr 0.0009999994	 wd 0.1000	time 0.5314 (0.4906)	loss 0.6377 (0.6215)	loss-cls 10.1981 (9.9416)	loss-aux 0.0047 (0.0031)	grad_norm 0.7171 (0.9862)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3968
[32m[2023-01-06 23:13:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2463/54685]	eta 7:06:52 lr 0.0009999994	 wd 0.1000	time 0.5874 (0.4904)	loss 0.6234 (0.6216)	loss-cls 9.9698 (9.9419)	loss-aux 0.0045 (0.0031)	grad_norm 0.7670 (0.9847)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4854
[32m[2023-01-06 23:13:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2479/54685]	eta 7:06:38 lr 0.0009999994	 wd 0.1000	time 0.5285 (0.4903)	loss 0.6222 (0.6216)	loss-cls 9.9496 (9.9422)	loss-aux 0.0050 (0.0032)	grad_norm 1.2602 (0.9865)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5795
[32m[2023-01-06 23:13:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2495/54685]	eta 7:06:21 lr 0.0009999994	 wd 0.1000	time 0.5201 (0.4902)	loss 0.6257 (0.6216)	loss-cls 10.0067 (9.9424)	loss-aux 0.0048 (0.0032)	grad_norm 0.7969 (0.9853)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3826
[32m[2023-01-06 23:13:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2511/54685]	eta 7:06:03 lr 0.0009999994	 wd 0.1000	time 0.5226 (0.4900)	loss 0.6181 (0.6216)	loss-cls 9.8848 (9.9425)	loss-aux 0.0047 (0.0032)	grad_norm 0.7781 (0.9840)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3880
[32m[2023-01-06 23:13:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2527/54685]	eta 7:05:48 lr 0.0009999994	 wd 0.1000	time 0.5359 (0.4898)	loss 0.6367 (0.6216)	loss-cls 10.1829 (9.9426)	loss-aux 0.0047 (0.0032)	grad_norm 0.7766 (0.9827)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4679
[32m[2023-01-06 23:13:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2543/54685]	eta 7:05:29 lr 0.0009999994	 wd 0.1000	time 0.5102 (0.4896)	loss 0.6134 (0.6216)	loss-cls 9.8088 (9.9431)	loss-aux 0.0050 (0.0032)	grad_norm 0.8381 (0.9818)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.2851
[32m[2023-01-06 23:13:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2559/54685]	eta 7:05:11 lr 0.0009999993	 wd 0.1000	time 0.5212 (0.4894)	loss 0.6294 (0.6217)	loss-cls 10.0662 (9.9433)	loss-aux 0.0049 (0.0032)	grad_norm 0.7915 (0.9806)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3730
[32m[2023-01-06 23:14:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2575/54685]	eta 7:04:57 lr 0.0009999993	 wd 0.1000	time 0.5368 (0.4893)	loss 0.6337 (0.6217)	loss-cls 10.1342 (9.9438)	loss-aux 0.0046 (0.0032)	grad_norm 0.7665 (0.9792)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4884
[32m[2023-01-06 23:14:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2591/54685]	eta 7:04:43 lr 0.0009999993	 wd 0.1000	time 0.5483 (0.4892)	loss 0.6086 (0.6217)	loss-cls 9.7331 (9.9438)	loss-aux 0.0052 (0.0032)	grad_norm 0.7906 (0.9781)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5552
[32m[2023-01-06 23:14:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2607/54685]	eta 7:04:29 lr 0.0009999993	 wd 0.1000	time 0.5591 (0.4891)	loss 0.6423 (0.6217)	loss-cls 10.2729 (9.9445)	loss-aux 0.0047 (0.0032)	grad_norm 0.9920 (0.9782)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4959
[32m[2023-01-06 23:14:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2623/54685]	eta 7:04:13 lr 0.0009999993	 wd 0.1000	time 0.5361 (0.4889)	loss 0.6218 (0.6218)	loss-cls 9.9458 (9.9453)	loss-aux 0.0035 (0.0032)	grad_norm 0.9601 (0.9780)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4271
[32m[2023-01-06 23:14:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2639/54685]	eta 7:03:59 lr 0.0009999993	 wd 0.1000	time 0.5285 (0.4888)	loss 0.6261 (0.6218)	loss-cls 10.0146 (9.9461)	loss-aux 0.0035 (0.0032)	grad_norm 0.9748 (0.9780)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4970
[32m[2023-01-06 23:14:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2655/54685]	eta 7:03:48 lr 0.0009999993	 wd 0.1000	time 0.6359 (0.4887)	loss 0.6495 (0.6219)	loss-cls 10.3876 (9.9467)	loss-aux 0.0036 (0.0032)	grad_norm 0.9493 (0.9778)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6480
[32m[2023-01-06 23:14:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2671/54685]	eta 7:03:36 lr 0.0009999993	 wd 0.1000	time 0.5521 (0.4886)	loss 0.6253 (0.6219)	loss-cls 10.0009 (9.9473)	loss-aux 0.0032 (0.0032)	grad_norm 0.9840 (0.9779)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6108
[32m[2023-01-06 23:14:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2687/54685]	eta 7:03:22 lr 0.0009999993	 wd 0.1000	time 0.5464 (0.4885)	loss 0.6202 (0.6220)	loss-cls 9.9205 (9.9482)	loss-aux 0.0032 (0.0032)	grad_norm 4.5382 (0.9991)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5088
[32m[2023-01-06 23:15:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2703/54685]	eta 7:03:11 lr 0.0009999993	 wd 0.1000	time 0.5281 (0.4885)	loss 0.6156 (0.6220)	loss-cls 9.8459 (9.9485)	loss-aux 0.0034 (0.0032)	grad_norm 0.8948 (0.9985)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6383
[32m[2023-01-06 23:15:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2719/54685]	eta 7:03:00 lr 0.0009999993	 wd 0.1000	time 0.5905 (0.4884)	loss 0.6332 (0.6220)	loss-cls 10.1282 (9.9492)	loss-aux 0.0035 (0.0032)	grad_norm 0.9339 (0.9981)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6386
[32m[2023-01-06 23:15:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2735/54685]	eta 7:02:47 lr 0.0009999993	 wd 0.1000	time 0.5345 (0.4883)	loss 0.6338 (0.6220)	loss-cls 10.1381 (9.9494)	loss-aux 0.0034 (0.0032)	grad_norm 0.8914 (0.9975)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5789
[32m[2023-01-06 23:15:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2751/54685]	eta 7:02:36 lr 0.0009999992	 wd 0.1000	time 0.5504 (0.4882)	loss 0.6253 (0.6221)	loss-cls 10.0022 (9.9500)	loss-aux 0.0034 (0.0032)	grad_norm 0.8504 (0.9966)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6364
[32m[2023-01-06 23:15:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2767/54685]	eta 7:02:22 lr 0.0009999992	 wd 0.1000	time 0.5353 (0.4881)	loss 0.6211 (0.6221)	loss-cls 9.9347 (9.9503)	loss-aux 0.0034 (0.0032)	grad_norm 1.2357 (0.9980)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4651
[32m[2023-01-06 23:15:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2783/54685]	eta 7:02:10 lr 0.0009999992	 wd 0.1000	time 0.5380 (0.4881)	loss 0.6404 (0.6221)	loss-cls 10.2437 (9.9507)	loss-aux 0.0034 (0.0032)	grad_norm 0.9102 (0.9975)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6199
[32m[2023-01-06 23:15:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2799/54685]	eta 7:02:00 lr 0.0009999992	 wd 0.1000	time 0.5342 (0.4880)	loss 0.6227 (0.6221)	loss-cls 9.9595 (9.9507)	loss-aux 0.0034 (0.0032)	grad_norm 1.1600 (0.9984)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6687
[32m[2023-01-06 23:15:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2815/54685]	eta 7:01:46 lr 0.0009999992	 wd 0.1000	time 0.5505 (0.4879)	loss 0.6119 (0.6221)	loss-cls 9.7867 (9.9511)	loss-aux 0.0035 (0.0032)	grad_norm 1.1122 (0.9991)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4824
[32m[2023-01-06 23:16:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2831/54685]	eta 7:01:33 lr 0.0009999992	 wd 0.1000	time 0.5084 (0.4878)	loss 0.6307 (0.6222)	loss-cls 10.0872 (9.9520)	loss-aux 0.0035 (0.0032)	grad_norm 0.9079 (0.9985)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5154
[32m[2023-01-06 23:16:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2847/54685]	eta 7:01:22 lr 0.0009999992	 wd 0.1000	time 0.5623 (0.4877)	loss 0.6423 (0.6222)	loss-cls 10.2730 (9.9524)	loss-aux 0.0034 (0.0032)	grad_norm 3.8259 (1.0144)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.6066
[32m[2023-01-06 23:16:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2863/54685]	eta 7:01:10 lr 0.0009999992	 wd 0.1000	time 0.5376 (0.4876)	loss 0.6449 (0.6223)	loss-cls 10.3157 (9.9533)	loss-aux 0.0033 (0.0032)	grad_norm 1.0559 (1.0147)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5993
[32m[2023-01-06 23:16:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2879/54685]	eta 7:00:57 lr 0.0009999992	 wd 0.1000	time 0.5328 (0.4875)	loss 0.6309 (0.6223)	loss-cls 10.0909 (9.9537)	loss-aux 0.0034 (0.0032)	grad_norm 0.8710 (1.0139)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5062
[32m[2023-01-06 23:16:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2895/54685]	eta 7:00:42 lr 0.0009999992	 wd 0.1000	time 0.5165 (0.4874)	loss 0.6390 (0.6223)	loss-cls 10.2199 (9.9542)	loss-aux 0.0035 (0.0032)	grad_norm 0.9101 (1.0133)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3806
[32m[2023-01-06 23:16:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2911/54685]	eta 7:00:30 lr 0.0009999992	 wd 0.1000	time 0.5240 (0.4873)	loss 0.6215 (0.6224)	loss-cls 9.9405 (9.9546)	loss-aux 0.0033 (0.0032)	grad_norm 1.2728 (1.0147)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5485
[32m[2023-01-06 23:16:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2927/54685]	eta 7:00:17 lr 0.0009999991	 wd 0.1000	time 0.5314 (0.4872)	loss 0.6274 (0.6224)	loss-cls 10.0359 (9.9552)	loss-aux 0.0033 (0.0032)	grad_norm 0.8454 (1.0138)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5189
[32m[2023-01-06 23:16:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2943/54685]	eta 7:00:03 lr 0.0009999991	 wd 0.1000	time 0.5772 (0.4871)	loss 0.6314 (0.6224)	loss-cls 10.0985 (9.9556)	loss-aux 0.0033 (0.0032)	grad_norm 0.8648 (1.0130)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4749
[32m[2023-01-06 23:17:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2959/54685]	eta 6:59:47 lr 0.0009999991	 wd 0.1000	time 0.5283 (0.4869)	loss 0.6224 (0.6224)	loss-cls 9.9547 (9.9557)	loss-aux 0.0034 (0.0033)	grad_norm 0.8016 (1.0118)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3080
[32m[2023-01-06 23:17:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2975/54685]	eta 6:59:33 lr 0.0009999991	 wd 0.1000	time 0.5275 (0.4868)	loss 0.6238 (0.6225)	loss-cls 9.9780 (9.9562)	loss-aux 0.0034 (0.0033)	grad_norm 0.8153 (1.0108)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4393
[32m[2023-01-06 23:17:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][2991/54685]	eta 6:59:17 lr 0.0009999991	 wd 0.1000	time 0.5172 (0.4867)	loss 0.6113 (0.6225)	loss-cls 9.7777 (9.9564)	loss-aux 0.0035 (0.0033)	grad_norm 0.8319 (1.0098)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3173
[32m[2023-01-06 23:17:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3007/54685]	eta 6:59:01 lr 0.0009999991	 wd 0.1000	time 0.4978 (0.4865)	loss 0.6128 (0.6225)	loss-cls 9.8013 (9.9564)	loss-aux 0.0034 (0.0033)	grad_norm 0.7676 (1.0085)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3089
[32m[2023-01-06 23:17:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3023/54685]	eta 6:58:46 lr 0.0009999991	 wd 0.1000	time 0.5230 (0.4864)	loss 0.6412 (0.6225)	loss-cls 10.2554 (9.9565)	loss-aux 0.0035 (0.0033)	grad_norm 0.7462 (1.0071)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3256
[32m[2023-01-06 23:17:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3039/54685]	eta 6:58:32 lr 0.0009999991	 wd 0.1000	time 0.5267 (0.4862)	loss 0.6409 (0.6225)	loss-cls 10.2503 (9.9568)	loss-aux 0.0034 (0.0033)	grad_norm 0.7510 (1.0058)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4069
[32m[2023-01-06 23:17:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3055/54685]	eta 6:58:17 lr 0.0009999991	 wd 0.1000	time 0.5115 (0.4861)	loss 0.6172 (0.6225)	loss-cls 9.8725 (9.9568)	loss-aux 0.0035 (0.0033)	grad_norm 0.7411 (1.0044)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3900
[32m[2023-01-06 23:17:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3071/54685]	eta 6:58:04 lr 0.0009999991	 wd 0.1000	time 0.5431 (0.4860)	loss 0.6026 (0.6225)	loss-cls 9.6387 (9.9569)	loss-aux 0.0033 (0.0033)	grad_norm 0.8002 (1.0033)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4340
[32m[2023-01-06 23:18:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3087/54685]	eta 6:57:51 lr 0.0009999990	 wd 0.1000	time 0.5657 (0.4859)	loss 0.6132 (0.6225)	loss-cls 9.8074 (9.9574)	loss-aux 0.0034 (0.0033)	grad_norm 0.7405 (1.0020)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4751
[32m[2023-01-06 23:18:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3103/54685]	eta 6:57:37 lr 0.0009999990	 wd 0.1000	time 0.5391 (0.4858)	loss 0.6122 (0.6225)	loss-cls 9.7911 (9.9569)	loss-aux 0.0034 (0.0033)	grad_norm 0.7306 (1.0006)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3986
[32m[2023-01-06 23:18:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3119/54685]	eta 6:57:23 lr 0.0009999990	 wd 0.1000	time 0.5318 (0.4857)	loss 0.6184 (0.6225)	loss-cls 9.8909 (9.9567)	loss-aux 0.0035 (0.0033)	grad_norm 0.6925 (0.9990)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4020
[32m[2023-01-06 23:18:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3135/54685]	eta 6:57:11 lr 0.0009999990	 wd 0.1000	time 0.5940 (0.4856)	loss 0.6209 (0.6225)	loss-cls 9.9313 (9.9568)	loss-aux 0.0034 (0.0033)	grad_norm 0.7135 (0.9975)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.5508
[32m[2023-01-06 23:18:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3151/54685]	eta 6:56:59 lr 0.0009999990	 wd 0.1000	time 0.5291 (0.4855)	loss 0.6322 (0.6225)	loss-cls 10.1125 (9.9569)	loss-aux 0.0035 (0.0033)	grad_norm 0.6971 (0.9960)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.4848
[32m[2023-01-06 23:18:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3167/54685]	eta 6:56:45 lr 0.0009999990	 wd 0.1000	time 0.5355 (0.4854)	loss 0.6412 (0.6225)	loss-cls 10.2559 (9.9569)	loss-aux 0.0034 (0.0033)	grad_norm 0.6421 (0.9942)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3622
[32m[2023-01-06 23:18:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3183/54685]	eta 6:56:30 lr 0.0009999990	 wd 0.1000	time 0.5282 (0.4852)	loss 0.6245 (0.6225)	loss-cls 9.9888 (9.9569)	loss-aux 0.0034 (0.0033)	grad_norm 0.6642 (0.9926)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3622
[32m[2023-01-06 23:18:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][3199/54685]	eta 6:56:17 lr 0.0009999990	 wd 0.1000	time 0.5355 (0.4851)	loss 0.6113 (0.6225)	loss-cls 9.7774 (9.9568)	loss-aux 0.0033 (0.0033)	grad_norm 0.6187 (0.9907)	loss_scale 65536.0000 (65536.0000)	mem 7557MB	batch_time 7.3943
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6850.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6669.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15811.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30043.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22995.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7953.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12142.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_14701.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13306.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10586.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_1796.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6247.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22166.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13419.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5664.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_16737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22719.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10242.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13396.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6710.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20118.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_16430.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_7026.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24544.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8812.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13244.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29025.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27221.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_1914.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24434.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12740.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_10353.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15830.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n06470073/n06470073_47249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25750.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22980.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_15341.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27611.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18729.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25530.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_26924.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28288.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29695.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7365.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11816.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2040.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6236.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2322.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33623.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18350.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12757.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9282.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12206.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13103.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9041.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14020.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19281.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7996.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6567.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9031.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27412.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11827.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9215.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7072.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13641.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13871.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12231.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28595.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17877.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n02368116/n02368116_318.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7013.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13261.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_16320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_21756.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7411.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22698.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19005.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03957420/n03957420_33553.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10675.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17471.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5051.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33630.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9401.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_11746.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8806.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15455.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_9566.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27296.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11766.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2852.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12064.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22581.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25408.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8645.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30926.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25717.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_3493.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15480.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4456.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4524.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_4539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9068.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8783.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33259.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27578.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27317.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9819.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15697.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_135.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8873.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_32625.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8925.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27627.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7728.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_34297.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14718.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10175248/n10175248_583.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13516.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24638.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12108.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_23316.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12364.JPEG
[32m[2023-01-06 23:18:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 282)[0m: INFO EPOCH 0 training takes 0:25:53
