+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ scontrol show JobId=102222
++ grep BatchHost
++ scontrol show JobId=102222
++ grep BatchHost
++ scontrol show JobId=102222
++ scontrol show JobId=102222
++ tr = ' '
++ scontrol show JobId=102222
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=102222
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=102222
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ scontrol show JobId=102222
++ scontrol show JobId=102222
++ grep BatchHost
++ scontrol show JobId=102222
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ scontrol show JobId=102222
++ scontrol show JobId=102222
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=102222
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=102222
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102222
++ tr = ' '
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export MASTER_ADDR=nico1
+ export NNODES=4
+ NNODES=4
+ MASTER_ADDR=nico1
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NNODES=4
+ NNODES=4
+ export RANK=7
+ RANK=7
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ python_args=
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export MASTER_ADDR=nico1
+ localrank=6
+ MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ localrank=4
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=102222
+ export CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
++ scontrol show JobId=102222
++ grep BatchHost
++ grep BatchHost
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ CUDA_VISIBLE_DEVICES=2
++ tr = ' '
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ export NNODES=4
+ NNODES=4
++ tr = ' '
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ awk '{print $2}'
++ awk '{print $2}'
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ MAX_JOBS=64
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args=
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=13
+ RANK=13
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ true
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ false
+ true
+ EXEC=./main_moe.py
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args=
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' OFF == ON ']'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ false
+ true
+ EXEC=./main_moe.py
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ export RANK=22
+ RANK=22
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ MASTER_ADDR=nico1
+ export RANK=21
+ RANK=21
+ export WORLD_SIZE=32
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ WORLD_SIZE=32
+ localrank=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export NNODES=4
+ NNODES=4
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export RANK=23
+ RANK=23
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ false
+ true
+ EXEC=./main_moe.py
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ false
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=16
+ RANK=16
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=29
+ RANK=29
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export MASTER_ADDR=nico1
+ export NNODES=4
+ NNODES=4
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ export RANK=30
+ RANK=30
+ export WORLD_SIZE=32
+ export RANK=28
+ WORLD_SIZE=32
+ localrank=6
+ RANK=28
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export NNODES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ NNODES=4
++ grep BatchHost
++ scontrol show JobId=102222
++ tr = ' '
++ awk '{print $2}'
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ '[' nico == nico ']'
+ cd /home/zms/model_training/MoE/FastSwin
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ false
+ true
+ EXEC=./main_moe.py
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ '[' OFF == ON ']'
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ false
+ true
+ EXEC=./main_moe.py
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=18
+ RANK=18
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
++ scontrol show JobId=102222
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=24
+ RANK=24
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=20
+ RANK=20
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=31
+ RANK=31
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=17
+ RANK=17
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=27
+ RANK=27
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=19
+ RANK=19
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=25
+ RANK=25
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102222
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=26
+ RANK=26
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-05T06:58:10+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 0/32
[32m[2023-01-05 06:58:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 403)[0m: INFO Full config saved to /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default/config.json
[32m[2023-01-05 06:58:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 406)[0m: INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: part
  DATASET: imagenet22K
  DATA_PATH: /mnt/znvme/dataset/imagenet22k
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 4.8
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    GATE_NOISE: 1.0
    INIT_STD: 0.005
    IN_CHANS: 3
    IS_GSHARD_LOSS: true
    MLP_FC2_BIAS: false
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - 1
      - 3
      - 5
      - 7
      - 9
      - 11
      - 13
      - 15
      - 17
    - - 1
    MOE_DROP: 0.1
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 2
    USE_BPR: true
    WINDOW_SIZE: 12
  TYPE: swin_fastmoe
OUTPUT: /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default
PRINT_FREQ: 1
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 8
  AUTO_RESUME: true
  BASE_LR: 0.001
  CHECKPOINT_MODE: full
  CLIP_GRAD: 3.0
  EPOCHS: 90
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.1

[32m[2023-01-05 06:58:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 407)[0m: INFO {"cfg": "configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml", "opts": null, "batch_size": 16, "data_path": "/mnt/znvme/dataset/imagenet22k", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": 8, "use_checkpoint": false, "checkpoint_mode": "full", "disable_amp": false, "amp_opt_level": null, "output": "/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": true, "fused_layernorm": false, "optim": null, "num_experts": 2, "top_k": 2, "balance_strategy": "naive", "expert_parallel_strategy": "EP+DP", "expert_ep_size": 16, "expert_dp_size": 2, "dump": false, "dynamic_placement": false, "dynamic_freq": 10, "new_shadow": false, "gshard_cap": 4.8, "init_method_std": 0.002, "num_layers": 12}
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
local rank 0 / global rank 0 successfully build train dataset
local rank 0 / global rank 0 successfully build val dataset
[32m[2023-01-05 06:58:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 120)[0m: INFO Creating model:swin_fastmoe/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 24/32
local rank 0 / global rank 24 successfully build train dataset
local rank 0 / global rank 24 successfully build val dataset
[INFO] 24 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 24 in DP group [8, 24]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 28/32
local rank 0 / global rank 28 successfully build train dataset
local rank 0 / global rank 28 successfully build val dataset
[INFO] 28 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 28 in DP group [12, 28]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 15/32
local rank 0 / global rank 15 successfully build train dataset
local rank 0 / global rank 15 successfully build val dataset
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15, 31]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 9/32
local rank 0 / global rank 9 successfully build train dataset
local rank 0 / global rank 9 successfully build val dataset
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9, 25]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 30/32
local rank 0 / global rank 30 successfully build train dataset
local rank 0 / global rank 30 successfully build val dataset
[INFO] 30 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 30 in DP group [14, 30]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 10/32
local rank 0 / global rank 10 successfully build train dataset
local rank 0 / global rank 10 successfully build val dataset
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10, 26]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 29/32
local rank 0 / global rank 29 successfully build train dataset
local rank 0 / global rank 29 successfully build val dataset
[INFO] 29 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 29 in DP group [13, 29]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 26/32
local rank 0 / global rank 26 successfully build train dataset
local rank 0 / global rank 26 successfully build val dataset
[INFO] 26 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 26 in DP group [10, 26]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 27/32
local rank 0 / global rank 27 successfully build train dataset
local rank 0 / global rank 27 successfully build val dataset
[INFO] 27 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 27 in DP group [11, 27]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 25/32
local rank 0 / global rank 25 successfully build train dataset
local rank 0 / global rank 25 successfully build val dataset
[INFO] 25 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 25 in DP group [9, 25]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 31/32
local rank 0 / global rank 31 successfully build train dataset
local rank 0 / global rank 31 successfully build val dataset
[INFO] 31 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 31 in DP group [15, 31]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 19/32
local rank 0 / global rank 19 successfully build train dataset
local rank 0 / global rank 19 successfully build val dataset
[INFO] 19 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 19 in DP group [3, 19]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 18/32
local rank 0 / global rank 18 successfully build train dataset
local rank 0 / global rank 18 successfully build val dataset
[INFO] 18 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 18 in DP group [2, 18]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 11/32
local rank 0 / global rank 11 successfully build train dataset
local rank 0 / global rank 11 successfully build val dataset
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11, 27]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 17/32
local rank 0 / global rank 17 successfully build train dataset
local rank 0 / global rank 17 successfully build val dataset
[INFO] 17 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 17 in DP group [1, 17]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 8/32
local rank 0 / global rank 8 successfully build train dataset
local rank 0 / global rank 8 successfully build val dataset
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8, 24]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 14/32
local rank 0 / global rank 14 successfully build train dataset
local rank 0 / global rank 14 successfully build val dataset
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14, 30]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 13/32
local rank 0 / global rank 13 successfully build train dataset
local rank 0 / global rank 13 successfully build val dataset
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13, 29]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 22/32
local rank 0 / global rank 22 successfully build train dataset
local rank 0 / global rank 22 successfully build val dataset
[INFO] 22 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 22 in DP group [6, 22]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 21/32
local rank 0 / global rank 21 successfully build train dataset
local rank 0 / global rank 21 successfully build val dataset
[INFO] 21 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 21 in DP group [5, 21]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 12/32
local rank 0 / global rank 12 successfully build train dataset
local rank 0 / global rank 12 successfully build val dataset
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12, 28]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0, 16]
[32m[2023-01-05 06:59:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 122)[0m: INFO SwinTransformerFastMoE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(48, 48), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(48, 48), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(24, 24), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(24, 24), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(12, 12), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(12, 12), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(6, 6), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=4096, out_features=1024, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 1024, hidden_features = 4096, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=4096, out_features=32, bias=True)
              )
            )
          )
        )
      )
    )
  )
  (norm): TimerModule(
    (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (avgpool): TimerModule(
    (model): AdaptiveAvgPool1d(output_size=1)
  )
  (head): TimerModule(
    (model): Linear(in_features=1024, out_features=21841, bias=True)
  )
)
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 20/32
local rank 0 / global rank 20 successfully build train dataset
local rank 0 / global rank 20 successfully build val dataset
[INFO] 20 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 20 in DP group [4, 20]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 23/32
local rank 0 / global rank 23 successfully build train dataset
local rank 0 / global rank 23 successfully build val dataset
[INFO] 23 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 23 in DP group [7, 23]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 4/32
local rank 0 / global rank 4 successfully build train dataset
local rank 0 / global rank 4 successfully build val dataset
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4, 20]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 16/32
local rank 0 / global rank 16 successfully build train dataset
local rank 0 / global rank 16 successfully build val dataset
[INFO] 16 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 16 in DP group [0, 16]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 5/32
local rank 0 / global rank 5 successfully build train dataset
local rank 0 / global rank 5 successfully build val dataset
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5, 21]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 6/32
local rank 0 / global rank 6 successfully build train dataset
local rank 0 / global rank 6 successfully build val dataset
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6, 22]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-05 06:59:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 156)[0m: INFO no checkpoint found in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default, ignoring auto resume
[32m[2023-01-05 06:59:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 176)[0m: INFO Start training
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 7/32
local rank 0 / global rank 7 successfully build train dataset
local rank 0 / global rank 7 successfully build val dataset
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7, 23]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 1/32
local rank 0 / global rank 1 successfully build train dataset
local rank 0 / global rank 1 successfully build val dataset
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1, 17]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 2/32
local rank 0 / global rank 2 successfully build train dataset
local rank 0 / global rank 2 successfully build val dataset
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2, 18]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 3/32
local rank 0 / global rank 3 successfully build train dataset
local rank 0 / global rank 3 successfully build val dataset
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3, 19]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-05 06:59:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][7/27342]	eta 11:26:36 lr 0.0010000000	 wd 0.1000	time 1.0424 (1.5071)	loss 1.2463 (1.2478)	loss-cls 9.9708 (9.9828)	loss-aux 0.0000 (0.0000)	grad_norm 2.9858 (2.9858)	loss_scale 65536.0000 (65536.0000)	mem 8339MB	batch_time 12.0569
[32m[2023-01-05 06:59:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][15/27342]	eta 8:08:08 lr 0.0010000000	 wd 0.1000	time 0.7337 (1.0718)	loss 1.2500 (1.2483)	loss-cls 10.0000 (9.9863)	loss-aux 0.0000 (0.0000)	grad_norm 1.9404 (2.4631)	loss_scale 65536.0000 (65536.0000)	mem 10897MB	batch_time 5.0913
[32m[2023-01-05 06:59:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][23/27342]	eta 7:02:40 lr 0.0010000000	 wd 0.1000	time 0.7230 (0.9283)	loss 1.2464 (1.2488)	loss-cls 9.9714 (9.9905)	loss-aux 0.0000 (0.0000)	grad_norm 1.7910 (2.2390)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.1316
[32m[2023-01-05 06:59:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][31/27342]	eta 6:32:07 lr 0.0010000000	 wd 0.1000	time 0.7579 (0.8615)	loss 1.2490 (1.2490)	loss-cls 9.9922 (9.9917)	loss-aux 0.0000 (0.0000)	grad_norm 1.9273 (2.1611)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.2876
[32m[2023-01-05 06:59:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][39/27342]	eta 6:12:38 lr 0.0010000000	 wd 0.1000	time 0.7373 (0.8189)	loss 1.2531 (1.2490)	loss-cls 10.0247 (9.9923)	loss-aux 0.0000 (0.0000)	grad_norm 1.9729 (2.1235)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.1889
[32m[2023-01-05 06:59:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][47/27342]	eta 6:00:23 lr 0.0010000000	 wd 0.1000	time 0.7499 (0.7922)	loss 1.2514 (1.2493)	loss-cls 10.0115 (9.9943)	loss-aux 0.0000 (0.0000)	grad_norm 1.8831 (2.0834)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.2692
[32m[2023-01-05 06:59:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][55/27342]	eta 5:52:39 lr 0.0010000000	 wd 0.1000	time 0.7733 (0.7754)	loss 1.2516 (1.2497)	loss-cls 10.0131 (9.9977)	loss-aux 0.0000 (0.0000)	grad_norm 1.7023 (2.0290)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.3994
[32m[2023-01-05 07:00:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][63/27342]	eta 5:46:17 lr 0.0010000000	 wd 0.1000	time 0.7746 (0.7616)	loss 1.2582 (1.2499)	loss-cls 10.0660 (9.9991)	loss-aux 0.0000 (0.0000)	grad_norm 1.8854 (2.0110)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.3207
[32m[2023-01-05 07:00:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][71/27342]	eta 5:41:47 lr 0.0010000000	 wd 0.1000	time 0.7516 (0.7520)	loss 1.2510 (1.2497)	loss-cls 10.0078 (9.9979)	loss-aux 0.0000 (0.0000)	grad_norm 1.6649 (1.9726)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.3972
[32m[2023-01-05 07:00:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][79/27342]	eta 5:38:57 lr 0.0010000000	 wd 0.1000	time 0.7954 (0.7460)	loss 1.2468 (1.2504)	loss-cls 9.9743 (10.0032)	loss-aux 0.0000 (0.0000)	grad_norm 2.0363 (1.9789)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.5342
[32m[2023-01-05 07:00:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][87/27342]	eta 5:37:06 lr 0.0010000000	 wd 0.1000	time 0.8079 (0.7421)	loss 1.2628 (1.2512)	loss-cls 10.1021 (10.0093)	loss-aux 0.0000 (0.0000)	grad_norm 2.0106 (1.9818)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.6309
[32m[2023-01-05 07:00:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][95/27342]	eta 5:35:00 lr 0.0010000000	 wd 0.1000	time 0.7665 (0.7377)	loss 1.2636 (1.2511)	loss-cls 10.1091 (10.0088)	loss-aux 0.0000 (0.0000)	grad_norm 1.7257 (1.9605)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.5132
[32m[2023-01-05 07:00:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][103/27342]	eta 5:32:52 lr 0.0010000000	 wd 0.1000	time 0.7734 (0.7332)	loss 1.2483 (1.2512)	loss-cls 9.9862 (10.0092)	loss-aux 0.0000 (0.0000)	grad_norm 1.8552 (1.9524)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.4357
[32m[2023-01-05 07:00:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][111/27342]	eta 5:31:03 lr 0.0010000000	 wd 0.1000	time 0.7955 (0.7294)	loss 1.2509 (1.2512)	loss-cls 10.0070 (10.0096)	loss-aux 0.0000 (0.0000)	grad_norm 1.7249 (1.9361)	loss_scale 65536.0000 (65536.0000)	mem 10925MB	batch_time 5.4402
[32m[2023-01-05 07:00:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][119/27342]	eta 5:29:07 lr 0.0010000000	 wd 0.1000	time 0.7703 (0.7254)	loss 1.2623 (1.2513)	loss-cls 10.0981 (10.0103)	loss-aux 0.0000 (0.0000)	grad_norm 2.1663 (1.9515)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3504
[32m[2023-01-05 07:00:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][127/27342]	eta 5:27:13 lr 0.0010000000	 wd 0.1000	time 0.7618 (0.7214)	loss 1.2535 (1.2511)	loss-cls 10.0282 (10.0089)	loss-aux 0.0000 (0.0000)	grad_norm 1.6134 (1.9303)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.2965
[32m[2023-01-05 07:00:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][135/27342]	eta 5:25:57 lr 0.0010000000	 wd 0.1000	time 0.7847 (0.7188)	loss 1.2534 (1.2512)	loss-cls 10.0274 (10.0092)	loss-aux 0.0000 (0.0000)	grad_norm 1.6152 (1.9118)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4171
[32m[2023-01-05 07:00:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][143/27342]	eta 5:24:42 lr 0.0010000000	 wd 0.1000	time 0.7653 (0.7163)	loss 1.2632 (1.2514)	loss-cls 10.1053 (10.0115)	loss-aux 0.0000 (0.0000)	grad_norm 1.7753 (1.9042)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3878
[32m[2023-01-05 07:01:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][151/27342]	eta 5:23:50 lr 0.0010000000	 wd 0.1000	time 0.7927 (0.7146)	loss 1.2651 (1.2515)	loss-cls 10.1209 (10.0116)	loss-aux 0.0000 (0.0000)	grad_norm 1.6418 (1.8904)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4668
[32m[2023-01-05 07:01:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][159/27342]	eta 5:22:44 lr 0.0010000000	 wd 0.1000	time 0.7480 (0.7124)	loss 1.2460 (1.2514)	loss-cls 9.9677 (10.0109)	loss-aux 0.0000 (0.0000)	grad_norm 1.5947 (1.8756)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3654
[32m[2023-01-05 07:01:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][167/27342]	eta 5:21:55 lr 0.0010000000	 wd 0.1000	time 0.7690 (0.7108)	loss 1.2728 (1.2518)	loss-cls 10.1828 (10.0140)	loss-aux 0.0000 (0.0000)	grad_norm 1.7533 (1.8698)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4299
[32m[2023-01-05 07:01:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][175/27342]	eta 5:21:11 lr 0.0010000000	 wd 0.1000	time 0.7625 (0.7094)	loss 1.2500 (1.2516)	loss-cls 10.0002 (10.0132)	loss-aux 0.0000 (0.0000)	grad_norm 1.6419 (1.8594)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4361
[32m[2023-01-05 07:01:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][183/27342]	eta 5:20:26 lr 0.0010000000	 wd 0.1000	time 0.7718 (0.7079)	loss 1.2237 (1.2517)	loss-cls 9.7899 (10.0138)	loss-aux 0.0000 (0.0000)	grad_norm 1.5373 (1.8454)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4101
[32m[2023-01-05 07:01:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][191/27342]	eta 5:19:36 lr 0.0010000000	 wd 0.1000	time 0.7737 (0.7063)	loss 1.2391 (1.2516)	loss-cls 9.9128 (10.0129)	loss-aux 0.0000 (0.0000)	grad_norm 1.3821 (1.8261)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3529
[32m[2023-01-05 07:01:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][199/27342]	eta 5:18:44 lr 0.0010000000	 wd 0.1000	time 0.7423 (0.7046)	loss 1.2337 (1.2515)	loss-cls 9.8696 (10.0119)	loss-aux 0.0000 (0.0000)	grad_norm 1.9906 (1.8327)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3069
[32m[2023-01-05 07:01:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][207/27342]	eta 5:18:07 lr 0.0010000000	 wd 0.1000	time 0.7838 (0.7034)	loss 1.2571 (1.2517)	loss-cls 10.0571 (10.0134)	loss-aux 0.0000 (0.0000)	grad_norm 1.5797 (1.8230)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3969
[32m[2023-01-05 07:01:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][215/27342]	eta 5:17:30 lr 0.0010000000	 wd 0.1000	time 0.8241 (0.7023)	loss 1.2353 (1.2516)	loss-cls 9.8828 (10.0125)	loss-aux 0.0000 (0.0000)	grad_norm 1.4625 (1.8096)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3795
[32m[2023-01-05 07:01:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][223/27342]	eta 5:16:54 lr 0.0010000000	 wd 0.1000	time 0.7649 (0.7012)	loss 1.2363 (1.2514)	loss-cls 9.8902 (10.0115)	loss-aux 0.0000 (0.0000)	grad_norm 1.8105 (1.8097)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3659
[32m[2023-01-05 07:01:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][231/27342]	eta 5:16:32 lr 0.0010000000	 wd 0.1000	time 0.7542 (0.7006)	loss 1.2494 (1.2515)	loss-cls 9.9951 (10.0119)	loss-aux 0.0000 (0.0000)	grad_norm 1.8425 (1.8108)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4686
[32m[2023-01-05 07:02:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][239/27342]	eta 5:16:01 lr 0.0010000000	 wd 0.1000	time 0.7962 (0.6996)	loss 1.2558 (1.2515)	loss-cls 10.0466 (10.0120)	loss-aux 0.0000 (0.0000)	grad_norm 2.0325 (1.8182)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3761
[32m[2023-01-05 07:02:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][247/27342]	eta 5:15:34 lr 0.0010000000	 wd 0.1000	time 0.7858 (0.6988)	loss 1.2586 (1.2516)	loss-cls 10.0684 (10.0126)	loss-aux 0.0000 (0.0000)	grad_norm 1.9290 (1.8218)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4018
[32m[2023-01-05 07:02:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][255/27342]	eta 5:15:06 lr 0.0010000000	 wd 0.1000	time 0.7806 (0.6980)	loss 1.2582 (1.2516)	loss-cls 10.0656 (10.0131)	loss-aux 0.0000 (0.0000)	grad_norm 2.0183 (1.8279)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3769
[32m[2023-01-05 07:02:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][263/27342]	eta 5:14:29 lr 0.0010000000	 wd 0.1000	time 0.7593 (0.6968)	loss 1.2596 (1.2516)	loss-cls 10.0768 (10.0131)	loss-aux 0.0000 (0.0000)	grad_norm 1.7832 (1.8265)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.2765
[32m[2023-01-05 07:02:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][271/27342]	eta 5:13:56 lr 0.0010000000	 wd 0.1000	time 0.7645 (0.6958)	loss 1.2472 (1.2517)	loss-cls 9.9773 (10.0134)	loss-aux 0.0000 (0.0000)	grad_norm 1.7210 (1.8234)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.3011
[32m[2023-01-05 07:02:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][279/27342]	eta 5:13:35 lr 0.0010000000	 wd 0.1000	time 0.7771 (0.6953)	loss 1.2572 (1.2517)	loss-cls 10.0575 (10.0134)	loss-aux 0.0000 (0.0000)	grad_norm 2.0859 (1.8309)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4113
[32m[2023-01-05 07:02:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][287/27342]	eta 5:13:27 lr 0.0010000000	 wd 0.1000	time 0.7842 (0.6952)	loss 1.2517 (1.2519)	loss-cls 10.0139 (10.0151)	loss-aux 0.0000 (0.0000)	grad_norm 1.9533 (1.8343)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.5343
[32m[2023-01-05 07:02:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][295/27342]	eta 5:13:25 lr 0.0010000000	 wd 0.1000	time 0.7746 (0.6953)	loss 1.2727 (1.2521)	loss-cls 10.1819 (10.0171)	loss-aux 0.0000 (0.0000)	grad_norm 1.9027 (1.8362)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.6039
[32m[2023-01-05 07:02:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][303/27342]	eta 5:13:25 lr 0.0010000000	 wd 0.1000	time 0.7775 (0.6955)	loss 1.2866 (1.2521)	loss-cls 10.2925 (10.0170)	loss-aux 0.0000 (0.0000)	grad_norm 3.0010 (1.8668)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.6195
[32m[2023-01-05 07:02:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][311/27342]	eta 5:13:35 lr 0.0010000000	 wd 0.1000	time 0.8003 (0.6961)	loss 1.2806 (1.2525)	loss-cls 10.2445 (10.0196)	loss-aux 0.0000 (0.0000)	grad_norm 1.8301 (1.8659)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.7487
[32m[2023-01-05 07:02:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][319/27342]	eta 5:13:36 lr 0.0010000000	 wd 0.1000	time 0.8105 (0.6963)	loss 1.2470 (1.2525)	loss-cls 9.9762 (10.0202)	loss-aux 0.0000 (0.0000)	grad_norm 2.0032 (1.8693)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.6483
[32m[2023-01-05 07:03:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][327/27342]	eta 5:13:35 lr 0.0010000000	 wd 0.1000	time 0.8016 (0.6965)	loss 1.2616 (1.2526)	loss-cls 10.0931 (10.0209)	loss-aux 0.0000 (0.0000)	grad_norm 1.9465 (1.8712)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.6178
[32m[2023-01-05 07:03:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][335/27342]	eta 5:13:33 lr 0.0010000000	 wd 0.1000	time 0.8153 (0.6966)	loss 1.2559 (1.2526)	loss-cls 10.0474 (10.0204)	loss-aux 0.0000 (0.0000)	grad_norm 2.2731 (1.8808)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.6151
[32m[2023-01-05 07:03:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][343/27342]	eta 5:13:27 lr 0.0010000000	 wd 0.1000	time 0.8088 (0.6966)	loss 1.2413 (1.2526)	loss-cls 9.9306 (10.0205)	loss-aux 0.0000 (0.0000)	grad_norm 1.9234 (1.8818)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.5737
[32m[2023-01-05 07:03:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][351/27342]	eta 5:13:26 lr 0.0010000000	 wd 0.1000	time 0.7754 (0.6968)	loss 1.2661 (1.2524)	loss-cls 10.1292 (10.0194)	loss-aux 0.0000 (0.0000)	grad_norm 1.8491 (1.8810)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.6268
[32m[2023-01-05 07:03:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][359/27342]	eta 5:13:15 lr 0.0009999999	 wd 0.1000	time 0.7693 (0.6966)	loss 1.2447 (1.2527)	loss-cls 9.9572 (10.0214)	loss-aux 0.0000 (0.0000)	grad_norm 1.9184 (1.8819)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.5056
[32m[2023-01-05 07:03:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][367/27342]	eta 5:13:03 lr 0.0009999999	 wd 0.1000	time 0.7793 (0.6963)	loss 1.2885 (1.2531)	loss-cls 10.3079 (10.0244)	loss-aux 0.0000 (0.0000)	grad_norm 1.9087 (1.8824)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4780
[32m[2023-01-05 07:03:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][375/27342]	eta 5:12:50 lr 0.0009999999	 wd 0.1000	time 0.7699 (0.6960)	loss 1.2351 (1.2535)	loss-cls 9.8805 (10.0281)	loss-aux 0.0000 (0.0000)	grad_norm 2.0160 (1.8853)	loss_scale 65536.0000 (65536.0000)	mem 10935MB	batch_time 5.4691
[32m[2023-01-05 07:03:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][383/27342]	eta 5:12:45 lr 0.0009999999	 wd 0.1000	time 0.7985 (0.6961)	loss 1.2637 (1.2538)	loss-cls 10.1093 (10.0306)	loss-aux 0.0000 (0.0000)	grad_norm 1.7636 (1.8827)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.5826
[32m[2023-01-05 07:03:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][391/27342]	eta 5:12:45 lr 0.0009999999	 wd 0.1000	time 0.8164 (0.6963)	loss 1.2535 (1.2539)	loss-cls 10.0284 (10.0310)	loss-aux 0.0000 (0.0000)	grad_norm 1.5454 (1.8759)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.6516
[32m[2023-01-05 07:03:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][399/27342]	eta 5:12:42 lr 0.0009999999	 wd 0.1000	time 0.7951 (0.6964)	loss 1.2335 (1.2537)	loss-cls 9.8677 (10.0300)	loss-aux 0.0000 (0.0000)	grad_norm 1.6092 (1.8705)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.6080
[32m[2023-01-05 07:03:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][407/27342]	eta 5:12:33 lr 0.0009999999	 wd 0.1000	time 0.7965 (0.6963)	loss 1.2557 (1.2537)	loss-cls 10.0452 (10.0292)	loss-aux 0.0000 (0.0000)	grad_norm 1.5202 (1.8637)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.5209
[32m[2023-01-05 07:04:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][415/27342]	eta 5:12:22 lr 0.0009999999	 wd 0.1000	time 0.7975 (0.6961)	loss 1.2528 (1.2536)	loss-cls 10.0221 (10.0289)	loss-aux 0.0000 (0.0000)	grad_norm 1.3922 (1.8546)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.4863
[32m[2023-01-05 07:04:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][423/27342]	eta 5:12:19 lr 0.0009999999	 wd 0.1000	time 0.7951 (0.6961)	loss 1.2543 (1.2535)	loss-cls 10.0346 (10.0281)	loss-aux 0.0000 (0.0000)	grad_norm 1.4474 (1.8469)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.6055
[32m[2023-01-05 07:04:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][431/27342]	eta 5:12:16 lr 0.0009999999	 wd 0.1000	time 0.8111 (0.6963)	loss 1.2456 (1.2535)	loss-cls 9.9646 (10.0277)	loss-aux 0.0000 (0.0000)	grad_norm 1.4541 (1.8396)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.6150
[32m[2023-01-05 07:04:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][439/27342]	eta 5:12:15 lr 0.0009999999	 wd 0.1000	time 0.7894 (0.6964)	loss 1.2204 (1.2535)	loss-cls 9.7628 (10.0277)	loss-aux 0.0000 (0.0000)	grad_norm 1.4641 (1.8328)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.6446
[32m[2023-01-05 07:04:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][447/27342]	eta 5:12:10 lr 0.0009999999	 wd 0.1000	time 0.8027 (0.6964)	loss 1.2795 (1.2535)	loss-cls 10.2357 (10.0280)	loss-aux 0.0000 (0.0000)	grad_norm 1.3826 (1.8248)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.5737
[32m[2023-01-05 07:04:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][455/27342]	eta 5:12:05 lr 0.0009999999	 wd 0.1000	time 0.7816 (0.6965)	loss 1.2539 (1.2536)	loss-cls 10.0311 (10.0286)	loss-aux 0.0000 (0.0000)	grad_norm 1.3891 (1.8171)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.5887
[32m[2023-01-05 07:04:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][463/27342]	eta 5:12:05 lr 0.0009999999	 wd 0.1000	time 0.7829 (0.6967)	loss 1.2525 (1.2535)	loss-cls 10.0203 (10.0283)	loss-aux 0.0000 (0.0000)	grad_norm 1.3392 (1.8089)	loss_scale 65536.0000 (65536.0000)	mem 10958MB	batch_time 5.6623
[32m[2023-01-05 07:04:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][471/27342]	eta 5:12:04 lr 0.0009999999	 wd 0.1000	time 0.8078 (0.6968)	loss 1.2285 (1.2536)	loss-cls 9.8281 (10.0291)	loss-aux 0.0000 (0.0000)	grad_norm 1.3076 (1.8004)	loss_scale 65536.0000 (65536.0000)	mem 10965MB	batch_time 5.6485
[32m[2023-01-05 07:04:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][479/27342]	eta 5:12:04 lr 0.0009999999	 wd 0.1000	time 0.8135 (0.6970)	loss 1.2686 (1.2535)	loss-cls 10.1490 (10.0282)	loss-aux 0.0000 (0.0000)	grad_norm 1.2160 (1.7907)	loss_scale 65536.0000 (65536.0000)	mem 10965MB	batch_time 5.6791
[32m[2023-01-05 07:04:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][487/27342]	eta 5:12:04 lr 0.0009999999	 wd 0.1000	time 0.8073 (0.6973)	loss 1.2520 (1.2535)	loss-cls 10.0160 (10.0277)	loss-aux 0.0000 (0.0000)	grad_norm 1.3265 (1.7830)	loss_scale 65536.0000 (65536.0000)	mem 10965MB	batch_time 5.6799
[32m[2023-01-05 07:05:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][495/27342]	eta 5:12:04 lr 0.0009999999	 wd 0.1000	time 0.8202 (0.6974)	loss 1.2597 (1.2534)	loss-cls 10.0775 (10.0271)	loss-aux 0.0000 (0.0000)	grad_norm 1.2706 (1.7748)	loss_scale 65536.0000 (65536.0000)	mem 10965MB	batch_time 5.6731
[32m[2023-01-05 07:05:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][503/27342]	eta 5:12:05 lr 0.0009999999	 wd 0.1000	time 0.8004 (0.6977)	loss 1.2496 (1.2533)	loss-cls 9.9969 (10.0262)	loss-aux 0.0000 (0.0000)	grad_norm 1.1902 (1.7655)	loss_scale 65536.0000 (65536.0000)	mem 10965MB	batch_time 5.7023
[32m[2023-01-05 07:05:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][511/27342]	eta 5:12:01 lr 0.0009999999	 wd 0.1000	time 0.7802 (0.6978)	loss 1.2463 (1.2532)	loss-cls 9.9702 (10.0257)	loss-aux 0.0000 (0.0000)	grad_norm 1.2491 (1.7574)	loss_scale 65536.0000 (65536.0000)	mem 10965MB	batch_time 5.6225
[32m[2023-01-05 07:05:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][519/27342]	eta 5:11:59 lr 0.0009999999	 wd 0.1000	time 0.8058 (0.6979)	loss 1.2758 (1.2534)	loss-cls 10.2067 (10.0268)	loss-aux 0.0000 (0.0000)	grad_norm 1.0998 (1.7473)	loss_scale 65536.0000 (65536.0000)	mem 10965MB	batch_time 5.6499
[32m[2023-01-05 07:05:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][527/27342]	eta 5:11:57 lr 0.0009999999	 wd 0.1000	time 0.7862 (0.6980)	loss 1.2432 (1.2532)	loss-cls 9.9454 (10.0259)	loss-aux 0.0000 (0.0000)	grad_norm 0.9957 (1.7359)	loss_scale 65536.0000 (65536.0000)	mem 10967MB	batch_time 5.6468
[32m[2023-01-05 07:05:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][535/27342]	eta 5:11:47 lr 0.0009999999	 wd 0.1000	time 0.7849 (0.6979)	loss 1.2465 (1.2532)	loss-cls 9.9718 (10.0258)	loss-aux 0.0000 (0.0000)	grad_norm 0.8628 (1.7229)	loss_scale 65536.0000 (65536.0000)	mem 10969MB	batch_time 5.5054
[32m[2023-01-05 07:05:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][543/27342]	eta 5:11:41 lr 0.0009999999	 wd 0.1000	time 0.7848 (0.6978)	loss 1.2589 (1.2532)	loss-cls 10.0715 (10.0255)	loss-aux 0.0000 (0.0000)	grad_norm 0.7729 (1.7089)	loss_scale 65536.0000 (65536.0000)	mem 10981MB	batch_time 5.5643
[32m[2023-01-05 07:05:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][551/27342]	eta 5:11:36 lr 0.0009999999	 wd 0.1000	time 0.8123 (0.6979)	loss 1.2551 (1.2532)	loss-cls 10.0409 (10.0254)	loss-aux 0.0000 (0.0000)	grad_norm 0.7321 (1.6948)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6051
[32m[2023-01-05 07:05:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][559/27342]	eta 5:11:34 lr 0.0009999999	 wd 0.1000	time 0.8092 (0.6980)	loss 1.2532 (1.2531)	loss-cls 10.0257 (10.0250)	loss-aux 0.0000 (0.0000)	grad_norm 0.6011 (1.6791)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6472
[32m[2023-01-05 07:05:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][567/27342]	eta 5:11:34 lr 0.0009999999	 wd 0.1000	time 0.8171 (0.6982)	loss 1.2480 (1.2530)	loss-cls 9.9838 (10.0240)	loss-aux 0.0000 (0.0000)	grad_norm 0.4365 (1.6616)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7024
[32m[2023-01-05 07:05:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][575/27342]	eta 5:11:30 lr 0.0009999999	 wd 0.1000	time 0.7772 (0.6983)	loss 1.2488 (1.2529)	loss-cls 9.9906 (10.0230)	loss-aux 0.0000 (0.0000)	grad_norm 0.4316 (1.6446)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6235
[32m[2023-01-05 07:06:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][583/27342]	eta 5:11:26 lr 0.0009999999	 wd 0.1000	time 0.7818 (0.6983)	loss 1.2445 (1.2528)	loss-cls 9.9563 (10.0222)	loss-aux 0.0000 (0.0000)	grad_norm 0.3625 (1.6270)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6257
[32m[2023-01-05 07:06:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][591/27342]	eta 5:11:25 lr 0.0009999999	 wd 0.1000	time 0.7908 (0.6985)	loss 1.2348 (1.2527)	loss-cls 9.8788 (10.0217)	loss-aux 0.0000 (0.0000)	grad_norm 0.5537 (1.6125)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6794
[32m[2023-01-05 07:06:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][599/27342]	eta 5:11:27 lr 0.0009999999	 wd 0.1000	time 0.8143 (0.6988)	loss 1.2355 (1.2526)	loss-cls 9.8840 (10.0208)	loss-aux 0.0000 (0.0000)	grad_norm 0.4184 (1.5966)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7528
[32m[2023-01-05 07:06:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][607/27342]	eta 5:11:29 lr 0.0009999999	 wd 0.1000	time 0.8344 (0.6991)	loss 1.2545 (1.2525)	loss-cls 10.0357 (10.0199)	loss-aux 0.0000 (0.0000)	grad_norm 0.4339 (1.5813)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7798
[32m[2023-01-05 07:06:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][615/27342]	eta 5:11:33 lr 0.0009999999	 wd 0.1000	time 0.8139 (0.6994)	loss 1.2513 (1.2524)	loss-cls 10.0107 (10.0194)	loss-aux 0.0000 (0.0000)	grad_norm 0.5845 (1.5683)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.8024
[32m[2023-01-05 07:06:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][623/27342]	eta 5:11:34 lr 0.0009999998	 wd 0.1000	time 0.8195 (0.6997)	loss 1.2566 (1.2524)	loss-cls 10.0527 (10.0189)	loss-aux 0.0000 (0.0000)	grad_norm 0.4146 (1.5535)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7636
[32m[2023-01-05 07:06:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][631/27342]	eta 5:11:38 lr 0.0009999998	 wd 0.1000	time 0.7941 (0.7000)	loss 1.2544 (1.2523)	loss-cls 10.0356 (10.0182)	loss-aux 0.0000 (0.0000)	grad_norm 0.4934 (1.5401)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.8104
[32m[2023-01-05 07:06:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][639/27342]	eta 5:11:39 lr 0.0009999998	 wd 0.1000	time 0.8121 (0.7003)	loss 1.2493 (1.2522)	loss-cls 9.9946 (10.0178)	loss-aux 0.0000 (0.0000)	grad_norm 0.5045 (1.5272)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7690
[32m[2023-01-05 07:06:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][647/27342]	eta 5:11:40 lr 0.0009999998	 wd 0.1000	time 0.8189 (0.7005)	loss 1.2411 (1.2521)	loss-cls 9.9289 (10.0165)	loss-aux 0.0000 (0.0000)	grad_norm 0.4944 (1.5144)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7608
[32m[2023-01-05 07:06:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][655/27342]	eta 5:11:40 lr 0.0009999998	 wd 0.1000	time 0.8165 (0.7007)	loss 1.2492 (1.2520)	loss-cls 9.9940 (10.0158)	loss-aux 0.0000 (0.0000)	grad_norm 0.5387 (1.5025)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7444
[32m[2023-01-05 07:07:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][663/27342]	eta 5:11:40 lr 0.0009999998	 wd 0.1000	time 0.7907 (0.7009)	loss 1.2216 (1.2518)	loss-cls 9.7728 (10.0145)	loss-aux 0.0000 (0.0000)	grad_norm 0.4197 (1.4895)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7408
[32m[2023-01-05 07:07:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][671/27342]	eta 5:11:43 lr 0.0009999998	 wd 0.1000	time 0.8031 (0.7013)	loss 1.2039 (1.2517)	loss-cls 9.6308 (10.0134)	loss-aux 0.0000 (0.0000)	grad_norm 0.4002 (1.4765)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.8229
[32m[2023-01-05 07:07:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][679/27342]	eta 5:11:46 lr 0.0009999998	 wd 0.1000	time 0.8531 (0.7016)	loss 1.2247 (1.2515)	loss-cls 9.7973 (10.0117)	loss-aux 0.0000 (0.0000)	grad_norm 0.4880 (1.4649)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.8403
[32m[2023-01-05 07:07:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][687/27342]	eta 5:11:48 lr 0.0009999998	 wd 0.1000	time 0.8177 (0.7019)	loss 1.2480 (1.2514)	loss-cls 9.9843 (10.0110)	loss-aux 0.0000 (0.0000)	grad_norm 0.5353 (1.4541)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7928
[32m[2023-01-05 07:07:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][695/27342]	eta 5:11:50 lr 0.0009999998	 wd 0.1000	time 0.7994 (0.7022)	loss 1.2348 (1.2513)	loss-cls 9.8783 (10.0108)	loss-aux 0.0000 (0.0000)	grad_norm 0.5261 (1.4434)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.8230
[32m[2023-01-05 07:07:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][703/27342]	eta 5:11:53 lr 0.0009999998	 wd 0.1000	time 0.8103 (0.7025)	loss 1.2441 (1.2512)	loss-cls 9.9532 (10.0100)	loss-aux 0.0000 (0.0000)	grad_norm 0.5141 (1.4328)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.8303
[32m[2023-01-05 07:07:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][711/27342]	eta 5:11:52 lr 0.0009999998	 wd 0.1000	time 0.8127 (0.7026)	loss 1.2444 (1.2512)	loss-cls 9.9551 (10.0096)	loss-aux 0.0000 (0.0000)	grad_norm 0.4508 (1.4218)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7445
[32m[2023-01-05 07:07:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][719/27342]	eta 5:11:49 lr 0.0009999998	 wd 0.1000	time 0.8069 (0.7028)	loss 1.2382 (1.2511)	loss-cls 9.9052 (10.0092)	loss-aux 0.0000 (0.0000)	grad_norm 0.5197 (1.4118)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7073
[32m[2023-01-05 07:07:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][727/27342]	eta 5:11:46 lr 0.0009999998	 wd 0.1000	time 0.8274 (0.7029)	loss 1.2437 (1.2510)	loss-cls 9.9494 (10.0084)	loss-aux 0.0000 (0.0000)	grad_norm 0.3635 (1.4003)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6914
[32m[2023-01-05 07:07:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][735/27342]	eta 5:11:45 lr 0.0009999998	 wd 0.1000	time 0.8182 (0.7030)	loss 1.2496 (1.2510)	loss-cls 9.9965 (10.0079)	loss-aux 0.0000 (0.0000)	grad_norm 0.3649 (1.3890)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7541
[32m[2023-01-05 07:07:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][743/27342]	eta 5:11:43 lr 0.0009999998	 wd 0.1000	time 0.8006 (0.7032)	loss 1.2396 (1.2509)	loss-cls 9.9165 (10.0073)	loss-aux 0.0000 (0.0000)	grad_norm 0.5279 (1.3798)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7158
[32m[2023-01-05 07:08:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][751/27342]	eta 5:11:37 lr 0.0009999998	 wd 0.1000	time 0.8012 (0.7031)	loss 1.2692 (1.2509)	loss-cls 10.1534 (10.0073)	loss-aux 0.0000 (0.0000)	grad_norm 0.5108 (1.3705)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6074
[32m[2023-01-05 07:08:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][759/27342]	eta 5:11:28 lr 0.0009999998	 wd 0.1000	time 0.7956 (0.7030)	loss 1.2610 (1.2508)	loss-cls 10.0882 (10.0064)	loss-aux 0.0000 (0.0000)	grad_norm 0.4881 (1.3612)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.5496
[32m[2023-01-05 07:08:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][767/27342]	eta 5:11:21 lr 0.0009999998	 wd 0.1000	time 0.7805 (0.7030)	loss 1.2308 (1.2506)	loss-cls 9.8466 (10.0052)	loss-aux 0.0000 (0.0000)	grad_norm 0.4510 (1.3517)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.5632
[32m[2023-01-05 07:08:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][775/27342]	eta 5:11:15 lr 0.0009999998	 wd 0.1000	time 0.7861 (0.7029)	loss 1.2469 (1.2505)	loss-cls 9.9753 (10.0042)	loss-aux 0.0000 (0.0000)	grad_norm 0.3583 (1.3415)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6089
[32m[2023-01-05 07:08:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][783/27342]	eta 5:11:08 lr 0.0009999998	 wd 0.1000	time 0.7860 (0.7029)	loss 1.2432 (1.2505)	loss-cls 9.9458 (10.0038)	loss-aux 0.0000 (0.0000)	grad_norm 0.5815 (1.3337)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6010
[32m[2023-01-05 07:08:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][791/27342]	eta 5:11:02 lr 0.0009999998	 wd 0.1000	time 0.7948 (0.7029)	loss 1.2375 (1.2504)	loss-cls 9.8999 (10.0034)	loss-aux 0.0000 (0.0000)	grad_norm 0.3758 (1.3241)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6158
[32m[2023-01-05 07:08:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][799/27342]	eta 5:10:56 lr 0.0009999997	 wd 0.1000	time 0.7920 (0.7029)	loss 1.2379 (1.2503)	loss-cls 9.9033 (10.0026)	loss-aux 0.0000 (0.0000)	grad_norm 0.4091 (1.3149)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.5951
[32m[2023-01-05 07:08:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][807/27342]	eta 5:10:50 lr 0.0009999997	 wd 0.1000	time 0.8040 (0.7029)	loss 1.2276 (1.2503)	loss-cls 9.8206 (10.0020)	loss-aux 0.0000 (0.0000)	grad_norm 0.5255 (1.3071)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6147
[32m[2023-01-05 07:08:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][815/27342]	eta 5:10:46 lr 0.0009999997	 wd 0.1000	time 0.8097 (0.7029)	loss 1.2398 (1.2502)	loss-cls 9.9183 (10.0013)	loss-aux 0.0000 (0.0000)	grad_norm 0.3814 (1.2980)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6871
[32m[2023-01-05 07:08:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][823/27342]	eta 5:10:44 lr 0.0009999997	 wd 0.1000	time 0.8058 (0.7030)	loss 1.2504 (1.2501)	loss-cls 10.0032 (10.0010)	loss-aux 0.0000 (0.0000)	grad_norm 0.4872 (1.2902)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7131
[32m[2023-01-05 07:08:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][831/27342]	eta 5:10:40 lr 0.0009999997	 wd 0.1000	time 0.8072 (0.7031)	loss 1.2298 (1.2500)	loss-cls 9.8383 (10.0003)	loss-aux 0.0000 (0.0000)	grad_norm 0.4210 (1.2818)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6761
[32m[2023-01-05 07:09:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][839/27342]	eta 5:10:38 lr 0.0009999997	 wd 0.1000	time 0.8143 (0.7032)	loss 1.2486 (1.2499)	loss-cls 9.9888 (9.9996)	loss-aux 0.0000 (0.0000)	grad_norm 0.4689 (1.2741)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7391
[32m[2023-01-05 07:09:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][847/27342]	eta 5:10:39 lr 0.0009999997	 wd 0.1000	time 0.8173 (0.7035)	loss 1.2307 (1.2498)	loss-cls 9.8459 (9.9987)	loss-aux 0.0000 (0.0000)	grad_norm 0.5254 (1.2670)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.8516
[32m[2023-01-05 07:09:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][855/27342]	eta 5:10:39 lr 0.0009999997	 wd 0.1000	time 0.8153 (0.7037)	loss 1.2498 (1.2498)	loss-cls 9.9987 (9.9981)	loss-aux 0.0000 (0.0000)	grad_norm 0.4029 (1.2589)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.8130
[32m[2023-01-05 07:09:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][863/27342]	eta 5:10:36 lr 0.0009999997	 wd 0.1000	time 0.8087 (0.7038)	loss 1.2278 (1.2496)	loss-cls 9.8226 (9.9969)	loss-aux 0.0000 (0.0000)	grad_norm 0.3545 (1.2505)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7133
[32m[2023-01-05 07:09:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][871/27342]	eta 5:10:34 lr 0.0009999997	 wd 0.1000	time 0.8232 (0.7039)	loss 1.2210 (1.2495)	loss-cls 9.7681 (9.9961)	loss-aux 0.0000 (0.0000)	grad_norm 0.4166 (1.2429)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7368
[32m[2023-01-05 07:09:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][879/27342]	eta 5:10:29 lr 0.0009999997	 wd 0.1000	time 0.8105 (0.7040)	loss 1.2242 (1.2494)	loss-cls 9.7940 (9.9955)	loss-aux 0.0000 (0.0000)	grad_norm 0.4315 (1.2355)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6740
[32m[2023-01-05 07:09:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][887/27342]	eta 5:10:24 lr 0.0009999997	 wd 0.1000	time 0.8082 (0.7040)	loss 1.2114 (1.2493)	loss-cls 9.6909 (9.9947)	loss-aux 0.0000 (0.0000)	grad_norm 0.4260 (1.2282)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6565
[32m[2023-01-05 07:09:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][895/27342]	eta 5:10:19 lr 0.0009999997	 wd 0.1000	time 0.7960 (0.7040)	loss 1.2521 (1.2493)	loss-cls 10.0167 (9.9941)	loss-aux 0.0000 (0.0000)	grad_norm 0.3879 (1.2207)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6380
[32m[2023-01-05 07:09:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][903/27342]	eta 5:10:14 lr 0.0009999997	 wd 0.1000	time 0.8094 (0.7041)	loss 1.2451 (1.2492)	loss-cls 9.9607 (9.9936)	loss-aux 0.0000 (0.0000)	grad_norm 0.3641 (1.2131)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6738
[32m[2023-01-05 07:09:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][911/27342]	eta 5:10:12 lr 0.0009999997	 wd 0.1000	time 0.8243 (0.7042)	loss 1.2292 (1.2491)	loss-cls 9.8334 (9.9932)	loss-aux 0.0000 (0.0000)	grad_norm 0.4504 (1.2064)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7406
[32m[2023-01-05 07:10:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][919/27342]	eta 5:10:10 lr 0.0009999997	 wd 0.1000	time 0.8123 (0.7043)	loss 1.2315 (1.2491)	loss-cls 9.8523 (9.9926)	loss-aux 0.0000 (0.0000)	grad_norm 0.5277 (1.2005)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7523
[32m[2023-01-05 07:10:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][927/27342]	eta 5:10:07 lr 0.0009999997	 wd 0.1000	time 0.7926 (0.7044)	loss 1.2339 (1.2490)	loss-cls 9.8714 (9.9920)	loss-aux 0.0000 (0.0000)	grad_norm 0.3448 (1.1932)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7230
[32m[2023-01-05 07:10:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][935/27342]	eta 5:10:03 lr 0.0009999997	 wd 0.1000	time 0.7923 (0.7045)	loss 1.2449 (1.2489)	loss-cls 9.9594 (9.9915)	loss-aux 0.0000 (0.0000)	grad_norm 0.4582 (1.1869)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6944
[32m[2023-01-05 07:10:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][943/27342]	eta 5:09:59 lr 0.0009999996	 wd 0.1000	time 0.8049 (0.7045)	loss 1.2375 (1.2489)	loss-cls 9.9003 (9.9913)	loss-aux 0.0000 (0.0000)	grad_norm 0.4871 (1.1810)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7034
[32m[2023-01-05 07:10:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][951/27342]	eta 5:09:54 lr 0.0009999996	 wd 0.1000	time 0.8018 (0.7046)	loss 1.2355 (1.2488)	loss-cls 9.8839 (9.9905)	loss-aux 0.0000 (0.0000)	grad_norm 0.4886 (1.1751)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6533
[32m[2023-01-05 07:10:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][959/27342]	eta 5:09:49 lr 0.0009999996	 wd 0.1000	time 0.8078 (0.7046)	loss 1.2590 (1.2487)	loss-cls 10.0721 (9.9897)	loss-aux 0.0000 (0.0000)	grad_norm 0.3895 (1.1686)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6607
[32m[2023-01-05 07:10:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][967/27342]	eta 5:09:43 lr 0.0009999996	 wd 0.1000	time 0.8022 (0.7046)	loss 1.2410 (1.2486)	loss-cls 9.9280 (9.9886)	loss-aux 0.0000 (0.0000)	grad_norm 0.3939 (1.1622)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6336
[32m[2023-01-05 07:10:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][975/27342]	eta 5:09:38 lr 0.0009999996	 wd 0.1000	time 0.8217 (0.7046)	loss 1.2491 (1.2485)	loss-cls 9.9928 (9.9881)	loss-aux 0.0000 (0.0000)	grad_norm 0.4382 (1.1563)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6628
[32m[2023-01-05 07:10:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][983/27342]	eta 5:09:33 lr 0.0009999996	 wd 0.1000	time 0.8094 (0.7046)	loss 1.2439 (1.2485)	loss-cls 9.9513 (9.9876)	loss-aux 0.0000 (0.0000)	grad_norm 0.3832 (1.1500)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6559
[32m[2023-01-05 07:10:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][991/27342]	eta 5:09:29 lr 0.0009999996	 wd 0.1000	time 0.8203 (0.7047)	loss 1.2294 (1.2484)	loss-cls 9.8349 (9.9872)	loss-aux 0.0000 (0.0000)	grad_norm 0.5423 (1.1451)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7047
[32m[2023-01-05 07:10:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][999/27342]	eta 5:09:25 lr 0.0009999996	 wd 0.1000	time 0.8012 (0.7048)	loss 1.2398 (1.2483)	loss-cls 9.9187 (9.9865)	loss-aux 0.0000 (0.0000)	grad_norm 0.3910 (1.1390)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7065
[32m[2023-01-05 07:11:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1007/27342]	eta 5:09:22 lr 0.0009999996	 wd 0.1000	time 0.8158 (0.7048)	loss 1.2531 (1.2483)	loss-cls 10.0247 (9.9861)	loss-aux 0.0000 (0.0000)	grad_norm 0.3847 (1.1330)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7097
[32m[2023-01-05 07:11:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1015/27342]	eta 5:09:18 lr 0.0009999996	 wd 0.1000	time 0.8110 (0.7049)	loss 1.2419 (1.2482)	loss-cls 9.9349 (9.9856)	loss-aux 0.0000 (0.0000)	grad_norm 0.3806 (1.1271)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7339
[32m[2023-01-05 07:11:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1023/27342]	eta 5:09:15 lr 0.0009999996	 wd 0.1000	time 0.8159 (0.7050)	loss 1.2486 (1.2482)	loss-cls 9.9889 (9.9856)	loss-aux 0.0000 (0.0000)	grad_norm 0.4353 (1.1217)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.7434
[32m[2023-01-05 07:11:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1031/27342]	eta 5:09:11 lr 0.0009999996	 wd 0.1000	time 0.8108 (0.7051)	loss 1.2431 (1.2482)	loss-cls 9.9446 (9.9856)	loss-aux 0.0000 (0.0000)	grad_norm 0.4936 (1.1168)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6814
[32m[2023-01-05 07:11:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1039/27342]	eta 5:09:05 lr 0.0009999996	 wd 0.1000	time 0.8089 (0.7051)	loss 1.2155 (1.2481)	loss-cls 9.7241 (9.9848)	loss-aux 0.0000 (0.0000)	grad_norm 0.4363 (1.1116)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6445
[32m[2023-01-05 07:11:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1047/27342]	eta 5:08:59 lr 0.0009999996	 wd 0.1000	time 0.7848 (0.7050)	loss 1.2473 (1.2480)	loss-cls 9.9782 (9.9841)	loss-aux 0.0000 (0.0000)	grad_norm 0.2725 (1.1052)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.6059
[32m[2023-01-05 07:11:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1055/27342]	eta 5:08:52 lr 0.0009999996	 wd 0.1000	time 0.7755 (0.7050)	loss 1.2351 (1.2480)	loss-cls 9.8811 (9.9841)	loss-aux 0.0000 (0.0000)	grad_norm 0.5829 (1.1012)	loss_scale 65536.0000 (65536.0000)	mem 10986MB	batch_time 5.5798
[32m[2023-01-05 07:11:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1063/27342]	eta 5:08:44 lr 0.0009999995	 wd 0.1000	time 0.7217 (0.7049)	loss 1.2383 (1.2479)	loss-cls 9.9064 (9.9834)	loss-aux 0.0000 (0.0000)	grad_norm nan (nan)	loss_scale 32768.0000 (65505.2030)	mem 10986MB	batch_time 5.5501
[32m[2023-01-05 07:11:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1071/27342]	eta 5:08:39 lr 0.0009999995	 wd 0.1000	time 0.8054 (0.7049)	loss 1.2373 (1.2478)	loss-cls 9.8984 (9.9827)	loss-aux 0.0000 (0.0000)	grad_norm 0.4234 (nan)	loss_scale 32768.0000 (65260.8955)	mem 10986MB	batch_time 5.6649
[32m[2023-01-05 07:11:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1079/27342]	eta 5:08:36 lr 0.0009999995	 wd 0.1000	time 0.8113 (0.7050)	loss 1.2345 (1.2478)	loss-cls 9.8764 (9.9822)	loss-aux 0.0000 (0.0000)	grad_norm 0.3357 (nan)	loss_scale 32768.0000 (65020.2074)	mem 10986MB	batch_time 5.7564
[32m[2023-01-05 07:12:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1087/27342]	eta 5:08:32 lr 0.0009999995	 wd 0.1000	time 0.8325 (0.7051)	loss 1.2439 (1.2477)	loss-cls 9.9513 (9.9820)	loss-aux 0.0000 (0.0000)	grad_norm 0.5040 (nan)	loss_scale 32768.0000 (64783.0588)	mem 10986MB	batch_time 5.7060
[32m[2023-01-05 07:12:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1095/27342]	eta 5:08:27 lr 0.0009999995	 wd 0.1000	time 0.7899 (0.7051)	loss 1.2467 (1.2477)	loss-cls 9.9734 (9.9814)	loss-aux 0.0000 (0.0000)	grad_norm 0.4514 (nan)	loss_scale 32768.0000 (64549.3723)	mem 10986MB	batch_time 5.6681
[32m[2023-01-05 07:12:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1103/27342]	eta 5:08:21 lr 0.0009999995	 wd 0.1000	time 0.7851 (0.7051)	loss 1.2329 (1.2476)	loss-cls 9.8634 (9.9805)	loss-aux 0.0000 (0.0000)	grad_norm 0.4599 (nan)	loss_scale 32768.0000 (64319.0725)	mem 10986MB	batch_time 5.6176
[32m[2023-01-05 07:12:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1111/27342]	eta 5:08:16 lr 0.0009999995	 wd 0.1000	time 0.8094 (0.7051)	loss 1.2204 (1.2475)	loss-cls 9.7635 (9.9799)	loss-aux 0.0000 (0.0000)	grad_norm 0.4653 (nan)	loss_scale 32768.0000 (64092.0863)	mem 10986MB	batch_time 5.6799
[32m[2023-01-05 07:12:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1119/27342]	eta 5:08:11 lr 0.0009999995	 wd 0.1000	time 0.7851 (0.7051)	loss 1.2265 (1.2474)	loss-cls 9.8120 (9.9789)	loss-aux 0.0000 (0.0000)	grad_norm 0.5297 (nan)	loss_scale 32768.0000 (63868.3429)	mem 10986MB	batch_time 5.6495
[32m[2023-01-05 07:12:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1127/27342]	eta 5:08:05 lr 0.0009999995	 wd 0.1000	time 0.7945 (0.7052)	loss 1.2353 (1.2473)	loss-cls 9.8823 (9.9785)	loss-aux 0.0000 (0.0000)	grad_norm 0.4860 (nan)	loss_scale 32768.0000 (63647.7730)	mem 10986MB	batch_time 5.6542
[32m[2023-01-05 07:12:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1135/27342]	eta 5:07:59 lr 0.0009999995	 wd 0.1000	time 0.7930 (0.7052)	loss 1.2238 (1.2473)	loss-cls 9.7904 (9.9783)	loss-aux 0.0000 (0.0000)	grad_norm 0.4837 (nan)	loss_scale 32768.0000 (63430.3099)	mem 10986MB	batch_time 5.6341
[32m[2023-01-05 07:12:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1143/27342]	eta 5:07:53 lr 0.0009999995	 wd 0.1000	time 0.7808 (0.7051)	loss 1.2756 (1.2472)	loss-cls 10.2047 (9.9777)	loss-aux 0.0000 (0.0000)	grad_norm 0.3965 (nan)	loss_scale 32768.0000 (63215.8881)	mem 10986MB	batch_time 5.6063
[32m[2023-01-05 07:12:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1151/27342]	eta 5:07:47 lr 0.0009999995	 wd 0.1000	time 0.8084 (0.7051)	loss 1.2451 (1.2472)	loss-cls 9.9605 (9.9772)	loss-aux 0.0000 (0.0000)	grad_norm 0.3898 (nan)	loss_scale 32768.0000 (63004.4444)	mem 10986MB	batch_time 5.6413
[32m[2023-01-05 07:12:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1159/27342]	eta 5:07:41 lr 0.0009999995	 wd 0.1000	time 0.7878 (0.7051)	loss 1.2375 (1.2471)	loss-cls 9.8998 (9.9769)	loss-aux 0.0000 (0.0000)	grad_norm 0.3683 (nan)	loss_scale 32768.0000 (62795.9172)	mem 10986MB	batch_time 5.6066
[32m[2023-01-05 07:12:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1167/27342]	eta 5:07:37 lr 0.0009999995	 wd 0.1000	time 0.8142 (0.7052)	loss 1.2635 (1.2470)	loss-cls 10.1082 (9.9764)	loss-aux 0.0000 (0.0000)	grad_norm 0.4337 (nan)	loss_scale 32768.0000 (62590.2466)	mem 10986MB	batch_time 5.7322
[32m[2023-01-05 07:13:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1175/27342]	eta 5:07:33 lr 0.0009999994	 wd 0.1000	time 0.8060 (0.7052)	loss 1.2403 (1.2470)	loss-cls 9.9225 (9.9759)	loss-aux 0.0000 (0.0000)	grad_norm 0.4728 (nan)	loss_scale 32768.0000 (62387.3741)	mem 10986MB	batch_time 5.7115
[32m[2023-01-05 07:13:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1183/27342]	eta 5:07:29 lr 0.0009999994	 wd 0.1000	time 0.8125 (0.7053)	loss 1.2380 (1.2469)	loss-cls 9.9038 (9.9753)	loss-aux 0.0000 (0.0000)	grad_norm 0.4577 (nan)	loss_scale 32768.0000 (62187.2432)	mem 10986MB	batch_time 5.7139
[32m[2023-01-05 07:13:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1191/27342]	eta 5:07:26 lr 0.0009999994	 wd 0.1000	time 0.8038 (0.7054)	loss 1.2311 (1.2468)	loss-cls 9.8486 (9.9746)	loss-aux 0.0000 (0.0000)	grad_norm 0.3873 (nan)	loss_scale 32768.0000 (61989.7987)	mem 10986MB	batch_time 5.7439
[32m[2023-01-05 07:13:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1199/27342]	eta 5:07:22 lr 0.0009999994	 wd 0.1000	time 0.8012 (0.7055)	loss 1.2432 (1.2468)	loss-cls 9.9455 (9.9742)	loss-aux 0.0000 (0.0000)	grad_norm 0.3116 (nan)	loss_scale 32768.0000 (61794.9867)	mem 10986MB	batch_time 5.7345
[32m[2023-01-05 07:13:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1207/27342]	eta 5:07:21 lr 0.0009999994	 wd 0.1000	time 0.8051 (0.7056)	loss 1.2463 (1.2467)	loss-cls 9.9704 (9.9736)	loss-aux 0.0000 (0.0000)	grad_norm 0.4049 (nan)	loss_scale 32768.0000 (61602.7550)	mem 10986MB	batch_time 5.8357
[32m[2023-01-05 07:13:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1215/27342]	eta 5:07:17 lr 0.0009999994	 wd 0.1000	time 0.8186 (0.7057)	loss 1.2291 (1.2466)	loss-cls 9.8326 (9.9728)	loss-aux 0.0000 (0.0000)	grad_norm 0.3319 (nan)	loss_scale 32768.0000 (61413.0526)	mem 10986MB	batch_time 5.7540
[32m[2023-01-05 07:13:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1223/27342]	eta 5:07:12 lr 0.0009999994	 wd 0.1000	time 0.8039 (0.7057)	loss 1.2577 (1.2466)	loss-cls 10.0616 (9.9726)	loss-aux 0.0000 (0.0000)	grad_norm 0.4063 (nan)	loss_scale 32768.0000 (61225.8301)	mem 10986MB	batch_time 5.6660
[32m[2023-01-05 07:13:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1231/27342]	eta 5:07:06 lr 0.0009999994	 wd 0.1000	time 0.7804 (0.7057)	loss 1.2421 (1.2466)	loss-cls 9.9368 (9.9725)	loss-aux 0.0000 (0.0000)	grad_norm 0.4576 (nan)	loss_scale 32768.0000 (61041.0390)	mem 10986MB	batch_time 5.6191
[32m[2023-01-05 07:13:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1239/27342]	eta 5:07:00 lr 0.0009999994	 wd 0.1000	time 0.7777 (0.7057)	loss 1.2465 (1.2465)	loss-cls 9.9718 (9.9723)	loss-aux 0.0000 (0.0000)	grad_norm 0.3685 (nan)	loss_scale 32768.0000 (60858.6323)	mem 10986MB	batch_time 5.6253
[32m[2023-01-05 07:13:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1247/27342]	eta 5:06:54 lr 0.0009999994	 wd 0.1000	time 0.8149 (0.7057)	loss 1.2243 (1.2465)	loss-cls 9.7941 (9.9719)	loss-aux 0.0000 (0.0000)	grad_norm 0.4254 (nan)	loss_scale 32768.0000 (60678.5641)	mem 10986MB	batch_time 5.6445
[32m[2023-01-05 07:14:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1255/27342]	eta 5:06:51 lr 0.0009999994	 wd 0.1000	time 0.8540 (0.7058)	loss 1.2492 (1.2464)	loss-cls 9.9934 (9.9715)	loss-aux 0.0000 (0.0000)	grad_norm 0.3631 (nan)	loss_scale 32768.0000 (60500.7898)	mem 10986MB	batch_time 5.7757
[32m[2023-01-05 07:14:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1263/27342]	eta 5:06:46 lr 0.0009999994	 wd 0.1000	time 0.8032 (0.7058)	loss 1.2472 (1.2464)	loss-cls 9.9773 (9.9715)	loss-aux 0.0000 (0.0000)	grad_norm 0.3431 (nan)	loss_scale 32768.0000 (60325.2658)	mem 10986MB	batch_time 5.6871
[32m[2023-01-05 07:14:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1271/27342]	eta 5:06:42 lr 0.0009999994	 wd 0.1000	time 0.8053 (0.7058)	loss 1.2467 (1.2464)	loss-cls 9.9733 (9.9714)	loss-aux 0.0000 (0.0000)	grad_norm 0.2969 (nan)	loss_scale 32768.0000 (60151.9497)	mem 10986MB	batch_time 5.6860
[32m[2023-01-05 07:14:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1279/27342]	eta 5:06:36 lr 0.0009999993	 wd 0.1000	time 0.8076 (0.7059)	loss 1.2503 (1.2464)	loss-cls 10.0025 (9.9714)	loss-aux 0.0000 (0.0000)	grad_norm 0.3170 (nan)	loss_scale 32768.0000 (59980.8000)	mem 10986MB	batch_time 5.6556
[32m[2023-01-05 07:14:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1287/27342]	eta 5:06:30 lr 0.0009999993	 wd 0.1000	time 0.8039 (0.7059)	loss 1.2583 (1.2464)	loss-cls 10.0667 (9.9712)	loss-aux 0.0000 (0.0000)	grad_norm 0.3803 (nan)	loss_scale 32768.0000 (59811.7764)	mem 10986MB	batch_time 5.6445
[32m[2023-01-05 07:14:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1295/27342]	eta 5:06:25 lr 0.0009999993	 wd 0.1000	time 0.8130 (0.7059)	loss 1.2372 (1.2463)	loss-cls 9.8976 (9.9707)	loss-aux 0.0000 (0.0000)	grad_norm 0.3278 (nan)	loss_scale 32768.0000 (59644.8395)	mem 10986MB	batch_time 5.6573
[32m[2023-01-05 07:14:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1303/27342]	eta 5:06:20 lr 0.0009999993	 wd 0.1000	time 0.8116 (0.7059)	loss 1.2458 (1.2463)	loss-cls 9.9664 (9.9702)	loss-aux 0.0000 (0.0000)	grad_norm 0.4034 (nan)	loss_scale 32768.0000 (59479.9509)	mem 10986MB	batch_time 5.6577
[32m[2023-01-05 07:14:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1311/27342]	eta 5:06:14 lr 0.0009999993	 wd 0.1000	time 0.7986 (0.7059)	loss 1.1912 (1.2462)	loss-cls 9.5297 (9.9697)	loss-aux 0.0000 (0.0000)	grad_norm 0.3765 (nan)	loss_scale 32768.0000 (59317.0732)	mem 10986MB	batch_time 5.6599
[32m[2023-01-05 07:14:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1319/27342]	eta 5:06:10 lr 0.0009999993	 wd 0.1000	time 0.8246 (0.7059)	loss 1.2180 (1.2461)	loss-cls 9.7441 (9.9689)	loss-aux 0.0000 (0.0000)	grad_norm 0.3688 (nan)	loss_scale 32768.0000 (59156.1697)	mem 10986MB	batch_time 5.7053
[32m[2023-01-05 07:14:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1327/27342]	eta 5:06:04 lr 0.0009999993	 wd 0.1000	time 0.7976 (0.7059)	loss 1.2518 (1.2460)	loss-cls 10.0143 (9.9683)	loss-aux 0.0000 (0.0000)	grad_norm 0.2896 (nan)	loss_scale 32768.0000 (58997.2048)	mem 10986MB	batch_time 5.6525
[32m[2023-01-05 07:14:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1335/27342]	eta 5:06:00 lr 0.0009999993	 wd 0.1000	time 0.8240 (0.7060)	loss 1.2386 (1.2460)	loss-cls 9.9088 (9.9679)	loss-aux 0.0000 (0.0000)	grad_norm 0.2921 (nan)	loss_scale 32768.0000 (58840.1437)	mem 10986MB	batch_time 5.7089
[32m[2023-01-05 07:15:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1343/27342]	eta 5:05:56 lr 0.0009999993	 wd 0.1000	time 0.8237 (0.7060)	loss 1.2517 (1.2460)	loss-cls 10.0139 (9.9681)	loss-aux 0.0000 (0.0000)	grad_norm 0.3748 (nan)	loss_scale 32768.0000 (58684.9524)	mem 10986MB	batch_time 5.7459
[32m[2023-01-05 07:15:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1351/27342]	eta 5:05:52 lr 0.0009999993	 wd 0.1000	time 0.8418 (0.7061)	loss 1.2411 (1.2460)	loss-cls 9.9289 (9.9678)	loss-aux 0.0000 (0.0000)	grad_norm 0.4744 (nan)	loss_scale 32768.0000 (58531.5976)	mem 10986MB	batch_time 5.7393
[32m[2023-01-05 07:15:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1359/27342]	eta 5:05:47 lr 0.0009999993	 wd 0.1000	time 0.8002 (0.7061)	loss 1.2490 (1.2459)	loss-cls 9.9917 (9.9673)	loss-aux 0.0000 (0.0000)	grad_norm 0.3324 (nan)	loss_scale 32768.0000 (58380.0471)	mem 10986MB	batch_time 5.6935
[32m[2023-01-05 07:15:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1367/27342]	eta 5:05:43 lr 0.0009999993	 wd 0.1000	time 0.8066 (0.7062)	loss 1.2336 (1.2459)	loss-cls 9.8690 (9.9669)	loss-aux 0.0000 (0.0000)	grad_norm 0.3760 (nan)	loss_scale 32768.0000 (58230.2690)	mem 10986MB	batch_time 5.7256
[32m[2023-01-05 07:15:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1375/27342]	eta 5:05:40 lr 0.0009999992	 wd 0.1000	time 0.8087 (0.7063)	loss 1.2408 (1.2458)	loss-cls 9.9265 (9.9667)	loss-aux 0.0000 (0.0000)	grad_norm 0.3484 (nan)	loss_scale 32768.0000 (58082.2326)	mem 10986MB	batch_time 5.7803
[32m[2023-01-05 07:15:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1383/27342]	eta 5:05:35 lr 0.0009999992	 wd 0.1000	time 0.8244 (0.7063)	loss 1.2436 (1.2458)	loss-cls 9.9486 (9.9661)	loss-aux 0.0000 (0.0000)	grad_norm 0.4331 (nan)	loss_scale 32768.0000 (57935.9075)	mem 10986MB	batch_time 5.7059
[32m[2023-01-05 07:15:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1391/27342]	eta 5:05:31 lr 0.0009999992	 wd 0.1000	time 0.8144 (0.7064)	loss 1.2282 (1.2457)	loss-cls 9.8253 (9.9654)	loss-aux 0.0000 (0.0000)	grad_norm 0.3210 (nan)	loss_scale 32768.0000 (57791.2644)	mem 10986MB	batch_time 5.7101
[32m[2023-01-05 07:15:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1399/27342]	eta 5:05:25 lr 0.0009999992	 wd 0.1000	time 0.8138 (0.7064)	loss 1.2353 (1.2456)	loss-cls 9.8828 (9.9648)	loss-aux 0.0000 (0.0000)	grad_norm 0.4385 (nan)	loss_scale 32768.0000 (57648.2743)	mem 10986MB	batch_time 5.6665
[32m[2023-01-05 07:15:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1407/27342]	eta 5:05:20 lr 0.0009999992	 wd 0.1000	time 0.7916 (0.7064)	loss 1.2015 (1.2455)	loss-cls 9.6119 (9.9639)	loss-aux 0.0000 (0.0000)	grad_norm 0.3675 (nan)	loss_scale 32768.0000 (57506.9091)	mem 10986MB	batch_time 5.6698
[32m[2023-01-05 07:15:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1415/27342]	eta 5:05:14 lr 0.0009999992	 wd 0.1000	time 0.7849 (0.7064)	loss 1.2583 (1.2454)	loss-cls 10.0664 (9.9636)	loss-aux 0.0000 (0.0000)	grad_norm 0.3513 (nan)	loss_scale 32768.0000 (57367.1412)	mem 10986MB	batch_time 5.6299
[32m[2023-01-05 07:16:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1423/27342]	eta 5:05:09 lr 0.0009999992	 wd 0.1000	time 0.7988 (0.7064)	loss 1.2243 (1.2454)	loss-cls 9.7947 (9.9632)	loss-aux 0.0000 (0.0000)	grad_norm 0.3083 (nan)	loss_scale 32768.0000 (57228.9438)	mem 10986MB	batch_time 5.6926
[32m[2023-01-05 07:16:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1431/27342]	eta 5:05:04 lr 0.0009999992	 wd 0.1000	time 0.7992 (0.7065)	loss 1.2482 (1.2453)	loss-cls 9.9853 (9.9628)	loss-aux 0.0000 (0.0000)	grad_norm 0.3078 (nan)	loss_scale 32768.0000 (57092.2905)	mem 10986MB	batch_time 5.7050
[32m[2023-01-05 07:16:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1439/27342]	eta 5:04:59 lr 0.0009999992	 wd 0.1000	time 0.7910 (0.7065)	loss 1.2580 (1.2454)	loss-cls 10.0637 (9.9628)	loss-aux 0.0000 (0.0000)	grad_norm 0.2796 (nan)	loss_scale 32768.0000 (56957.1556)	mem 10986MB	batch_time 5.6738
[32m[2023-01-05 07:16:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1447/27342]	eta 5:04:52 lr 0.0009999992	 wd 0.1000	time 0.7936 (0.7064)	loss 1.2085 (1.2453)	loss-cls 9.6679 (9.9622)	loss-aux 0.0000 (0.0000)	grad_norm 0.3634 (nan)	loss_scale 32768.0000 (56823.5138)	mem 10986MB	batch_time 5.5878
[32m[2023-01-05 07:16:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1455/27342]	eta 5:04:46 lr 0.0009999992	 wd 0.1000	time 0.7711 (0.7064)	loss 1.2411 (1.2452)	loss-cls 9.9287 (9.9619)	loss-aux 0.0000 (0.0000)	grad_norm 0.2834 (nan)	loss_scale 32768.0000 (56691.3407)	mem 10986MB	batch_time 5.5872
[32m[2023-01-05 07:16:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1463/27342]	eta 5:04:40 lr 0.0009999991	 wd 0.1000	time 0.7821 (0.7064)	loss 1.2183 (1.2452)	loss-cls 9.7462 (9.9615)	loss-aux 0.0000 (0.0000)	grad_norm 0.3062 (nan)	loss_scale 32768.0000 (56560.6120)	mem 10986MB	batch_time 5.6305
[32m[2023-01-05 07:16:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1471/27342]	eta 5:04:34 lr 0.0009999991	 wd 0.1000	time 0.7857 (0.7064)	loss 1.2321 (1.2452)	loss-cls 9.8569 (9.9613)	loss-aux 0.0000 (0.0000)	grad_norm 0.3695 (nan)	loss_scale 32768.0000 (56431.3043)	mem 10986MB	batch_time 5.6393
[32m[2023-01-05 07:16:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1479/27342]	eta 5:04:27 lr 0.0009999991	 wd 0.1000	time 0.7919 (0.7063)	loss 1.2266 (1.2451)	loss-cls 9.8131 (9.9609)	loss-aux 0.0000 (0.0000)	grad_norm 0.3318 (nan)	loss_scale 32768.0000 (56303.3946)	mem 10986MB	batch_time 5.6080
[32m[2023-01-05 07:16:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1487/27342]	eta 5:04:21 lr 0.0009999991	 wd 0.1000	time 0.7898 (0.7063)	loss 1.2553 (1.2451)	loss-cls 10.0426 (9.9608)	loss-aux 0.0000 (0.0000)	grad_norm 0.4945 (nan)	loss_scale 32768.0000 (56176.8602)	mem 10986MB	batch_time 5.5869
[32m[2023-01-05 07:16:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1495/27342]	eta 5:04:14 lr 0.0009999991	 wd 0.1000	time 0.8044 (0.7063)	loss 1.2477 (1.2451)	loss-cls 9.9815 (9.9606)	loss-aux 0.0000 (0.0000)	grad_norm 0.2394 (nan)	loss_scale 32768.0000 (56051.6791)	mem 10986MB	batch_time 5.6045
[32m[2023-01-05 07:16:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1503/27342]	eta 5:04:07 lr 0.0009999991	 wd 0.1000	time 0.7913 (0.7062)	loss 1.2362 (1.2450)	loss-cls 9.8895 (9.9603)	loss-aux 0.0000 (0.0000)	grad_norm 0.3126 (nan)	loss_scale 32768.0000 (55927.8298)	mem 10986MB	batch_time 5.5509
[32m[2023-01-05 07:17:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1511/27342]	eta 5:04:02 lr 0.0009999991	 wd 0.1000	time 0.8133 (0.7062)	loss 1.2434 (1.2450)	loss-cls 9.9472 (9.9598)	loss-aux 0.0000 (0.0000)	grad_norm 0.3249 (nan)	loss_scale 32768.0000 (55805.2910)	mem 10986MB	batch_time 5.6770
[32m[2023-01-05 07:17:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1519/27342]	eta 5:03:57 lr 0.0009999991	 wd 0.1000	time 0.8138 (0.7062)	loss 1.2432 (1.2450)	loss-cls 9.9455 (9.9596)	loss-aux 0.0000 (0.0000)	grad_norm 0.2731 (nan)	loss_scale 32768.0000 (55684.0421)	mem 10986MB	batch_time 5.6975
[32m[2023-01-05 07:17:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1527/27342]	eta 5:03:53 lr 0.0009999991	 wd 0.1000	time 0.8338 (0.7063)	loss 1.2310 (1.2449)	loss-cls 9.8479 (9.9592)	loss-aux 0.0000 (0.0000)	grad_norm 0.2780 (nan)	loss_scale 32768.0000 (55564.0628)	mem 10986MB	batch_time 5.7715
[32m[2023-01-05 07:17:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1535/27342]	eta 5:03:49 lr 0.0009999991	 wd 0.1000	time 0.8055 (0.7064)	loss 1.2351 (1.2449)	loss-cls 9.8806 (9.9588)	loss-aux 0.0000 (0.0000)	grad_norm 0.2637 (nan)	loss_scale 32768.0000 (55445.3333)	mem 10986MB	batch_time 5.7251
[32m[2023-01-05 07:17:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1543/27342]	eta 5:03:44 lr 0.0009999990	 wd 0.1000	time 0.8126 (0.7064)	loss 1.2162 (1.2448)	loss-cls 9.7294 (9.9587)	loss-aux 0.0000 (0.0000)	grad_norm 0.3828 (nan)	loss_scale 32768.0000 (55327.8342)	mem 10986MB	batch_time 5.7051
[32m[2023-01-05 07:17:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1551/27342]	eta 5:03:39 lr 0.0009999990	 wd 0.1000	time 0.7966 (0.7064)	loss 1.2298 (1.2448)	loss-cls 9.8382 (9.9580)	loss-aux 0.0000 (0.0000)	grad_norm 0.3833 (nan)	loss_scale 32768.0000 (55211.5464)	mem 10986MB	batch_time 5.6949
[32m[2023-01-05 07:17:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1559/27342]	eta 5:03:34 lr 0.0009999990	 wd 0.1000	time 0.8082 (0.7065)	loss 1.2313 (1.2447)	loss-cls 9.8503 (9.9576)	loss-aux 0.0000 (0.0000)	grad_norm 0.3661 (nan)	loss_scale 32768.0000 (55096.4513)	mem 10986MB	batch_time 5.7170
[32m[2023-01-05 07:17:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1567/27342]	eta 5:03:29 lr 0.0009999990	 wd 0.1000	time 0.7975 (0.7065)	loss 1.2351 (1.2447)	loss-cls 9.8811 (9.9572)	loss-aux 0.0000 (0.0000)	grad_norm 0.3590 (nan)	loss_scale 32768.0000 (54982.5306)	mem 10986MB	batch_time 5.6755
[32m[2023-01-05 07:17:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1575/27342]	eta 5:03:24 lr 0.0009999990	 wd 0.1000	time 0.7903 (0.7065)	loss 1.2288 (1.2447)	loss-cls 9.8300 (9.9572)	loss-aux 0.0000 (0.0000)	grad_norm 0.3420 (nan)	loss_scale 32768.0000 (54869.7665)	mem 10986MB	batch_time 5.6778
[32m[2023-01-05 07:17:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1583/27342]	eta 5:03:19 lr 0.0009999990	 wd 0.1000	time 0.7970 (0.7065)	loss 1.2308 (1.2446)	loss-cls 9.8467 (9.9565)	loss-aux 0.0000 (0.0000)	grad_norm 0.3599 (nan)	loss_scale 32768.0000 (54758.1414)	mem 10986MB	batch_time 5.6673
[32m[2023-01-05 07:17:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1591/27342]	eta 5:03:13 lr 0.0009999990	 wd 0.1000	time 0.7976 (0.7065)	loss 1.2122 (1.2445)	loss-cls 9.6976 (9.9559)	loss-aux 0.0000 (0.0000)	grad_norm 0.2466 (nan)	loss_scale 32768.0000 (54647.6382)	mem 10986MB	batch_time 5.6326
[32m[2023-01-05 07:18:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1599/27342]	eta 5:03:08 lr 0.0009999990	 wd 0.1000	time 0.8214 (0.7066)	loss 1.2301 (1.2444)	loss-cls 9.8412 (9.9555)	loss-aux 0.0000 (0.0000)	grad_norm 0.2540 (nan)	loss_scale 32768.0000 (54538.2400)	mem 10986MB	batch_time 5.7424
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19281.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15811.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11827.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13641.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6236.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9041.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10175248/n10175248_583.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7365.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11816.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_26924.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12757.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12740.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29695.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22698.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15455.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27412.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13516.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25408.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_9566.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7411.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2852.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25717.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12064.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4456.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7013.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28595.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_4539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33259.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27578.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5051.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15697.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03957420/n03957420_33553.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10675.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17471.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9068.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n06470073/n06470073_47249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8783.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17877.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_15341.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8925.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5664.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22719.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25750.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9282.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8812.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22980.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_16430.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_7026.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24638.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_23316.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12206.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13871.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12108.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29025.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13244.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12231.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20118.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6669.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13396.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18350.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27317.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28288.JPEG
[32m[2023-01-05 07:18:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 282)[0m: INFO EPOCH 0 training takes 0:18:51
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9819.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_135.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8873.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_32625.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7996.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7072.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6567.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4524.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9215.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_16320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13103.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2040.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2322.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7728.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_34297.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25530.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10242.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27627.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12364.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_16737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7953.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22166.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10586.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_1796.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6247.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6710.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19005.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22581.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27221.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24544.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_10353.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14718.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18729.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_1914.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27611.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24434.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15830.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_21756.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_11746.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n02368116/n02368116_318.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9401.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13261.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8806.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27296.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30926.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9031.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14020.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_14701.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20180.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8645.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6850.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33630.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30043.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13306.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22995.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11766.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33623.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15480.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13419.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_3493.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12142.JPEG
