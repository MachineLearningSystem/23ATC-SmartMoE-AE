+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102158
++ scontrol show JobId=102158
++ grep BatchHost
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102158
++ grep BatchHost
++ scontrol show JobId=102158
++ scontrol show JobId=102158
++ tr = ' '
++ scontrol show JobId=102158
++ awk '{print $2}'
++ grep BatchHost
++ scontrol show JobId=102158
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=102158
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102158
++ scontrol show JobId=102158
++ scontrol show JobId=102158
++ scontrol show JobId=102158
++ scontrol show JobId=102158
++ scontrol show JobId=102158
++ grep BatchHost
++ scontrol show JobId=102158
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102158
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
++ scontrol show JobId=102158
+ export RANK=17
+ RANK=17
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
++ scontrol show JobId=102158
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
++ scontrol show JobId=102158
+ export RANK=21
+ RANK=21
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export NNODES=4
+ NNODES=4
+ localrank=5
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NNODES=4
+ NNODES=4
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
++ grep BatchHost
++ grep BatchHost
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
++ grep BatchHost
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ export RANK=16
+ RANK=16
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export RANK=22
+ RANK=22
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
++ grep BatchHost
+ localrank=6
++ scontrol show JobId=102158
+ export NNODES=4
+ NNODES=4
++ scontrol show JobId=102158
+ export NODE_RANK=4
+ NODE_RANK=4
++ scontrol show JobId=102158
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
++ tr = ' '
+ export MAX_JOBS=64
+ MAX_JOBS=64
++ tr = ' '
+ export NNODES=4
+ NNODES=4
++ tr = ' '
+ export NODE_RANK=5
++ grep BatchHost
+ NODE_RANK=5
+ export MAX_JOBS=64
++ tr = ' '
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
++ grep BatchHost
+ MAX_JOBS=64
++ scontrol show JobId=102158
+ DATASET_PREFIX=/mnt/znvme/zms
++ grep BatchHost
+ true
++ awk '{print $2}'
+ cd /home/zms/model_training/MoE/FastSwin
++ tr = ' '
+ '[' nico == nico ']'
++ awk '{print $2}'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
++ awk '{print $2}'
+ true
++ awk '{print $2}'
+ cd /home/zms/model_training/MoE/FastSwin
++ tr = ' '
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=19
+ RANK=19
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
++ tr = ' '
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
++ awk '{print $2}'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ export NNODES=4
+ NNODES=4
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export NODE_RANK=4
+ NODE_RANK=4
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ awk '{print $2}'
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args=
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
++ awk '{print $2}'
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
++ scontrol show JobId=102158
+ '[' OFF == ON ']'
++ grep BatchHost
+ python_args+='
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
                --expert-dp-size 2 '
+ false
++ tr = ' '
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ false
+ true
+ EXEC=./main_moe.py
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=20
+ RANK=20
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
++ awk '{print $2}'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=23
+ RANK=23
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ export RANK=18
+ RANK=18
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ export NNODES=4
+ NNODES=4
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export NODE_RANK=4
+ NODE_RANK=4
+ '[' OFF == ON ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ false
+ true
+ EXEC=./main_moe.py
+ false
+ true
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ EXEC=./main_moe.py
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102158
++ scontrol show JobId=102158
++ scontrol show JobId=102158
++ grep BatchHost
++ scontrol show JobId=102158
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ scontrol show JobId=102158
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=102158
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=102158
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=102158
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=29
+ RANK=29
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=28
+ RANK=28
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ export RANK=27
+ RANK=27
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export NNODES=4
+ NNODES=4
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args=
+ TRAIN_SAMPLES=20520960
+ export MASTER_ADDR=nico1
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ export RANK=25
+ RANK=25
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ DATASET_PREFIX=/mnt/znvme/zms
+ export RANK=24
+ RANK=24
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ python_args=
+ false
+ true
+ EXEC=./main_moe.py
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ false
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ false
+ true
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ EXEC=./main_moe.py
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=30
+ RANK=30
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ export RANK=26
+ RANK=26
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=31
+ RANK=31
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ '[' OFF == ON ']'
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' OFF == ON ']'
+ python_args+='
+ '[' OFF == ON ']'
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ false
+ true
+ EXEC=./main_moe.py
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ USE_MEGATRON=0
+ false
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MASTER_ADDR=nico1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ false
+ true
+ EXEC=./main_moe.py
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ localrank=3
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=4
+ NNODES=4
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ false
+ true
+ EXEC=./main_moe.py
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ python_args=
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ false
+ true
+ EXEC=./main_moe.py
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NNODES=4
+ NNODES=4
+ export RANK=11
+ RANK=11
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NNODES=4
+ NNODES=4
+ true
+ export NODE_RANK=2
+ NODE_RANK=2
+ cd /home/zms/model_training/MoE/FastSwin
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export RANK=14
+ RANK=14
+ RANK=8
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export RANK=12
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ RANK=12
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ '[' nico == nico ']'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ cd /home/zms/model_training/MoE/FastSwin
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ python_args=
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof/table
+ python_args=
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args+='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 16                     --expert-dp-size 2 '
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 2 
                --expert-ep-size 16
                --expert-dp-size 2 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep16_dp2_totalE32_localE2_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T19:16:02+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 2 --expert-ep-size 16 --expert-dp-size 2
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 0/32
[32m[2023-01-04 19:16:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 403)[0m: INFO Full config saved to /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default/config.json
[32m[2023-01-04 19:16:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 406)[0m: INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: part
  DATASET: imagenet22K
  DATA_PATH: /mnt/znvme/dataset/imagenet22k
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    GATE_NOISE: 1.0
    INIT_STD: 0.005
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: false
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - 1
      - 3
      - 5
      - 7
      - 9
      - 11
      - 13
      - 15
      - 17
    - - 1
    MOE_DROP: 0.1
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_LOCAL_EXPERTS: 2
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 2
    USE_BPR: true
    WINDOW_SIZE: 12
  TYPE: swin_fastmoe
OUTPUT: /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default
PRINT_FREQ: 1
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 8
  AUTO_RESUME: true
  BASE_LR: 0.001
  CHECKPOINT_MODE: full
  CLIP_GRAD: 3.0
  EPOCHS: 90
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.1

[32m[2023-01-04 19:16:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 407)[0m: INFO {"cfg": "configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml", "opts": null, "batch_size": 16, "data_path": "/mnt/znvme/dataset/imagenet22k", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": 8, "use_checkpoint": false, "checkpoint_mode": "full", "disable_amp": false, "amp_opt_level": null, "output": "/home/zms/model_training/Auto-Megatron/logs/test_swin", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": true, "fused_layernorm": false, "optim": null, "num_experts": 2, "top_k": 2, "balance_strategy": "naive", "expert_parallel_strategy": "EP+DP", "expert_ep_size": 16, "expert_dp_size": 2, "dump": false, "dynamic_placement": false, "dynamic_freq": 10, "new_shadow": false, "gshard_cap": 4.8, "init_method_std": 0.002, "num_layers": 12}
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
local rank 0 / global rank 0 successfully build train dataset
local rank 0 / global rank 0 successfully build val dataset
[32m[2023-01-04 19:16:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 120)[0m: INFO Creating model:swin_fastmoe/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 23/32
local rank 0 / global rank 23 successfully build train dataset
local rank 0 / global rank 23 successfully build val dataset
[INFO] 23 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 23 in DP group [7, 23]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 25/32
local rank 0 / global rank 25 successfully build train dataset
local rank 0 / global rank 25 successfully build val dataset
[INFO] 25 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 25 in DP group [9, 25]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 20/32
local rank 0 / global rank 20 successfully build train dataset
local rank 0 / global rank 20 successfully build val dataset
[INFO] 20 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 20 in DP group [4, 20]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 10/32
local rank 0 / global rank 10 successfully build train dataset
local rank 0 / global rank 10 successfully build val dataset
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10, 26]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 28/32
local rank 0 / global rank 28 successfully build train dataset
local rank 0 / global rank 28 successfully build val dataset
[INFO] 28 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 28 in DP group [12, 28]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 31/32
local rank 0 / global rank 31 successfully build train dataset
local rank 0 / global rank 31 successfully build val dataset
[INFO] 31 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 31 in DP group [15, 31]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0, 16]
[32m[2023-01-04 19:16:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 122)[0m: INFO SwinTransformerFastMoE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(48, 48), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(48, 48), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(24, 24), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(24, 24), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(12, 12), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(12, 12), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(6, 6), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=4096, out_features=1024, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 1024, hidden_features = 4096, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
                (1): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=4096, out_features=32, bias=True)
              )
            )
          )
        )
      )
    )
  )
  (norm): TimerModule(
    (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (avgpool): TimerModule(
    (model): AdaptiveAvgPool1d(output_size=1)
  )
  (head): TimerModule(
    (model): Linear(in_features=1024, out_features=21841, bias=True)
  )
)
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 15/32
local rank 0 / global rank 15 successfully build train dataset
local rank 0 / global rank 15 successfully build val dataset
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15, 31]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 18/32
local rank 0 / global rank 18 successfully build train dataset
local rank 0 / global rank 18 successfully build val dataset
[INFO] 18 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 18 in DP group [2, 18]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 29/32
local rank 0 / global rank 29 successfully build train dataset
local rank 0 / global rank 29 successfully build val dataset
[INFO] 29 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 29 in DP group [13, 29]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 17/32
local rank 0 / global rank 17 successfully build train dataset
local rank 0 / global rank 17 successfully build val dataset
[INFO] 17 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 17 in DP group [1, 17]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 4/32
local rank 0 / global rank 4 successfully build train dataset
local rank 0 / global rank 4 successfully build val dataset
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4, 20]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 22/32
local rank 0 / global rank 22 successfully build train dataset
local rank 0 / global rank 22 successfully build val dataset
[INFO] 22 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 22 in DP group [6, 22]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 24/32
local rank 0 / global rank 24 successfully build train dataset
local rank 0 / global rank 24 successfully build val dataset
[INFO] 24 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 24 in DP group [8, 24]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 11/32
local rank 0 / global rank 11 successfully build train dataset
local rank 0 / global rank 11 successfully build val dataset
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11, 27]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 8/32
local rank 0 / global rank 8 successfully build train dataset
local rank 0 / global rank 8 successfully build val dataset
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8, 24]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 19/32
local rank 0 / global rank 19 successfully build train dataset
local rank 0 / global rank 19 successfully build val dataset
[INFO] 19 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 19 in DP group [3, 19]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 3/32
local rank 0 / global rank 3 successfully build train dataset
local rank 0 / global rank 3 successfully build val dataset
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3, 19]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 7/32
local rank 0 / global rank 7 successfully build train dataset
local rank 0 / global rank 7 successfully build val dataset
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7, 23]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-04 19:16:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 156)[0m: INFO no checkpoint found in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default, ignoring auto resume
[32m[2023-01-04 19:16:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 176)[0m: INFO Start training
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 1/32
local rank 0 / global rank 1 successfully build train dataset
local rank 0 / global rank 1 successfully build val dataset
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1, 17]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 9/32
local rank 0 / global rank 9 successfully build train dataset
local rank 0 / global rank 9 successfully build val dataset
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9, 25]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 21/32
local rank 0 / global rank 21 successfully build train dataset
local rank 0 / global rank 21 successfully build val dataset
[INFO] 21 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 21 in DP group [5, 21]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 16/32
local rank 0 / global rank 16 successfully build train dataset
local rank 0 / global rank 16 successfully build val dataset
[INFO] 16 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 16 in DP group [0, 16]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 26/32
local rank 0 / global rank 26 successfully build train dataset
local rank 0 / global rank 26 successfully build val dataset
[INFO] 26 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 26 in DP group [10, 26]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 2/32
local rank 0 / global rank 2 successfully build train dataset
local rank 0 / global rank 2 successfully build val dataset
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2, 18]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 6/32
local rank 0 / global rank 6 successfully build train dataset
local rank 0 / global rank 6 successfully build val dataset
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6, 22]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 14/32
local rank 0 / global rank 14 successfully build train dataset
local rank 0 / global rank 14 successfully build val dataset
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14, 30]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 5/32
local rank 0 / global rank 5 successfully build train dataset
local rank 0 / global rank 5 successfully build val dataset
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5, 21]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 12/32
local rank 0 / global rank 12 successfully build train dataset
local rank 0 / global rank 12 successfully build val dataset
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12, 28]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 30/32
local rank 0 / global rank 30 successfully build train dataset
local rank 0 / global rank 30 successfully build val dataset
[INFO] 30 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 30 in DP group [14, 30]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 13/32
local rank 0 / global rank 13 successfully build train dataset
local rank 0 / global rank 13 successfully build val dataset
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13, 29]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 27/32
local rank 0 / global rank 27 successfully build train dataset
local rank 0 / global rank 27 successfully build val dataset
[INFO] 27 in EP group [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 27 in DP group [11, 27]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-04 19:17:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][7/27342]	eta 11:10:06 lr 0.0010000000	 wd 0.1000	time 1.0127 (1.4709)	loss 1.2463 (1.2478)	loss-cls 9.9708 (9.9828)	loss-aux 0.0000 (0.0000)	grad_norm 2.9858 (2.9858)	loss_scale 65536.0000 (65536.0000)	mem 8339MB	batch_time 11.7670
[32m[2023-01-04 19:17:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][15/27342]	eta 8:00:11 lr 0.0010000000	 wd 0.1000	time 0.7402 (1.0543)	loss 1.2500 (1.2483)	loss-cls 10.0000 (9.9863)	loss-aux 0.0000 (0.0000)	grad_norm 1.9404 (2.4631)	loss_scale 65536.0000 (65536.0000)	mem 10897MB	batch_time 5.1022
[32m[2023-01-04 19:17:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][23/27342]	eta 6:56:51 lr 0.0010000000	 wd 0.1000	time 0.7464 (0.9155)	loss 1.2464 (1.2488)	loss-cls 9.9714 (9.9905)	loss-aux 0.0000 (0.0000)	grad_norm 1.7911 (2.2391)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.1035
[32m[2023-01-04 19:17:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][31/27342]	eta 6:27:46 lr 0.0010000000	 wd 0.1000	time 0.7562 (0.8519)	loss 1.2488 (1.2490)	loss-cls 9.9907 (9.9916)	loss-aux 0.0000 (0.0000)	grad_norm 1.9353 (2.1631)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.2878
[32m[2023-01-04 19:17:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][39/27342]	eta 6:09:43 lr 0.0010000000	 wd 0.1000	time 0.7487 (0.8125)	loss 1.2533 (1.2490)	loss-cls 10.0264 (9.9924)	loss-aux 0.0000 (0.0000)	grad_norm 1.9833 (2.1272)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.2397
[32m[2023-01-04 19:17:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][47/27342]	eta 5:58:15 lr 0.0010000000	 wd 0.1000	time 0.7375 (0.7875)	loss 1.2523 (1.2493)	loss-cls 10.0188 (9.9944)	loss-aux 0.0000 (0.0000)	grad_norm 2.0108 (2.1078)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.3002
[32m[2023-01-04 19:17:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][55/27342]	eta 5:51:00 lr 0.0010000000	 wd 0.1000	time 0.7855 (0.7718)	loss 1.2513 (1.2496)	loss-cls 10.0107 (9.9966)	loss-aux 0.0000 (0.0000)	grad_norm 1.8825 (2.0756)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.4211
[32m[2023-01-04 19:17:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][63/27342]	eta 5:45:22 lr 0.0010000000	 wd 0.1000	time 0.7844 (0.7596)	loss 1.2578 (1.2499)	loss-cls 10.0626 (9.9990)	loss-aux 0.0000 (0.0000)	grad_norm 2.1060 (2.0794)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.3959
[32m[2023-01-04 19:17:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][71/27342]	eta 5:40:34 lr 0.0010000000	 wd 0.1000	time 0.7480 (0.7493)	loss 1.2503 (1.2497)	loss-cls 10.0023 (9.9979)	loss-aux 0.0000 (0.0000)	grad_norm 1.7596 (2.0439)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.3333
[32m[2023-01-04 19:17:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][79/27342]	eta 5:36:27 lr 0.0010000000	 wd 0.1000	time 0.7393 (0.7405)	loss 1.2457 (1.2503)	loss-cls 9.9655 (10.0021)	loss-aux 0.0000 (0.0000)	grad_norm 1.8960 (2.0291)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.2875
[32m[2023-01-04 19:18:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][87/27342]	eta 5:33:35 lr 0.0010000000	 wd 0.1000	time 0.7948 (0.7344)	loss 1.2598 (1.2508)	loss-cls 10.0783 (10.0061)	loss-aux 0.0000 (0.0000)	grad_norm 1.8259 (2.0106)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.3858
[32m[2023-01-04 19:18:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][95/27342]	eta 5:30:31 lr 0.0010000000	 wd 0.1000	time 0.7478 (0.7279)	loss 1.2600 (1.2507)	loss-cls 10.0798 (10.0056)	loss-aux 0.0000 (0.0000)	grad_norm 1.6243 (1.9784)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.2500
[32m[2023-01-04 19:18:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][103/27342]	eta 5:28:42 lr 0.0010000000	 wd 0.1000	time 0.7734 (0.7240)	loss 1.2376 (1.2505)	loss-cls 9.9008 (10.0039)	loss-aux 0.0000 (0.0000)	grad_norm 1.6633 (1.9542)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.4270
[32m[2023-01-04 19:18:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][111/27342]	eta 5:27:09 lr 0.0010000000	 wd 0.1000	time 0.7805 (0.7209)	loss 1.2561 (1.2504)	loss-cls 10.0491 (10.0035)	loss-aux 0.0000 (0.0000)	grad_norm 1.5768 (1.9272)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.4344
[32m[2023-01-04 19:18:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][119/27342]	eta 5:26:05 lr 0.0010000000	 wd 0.1000	time 0.8035 (0.7187)	loss 1.2520 (1.2505)	loss-cls 10.0158 (10.0037)	loss-aux 0.0000 (0.0000)	grad_norm 1.7056 (1.9124)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5110
[32m[2023-01-04 19:18:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][127/27342]	eta 5:25:14 lr 0.0010000000	 wd 0.1000	time 0.7775 (0.7171)	loss 1.2504 (1.2501)	loss-cls 10.0029 (10.0009)	loss-aux 0.0000 (0.0000)	grad_norm 1.6014 (1.8930)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5366
[32m[2023-01-04 19:18:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][135/27342]	eta 5:24:23 lr 0.0010000000	 wd 0.1000	time 0.7911 (0.7154)	loss 1.2481 (1.2501)	loss-cls 9.9851 (10.0008)	loss-aux 0.0000 (0.0000)	grad_norm 1.5877 (1.8750)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5093
[32m[2023-01-04 19:18:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][143/27342]	eta 5:24:19 lr 0.0010000000	 wd 0.1000	time 0.8147 (0.7154)	loss 1.2659 (1.2505)	loss-cls 10.1273 (10.0039)	loss-aux 0.0000 (0.0000)	grad_norm 1.7503 (1.8681)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7300
[32m[2023-01-04 19:18:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][151/27342]	eta 5:24:12 lr 0.0010000000	 wd 0.1000	time 0.8153 (0.7154)	loss 1.2617 (1.2506)	loss-cls 10.0938 (10.0049)	loss-aux 0.0000 (0.0000)	grad_norm 1.6631 (1.8573)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7181
[32m[2023-01-04 19:18:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][159/27342]	eta 5:23:54 lr 0.0010000000	 wd 0.1000	time 0.8027 (0.7150)	loss 1.2506 (1.2507)	loss-cls 10.0048 (10.0052)	loss-aux 0.0000 (0.0000)	grad_norm 1.6186 (1.8454)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6541
[32m[2023-01-04 19:18:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][167/27342]	eta 5:23:37 lr 0.0010000000	 wd 0.1000	time 0.8109 (0.7145)	loss 1.2429 (1.2507)	loss-cls 9.9431 (10.0053)	loss-aux 0.0000 (0.0000)	grad_norm 1.7240 (1.8396)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6454
[32m[2023-01-04 19:19:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][175/27342]	eta 5:23:00 lr 0.0010000000	 wd 0.1000	time 0.7871 (0.7134)	loss 1.2494 (1.2507)	loss-cls 9.9953 (10.0054)	loss-aux 0.0000 (0.0000)	grad_norm 1.5110 (1.8247)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5164
[32m[2023-01-04 19:19:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][183/27342]	eta 5:22:19 lr 0.0010000000	 wd 0.1000	time 0.7555 (0.7121)	loss 1.2302 (1.2508)	loss-cls 9.8415 (10.0064)	loss-aux 0.0000 (0.0000)	grad_norm 1.6493 (1.8170)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.4681
[32m[2023-01-04 19:19:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][191/27342]	eta 5:21:22 lr 0.0010000000	 wd 0.1000	time 0.7596 (0.7102)	loss 1.2414 (1.2507)	loss-cls 9.9313 (10.0059)	loss-aux 0.0000 (0.0000)	grad_norm 1.4098 (1.8001)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.3354
[32m[2023-01-04 19:19:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][199/27342]	eta 5:20:38 lr 0.0010000000	 wd 0.1000	time 0.7824 (0.7088)	loss 1.2406 (1.2507)	loss-cls 9.9252 (10.0058)	loss-aux 0.0000 (0.0000)	grad_norm 1.4073 (1.7844)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.3953
[32m[2023-01-04 19:19:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][207/27342]	eta 5:20:02 lr 0.0010000000	 wd 0.1000	time 0.7695 (0.7077)	loss 1.2502 (1.2509)	loss-cls 10.0015 (10.0073)	loss-aux 0.0000 (0.0000)	grad_norm 1.3638 (1.7682)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.4372
[32m[2023-01-04 19:19:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][215/27342]	eta 5:19:33 lr 0.0010000000	 wd 0.1000	time 0.7900 (0.7068)	loss 1.2564 (1.2514)	loss-cls 10.0512 (10.0109)	loss-aux 0.0000 (0.0000)	grad_norm 1.7440 (1.7673)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.4790
[32m[2023-01-04 19:19:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][223/27342]	eta 5:19:18 lr 0.0010000000	 wd 0.1000	time 0.7974 (0.7064)	loss 1.2493 (1.2514)	loss-cls 9.9946 (10.0113)	loss-aux 0.0000 (0.0000)	grad_norm 1.6588 (1.7634)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5728
[32m[2023-01-04 19:19:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][231/27342]	eta 5:19:13 lr 0.0010000000	 wd 0.1000	time 0.7975 (0.7065)	loss 1.2528 (1.2514)	loss-cls 10.0221 (10.0108)	loss-aux 0.0000 (0.0000)	grad_norm 1.6382 (1.7591)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6618
[32m[2023-01-04 19:19:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][239/27342]	eta 5:18:54 lr 0.0010000000	 wd 0.1000	time 0.7765 (0.7060)	loss 1.2451 (1.2513)	loss-cls 9.9608 (10.0100)	loss-aux 0.0000 (0.0000)	grad_norm 1.7351 (1.7583)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5304
[32m[2023-01-04 19:19:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][247/27342]	eta 5:18:28 lr 0.0010000000	 wd 0.1000	time 0.7729 (0.7052)	loss 1.2530 (1.2512)	loss-cls 10.0236 (10.0097)	loss-aux 0.0000 (0.0000)	grad_norm 1.6555 (1.7550)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.4641
[32m[2023-01-04 19:19:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][255/27342]	eta 5:18:05 lr 0.0010000000	 wd 0.1000	time 0.7751 (0.7046)	loss 1.2638 (1.2512)	loss-cls 10.1101 (10.0096)	loss-aux 0.0000 (0.0000)	grad_norm 1.6834 (1.7527)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.4735
[32m[2023-01-04 19:20:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][263/27342]	eta 5:17:47 lr 0.0010000000	 wd 0.1000	time 0.8015 (0.7041)	loss 1.2690 (1.2513)	loss-cls 10.1517 (10.0104)	loss-aux 0.0000 (0.0000)	grad_norm 1.6785 (1.7505)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5200
[32m[2023-01-04 19:20:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][271/27342]	eta 5:17:33 lr 0.0010000000	 wd 0.1000	time 0.7851 (0.7039)	loss 1.2438 (1.2513)	loss-cls 9.9504 (10.0104)	loss-aux 0.0000 (0.0000)	grad_norm 1.6863 (1.7486)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5538
[32m[2023-01-04 19:20:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][279/27342]	eta 5:17:23 lr 0.0010000000	 wd 0.1000	time 0.8049 (0.7037)	loss 1.2760 (1.2513)	loss-cls 10.2084 (10.0107)	loss-aux 0.0000 (0.0000)	grad_norm 1.7356 (1.7482)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5822
[32m[2023-01-04 19:20:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][287/27342]	eta 5:17:09 lr 0.0010000000	 wd 0.1000	time 0.7699 (0.7034)	loss 1.2460 (1.2514)	loss-cls 9.9680 (10.0114)	loss-aux 0.0000 (0.0000)	grad_norm 1.6686 (1.7460)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5420
[32m[2023-01-04 19:20:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][295/27342]	eta 5:16:56 lr 0.0010000000	 wd 0.1000	time 0.7821 (0.7031)	loss 1.2576 (1.2515)	loss-cls 10.0608 (10.0118)	loss-aux 0.0000 (0.0000)	grad_norm 1.6407 (1.7432)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5437
[32m[2023-01-04 19:20:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][303/27342]	eta 5:16:36 lr 0.0010000000	 wd 0.1000	time 0.7767 (0.7026)	loss 1.2831 (1.2515)	loss-cls 10.2646 (10.0118)	loss-aux 0.0000 (0.0000)	grad_norm 1.6745 (1.7414)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.4613
[32m[2023-01-04 19:20:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][311/27342]	eta 5:16:21 lr 0.0010000000	 wd 0.1000	time 0.7961 (0.7022)	loss 1.2684 (1.2515)	loss-cls 10.1475 (10.0120)	loss-aux 0.0000 (0.0000)	grad_norm 1.4674 (1.7343)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5171
[32m[2023-01-04 19:20:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][319/27342]	eta 5:16:15 lr 0.0010000000	 wd 0.1000	time 0.7891 (0.7022)	loss 1.2631 (1.2517)	loss-cls 10.1045 (10.0136)	loss-aux 0.0000 (0.0000)	grad_norm 1.6588 (1.7325)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6104
[32m[2023-01-04 19:20:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][327/27342]	eta 5:16:14 lr 0.0010000000	 wd 0.1000	time 0.7827 (0.7024)	loss 1.2806 (1.2519)	loss-cls 10.2447 (10.0148)	loss-aux 0.0000 (0.0000)	grad_norm 1.3955 (1.7242)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6722
[32m[2023-01-04 19:20:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][335/27342]	eta 5:16:20 lr 0.0010000000	 wd 0.1000	time 0.8285 (0.7028)	loss 1.2573 (1.2521)	loss-cls 10.0587 (10.0166)	loss-aux 0.0000 (0.0000)	grad_norm 1.3734 (1.7159)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7610
[32m[2023-01-04 19:20:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][343/27342]	eta 5:16:20 lr 0.0010000000	 wd 0.1000	time 0.8150 (0.7030)	loss 1.2520 (1.2523)	loss-cls 10.0158 (10.0181)	loss-aux 0.0000 (0.0000)	grad_norm 1.4334 (1.7093)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7015
[32m[2023-01-04 19:21:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][351/27342]	eta 5:16:16 lr 0.0010000000	 wd 0.1000	time 0.7872 (0.7031)	loss 1.2514 (1.2523)	loss-cls 10.0112 (10.0186)	loss-aux 0.0000 (0.0000)	grad_norm 1.4887 (1.7043)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6375
[32m[2023-01-04 19:21:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][359/27342]	eta 5:16:08 lr 0.0009999999	 wd 0.1000	time 0.8044 (0.7030)	loss 1.2247 (1.2523)	loss-cls 9.7979 (10.0188)	loss-aux 0.0000 (0.0000)	grad_norm 1.2666 (1.6946)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6027
[32m[2023-01-04 19:21:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][367/27342]	eta 5:15:57 lr 0.0009999999	 wd 0.1000	time 0.7940 (0.7028)	loss 1.2685 (1.2524)	loss-cls 10.1476 (10.0189)	loss-aux 0.0000 (0.0000)	grad_norm 1.1981 (1.6838)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5488
[32m[2023-01-04 19:21:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][375/27342]	eta 5:15:46 lr 0.0009999999	 wd 0.1000	time 0.7677 (0.7026)	loss 1.2412 (1.2522)	loss-cls 9.9298 (10.0180)	loss-aux 0.0000 (0.0000)	grad_norm 1.4754 (1.6793)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5427
[32m[2023-01-04 19:21:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][383/27342]	eta 5:15:40 lr 0.0009999999	 wd 0.1000	time 0.8147 (0.7026)	loss 1.2470 (1.2522)	loss-cls 9.9759 (10.0178)	loss-aux 0.0000 (0.0000)	grad_norm 1.7703 (1.6812)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6112
[32m[2023-01-04 19:21:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][391/27342]	eta 5:15:33 lr 0.0009999999	 wd 0.1000	time 0.8092 (0.7025)	loss 1.2431 (1.2523)	loss-cls 9.9445 (10.0181)	loss-aux 0.0000 (0.0000)	grad_norm 1.0378 (1.6681)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6109
[32m[2023-01-04 19:21:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][399/27342]	eta 5:15:28 lr 0.0009999999	 wd 0.1000	time 0.7917 (0.7026)	loss 1.2497 (1.2522)	loss-cls 9.9973 (10.0174)	loss-aux 0.0000 (0.0000)	grad_norm 1.0861 (1.6565)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6296
[32m[2023-01-04 19:21:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][407/27342]	eta 5:15:26 lr 0.0009999999	 wd 0.1000	time 0.8107 (0.7027)	loss 1.2349 (1.2521)	loss-cls 9.8795 (10.0169)	loss-aux 0.0000 (0.0000)	grad_norm 1.1953 (1.6474)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6703
[32m[2023-01-04 19:21:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][415/27342]	eta 5:15:24 lr 0.0009999999	 wd 0.1000	time 0.7967 (0.7028)	loss 1.2620 (1.2521)	loss-cls 10.0959 (10.0164)	loss-aux 0.0000 (0.0000)	grad_norm 1.2606 (1.6400)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6739
[32m[2023-01-04 19:21:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][423/27342]	eta 5:15:19 lr 0.0009999999	 wd 0.1000	time 0.7812 (0.7028)	loss 1.2373 (1.2520)	loss-cls 9.8986 (10.0160)	loss-aux 0.0000 (0.0000)	grad_norm 0.9785 (1.6275)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6399
[32m[2023-01-04 19:21:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][431/27342]	eta 5:15:08 lr 0.0009999999	 wd 0.1000	time 0.7958 (0.7026)	loss 1.2316 (1.2519)	loss-cls 9.8529 (10.0155)	loss-aux 0.0000 (0.0000)	grad_norm 5.2193 (1.6940)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5273
[32m[2023-01-04 19:22:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][439/27342]	eta 5:15:02 lr 0.0009999999	 wd 0.1000	time 0.7828 (0.7026)	loss 1.2255 (1.2519)	loss-cls 9.8036 (10.0154)	loss-aux 0.0000 (0.0000)	grad_norm 1.0217 (1.6818)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6133
[32m[2023-01-04 19:22:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][447/27342]	eta 5:14:57 lr 0.0009999999	 wd 0.1000	time 0.8072 (0.7026)	loss 1.2503 (1.2519)	loss-cls 10.0024 (10.0149)	loss-aux 0.0000 (0.0000)	grad_norm 12.7128 (1.8788)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6274
[32m[2023-01-04 19:22:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][455/27342]	eta 5:14:48 lr 0.0009999999	 wd 0.1000	time 0.7978 (0.7025)	loss 1.2482 (1.2519)	loss-cls 9.9857 (10.0152)	loss-aux 0.0000 (0.0000)	grad_norm 0.8695 (1.8611)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5759
[32m[2023-01-04 19:22:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][463/27342]	eta 5:14:44 lr 0.0009999999	 wd 0.1000	time 0.8255 (0.7026)	loss 1.2449 (1.2518)	loss-cls 9.9589 (10.0147)	loss-aux 0.0000 (0.0000)	grad_norm 0.8237 (1.8432)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6363
[32m[2023-01-04 19:22:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][471/27342]	eta 5:14:35 lr 0.0009999999	 wd 0.1000	time 0.7827 (0.7024)	loss 1.2273 (1.2518)	loss-cls 9.8185 (10.0145)	loss-aux 0.0000 (0.0000)	grad_norm 2.0547 (1.8468)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5639
[32m[2023-01-04 19:22:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][479/27342]	eta 5:14:29 lr 0.0009999999	 wd 0.1000	time 0.8094 (0.7024)	loss 1.2533 (1.2517)	loss-cls 10.0264 (10.0138)	loss-aux 0.0000 (0.0000)	grad_norm 1.4179 (1.8396)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6189
[32m[2023-01-04 19:22:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][487/27342]	eta 5:14:23 lr 0.0009999999	 wd 0.1000	time 0.7953 (0.7024)	loss 1.2197 (1.2516)	loss-cls 9.7574 (10.0125)	loss-aux 0.0000 (0.0000)	grad_norm 0.9978 (1.8258)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6044
[32m[2023-01-04 19:22:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][495/27342]	eta 5:14:20 lr 0.0009999999	 wd 0.1000	time 0.8153 (0.7025)	loss 1.2566 (1.2516)	loss-cls 10.0529 (10.0128)	loss-aux 0.0000 (0.0000)	grad_norm 21.2418 (2.1390)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6823
[32m[2023-01-04 19:22:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][503/27342]	eta 5:14:18 lr 0.0009999999	 wd 0.1000	time 0.8076 (0.7026)	loss 1.2446 (1.2514)	loss-cls 9.9566 (10.0116)	loss-aux 0.0000 (0.0000)	grad_norm 1.5491 (2.1296)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6718
[32m[2023-01-04 19:22:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][511/27342]	eta 5:14:19 lr 0.0009999999	 wd 0.1000	time 0.8095 (0.7029)	loss 1.2358 (1.2513)	loss-cls 9.8862 (10.0105)	loss-aux 0.0000 (0.0000)	grad_norm 0.7772 (2.1085)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7488
[32m[2023-01-04 19:23:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][519/27342]	eta 5:14:20 lr 0.0009999999	 wd 0.1000	time 0.8129 (0.7032)	loss 1.2534 (1.2513)	loss-cls 10.0276 (10.0105)	loss-aux 0.0000 (0.0000)	grad_norm 0.6550 (2.0861)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7625
[32m[2023-01-04 19:23:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][527/27342]	eta 5:14:20 lr 0.0009999999	 wd 0.1000	time 0.8339 (0.7034)	loss 1.2371 (1.2512)	loss-cls 9.8965 (10.0099)	loss-aux 0.0000 (0.0000)	grad_norm 0.6162 (2.0639)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7404
[32m[2023-01-04 19:23:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][535/27342]	eta 5:14:17 lr 0.0009999999	 wd 0.1000	time 0.8067 (0.7035)	loss 1.2395 (1.2512)	loss-cls 9.9157 (10.0097)	loss-aux 0.0000 (0.0000)	grad_norm 0.6272 (2.0424)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6727
[32m[2023-01-04 19:23:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][543/27342]	eta 5:14:16 lr 0.0009999999	 wd 0.1000	time 0.8276 (0.7036)	loss 1.2433 (1.2512)	loss-cls 9.9464 (10.0093)	loss-aux 0.0000 (0.0000)	grad_norm 0.6441 (2.0219)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7297
[32m[2023-01-04 19:23:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][551/27342]	eta 5:14:12 lr 0.0009999999	 wd 0.1000	time 0.7982 (0.7037)	loss 1.2490 (1.2512)	loss-cls 9.9916 (10.0094)	loss-aux 0.0000 (0.0000)	grad_norm 0.6214 (2.0016)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6460
[32m[2023-01-04 19:23:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][559/27342]	eta 5:14:09 lr 0.0009999999	 wd 0.1000	time 0.8254 (0.7038)	loss 1.2342 (1.2511)	loss-cls 9.8735 (10.0087)	loss-aux 0.0000 (0.0000)	grad_norm 1.3108 (1.9917)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6944
[32m[2023-01-04 19:23:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][567/27342]	eta 5:14:06 lr 0.0009999999	 wd 0.1000	time 0.8107 (0.7039)	loss 1.2429 (1.2510)	loss-cls 9.9431 (10.0080)	loss-aux 0.0000 (0.0000)	grad_norm 1.6689 (1.9871)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6881
[32m[2023-01-04 19:23:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][575/27342]	eta 5:14:04 lr 0.0009999999	 wd 0.1000	time 0.8214 (0.7040)	loss 1.2583 (1.2509)	loss-cls 10.0667 (10.0075)	loss-aux 0.0000 (0.0000)	grad_norm 0.6751 (1.9689)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7025
[32m[2023-01-04 19:23:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][583/27342]	eta 5:13:59 lr 0.0009999999	 wd 0.1000	time 0.7891 (0.7040)	loss 1.2489 (1.2509)	loss-cls 9.9909 (10.0071)	loss-aux 0.0000 (0.0000)	grad_norm 0.6907 (1.9514)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6467
[32m[2023-01-04 19:23:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][591/27342]	eta 5:13:50 lr 0.0009999999	 wd 0.1000	time 0.7456 (0.7039)	loss 1.2373 (1.2508)	loss-cls 9.8987 (10.0065)	loss-aux 0.0000 (0.0000)	grad_norm 0.7840 (1.9356)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5519
[32m[2023-01-04 19:23:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][599/27342]	eta 5:13:41 lr 0.0009999999	 wd 0.1000	time 0.7987 (0.7038)	loss 1.2322 (1.2508)	loss-cls 9.8580 (10.0062)	loss-aux 0.0000 (0.0000)	grad_norm 0.6219 (1.9181)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.5566
[32m[2023-01-04 19:24:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][607/27342]	eta 5:13:34 lr 0.0009999999	 wd 0.1000	time 0.8029 (0.7038)	loss 1.2461 (1.2507)	loss-cls 9.9688 (10.0057)	loss-aux 0.0000 (0.0000)	grad_norm 6.0926 (1.9730)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6145
[32m[2023-01-04 19:24:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][615/27342]	eta 5:13:32 lr 0.0009999999	 wd 0.1000	time 0.8142 (0.7039)	loss 1.2620 (1.2507)	loss-cls 10.0959 (10.0057)	loss-aux 0.0000 (0.0000)	grad_norm 0.6569 (1.9559)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6961
[32m[2023-01-04 19:24:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][623/27342]	eta 5:13:27 lr 0.0009999998	 wd 0.1000	time 0.7949 (0.7039)	loss 1.2507 (1.2507)	loss-cls 10.0057 (10.0054)	loss-aux 0.0000 (0.0000)	grad_norm 0.5932 (1.9385)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6541
[32m[2023-01-04 19:24:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][631/27342]	eta 5:13:25 lr 0.0009999998	 wd 0.1000	time 0.8082 (0.7040)	loss 1.2534 (1.2506)	loss-cls 10.0271 (10.0052)	loss-aux 0.0000 (0.0000)	grad_norm 0.7654 (1.9236)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7101
[32m[2023-01-04 19:24:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][639/27342]	eta 5:13:22 lr 0.0009999998	 wd 0.1000	time 0.8183 (0.7041)	loss 1.2451 (1.2506)	loss-cls 9.9606 (10.0048)	loss-aux 0.0000 (0.0000)	grad_norm 0.6050 (1.9071)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.6934
[32m[2023-01-04 19:24:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][647/27342]	eta 5:13:21 lr 0.0009999998	 wd 0.1000	time 0.8291 (0.7043)	loss 1.2158 (1.2504)	loss-cls 9.7263 (10.0034)	loss-aux 0.0000 (0.0000)	grad_norm 0.6278 (1.8914)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7647
[32m[2023-01-04 19:24:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][655/27342]	eta 5:13:21 lr 0.0009999998	 wd 0.1000	time 0.8463 (0.7045)	loss 1.2566 (1.2505)	loss-cls 10.0530 (10.0037)	loss-aux 0.0000 (0.0000)	grad_norm 0.7941 (1.8780)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.7708
[32m[2023-01-04 19:24:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][663/27342]	eta 5:13:24 lr 0.0009999998	 wd 0.1000	time 0.8319 (0.7049)	loss 1.2265 (1.2504)	loss-cls 9.8124 (10.0031)	loss-aux 0.0000 (0.0000)	grad_norm 0.6559 (1.8632)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.8537
[32m[2023-01-04 19:24:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][671/27342]	eta 5:13:31 lr 0.0009999998	 wd 0.1000	time 0.8398 (0.7053)	loss 1.2197 (1.2503)	loss-cls 9.7578 (10.0022)	loss-aux 0.0000 (0.0000)	grad_norm 0.5957 (1.8482)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.9533
[32m[2023-01-04 19:24:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][679/27342]	eta 5:13:37 lr 0.0009999998	 wd 0.1000	time 0.8303 (0.7058)	loss 1.2307 (1.2501)	loss-cls 9.8455 (10.0007)	loss-aux 0.0000 (0.0000)	grad_norm 0.6083 (1.8336)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.9425
[32m[2023-01-04 19:25:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][687/27342]	eta 5:13:42 lr 0.0009999998	 wd 0.1000	time 0.8338 (0.7061)	loss 1.2413 (1.2500)	loss-cls 9.9300 (10.0001)	loss-aux 0.0000 (0.0000)	grad_norm 0.6577 (1.8199)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.9008
[32m[2023-01-04 19:25:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][695/27342]	eta 5:13:49 lr 0.0009999998	 wd 0.1000	time 0.8533 (0.7066)	loss 1.2401 (1.2500)	loss-cls 9.9207 (9.9997)	loss-aux 0.0000 (0.0000)	grad_norm 0.6350 (1.8063)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.9852
[32m[2023-01-04 19:25:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][703/27342]	eta 5:13:53 lr 0.0009999998	 wd 0.1000	time 0.8323 (0.7070)	loss 1.2442 (1.2499)	loss-cls 9.9533 (9.9993)	loss-aux 0.0000 (0.0000)	grad_norm 0.6008 (1.7926)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.9184
[32m[2023-01-04 19:25:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][711/27342]	eta 5:13:58 lr 0.0009999998	 wd 0.1000	time 0.8428 (0.7074)	loss 1.2410 (1.2499)	loss-cls 9.9282 (9.9989)	loss-aux 0.0000 (0.0000)	grad_norm 0.6520 (1.7798)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.9417
[32m[2023-01-04 19:25:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][719/27342]	eta 5:14:02 lr 0.0009999998	 wd 0.1000	time 0.8402 (0.7077)	loss 1.2388 (1.2499)	loss-cls 9.9101 (9.9992)	loss-aux 0.0000 (0.0000)	grad_norm 0.6903 (1.7677)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.9047
[32m[2023-01-04 19:25:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][727/27342]	eta 5:14:04 lr 0.0009999998	 wd 0.1000	time 0.8411 (0.7080)	loss 1.2502 (1.2498)	loss-cls 10.0015 (9.9985)	loss-aux 0.0000 (0.0000)	grad_norm 0.5678 (1.7545)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.8846
[32m[2023-01-04 19:25:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][735/27342]	eta 5:14:07 lr 0.0009999998	 wd 0.1000	time 0.8487 (0.7084)	loss 1.2490 (1.2498)	loss-cls 9.9923 (9.9982)	loss-aux 0.0000 (0.0000)	grad_norm 0.6772 (1.7428)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.9018
[32m[2023-01-04 19:25:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][743/27342]	eta 5:14:08 lr 0.0009999998	 wd 0.1000	time 0.8284 (0.7086)	loss 1.2397 (1.2498)	loss-cls 9.9176 (9.9980)	loss-aux 0.0000 (0.0000)	grad_norm 0.8803 (1.7335)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.8581
[32m[2023-01-04 19:25:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][751/27342]	eta 5:14:14 lr 0.0009999998	 wd 0.1000	time 0.8594 (0.7090)	loss 1.2518 (1.2498)	loss-cls 10.0141 (9.9981)	loss-aux 0.0000 (0.0000)	grad_norm 0.6184 (1.7216)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.9799
[32m[2023-01-04 19:25:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][759/27342]	eta 5:14:16 lr 0.0009999998	 wd 0.1000	time 0.8433 (0.7093)	loss 1.2605 (1.2496)	loss-cls 10.0840 (9.9971)	loss-aux 0.0000 (0.0000)	grad_norm 0.8858 (1.7128)	loss_scale 65536.0000 (65536.0000)	mem 10930MB	batch_time 5.8997
[32m[2023-01-04 19:26:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][767/27342]	eta 5:14:13 lr 0.0009999998	 wd 0.1000	time 0.7474 (0.7095)	loss 1.2336 (1.2496)	loss-cls 9.8686 (9.9964)	loss-aux 0.0000 (0.0000)	grad_norm nan (nan)	loss_scale 32768.0000 (65493.3333)	mem 10930MB	batch_time 5.7630
[32m[2023-01-04 19:26:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][775/27342]	eta 5:14:17 lr 0.0009999998	 wd 0.1000	time 0.8392 (0.7098)	loss 1.2447 (1.2494)	loss-cls 9.9573 (9.9956)	loss-aux 0.0000 (0.0000)	grad_norm 0.5678 (nan)	loss_scale 32768.0000 (65155.9588)	mem 10930MB	batch_time 5.9448
[32m[2023-01-04 19:26:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][783/27342]	eta 5:14:16 lr 0.0009999998	 wd 0.1000	time 0.8270 (0.7100)	loss 1.2295 (1.2494)	loss-cls 9.8358 (9.9954)	loss-aux 0.0000 (0.0000)	grad_norm 0.8113 (nan)	loss_scale 32768.0000 (64825.4694)	mem 10930MB	batch_time 5.8320
[32m[2023-01-04 19:26:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][791/27342]	eta 5:14:17 lr 0.0009999998	 wd 0.1000	time 0.8367 (0.7102)	loss 1.2402 (1.2494)	loss-cls 9.9217 (9.9953)	loss-aux 0.0000 (0.0000)	grad_norm 1.9939 (nan)	loss_scale 32768.0000 (64501.6566)	mem 10930MB	batch_time 5.8787
[32m[2023-01-04 19:26:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][799/27342]	eta 5:14:18 lr 0.0009999997	 wd 0.1000	time 0.8301 (0.7105)	loss 1.2634 (1.2494)	loss-cls 10.1074 (9.9951)	loss-aux 0.0000 (0.0000)	grad_norm 2.8656 (nan)	loss_scale 32768.0000 (64184.3200)	mem 10930MB	batch_time 5.8678
[32m[2023-01-04 19:26:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][807/27342]	eta 5:14:20 lr 0.0009999997	 wd 0.1000	time 0.8424 (0.7108)	loss 1.2310 (1.2493)	loss-cls 9.8483 (9.9947)	loss-aux 0.0000 (0.0000)	grad_norm 0.8225 (nan)	loss_scale 32768.0000 (63873.2673)	mem 10930MB	batch_time 5.9315
[32m[2023-01-04 19:26:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][815/27342]	eta 5:14:19 lr 0.0009999997	 wd 0.1000	time 0.8313 (0.7109)	loss 1.2393 (1.2493)	loss-cls 9.9142 (9.9943)	loss-aux 0.0000 (0.0000)	grad_norm 0.8936 (nan)	loss_scale 32768.0000 (63568.3137)	mem 10930MB	batch_time 5.8163
[32m[2023-01-04 19:26:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][823/27342]	eta 5:14:17 lr 0.0009999997	 wd 0.1000	time 0.7956 (0.7111)	loss 1.2571 (1.2493)	loss-cls 10.0564 (9.9942)	loss-aux 0.0000 (0.0000)	grad_norm 0.5833 (nan)	loss_scale 32768.0000 (63269.2816)	mem 10930MB	batch_time 5.8135
[32m[2023-01-04 19:26:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][831/27342]	eta 5:14:17 lr 0.0009999997	 wd 0.1000	time 0.8238 (0.7113)	loss 1.2210 (1.2492)	loss-cls 9.7683 (9.9936)	loss-aux 0.0000 (0.0000)	grad_norm 0.6082 (nan)	loss_scale 32768.0000 (62976.0000)	mem 10930MB	batch_time 5.8711
[32m[2023-01-04 19:26:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][839/27342]	eta 5:14:20 lr 0.0009999997	 wd 0.1000	time 0.8349 (0.7116)	loss 1.2583 (1.2492)	loss-cls 10.0666 (9.9932)	loss-aux 0.0000 (0.0000)	grad_norm 0.7960 (nan)	loss_scale 32768.0000 (62688.3048)	mem 10930MB	batch_time 5.9444
[32m[2023-01-04 19:27:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][847/27342]	eta 5:14:19 lr 0.0009999997	 wd 0.1000	time 0.8324 (0.7118)	loss 1.2539 (1.2491)	loss-cls 10.0309 (9.9927)	loss-aux 0.0000 (0.0000)	grad_norm 13.8275 (nan)	loss_scale 32768.0000 (62406.0377)	mem 10930MB	batch_time 5.8728
[32m[2023-01-04 19:27:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][855/27342]	eta 5:14:21 lr 0.0009999997	 wd 0.1000	time 0.8017 (0.7121)	loss 1.2435 (1.2491)	loss-cls 9.9480 (9.9925)	loss-aux 0.0000 (0.0000)	grad_norm 2.0525 (nan)	loss_scale 32768.0000 (62129.0467)	mem 10930MB	batch_time 5.9378
[32m[2023-01-04 19:27:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][863/27342]	eta 5:14:21 lr 0.0009999997	 wd 0.1000	time 0.8265 (0.7123)	loss 1.2348 (1.2489)	loss-cls 9.8784 (9.9914)	loss-aux 0.0000 (0.0000)	grad_norm 1.6941 (nan)	loss_scale 32768.0000 (61857.1852)	mem 10930MB	batch_time 5.8846
[32m[2023-01-04 19:27:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][871/27342]	eta 5:14:22 lr 0.0009999997	 wd 0.1000	time 0.8401 (0.7126)	loss 1.2345 (1.2489)	loss-cls 9.8760 (9.9909)	loss-aux 0.0000 (0.0000)	grad_norm 8.4681 (nan)	loss_scale 32768.0000 (61590.3119)	mem 10930MB	batch_time 5.9169
[32m[2023-01-04 19:27:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][879/27342]	eta 5:14:23 lr 0.0009999997	 wd 0.1000	time 0.8217 (0.7128)	loss 1.2281 (1.2489)	loss-cls 9.8248 (9.9909)	loss-aux 0.0000 (0.0000)	grad_norm 0.9449 (nan)	loss_scale 32768.0000 (61328.2909)	mem 10930MB	batch_time 5.9139
[32m[2023-01-04 19:27:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][887/27342]	eta 5:14:20 lr 0.0009999997	 wd 0.1000	time 0.7911 (0.7129)	loss 1.2214 (1.2488)	loss-cls 9.7710 (9.9903)	loss-aux 0.0000 (0.0000)	grad_norm 2.5071 (nan)	loss_scale 32768.0000 (61070.9910)	mem 10930MB	batch_time 5.7992
[32m[2023-01-04 19:27:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][895/27342]	eta 5:14:19 lr 0.0009999997	 wd 0.1000	time 0.8214 (0.7131)	loss 1.2374 (1.2487)	loss-cls 9.8992 (9.9898)	loss-aux 0.0000 (0.0000)	grad_norm 1.2698 (nan)	loss_scale 32768.0000 (60818.2857)	mem 10930MB	batch_time 5.8464
[32m[2023-01-04 19:27:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][903/27342]	eta 5:14:18 lr 0.0009999997	 wd 0.1000	time 0.8231 (0.7133)	loss 1.2381 (1.2487)	loss-cls 9.9051 (9.9894)	loss-aux 0.0000 (0.0000)	grad_norm 0.7434 (nan)	loss_scale 32768.0000 (60570.0531)	mem 10930MB	batch_time 5.8811
[32m[2023-01-04 19:27:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][911/27342]	eta 5:14:20 lr 0.0009999997	 wd 0.1000	time 0.8427 (0.7136)	loss 1.2239 (1.2487)	loss-cls 9.7911 (9.9895)	loss-aux 0.0000 (0.0000)	grad_norm 0.7452 (nan)	loss_scale 32768.0000 (60326.1754)	mem 10930MB	batch_time 5.9698
[32m[2023-01-04 19:27:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][919/27342]	eta 5:14:20 lr 0.0009999997	 wd 0.1000	time 0.8222 (0.7138)	loss 1.2593 (1.2487)	loss-cls 10.0743 (9.9899)	loss-aux 0.0000 (0.0000)	grad_norm 1.0904 (nan)	loss_scale 32768.0000 (60086.5391)	mem 10930MB	batch_time 5.9071
[32m[2023-01-04 19:27:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][927/27342]	eta 5:14:20 lr 0.0009999997	 wd 0.1000	time 0.8127 (0.7140)	loss 1.2366 (1.2487)	loss-cls 9.8927 (9.9894)	loss-aux 0.0000 (0.0000)	grad_norm 0.9646 (nan)	loss_scale 32768.0000 (59851.0345)	mem 10930MB	batch_time 5.9042
[32m[2023-01-04 19:28:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][935/27342]	eta 5:14:20 lr 0.0009999997	 wd 0.1000	time 0.8499 (0.7142)	loss 1.2526 (1.2486)	loss-cls 10.0211 (9.9890)	loss-aux 0.0000 (0.0000)	grad_norm 0.7019 (nan)	loss_scale 32768.0000 (59619.5556)	mem 10930MB	batch_time 5.9205
[32m[2023-01-04 19:28:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][943/27342]	eta 5:14:18 lr 0.0009999996	 wd 0.1000	time 0.8186 (0.7144)	loss 1.2645 (1.2486)	loss-cls 10.1163 (9.9890)	loss-aux 0.0000 (0.0000)	grad_norm 0.6876 (nan)	loss_scale 32768.0000 (59392.0000)	mem 10930MB	batch_time 5.8580
[32m[2023-01-04 19:28:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][951/27342]	eta 5:14:16 lr 0.0009999996	 wd 0.1000	time 0.8264 (0.7145)	loss 1.2709 (1.2486)	loss-cls 10.1670 (9.9891)	loss-aux 0.0000 (0.0000)	grad_norm 0.7406 (nan)	loss_scale 32768.0000 (59168.2689)	mem 10930MB	batch_time 5.8303
[32m[2023-01-04 19:28:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][959/27342]	eta 5:14:13 lr 0.0009999996	 wd 0.1000	time 0.8227 (0.7146)	loss 1.2687 (1.2486)	loss-cls 10.1496 (9.9890)	loss-aux 0.0000 (0.0000)	grad_norm 0.7148 (nan)	loss_scale 32768.0000 (58948.2667)	mem 10930MB	batch_time 5.8059
[32m[2023-01-04 19:28:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][967/27342]	eta 5:14:10 lr 0.0009999996	 wd 0.1000	time 0.8348 (0.7147)	loss 1.2434 (1.2486)	loss-cls 9.9476 (9.9887)	loss-aux 0.0000 (0.0000)	grad_norm 0.8275 (nan)	loss_scale 32768.0000 (58731.9008)	mem 10930MB	batch_time 5.8193
[32m[2023-01-04 19:28:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][975/27342]	eta 5:14:07 lr 0.0009999996	 wd 0.1000	time 0.8154 (0.7148)	loss 1.2748 (1.2486)	loss-cls 10.1985 (9.9889)	loss-aux 0.0000 (0.0000)	grad_norm 0.9218 (nan)	loss_scale 32768.0000 (58519.0820)	mem 10930MB	batch_time 5.8322
[32m[2023-01-04 19:28:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][983/27342]	eta 5:14:06 lr 0.0009999996	 wd 0.1000	time 0.8463 (0.7150)	loss 1.2568 (1.2486)	loss-cls 10.0545 (9.9885)	loss-aux 0.0000 (0.0000)	grad_norm 3.2524 (nan)	loss_scale 32768.0000 (58309.7236)	mem 10930MB	batch_time 5.8933
[32m[2023-01-04 19:28:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][991/27342]	eta 5:14:04 lr 0.0009999996	 wd 0.1000	time 0.7458 (0.7151)	loss 1.2229 (1.2485)	loss-cls 9.7831 (9.9878)	loss-aux 0.0000 (0.0000)	grad_norm nan (nan)	loss_scale 16384.0000 (58087.2258)	mem 10930MB	batch_time 5.8447
[32m[2023-01-04 19:28:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][999/27342]	eta 5:14:07 lr 0.0009999996	 wd 0.1000	time 0.8408 (0.7154)	loss 1.2386 (1.2484)	loss-cls 9.9085 (9.9874)	loss-aux 0.0000 (0.0000)	grad_norm 10.3683 (nan)	loss_scale 16384.0000 (57753.6000)	mem 10930MB	batch_time 6.0520
[32m[2023-01-04 19:28:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1007/27342]	eta 5:14:06 lr 0.0009999996	 wd 0.1000	time 0.8179 (0.7157)	loss 1.2731 (1.2484)	loss-cls 10.1850 (9.9872)	loss-aux 0.0000 (0.0000)	grad_norm 0.5071 (nan)	loss_scale 16384.0000 (57425.2698)	mem 10930MB	batch_time 5.9286
[32m[2023-01-04 19:29:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1015/27342]	eta 5:14:06 lr 0.0009999996	 wd 0.1000	time 0.8291 (0.7159)	loss 1.2350 (1.2483)	loss-cls 9.8796 (9.9868)	loss-aux 0.0000 (0.0000)	grad_norm 0.5969 (nan)	loss_scale 16384.0000 (57102.1102)	mem 10930MB	batch_time 5.9530
[32m[2023-01-04 19:29:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1023/27342]	eta 5:14:06 lr 0.0009999996	 wd 0.1000	time 0.8393 (0.7161)	loss 1.2572 (1.2484)	loss-cls 10.0580 (9.9869)	loss-aux 0.0000 (0.0000)	grad_norm 0.7926 (nan)	loss_scale 16384.0000 (56784.0000)	mem 10930MB	batch_time 5.9534
[32m[2023-01-04 19:29:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1031/27342]	eta 5:14:07 lr 0.0009999996	 wd 0.1000	time 0.8353 (0.7163)	loss 1.2395 (1.2483)	loss-cls 9.9162 (9.9868)	loss-aux 0.0000 (0.0000)	grad_norm 0.4715 (nan)	loss_scale 16384.0000 (56470.8217)	mem 10930MB	batch_time 5.9631
[32m[2023-01-04 19:29:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1039/27342]	eta 5:14:05 lr 0.0009999996	 wd 0.1000	time 0.8390 (0.7165)	loss 1.2175 (1.2482)	loss-cls 9.7396 (9.9859)	loss-aux 0.0000 (0.0000)	grad_norm 0.9636 (nan)	loss_scale 16384.0000 (56162.4615)	mem 10930MB	batch_time 5.8892
[32m[2023-01-04 19:29:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1047/27342]	eta 5:14:03 lr 0.0009999996	 wd 0.1000	time 0.8354 (0.7166)	loss 1.2480 (1.2482)	loss-cls 9.9841 (9.9853)	loss-aux 0.0000 (0.0000)	grad_norm 0.9475 (nan)	loss_scale 16384.0000 (55858.8092)	mem 10930MB	batch_time 5.8835
[32m[2023-01-04 19:29:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1055/27342]	eta 5:14:00 lr 0.0009999996	 wd 0.1000	time 0.7907 (0.7167)	loss 1.2407 (1.2482)	loss-cls 9.9258 (9.9856)	loss-aux 0.0000 (0.0000)	grad_norm 0.5054 (nan)	loss_scale 16384.0000 (55559.7576)	mem 10930MB	batch_time 5.8487
[32m[2023-01-04 19:29:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1063/27342]	eta 5:13:58 lr 0.0009999995	 wd 0.1000	time 0.8098 (0.7168)	loss 1.2443 (1.2481)	loss-cls 9.9542 (9.9849)	loss-aux 0.0000 (0.0000)	grad_norm 0.3300 (nan)	loss_scale 16384.0000 (55265.2030)	mem 10930MB	batch_time 5.8561
[32m[2023-01-04 19:29:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1071/27342]	eta 5:13:54 lr 0.0009999995	 wd 0.1000	time 0.8194 (0.7169)	loss 1.2348 (1.2480)	loss-cls 9.8785 (9.9843)	loss-aux 0.0000 (0.0000)	grad_norm 0.3804 (nan)	loss_scale 16384.0000 (54975.0448)	mem 10930MB	batch_time 5.8062
[32m[2023-01-04 19:29:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1079/27342]	eta 5:13:52 lr 0.0009999995	 wd 0.1000	time 0.8402 (0.7171)	loss 1.2306 (1.2480)	loss-cls 9.8451 (9.9840)	loss-aux 0.0000 (0.0000)	grad_norm 0.8569 (nan)	loss_scale 16384.0000 (54689.1852)	mem 10930MB	batch_time 5.9018
[32m[2023-01-04 19:29:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1087/27342]	eta 5:13:50 lr 0.0009999995	 wd 0.1000	time 0.8101 (0.7172)	loss 1.2455 (1.2480)	loss-cls 9.9638 (9.9838)	loss-aux 0.0000 (0.0000)	grad_norm 0.6399 (nan)	loss_scale 16384.0000 (54407.5294)	mem 10930MB	batch_time 5.9082
[32m[2023-01-04 19:30:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1095/27342]	eta 5:13:49 lr 0.0009999995	 wd 0.1000	time 0.8235 (0.7174)	loss 1.2484 (1.2479)	loss-cls 9.9872 (9.9833)	loss-aux 0.0000 (0.0000)	grad_norm 0.5269 (nan)	loss_scale 16384.0000 (54129.9854)	mem 10930MB	batch_time 5.9099
[32m[2023-01-04 19:30:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1103/27342]	eta 5:13:47 lr 0.0009999995	 wd 0.1000	time 0.8083 (0.7175)	loss 1.2293 (1.2478)	loss-cls 9.8345 (9.9821)	loss-aux 0.0000 (0.0000)	grad_norm 0.4899 (nan)	loss_scale 16384.0000 (53856.4638)	mem 10930MB	batch_time 5.9171
[32m[2023-01-04 19:30:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1111/27342]	eta 5:13:47 lr 0.0009999995	 wd 0.1000	time 0.8351 (0.7177)	loss 1.2130 (1.2477)	loss-cls 9.7037 (9.9814)	loss-aux 0.0000 (0.0000)	grad_norm 2.6785 (nan)	loss_scale 16384.0000 (53586.8777)	mem 10930MB	batch_time 5.9680
[32m[2023-01-04 19:30:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1119/27342]	eta 5:13:46 lr 0.0009999995	 wd 0.1000	time 0.8328 (0.7179)	loss 1.2216 (1.2476)	loss-cls 9.7730 (9.9804)	loss-aux 0.0000 (0.0000)	grad_norm 0.5645 (nan)	loss_scale 16384.0000 (53321.1429)	mem 10930MB	batch_time 5.9506
[32m[2023-01-04 19:30:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1127/27342]	eta 5:13:45 lr 0.0009999995	 wd 0.1000	time 0.8515 (0.7181)	loss 1.2333 (1.2475)	loss-cls 9.8661 (9.9801)	loss-aux 0.0000 (0.0000)	grad_norm 0.4591 (nan)	loss_scale 16384.0000 (53059.1773)	mem 10930MB	batch_time 5.9644
[32m[2023-01-04 19:30:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1135/27342]	eta 5:13:44 lr 0.0009999995	 wd 0.1000	time 0.8347 (0.7183)	loss 1.2215 (1.2475)	loss-cls 9.7724 (9.9798)	loss-aux 0.0000 (0.0000)	grad_norm 0.7223 (nan)	loss_scale 16384.0000 (52800.9014)	mem 10930MB	batch_time 5.9504
[32m[2023-01-04 19:30:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1143/27342]	eta 5:13:45 lr 0.0009999995	 wd 0.1000	time 0.8402 (0.7185)	loss 1.2722 (1.2474)	loss-cls 10.1778 (9.9793)	loss-aux 0.0000 (0.0000)	grad_norm 0.4322 (nan)	loss_scale 16384.0000 (52546.2378)	mem 10930MB	batch_time 6.0187
[32m[2023-01-04 19:30:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1151/27342]	eta 5:13:45 lr 0.0009999995	 wd 0.1000	time 0.8336 (0.7188)	loss 1.2484 (1.2473)	loss-cls 9.9869 (9.9787)	loss-aux 0.0000 (0.0000)	grad_norm 0.4046 (nan)	loss_scale 16384.0000 (52295.1111)	mem 10930MB	batch_time 5.9969
[32m[2023-01-04 19:30:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1159/27342]	eta 5:13:45 lr 0.0009999995	 wd 0.1000	time 0.8599 (0.7190)	loss 1.2375 (1.2473)	loss-cls 9.9002 (9.9782)	loss-aux 0.0000 (0.0000)	grad_norm 0.3144 (nan)	loss_scale 16384.0000 (52047.4483)	mem 10930MB	batch_time 5.9988
[32m[2023-01-04 19:30:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1167/27342]	eta 5:13:45 lr 0.0009999995	 wd 0.1000	time 0.8494 (0.7192)	loss 1.2417 (1.2472)	loss-cls 9.9333 (9.9777)	loss-aux 0.0000 (0.0000)	grad_norm 0.4357 (nan)	loss_scale 16384.0000 (51803.1781)	mem 10930MB	batch_time 6.0398
[32m[2023-01-04 19:31:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1175/27342]	eta 5:13:48 lr 0.0009999994	 wd 0.1000	time 0.8711 (0.7195)	loss 1.2403 (1.2472)	loss-cls 9.9224 (9.9773)	loss-aux 0.0000 (0.0000)	grad_norm 0.9124 (nan)	loss_scale 16384.0000 (51562.2313)	mem 10930MB	batch_time 6.1198
[32m[2023-01-04 19:31:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1183/27342]	eta 5:13:48 lr 0.0009999994	 wd 0.1000	time 0.8495 (0.7198)	loss 1.2283 (1.2471)	loss-cls 9.8261 (9.9766)	loss-aux 0.0000 (0.0000)	grad_norm 0.4835 (nan)	loss_scale 16384.0000 (51324.5405)	mem 10930MB	batch_time 6.0511
[32m[2023-01-04 19:31:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1191/27342]	eta 5:13:48 lr 0.0009999994	 wd 0.1000	time 0.8532 (0.7200)	loss 1.2349 (1.2470)	loss-cls 9.8793 (9.9759)	loss-aux 0.0000 (0.0000)	grad_norm 4.2367 (nan)	loss_scale 16384.0000 (51090.0403)	mem 10930MB	batch_time 6.0198
[32m[2023-01-04 19:31:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1199/27342]	eta 5:13:48 lr 0.0009999994	 wd 0.1000	time 0.8349 (0.7202)	loss 1.2375 (1.2469)	loss-cls 9.9002 (9.9755)	loss-aux 0.0000 (0.0000)	grad_norm 0.2916 (nan)	loss_scale 16384.0000 (50858.6667)	mem 10930MB	batch_time 6.0033
[32m[2023-01-04 19:31:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1207/27342]	eta 5:13:48 lr 0.0009999994	 wd 0.1000	time 0.8600 (0.7204)	loss 1.2448 (1.2469)	loss-cls 9.9585 (9.9751)	loss-aux 0.0000 (0.0000)	grad_norm 0.4766 (nan)	loss_scale 16384.0000 (50630.3576)	mem 10930MB	batch_time 6.0422
[32m[2023-01-04 19:31:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1215/27342]	eta 5:13:46 lr 0.0009999994	 wd 0.1000	time 0.8342 (0.7206)	loss 1.2328 (1.2468)	loss-cls 9.8627 (9.9744)	loss-aux 0.0000 (0.0000)	grad_norm 3.2088 (nan)	loss_scale 16384.0000 (50405.0526)	mem 10930MB	batch_time 5.9274
[32m[2023-01-04 19:31:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1223/27342]	eta 5:13:44 lr 0.0009999994	 wd 0.1000	time 0.8312 (0.7207)	loss 1.2537 (1.2468)	loss-cls 10.0296 (9.9743)	loss-aux 0.0000 (0.0000)	grad_norm 0.4189 (nan)	loss_scale 16384.0000 (50182.6928)	mem 10930MB	batch_time 5.9224
[32m[2023-01-04 19:31:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1231/27342]	eta 5:13:41 lr 0.0009999994	 wd 0.1000	time 0.8424 (0.7208)	loss 1.2404 (1.2468)	loss-cls 9.9233 (9.9741)	loss-aux 0.0000 (0.0000)	grad_norm 0.4873 (nan)	loss_scale 16384.0000 (49963.2208)	mem 10930MB	batch_time 5.9271
[32m[2023-01-04 19:31:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1239/27342]	eta 5:13:37 lr 0.0009999994	 wd 0.1000	time 0.8128 (0.7209)	loss 1.2446 (1.2467)	loss-cls 9.9569 (9.9740)	loss-aux 0.0000 (0.0000)	grad_norm 0.4271 (nan)	loss_scale 16384.0000 (49746.5806)	mem 10930MB	batch_time 5.8537
[32m[2023-01-04 19:31:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1247/27342]	eta 5:13:36 lr 0.0009999994	 wd 0.1000	time 0.8424 (0.7211)	loss 1.2364 (1.2467)	loss-cls 9.8912 (9.9738)	loss-aux 0.0000 (0.0000)	grad_norm 0.4329 (nan)	loss_scale 16384.0000 (49532.7179)	mem 10930MB	batch_time 5.9580
[32m[2023-01-04 19:32:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1255/27342]	eta 5:13:32 lr 0.0009999994	 wd 0.1000	time 0.8308 (0.7211)	loss 1.2467 (1.2467)	loss-cls 9.9737 (9.9734)	loss-aux 0.0000 (0.0000)	grad_norm 15.8658 (nan)	loss_scale 16384.0000 (49321.5796)	mem 10930MB	batch_time 5.8622
[32m[2023-01-04 19:32:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1263/27342]	eta 5:13:28 lr 0.0009999994	 wd 0.1000	time 0.8307 (0.7212)	loss 1.2486 (1.2467)	loss-cls 9.9892 (9.9735)	loss-aux 0.0000 (0.0000)	grad_norm 0.3817 (nan)	loss_scale 16384.0000 (49113.1139)	mem 10930MB	batch_time 5.8785
[32m[2023-01-04 19:32:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1271/27342]	eta 5:13:26 lr 0.0009999994	 wd 0.1000	time 0.8411 (0.7214)	loss 1.2499 (1.2467)	loss-cls 9.9988 (9.9734)	loss-aux 0.0000 (0.0000)	grad_norm 0.3632 (nan)	loss_scale 16384.0000 (48907.2704)	mem 10930MB	batch_time 5.9420
[32m[2023-01-04 19:32:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1279/27342]	eta 5:13:22 lr 0.0009999993	 wd 0.1000	time 0.8226 (0.7214)	loss 1.2499 (1.2467)	loss-cls 9.9994 (9.9733)	loss-aux 0.0000 (0.0000)	grad_norm 0.3181 (nan)	loss_scale 16384.0000 (48704.0000)	mem 10930MB	batch_time 5.8837
[32m[2023-01-04 19:32:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1287/27342]	eta 5:13:19 lr 0.0009999993	 wd 0.1000	time 0.8504 (0.7215)	loss 1.2671 (1.2467)	loss-cls 10.1368 (9.9732)	loss-aux 0.0000 (0.0000)	grad_norm 0.4415 (nan)	loss_scale 16384.0000 (48503.2547)	mem 10930MB	batch_time 5.8971
[32m[2023-01-04 19:32:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1295/27342]	eta 5:13:16 lr 0.0009999993	 wd 0.1000	time 0.8521 (0.7216)	loss 1.2340 (1.2466)	loss-cls 9.8723 (9.9728)	loss-aux 0.0000 (0.0000)	grad_norm 0.3529 (nan)	loss_scale 16384.0000 (48304.9877)	mem 10930MB	batch_time 5.9030
[32m[2023-01-04 19:32:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1303/27342]	eta 5:13:13 lr 0.0009999993	 wd 0.1000	time 0.8327 (0.7218)	loss 1.2454 (1.2465)	loss-cls 9.9628 (9.9723)	loss-aux 0.0000 (0.0000)	grad_norm 0.3953 (nan)	loss_scale 16384.0000 (48109.1534)	mem 10930MB	batch_time 5.9221
[32m[2023-01-04 19:32:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1311/27342]	eta 5:13:09 lr 0.0009999993	 wd 0.1000	time 0.8275 (0.7218)	loss 1.1926 (1.2465)	loss-cls 9.5408 (9.9719)	loss-aux 0.0000 (0.0000)	grad_norm 0.4147 (nan)	loss_scale 16384.0000 (47915.7073)	mem 10930MB	batch_time 5.8735
[32m[2023-01-04 19:32:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1319/27342]	eta 5:13:06 lr 0.0009999993	 wd 0.1000	time 0.8308 (0.7219)	loss 1.2207 (1.2464)	loss-cls 9.7656 (9.9713)	loss-aux 0.0000 (0.0000)	grad_norm 0.3810 (nan)	loss_scale 16384.0000 (47724.6061)	mem 10930MB	batch_time 5.9021
[32m[2023-01-04 19:32:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1327/27342]	eta 5:13:04 lr 0.0009999993	 wd 0.1000	time 0.8392 (0.7221)	loss 1.2455 (1.2463)	loss-cls 9.9642 (9.9707)	loss-aux 0.0000 (0.0000)	grad_norm 0.2925 (nan)	loss_scale 16384.0000 (47535.8072)	mem 10930MB	batch_time 5.9713
[32m[2023-01-04 19:33:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1335/27342]	eta 5:13:03 lr 0.0009999993	 wd 0.1000	time 0.8385 (0.7222)	loss 1.2448 (1.2463)	loss-cls 9.9580 (9.9703)	loss-aux 0.0000 (0.0000)	grad_norm 0.2928 (nan)	loss_scale 16384.0000 (47349.2695)	mem 10930MB	batch_time 6.0047
[32m[2023-01-04 19:33:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1343/27342]	eta 5:13:01 lr 0.0009999993	 wd 0.1000	time 0.8544 (0.7224)	loss 1.2546 (1.2463)	loss-cls 10.0371 (9.9704)	loss-aux 0.0000 (0.0000)	grad_norm 0.4066 (nan)	loss_scale 16384.0000 (47164.9524)	mem 10930MB	batch_time 5.9665
[32m[2023-01-04 19:33:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1351/27342]	eta 5:12:59 lr 0.0009999993	 wd 0.1000	time 0.8431 (0.7225)	loss 1.2424 (1.2463)	loss-cls 9.9390 (9.9702)	loss-aux 0.0000 (0.0000)	grad_norm 0.5217 (nan)	loss_scale 16384.0000 (46982.8166)	mem 10930MB	batch_time 5.9745
[32m[2023-01-04 19:33:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1359/27342]	eta 5:12:57 lr 0.0009999993	 wd 0.1000	time 0.8354 (0.7227)	loss 1.2482 (1.2462)	loss-cls 9.9855 (9.9698)	loss-aux 0.0000 (0.0000)	grad_norm 0.3960 (nan)	loss_scale 16384.0000 (46802.8235)	mem 10930MB	batch_time 5.9896
[32m[2023-01-04 19:33:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1367/27342]	eta 5:12:55 lr 0.0009999993	 wd 0.1000	time 0.8390 (0.7228)	loss 1.2326 (1.2462)	loss-cls 9.8611 (9.9694)	loss-aux 0.0000 (0.0000)	grad_norm 0.4166 (nan)	loss_scale 16384.0000 (46624.9357)	mem 10930MB	batch_time 5.9668
[32m[2023-01-04 19:33:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1375/27342]	eta 5:12:51 lr 0.0009999992	 wd 0.1000	time 0.8260 (0.7229)	loss 1.2428 (1.2462)	loss-cls 9.9428 (9.9692)	loss-aux 0.0000 (0.0000)	grad_norm 0.3804 (nan)	loss_scale 16384.0000 (46449.1163)	mem 10930MB	batch_time 5.9037
[32m[2023-01-04 19:33:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1383/27342]	eta 5:12:48 lr 0.0009999992	 wd 0.1000	time 0.8470 (0.7230)	loss 1.2448 (1.2461)	loss-cls 9.9585 (9.9685)	loss-aux 0.0000 (0.0000)	grad_norm 0.4495 (nan)	loss_scale 16384.0000 (46275.3295)	mem 10930MB	batch_time 5.9245
[32m[2023-01-04 19:33:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1391/27342]	eta 5:12:45 lr 0.0009999992	 wd 0.1000	time 0.8317 (0.7231)	loss 1.2319 (1.2460)	loss-cls 9.8551 (9.9680)	loss-aux 0.0000 (0.0000)	grad_norm 0.4733 (nan)	loss_scale 16384.0000 (46103.5402)	mem 10930MB	batch_time 5.9327
[32m[2023-01-04 19:33:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1399/27342]	eta 5:12:43 lr 0.0009999992	 wd 0.1000	time 0.8363 (0.7233)	loss 1.2355 (1.2459)	loss-cls 9.8837 (9.9674)	loss-aux 0.0000 (0.0000)	grad_norm 0.4380 (nan)	loss_scale 16384.0000 (45933.7143)	mem 10930MB	batch_time 5.9799
[32m[2023-01-04 19:33:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1407/27342]	eta 5:12:40 lr 0.0009999992	 wd 0.1000	time 0.8446 (0.7234)	loss 1.2076 (1.2458)	loss-cls 9.6609 (9.9664)	loss-aux 0.0000 (0.0000)	grad_norm 0.4195 (nan)	loss_scale 16384.0000 (45765.8182)	mem 10930MB	batch_time 5.9662
[32m[2023-01-04 19:34:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1415/27342]	eta 5:12:39 lr 0.0009999992	 wd 0.1000	time 0.8398 (0.7235)	loss 1.2593 (1.2458)	loss-cls 10.0745 (9.9662)	loss-aux 0.0000 (0.0000)	grad_norm 0.4723 (nan)	loss_scale 16384.0000 (45599.8192)	mem 10930MB	batch_time 6.0138
[32m[2023-01-04 19:34:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1423/27342]	eta 5:12:37 lr 0.0009999992	 wd 0.1000	time 0.8583 (0.7237)	loss 1.2202 (1.2457)	loss-cls 9.7615 (9.9659)	loss-aux 0.0000 (0.0000)	grad_norm 0.3520 (nan)	loss_scale 16384.0000 (45435.6854)	mem 10930MB	batch_time 6.0078
[32m[2023-01-04 19:34:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1431/27342]	eta 5:12:34 lr 0.0009999992	 wd 0.1000	time 0.8391 (0.7238)	loss 1.2470 (1.2457)	loss-cls 9.9764 (9.9655)	loss-aux 0.0000 (0.0000)	grad_norm 0.2963 (nan)	loss_scale 16384.0000 (45273.3855)	mem 10930MB	batch_time 5.9234
[32m[2023-01-04 19:34:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1439/27342]	eta 5:12:31 lr 0.0009999992	 wd 0.1000	time 0.8314 (0.7239)	loss 1.2621 (1.2457)	loss-cls 10.0965 (9.9656)	loss-aux 0.0000 (0.0000)	grad_norm 0.2846 (nan)	loss_scale 16384.0000 (45112.8889)	mem 10930MB	batch_time 5.9577
[32m[2023-01-04 19:34:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1447/27342]	eta 5:12:28 lr 0.0009999992	 wd 0.1000	time 0.8582 (0.7240)	loss 1.2086 (1.2456)	loss-cls 9.6690 (9.9649)	loss-aux 0.0000 (0.0000)	grad_norm 0.4123 (nan)	loss_scale 16384.0000 (44954.1657)	mem 10930MB	batch_time 5.9269
[32m[2023-01-04 19:34:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1455/27342]	eta 5:12:24 lr 0.0009999992	 wd 0.1000	time 0.8516 (0.7241)	loss 1.2367 (1.2456)	loss-cls 9.8939 (9.9645)	loss-aux 0.0000 (0.0000)	grad_norm 0.3147 (nan)	loss_scale 16384.0000 (44797.1868)	mem 10930MB	batch_time 5.9306
[32m[2023-01-04 19:34:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1463/27342]	eta 5:12:21 lr 0.0009999991	 wd 0.1000	time 0.8477 (0.7242)	loss 1.2157 (1.2455)	loss-cls 9.7255 (9.9642)	loss-aux 0.0000 (0.0000)	grad_norm 0.3297 (nan)	loss_scale 16384.0000 (44641.9235)	mem 10930MB	batch_time 5.9445
[32m[2023-01-04 19:34:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1471/27342]	eta 5:12:18 lr 0.0009999991	 wd 0.1000	time 0.8511 (0.7243)	loss 1.2329 (1.2455)	loss-cls 9.8634 (9.9642)	loss-aux 0.0000 (0.0000)	grad_norm 0.3851 (nan)	loss_scale 16384.0000 (44488.3478)	mem 10930MB	batch_time 5.9739
[32m[2023-01-04 19:34:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1479/27342]	eta 5:12:16 lr 0.0009999991	 wd 0.1000	time 0.8399 (0.7244)	loss 1.2298 (1.2455)	loss-cls 9.8386 (9.9639)	loss-aux 0.0000 (0.0000)	grad_norm 0.4091 (nan)	loss_scale 16384.0000 (44336.4324)	mem 10930MB	batch_time 5.9708
[32m[2023-01-04 19:34:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1487/27342]	eta 5:12:13 lr 0.0009999991	 wd 0.1000	time 0.8386 (0.7246)	loss 1.2613 (1.2455)	loss-cls 10.0906 (9.9638)	loss-aux 0.0000 (0.0000)	grad_norm 0.4438 (nan)	loss_scale 16384.0000 (44186.1505)	mem 10930MB	batch_time 5.9594
[32m[2023-01-04 19:35:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1495/27342]	eta 5:12:12 lr 0.0009999991	 wd 0.1000	time 0.8318 (0.7247)	loss 1.2443 (1.2454)	loss-cls 9.9540 (9.9635)	loss-aux 0.0000 (0.0000)	grad_norm 0.2386 (nan)	loss_scale 16384.0000 (44037.4759)	mem 10930MB	batch_time 6.0892
[32m[2023-01-04 19:35:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1503/27342]	eta 5:12:11 lr 0.0009999991	 wd 0.1000	time 0.8624 (0.7249)	loss 1.2352 (1.2454)	loss-cls 9.8814 (9.9632)	loss-aux 0.0000 (0.0000)	grad_norm 0.3552 (nan)	loss_scale 16384.0000 (43890.3830)	mem 10930MB	batch_time 6.0464
[32m[2023-01-04 19:35:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1511/27342]	eta 5:12:08 lr 0.0009999991	 wd 0.1000	time 0.8353 (0.7250)	loss 1.2374 (1.2453)	loss-cls 9.8991 (9.9626)	loss-aux 0.0000 (0.0000)	grad_norm 0.3756 (nan)	loss_scale 16384.0000 (43744.8466)	mem 10930MB	batch_time 5.9880
[32m[2023-01-04 19:35:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1519/27342]	eta 5:12:05 lr 0.0009999991	 wd 0.1000	time 0.8252 (0.7251)	loss 1.2529 (1.2453)	loss-cls 10.0230 (9.9624)	loss-aux 0.0000 (0.0000)	grad_norm 0.4092 (nan)	loss_scale 16384.0000 (43600.8421)	mem 10930MB	batch_time 5.9631
[32m[2023-01-04 19:35:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1527/27342]	eta 5:12:02 lr 0.0009999991	 wd 0.1000	time 0.8334 (0.7253)	loss 1.2295 (1.2453)	loss-cls 9.8363 (9.9622)	loss-aux 0.0000 (0.0000)	grad_norm 0.3775 (nan)	loss_scale 16384.0000 (43458.3455)	mem 10930MB	batch_time 5.9676
[32m[2023-01-04 19:35:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1535/27342]	eta 5:11:59 lr 0.0009999991	 wd 0.1000	time 0.8320 (0.7254)	loss 1.2369 (1.2452)	loss-cls 9.8954 (9.9618)	loss-aux 0.0000 (0.0000)	grad_norm 0.3244 (nan)	loss_scale 16384.0000 (43317.3333)	mem 10930MB	batch_time 5.9850
[32m[2023-01-04 19:35:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1543/27342]	eta 5:11:57 lr 0.0009999990	 wd 0.1000	time 0.8598 (0.7255)	loss 1.2168 (1.2452)	loss-cls 9.7344 (9.9616)	loss-aux 0.0000 (0.0000)	grad_norm 0.4789 (nan)	loss_scale 16384.0000 (43177.7824)	mem 10930MB	batch_time 6.0127
[32m[2023-01-04 19:35:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1551/27342]	eta 5:11:54 lr 0.0009999990	 wd 0.1000	time 0.8457 (0.7256)	loss 1.2324 (1.2451)	loss-cls 9.8595 (9.9610)	loss-aux 0.0000 (0.0000)	grad_norm 0.4448 (nan)	loss_scale 16384.0000 (43039.6701)	mem 10930MB	batch_time 5.9586
[32m[2023-01-04 19:35:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1559/27342]	eta 5:11:48 lr 0.0009999990	 wd 0.1000	time 0.8413 (0.7256)	loss 1.2313 (1.2451)	loss-cls 9.8506 (9.9605)	loss-aux 0.0000 (0.0000)	grad_norm 0.3357 (nan)	loss_scale 16384.0000 (42902.9744)	mem 10930MB	batch_time 5.8414
[32m[2023-01-04 19:35:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1567/27342]	eta 5:11:43 lr 0.0009999990	 wd 0.1000	time 0.8257 (0.7256)	loss 1.2423 (1.2450)	loss-cls 9.9384 (9.9600)	loss-aux 0.0000 (0.0000)	grad_norm 0.3171 (nan)	loss_scale 16384.0000 (42767.6735)	mem 10930MB	batch_time 5.8308
[32m[2023-01-04 19:36:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1575/27342]	eta 5:11:38 lr 0.0009999990	 wd 0.1000	time 0.8262 (0.7257)	loss 1.2325 (1.2450)	loss-cls 9.8598 (9.9602)	loss-aux 0.0000 (0.0000)	grad_norm 0.4192 (nan)	loss_scale 16384.0000 (42633.7462)	mem 10930MB	batch_time 5.8573
[32m[2023-01-04 19:36:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1583/27342]	eta 5:11:34 lr 0.0009999990	 wd 0.1000	time 0.8311 (0.7257)	loss 1.2282 (1.2449)	loss-cls 9.8260 (9.9595)	loss-aux 0.0000 (0.0000)	grad_norm 1.0021 (nan)	loss_scale 16384.0000 (42501.1717)	mem 10930MB	batch_time 5.8924
[32m[2023-01-04 19:36:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1591/27342]	eta 5:11:28 lr 0.0009999990	 wd 0.1000	time 0.8208 (0.7257)	loss 1.2122 (1.2449)	loss-cls 9.6972 (9.9590)	loss-aux 0.0000 (0.0000)	grad_norm 0.2995 (nan)	loss_scale 16384.0000 (42369.9296)	mem 10930MB	batch_time 5.8147
[32m[2023-01-04 19:36:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1599/27342]	eta 5:11:24 lr 0.0009999990	 wd 0.1000	time 0.8705 (0.7258)	loss 1.2301 (1.2448)	loss-cls 9.8407 (9.9587)	loss-aux 0.0000 (0.0000)	grad_norm 0.2740 (nan)	loss_scale 16384.0000 (42240.0000)	mem 10930MB	batch_time 5.9120
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13641.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19281.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27412.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29025.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_16430.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_7026.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13396.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20118.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6669.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29695.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6236.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9041.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15811.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24638.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_23316.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7411.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2852.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12757.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9282.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15455.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12064.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7013.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12740.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5051.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12206.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13871.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10175248/n10175248_583.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12108.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13516.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25408.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28595.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22698.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25717.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03957420/n03957420_33553.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10675.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17471.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_9566.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n06470073/n06470073_47249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12231.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_15341.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8925.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22980.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_4539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11827.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8812.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5664.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22719.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7365.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11816.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_26924.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13244.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33259.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27578.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15697.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9068.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8783.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4456.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25750.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17877.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18350.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28288.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27317.JPEG
[32m[2023-01-04 19:36:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 282)[0m: INFO EPOCH 0 training takes 0:19:22
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15830.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8873.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4524.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_32625.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27221.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19005.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22581.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_11746.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n02368116/n02368116_318.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18729.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27611.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9401.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10586.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_1796.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6247.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6710.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33630.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_16737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11766.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30043.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13306.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22995.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22166.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10242.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27627.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14718.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24544.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_10353.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24434.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12364.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13261.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8806.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_1914.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7953.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_21756.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7728.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_34297.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7996.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7072.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6567.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_16320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9215.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25530.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9819.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_14701.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20180.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6850.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9031.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27296.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30926.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8645.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_135.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13103.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2040.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2322.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14020.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33623.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_3493.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15480.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13419.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12142.JPEG
