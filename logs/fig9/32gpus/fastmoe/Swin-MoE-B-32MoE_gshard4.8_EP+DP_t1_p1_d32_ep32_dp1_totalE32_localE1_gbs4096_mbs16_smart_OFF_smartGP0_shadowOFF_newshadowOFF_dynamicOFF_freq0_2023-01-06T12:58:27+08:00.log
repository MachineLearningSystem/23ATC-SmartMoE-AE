+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ grep BatchHost
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ tr = ' '
++ tr = ' '
++ grep BatchHost
++ grep BatchHost
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=102381
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ grep BatchHost
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=102381
++ grep BatchHost
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ tr = ' '
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ awk '{print $2}'
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ scontrol show JobId=102381
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102381
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=102381
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
++ awk '{print $2}'
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
++ awk '{print $2}'
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ MASTER_ADDR=nico1
+ localrank=7
+ export RANK=6
+ RANK=6
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ MAX_JOBS=64
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=32
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ WORLD_SIZE=32
+ localrank=4
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args=
+ python_args=
+ python_args+='
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ python_args=
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ false
+ true
+ EXEC=./main_moe.py
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ false
+ true
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export RANK=8
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ RANK=8
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ CUDA_VISIBLE_DEVICES=6
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export RANK=15
+ RANK=15
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ false
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ true
+ EXEC=./main_moe.py
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ NODE_RANK=2
+ export MAX_JOBS=64
+ false
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ MAX_JOBS=64
+ USE_MEGATRON=0
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=2
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NNODES=4
+ NNODES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=13
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ RANK=13
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ export RANK=9
+ RANK=9
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ export RANK=12
+ RANK=12
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ cd /home/zms/model_training/MoE/FastSwin
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export CUDA_VISIBLE_DEVICES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ CUDA_VISIBLE_DEVICES=4
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NNODES=4
+ NNODES=4
+ true
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ export NODE_RANK=3
+ NODE_RANK=3
+ cd /home/zms/model_training/MoE/FastSwin
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ export RANK=21
+ RANK=21
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ localrank=5
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export NNODES=4
+ NNODES=4
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export NODE_RANK=5
+ NODE_RANK=5
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ export MAX_JOBS=64
+ MAX_JOBS=64
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' nico == nico ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=17
+ RANK=17
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ export RANK=19
+ RANK=19
+ export WORLD_SIZE=32
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ python_args+='
+ CUDA_VISIBLE_DEVICES=3
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ '[' OFF == ON ']'
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ false
+ true
+ EXEC=./main_moe.py
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ python_args+='
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=31
+ RANK=31
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=29
+ RANK=29
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ export RANK=28
+ RANK=28
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export RANK=30
+ RANK=30
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ cd /home/zms/model_training/MoE/FastSwin
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ export NODE_RANK=7
+ NODE_RANK=7
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ '[' OFF == ON ']'
+ python_args=
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ false
+ true
+ EXEC=./main_moe.py
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ export RANK=16
+ RANK=16
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ '[' OFF == ON ']'
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' OFF == ON ']'
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ true
+ EXEC=./main_moe.py
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=18
+ RANK=18
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=24
+ RANK=24
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args+='
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=27
+ RANK=27
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=23
+ RANK=23
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=22
+ RANK=22
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=26
+ RANK=26
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=20
+ RANK=20
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102381
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=25
+ RANK=25
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-06T12:58:27+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8 --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 0/32
[32m[2023-01-06 12:58:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 403)[0m: INFO Full config saved to /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default/config.json
[32m[2023-01-06 12:58:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 406)[0m: INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: part
  DATASET: imagenet22K
  DATA_PATH: /mnt/znvme/dataset/imagenet22k
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 4.8
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    GATE_NOISE: 1.0
    INIT_STD: 0.005
    IN_CHANS: 3
    IS_GSHARD_LOSS: true
    MLP_FC2_BIAS: false
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - 1
      - 3
      - 5
      - 7
      - 9
      - 11
      - 13
      - 15
      - 17
    - - 1
    MOE_DROP: 0.1
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 2
    USE_BPR: true
    WINDOW_SIZE: 12
  TYPE: swin_fastmoe
OUTPUT: /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default
PRINT_FREQ: 1
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 8
  AUTO_RESUME: true
  BASE_LR: 0.001
  CHECKPOINT_MODE: full
  CLIP_GRAD: 3.0
  EPOCHS: 90
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.1

[32m[2023-01-06 12:58:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 407)[0m: INFO {"cfg": "configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml", "opts": null, "batch_size": 16, "data_path": "/mnt/znvme/dataset/imagenet22k", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": 8, "use_checkpoint": false, "checkpoint_mode": "full", "disable_amp": false, "amp_opt_level": null, "output": "/home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": true, "fused_layernorm": false, "optim": null, "num_experts": 1, "top_k": 2, "balance_strategy": "naive", "expert_parallel_strategy": "EP+DP", "expert_ep_size": 32, "expert_dp_size": 1, "dump": false, "dynamic_placement": false, "dynamic_freq": 10, "new_shadow": false, "gshard_cap": 4.8, "init_method_std": 0.002, "num_layers": 12}
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
local rank 0 / global rank 0 successfully build train dataset
local rank 0 / global rank 0 successfully build val dataset
[32m[2023-01-06 12:59:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 120)[0m: INFO Creating model:swin_fastmoe/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 29/32
local rank 0 / global rank 29 successfully build train dataset
local rank 0 / global rank 29 successfully build val dataset
[INFO] 29 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 29 in DP group [29]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 17/32
local rank 0 / global rank 17 successfully build train dataset
local rank 0 / global rank 17 successfully build val dataset
[INFO] 17 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 17 in DP group [17]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 13/32
local rank 0 / global rank 13 successfully build train dataset
local rank 0 / global rank 13 successfully build val dataset
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 13 in DP group [13]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 5/32
local rank 0 / global rank 5 successfully build train dataset
local rank 0 / global rank 5 successfully build val dataset
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 5 in DP group [5]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 4/32
local rank 0 / global rank 4 successfully build train dataset
local rank 0 / global rank 4 successfully build val dataset
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 4 in DP group [4]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 0 in DP group [0]
[32m[2023-01-06 12:59:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 122)[0m: INFO SwinTransformerFastMoE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(48, 48), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(48, 48), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(24, 24), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(24, 24), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(12, 12), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(12, 12), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(6, 6), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=4096, out_features=1024, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 1024, hidden_features = 4096, num_local_experts = 1, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=4096, out_features=32, bias=True)
              )
            )
          )
        )
      )
    )
  )
  (norm): TimerModule(
    (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (avgpool): TimerModule(
    (model): AdaptiveAvgPool1d(output_size=1)
  )
  (head): TimerModule(
    (model): Linear(in_features=1024, out_features=21841, bias=True)
  )
)
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 31/32
local rank 0 / global rank 31 successfully build train dataset
local rank 0 / global rank 31 successfully build val dataset
[INFO] 31 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 31 in DP group [31]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 2/32
local rank 0 / global rank 2 successfully build train dataset
local rank 0 / global rank 2 successfully build val dataset
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 2 in DP group [2]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 30/32
local rank 0 / global rank 30 successfully build train dataset
local rank 0 / global rank 30 successfully build val dataset
[INFO] 30 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 30 in DP group [30]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 16/32
local rank 0 / global rank 16 successfully build train dataset
local rank 0 / global rank 16 successfully build val dataset
[INFO] 16 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 16 in DP group [16]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 19/32
local rank 0 / global rank 19 successfully build train dataset
local rank 0 / global rank 19 successfully build val dataset
[INFO] 19 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 19 in DP group [19]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 24/32
local rank 0 / global rank 24 successfully build train dataset
local rank 0 / global rank 24 successfully build val dataset
[INFO] 24 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 24 in DP group [24]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 3/32
local rank 0 / global rank 3 successfully build train dataset
local rank 0 / global rank 3 successfully build val dataset
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 3 in DP group [3]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 22/32
local rank 0 / global rank 22 successfully build train dataset
local rank 0 / global rank 22 successfully build val dataset
[INFO] 22 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 22 in DP group [22]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-06 12:59:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 156)[0m: INFO no checkpoint found in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default, ignoring auto resume
[32m[2023-01-06 12:59:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 176)[0m: INFO Start training
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 14/32
local rank 0 / global rank 14 successfully build train dataset
local rank 0 / global rank 14 successfully build val dataset
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 14 in DP group [14]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 7/32
local rank 0 / global rank 7 successfully build train dataset
local rank 0 / global rank 7 successfully build val dataset
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 7 in DP group [7]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 11/32
local rank 0 / global rank 11 successfully build train dataset
local rank 0 / global rank 11 successfully build val dataset
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 11 in DP group [11]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 28/32
local rank 0 / global rank 28 successfully build train dataset
local rank 0 / global rank 28 successfully build val dataset
[INFO] 28 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 28 in DP group [28]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 12/32
local rank 0 / global rank 12 successfully build train dataset
local rank 0 / global rank 12 successfully build val dataset
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 12 in DP group [12]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 10/32
local rank 0 / global rank 10 successfully build train dataset
local rank 0 / global rank 10 successfully build val dataset
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 10 in DP group [10]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 1/32
local rank 0 / global rank 1 successfully build train dataset
local rank 0 / global rank 1 successfully build val dataset
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 1 in DP group [1]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 8/32
local rank 0 / global rank 8 successfully build train dataset
local rank 0 / global rank 8 successfully build val dataset
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 8 in DP group [8]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 6/32
local rank 0 / global rank 6 successfully build train dataset
local rank 0 / global rank 6 successfully build val dataset
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 6 in DP group [6]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 27/32
local rank 0 / global rank 27 successfully build train dataset
local rank 0 / global rank 27 successfully build val dataset
[INFO] 27 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 27 in DP group [27]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 25/32
local rank 0 / global rank 25 successfully build train dataset
local rank 0 / global rank 25 successfully build val dataset
[INFO] 25 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 25 in DP group [25]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 26/32
local rank 0 / global rank 26 successfully build train dataset
local rank 0 / global rank 26 successfully build val dataset
[INFO] 26 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 26 in DP group [26]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 15/32
local rank 0 / global rank 15 successfully build train dataset
local rank 0 / global rank 15 successfully build val dataset
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 15 in DP group [15]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 18/32
local rank 0 / global rank 18 successfully build train dataset
local rank 0 / global rank 18 successfully build val dataset
[INFO] 18 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 18 in DP group [18]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 9/32
local rank 0 / global rank 9 successfully build train dataset
local rank 0 / global rank 9 successfully build val dataset
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 9 in DP group [9]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 21/32
local rank 0 / global rank 21 successfully build train dataset
local rank 0 / global rank 21 successfully build val dataset
[INFO] 21 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 21 in DP group [21]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 20/32
local rank 0 / global rank 20 successfully build train dataset
local rank 0 / global rank 20 successfully build val dataset
[INFO] 20 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 20 in DP group [20]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 23/32
local rank 0 / global rank 23 successfully build train dataset
local rank 0 / global rank 23 successfully build val dataset
[INFO] 23 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 23 in DP group [23]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/swin_gshard4.8/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-06 12:59:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][7/27342]	eta 20:09:40 lr 0.0010000000	 wd 0.1000	time 0.9898 (2.6552)	loss 1.2463 (1.2481)	loss-cls 9.9703 (9.9847)	loss-aux 0.0000 (0.0000)	grad_norm 3.0917 (3.0917)	loss_scale 65536.0000 (65536.0000)	mem 5198MB	batch_time 21.2416
[32m[2023-01-06 12:59:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][15/27342]	eta 13:08:06 lr 0.0010000000	 wd 0.1000	time 0.8571 (1.7304)	loss 1.2504 (1.2491)	loss-cls 10.0035 (9.9926)	loss-aux 0.0000 (0.0000)	grad_norm 2.2229 (2.6573)	loss_scale 65536.0000 (65536.0000)	mem 7523MB	batch_time 6.4445
[32m[2023-01-06 13:00:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][23/27342]	eta 10:47:07 lr 0.0010000000	 wd 0.1000	time 0.8658 (1.4213)	loss 1.2530 (1.2495)	loss-cls 10.0244 (9.9963)	loss-aux 0.0000 (0.0000)	grad_norm 2.0027 (2.4391)	loss_scale 65536.0000 (65536.0000)	mem 7523MB	batch_time 6.4242
[32m[2023-01-06 13:00:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][31/27342]	eta 9:36:55 lr 0.0010000000	 wd 0.1000	time 0.8566 (1.2675)	loss 1.2464 (1.2498)	loss-cls 9.9709 (9.9981)	loss-aux 0.0000 (0.0000)	grad_norm 1.8352 (2.2881)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4482
[32m[2023-01-06 13:00:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][39/27342]	eta 8:56:14 lr 0.0010000000	 wd 0.1000	time 0.9083 (1.1784)	loss 1.2535 (1.2500)	loss-cls 10.0279 (10.0003)	loss-aux 0.0000 (0.0000)	grad_norm 1.7739 (2.1853)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5784
[32m[2023-01-06 13:00:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][47/27342]	eta 8:28:51 lr 0.0010000000	 wd 0.1000	time 0.8801 (1.1186)	loss 1.2448 (1.2498)	loss-cls 9.9586 (9.9984)	loss-aux 0.0000 (0.0000)	grad_norm 1.9504 (2.1461)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5552
[32m[2023-01-06 13:00:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][55/27342]	eta 8:09:37 lr 0.0010000000	 wd 0.1000	time 0.9188 (1.0766)	loss 1.2543 (1.2503)	loss-cls 10.0348 (10.0023)	loss-aux 0.0000 (0.0000)	grad_norm 2.2643 (2.1630)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5981
[32m[2023-01-06 13:00:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][63/27342]	eta 7:54:02 lr 0.0010000000	 wd 0.1000	time 0.8792 (1.0426)	loss 1.2595 (1.2506)	loss-cls 10.0759 (10.0044)	loss-aux 0.0000 (0.0000)	grad_norm 2.1325 (2.1592)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4387
[32m[2023-01-06 13:00:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][71/27342]	eta 7:41:54 lr 0.0010000000	 wd 0.1000	time 0.8881 (1.0163)	loss 1.2610 (1.2508)	loss-cls 10.0881 (10.0061)	loss-aux 0.0000 (0.0000)	grad_norm 1.9936 (2.1408)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4417
[32m[2023-01-06 13:00:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][79/27342]	eta 7:31:51 lr 0.0010000000	 wd 0.1000	time 0.8698 (0.9945)	loss 1.2471 (1.2510)	loss-cls 9.9769 (10.0079)	loss-aux 0.0000 (0.0000)	grad_norm 1.7896 (2.1057)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3858
[32m[2023-01-06 13:00:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][87/27342]	eta 7:23:58 lr 0.0010000000	 wd 0.1000	time 0.8710 (0.9774)	loss 1.2521 (1.2511)	loss-cls 10.0169 (10.0088)	loss-aux 0.0000 (0.0000)	grad_norm 1.7204 (2.0706)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4524
[32m[2023-01-06 13:01:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][95/27342]	eta 7:17:21 lr 0.0010000000	 wd 0.1000	time 0.9050 (0.9631)	loss 1.2563 (1.2510)	loss-cls 10.0504 (10.0080)	loss-aux 0.0000 (0.0000)	grad_norm 1.7696 (2.0456)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4470
[32m[2023-01-06 13:01:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][103/27342]	eta 7:11:39 lr 0.0010000000	 wd 0.1000	time 0.8596 (0.9508)	loss 1.2532 (1.2509)	loss-cls 10.0258 (10.0069)	loss-aux 0.0000 (0.0000)	grad_norm 1.6259 (2.0133)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4300
[32m[2023-01-06 13:01:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][111/27342]	eta 7:07:04 lr 0.0010000000	 wd 0.1000	time 0.8727 (0.9410)	loss 1.2371 (1.2506)	loss-cls 9.8966 (10.0052)	loss-aux 0.0000 (0.0000)	grad_norm 1.6705 (1.9888)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5062
[32m[2023-01-06 13:01:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][119/27342]	eta 7:03:11 lr 0.0010000000	 wd 0.1000	time 0.8969 (0.9327)	loss 1.2489 (1.2509)	loss-cls 9.9912 (10.0069)	loss-aux 0.0000 (0.0000)	grad_norm 1.7258 (1.9713)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5365
[32m[2023-01-06 13:01:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][127/27342]	eta 6:59:28 lr 0.0010000000	 wd 0.1000	time 0.8806 (0.9248)	loss 1.2449 (1.2507)	loss-cls 9.9590 (10.0058)	loss-aux 0.0000 (0.0000)	grad_norm 1.7112 (1.9550)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4443
[32m[2023-01-06 13:01:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][135/27342]	eta 6:56:23 lr 0.0010000000	 wd 0.1000	time 0.8898 (0.9183)	loss 1.2457 (1.2508)	loss-cls 9.9655 (10.0061)	loss-aux 0.0000 (0.0000)	grad_norm 1.8121 (1.9466)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5107
[32m[2023-01-06 13:01:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][143/27342]	eta 6:53:38 lr 0.0010000000	 wd 0.1000	time 0.8838 (0.9125)	loss 1.2542 (1.2509)	loss-cls 10.0337 (10.0070)	loss-aux 0.0000 (0.0000)	grad_norm 1.8982 (1.9439)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5129
[32m[2023-01-06 13:01:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][151/27342]	eta 6:51:13 lr 0.0010000000	 wd 0.1000	time 0.8855 (0.9074)	loss 1.2508 (1.2509)	loss-cls 10.0062 (10.0075)	loss-aux 0.0000 (0.0000)	grad_norm 1.5606 (1.9237)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5297
[32m[2023-01-06 13:01:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][159/27342]	eta 6:48:47 lr 0.0010000000	 wd 0.1000	time 0.8730 (0.9023)	loss 1.2499 (1.2509)	loss-cls 9.9988 (10.0071)	loss-aux 0.0000 (0.0000)	grad_norm 1.6100 (1.9081)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4464
[32m[2023-01-06 13:02:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][167/27342]	eta 6:46:49 lr 0.0010000000	 wd 0.1000	time 0.8743 (0.8982)	loss 1.2464 (1.2509)	loss-cls 9.9711 (10.0072)	loss-aux 0.0000 (0.0000)	grad_norm 1.7162 (1.8989)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5287
[32m[2023-01-06 13:02:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][175/27342]	eta 6:44:57 lr 0.0010000000	 wd 0.1000	time 0.8842 (0.8944)	loss 1.2551 (1.2510)	loss-cls 10.0408 (10.0079)	loss-aux 0.0000 (0.0000)	grad_norm 1.7038 (1.8900)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5090
[32m[2023-01-06 13:02:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][183/27342]	eta 6:43:15 lr 0.0010000000	 wd 0.1000	time 0.8184 (0.8909)	loss 1.2457 (1.2511)	loss-cls 9.9658 (10.0088)	loss-aux 0.0000 (0.0000)	grad_norm 1.7945 (1.8859)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5114
[32m[2023-01-06 13:02:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][191/27342]	eta 6:41:42 lr 0.0010000000	 wd 0.1000	time 0.8744 (0.8877)	loss 1.2539 (1.2511)	loss-cls 10.0310 (10.0090)	loss-aux 0.0000 (0.0000)	grad_norm 1.8560 (1.8846)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5225
[32m[2023-01-06 13:02:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][199/27342]	eta 6:40:16 lr 0.0010000000	 wd 0.1000	time 0.8831 (0.8848)	loss 1.2333 (1.2512)	loss-cls 9.8667 (10.0094)	loss-aux 0.0000 (0.0000)	grad_norm 2.3098 (1.9017)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5162
[32m[2023-01-06 13:02:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][207/27342]	eta 6:38:54 lr 0.0010000000	 wd 0.1000	time 0.8799 (0.8821)	loss 1.2606 (1.2514)	loss-cls 10.0846 (10.0109)	loss-aux 0.0000 (0.0000)	grad_norm 1.6787 (1.8931)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5090
[32m[2023-01-06 13:02:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][215/27342]	eta 6:37:45 lr 0.0010000000	 wd 0.1000	time 0.9221 (0.8798)	loss 1.2291 (1.2515)	loss-cls 9.8328 (10.0117)	loss-aux 0.0000 (0.0000)	grad_norm 1.6939 (1.8857)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5578
[32m[2023-01-06 13:02:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][223/27342]	eta 6:36:36 lr 0.0010000000	 wd 0.1000	time 0.8996 (0.8775)	loss 1.2363 (1.2514)	loss-cls 9.8902 (10.0115)	loss-aux 0.0000 (0.0000)	grad_norm 1.5534 (1.8738)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5303
[32m[2023-01-06 13:02:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][231/27342]	eta 6:35:33 lr 0.0010000000	 wd 0.1000	time 0.8788 (0.8754)	loss 1.2553 (1.2514)	loss-cls 10.0421 (10.0113)	loss-aux 0.0000 (0.0000)	grad_norm 1.6371 (1.8657)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5371
[32m[2023-01-06 13:03:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][239/27342]	eta 6:34:30 lr 0.0010000000	 wd 0.1000	time 0.8945 (0.8734)	loss 1.2398 (1.2514)	loss-cls 9.9182 (10.0114)	loss-aux 0.0000 (0.0000)	grad_norm 1.7584 (1.8621)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5136
[32m[2023-01-06 13:03:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][247/27342]	eta 6:33:35 lr 0.0010000000	 wd 0.1000	time 0.8933 (0.8716)	loss 1.2539 (1.2516)	loss-cls 10.0314 (10.0125)	loss-aux 0.0000 (0.0000)	grad_norm 1.7480 (1.8584)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5455
[32m[2023-01-06 13:03:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][255/27342]	eta 6:32:48 lr 0.0010000000	 wd 0.1000	time 0.8940 (0.8701)	loss 1.2619 (1.2516)	loss-cls 10.0949 (10.0127)	loss-aux 0.0000 (0.0000)	grad_norm 1.7711 (1.8557)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5896
[32m[2023-01-06 13:03:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][263/27342]	eta 6:31:56 lr 0.0010000000	 wd 0.1000	time 0.8936 (0.8685)	loss 1.2489 (1.2516)	loss-cls 9.9908 (10.0130)	loss-aux 0.0000 (0.0000)	grad_norm 1.6500 (1.8495)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5285
[32m[2023-01-06 13:03:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][271/27342]	eta 6:31:10 lr 0.0010000000	 wd 0.1000	time 0.8828 (0.8670)	loss 1.2538 (1.2518)	loss-cls 10.0305 (10.0141)	loss-aux 0.0000 (0.0000)	grad_norm 1.5881 (1.8418)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5470
[32m[2023-01-06 13:03:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][279/27342]	eta 6:30:26 lr 0.0010000000	 wd 0.1000	time 0.8967 (0.8656)	loss 1.2541 (1.2517)	loss-cls 10.0328 (10.0133)	loss-aux 0.0000 (0.0000)	grad_norm 1.9193 (1.8440)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5525
[32m[2023-01-06 13:03:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][287/27342]	eta 6:29:44 lr 0.0010000000	 wd 0.1000	time 0.8759 (0.8643)	loss 1.2647 (1.2517)	loss-cls 10.1179 (10.0139)	loss-aux 0.0000 (0.0000)	grad_norm 2.0638 (1.8501)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5537
[32m[2023-01-06 13:03:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][295/27342]	eta 6:29:07 lr 0.0010000000	 wd 0.1000	time 0.8866 (0.8632)	loss 1.2729 (1.2522)	loss-cls 10.1833 (10.0178)	loss-aux 0.0000 (0.0000)	grad_norm 4.9483 (1.9338)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5827
[32m[2023-01-06 13:03:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][303/27342]	eta 6:28:23 lr 0.0010000000	 wd 0.1000	time 0.8697 (0.8618)	loss 1.2580 (1.2523)	loss-cls 10.0644 (10.0181)	loss-aux 0.0000 (0.0000)	grad_norm 2.1414 (1.9393)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4930
[32m[2023-01-06 13:03:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][311/27342]	eta 6:27:45 lr 0.0010000000	 wd 0.1000	time 0.8952 (0.8607)	loss 1.2588 (1.2522)	loss-cls 10.0707 (10.0180)	loss-aux 0.0000 (0.0000)	grad_norm 1.8480 (1.9369)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5325
[32m[2023-01-06 13:04:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][319/27342]	eta 6:27:04 lr 0.0010000000	 wd 0.1000	time 0.8814 (0.8594)	loss 1.2512 (1.2522)	loss-cls 10.0098 (10.0179)	loss-aux 0.0000 (0.0000)	grad_norm 1.7634 (1.9326)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4867
[32m[2023-01-06 13:04:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][327/27342]	eta 6:26:26 lr 0.0010000000	 wd 0.1000	time 0.8722 (0.8583)	loss 1.2642 (1.2523)	loss-cls 10.1135 (10.0182)	loss-aux 0.0000 (0.0000)	grad_norm 2.8625 (1.9553)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4940
[32m[2023-01-06 13:04:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][335/27342]	eta 6:25:50 lr 0.0010000000	 wd 0.1000	time 0.8652 (0.8572)	loss 1.2369 (1.2525)	loss-cls 9.8952 (10.0198)	loss-aux 0.0000 (0.0000)	grad_norm 1.9128 (1.9543)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5039
[32m[2023-01-06 13:04:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][343/27342]	eta 6:25:12 lr 0.0010000000	 wd 0.1000	time 0.8798 (0.8561)	loss 1.2563 (1.2527)	loss-cls 10.0501 (10.0214)	loss-aux 0.0000 (0.0000)	grad_norm 1.9064 (1.9532)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4663
[32m[2023-01-06 13:04:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][351/27342]	eta 6:24:34 lr 0.0010000000	 wd 0.1000	time 0.8811 (0.8549)	loss 1.2486 (1.2525)	loss-cls 9.9889 (10.0198)	loss-aux 0.0000 (0.0000)	grad_norm 2.1089 (1.9567)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4382
[32m[2023-01-06 13:04:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][359/27342]	eta 6:23:58 lr 0.0009999999	 wd 0.1000	time 0.8811 (0.8538)	loss 1.2513 (1.2528)	loss-cls 10.0100 (10.0225)	loss-aux 0.0000 (0.0000)	grad_norm 1.8743 (1.9549)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4544
[32m[2023-01-06 13:04:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][367/27342]	eta 6:23:25 lr 0.0009999999	 wd 0.1000	time 0.8814 (0.8529)	loss 1.2629 (1.2528)	loss-cls 10.1036 (10.0223)	loss-aux 0.0000 (0.0000)	grad_norm 1.8427 (1.9524)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4722
[32m[2023-01-06 13:04:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][375/27342]	eta 6:22:51 lr 0.0009999999	 wd 0.1000	time 0.8667 (0.8518)	loss 1.2266 (1.2526)	loss-cls 9.8131 (10.0211)	loss-aux 0.0000 (0.0000)	grad_norm 2.6262 (1.9668)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4408
[32m[2023-01-06 13:04:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][383/27342]	eta 6:22:18 lr 0.0009999999	 wd 0.1000	time 0.8895 (0.8509)	loss 1.2530 (1.2527)	loss-cls 10.0239 (10.0217)	loss-aux 0.0000 (0.0000)	grad_norm 2.9954 (1.9882)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4473
[32m[2023-01-06 13:05:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][391/27342]	eta 6:21:48 lr 0.0009999999	 wd 0.1000	time 0.8731 (0.8500)	loss 1.2425 (1.2527)	loss-cls 9.9402 (10.0219)	loss-aux 0.0000 (0.0000)	grad_norm 1.5103 (1.9784)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4596
[32m[2023-01-06 13:05:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][399/27342]	eta 6:21:16 lr 0.0009999999	 wd 0.1000	time 0.8702 (0.8491)	loss 1.2427 (1.2526)	loss-cls 9.9416 (10.0205)	loss-aux 0.0000 (0.0000)	grad_norm 1.8407 (1.9757)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4247
[32m[2023-01-06 13:05:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][407/27342]	eta 6:20:51 lr 0.0009999999	 wd 0.1000	time 0.8810 (0.8484)	loss 1.2328 (1.2525)	loss-cls 9.8627 (10.0201)	loss-aux 0.0000 (0.0000)	grad_norm 1.4942 (1.9663)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5195
[32m[2023-01-06 13:05:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][415/27342]	eta 6:20:27 lr 0.0009999999	 wd 0.1000	time 0.8854 (0.8478)	loss 1.2491 (1.2524)	loss-cls 9.9928 (10.0196)	loss-aux 0.0000 (0.0000)	grad_norm 1.7562 (1.9622)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5300
[32m[2023-01-06 13:05:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][423/27342]	eta 6:20:04 lr 0.0009999999	 wd 0.1000	time 0.8811 (0.8471)	loss 1.2535 (1.2525)	loss-cls 10.0280 (10.0197)	loss-aux 0.0000 (0.0000)	grad_norm 1.3482 (1.9506)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5157
[32m[2023-01-06 13:05:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][431/27342]	eta 6:19:42 lr 0.0009999999	 wd 0.1000	time 0.8874 (0.8466)	loss 1.2305 (1.2523)	loss-cls 9.8444 (10.0188)	loss-aux 0.0000 (0.0000)	grad_norm 1.2072 (1.9369)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5350
[32m[2023-01-06 13:05:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][439/27342]	eta 6:19:24 lr 0.0009999999	 wd 0.1000	time 0.9085 (0.8462)	loss 1.2308 (1.2524)	loss-cls 9.8460 (10.0191)	loss-aux 0.0000 (0.0000)	grad_norm 1.1632 (1.9228)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.6002
[32m[2023-01-06 13:05:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][447/27342]	eta 6:19:07 lr 0.0009999999	 wd 0.1000	time 0.9517 (0.8458)	loss 1.2568 (1.2523)	loss-cls 10.0547 (10.0184)	loss-aux 0.0000 (0.0000)	grad_norm 1.0437 (1.9071)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5904
[32m[2023-01-06 13:05:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][455/27342]	eta 6:18:40 lr 0.0009999999	 wd 0.1000	time 0.8809 (0.8451)	loss 1.2444 (1.2523)	loss-cls 9.9555 (10.0186)	loss-aux 0.0000 (0.0000)	grad_norm 1.7047 (1.9035)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4320
[32m[2023-01-06 13:06:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][463/27342]	eta 6:18:12 lr 0.0009999999	 wd 0.1000	time 0.8532 (0.8442)	loss 1.2396 (1.2522)	loss-cls 9.9166 (10.0177)	loss-aux 0.0000 (0.0000)	grad_norm 0.9210 (1.8866)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3850
[32m[2023-01-06 13:06:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][471/27342]	eta 6:17:48 lr 0.0009999999	 wd 0.1000	time 0.8968 (0.8436)	loss 1.2348 (1.2522)	loss-cls 9.8782 (10.0175)	loss-aux 0.0000 (0.0000)	grad_norm 1.5167 (1.8803)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4527
[32m[2023-01-06 13:06:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][479/27342]	eta 6:17:19 lr 0.0009999999	 wd 0.1000	time 0.8821 (0.8428)	loss 1.2532 (1.2520)	loss-cls 10.0257 (10.0163)	loss-aux 0.0000 (0.0000)	grad_norm 0.7800 (1.8620)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3542
[32m[2023-01-06 13:06:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][487/27342]	eta 6:16:58 lr 0.0009999999	 wd 0.1000	time 0.8722 (0.8423)	loss 1.2289 (1.2519)	loss-cls 9.8314 (10.0151)	loss-aux 0.0000 (0.0000)	grad_norm 0.8508 (1.8454)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4835
[32m[2023-01-06 13:06:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][495/27342]	eta 6:16:33 lr 0.0009999999	 wd 0.1000	time 0.8699 (0.8416)	loss 1.2472 (1.2518)	loss-cls 9.9776 (10.0147)	loss-aux 0.0000 (0.0000)	grad_norm 0.8384 (1.8292)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3929
[32m[2023-01-06 13:06:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][503/27342]	eta 6:16:06 lr 0.0009999999	 wd 0.1000	time 0.8576 (0.8408)	loss 1.2373 (1.2517)	loss-cls 9.8986 (10.0137)	loss-aux 0.0000 (0.0000)	grad_norm 0.7814 (1.8125)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3468
[32m[2023-01-06 13:06:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][511/27342]	eta 6:15:40 lr 0.0009999999	 wd 0.1000	time 0.8666 (0.8401)	loss 1.2354 (1.2515)	loss-cls 9.8831 (10.0124)	loss-aux 0.0000 (0.0000)	grad_norm 0.8541 (1.7976)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3756
[32m[2023-01-06 13:06:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][519/27342]	eta 6:15:14 lr 0.0009999999	 wd 0.1000	time 0.8691 (0.8394)	loss 1.2621 (1.2516)	loss-cls 10.0969 (10.0124)	loss-aux 0.0000 (0.0000)	grad_norm 0.8250 (1.7826)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3476
[32m[2023-01-06 13:06:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][527/27342]	eta 6:14:49 lr 0.0009999999	 wd 0.1000	time 0.8553 (0.8387)	loss 1.2406 (1.2514)	loss-cls 9.9245 (10.0116)	loss-aux 0.0000 (0.0000)	grad_norm 1.2516 (1.7746)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3406
[32m[2023-01-06 13:07:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][535/27342]	eta 6:14:24 lr 0.0009999999	 wd 0.1000	time 0.8470 (0.8380)	loss 1.2415 (1.2514)	loss-cls 9.9321 (10.0111)	loss-aux 0.0000 (0.0000)	grad_norm 0.8567 (1.7609)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3530
[32m[2023-01-06 13:07:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][543/27342]	eta 6:14:00 lr 0.0009999999	 wd 0.1000	time 0.8632 (0.8374)	loss 1.2613 (1.2513)	loss-cls 10.0902 (10.0105)	loss-aux 0.0000 (0.0000)	grad_norm 0.7813 (1.7465)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3569
[32m[2023-01-06 13:07:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][551/27342]	eta 6:13:37 lr 0.0009999999	 wd 0.1000	time 0.8288 (0.8368)	loss 1.2496 (1.2514)	loss-cls 9.9965 (10.0109)	loss-aux 0.0000 (0.0000)	grad_norm 0.7300 (1.7317)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3632
[32m[2023-01-06 13:07:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][559/27342]	eta 6:13:16 lr 0.0009999999	 wd 0.1000	time 0.8478 (0.8362)	loss 1.2408 (1.2512)	loss-cls 9.9262 (10.0100)	loss-aux 0.0000 (0.0000)	grad_norm 0.7002 (1.7170)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3945
[32m[2023-01-06 13:07:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][567/27342]	eta 6:12:56 lr 0.0009999999	 wd 0.1000	time 0.8516 (0.8357)	loss 1.2373 (1.2511)	loss-cls 9.8984 (10.0090)	loss-aux 0.0000 (0.0000)	grad_norm 0.6490 (1.7019)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.3912
[32m[2023-01-06 13:07:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][575/27342]	eta 6:12:40 lr 0.0009999999	 wd 0.1000	time 0.8868 (0.8354)	loss 1.2566 (1.2512)	loss-cls 10.0524 (10.0093)	loss-aux 0.0000 (0.0000)	grad_norm 0.6232 (1.6870)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4930
[32m[2023-01-06 13:07:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][583/27342]	eta 6:12:25 lr 0.0009999999	 wd 0.1000	time 0.9041 (0.8351)	loss 1.2387 (1.2510)	loss-cls 9.9094 (10.0084)	loss-aux 0.0000 (0.0000)	grad_norm 0.5577 (1.6715)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.5121
[32m[2023-01-06 13:07:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][591/27342]	eta 6:12:07 lr 0.0009999999	 wd 0.1000	time 0.8700 (0.8346)	loss 1.2463 (1.2510)	loss-cls 9.9706 (10.0076)	loss-aux 0.0000 (0.0000)	grad_norm 0.5914 (1.6569)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4115
[32m[2023-01-06 13:07:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][599/27342]	eta 6:11:49 lr 0.0009999999	 wd 0.1000	time 0.8590 (0.8342)	loss 1.2451 (1.2509)	loss-cls 9.9605 (10.0073)	loss-aux 0.0000 (0.0000)	grad_norm 0.5529 (1.6422)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4394
[32m[2023-01-06 13:07:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][607/27342]	eta 6:11:35 lr 0.0009999999	 wd 0.1000	time 0.8795 (0.8339)	loss 1.2447 (1.2508)	loss-cls 9.9577 (10.0067)	loss-aux 0.0000 (0.0000)	grad_norm 0.9572 (1.6332)	loss_scale 65536.0000 (65536.0000)	mem 7531MB	batch_time 6.4921
[32m[2023-01-06 13:08:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][615/27342]	eta 6:11:15 lr 0.0009999999	 wd 0.1000	time 0.7983 (0.8335)	loss 1.2580 (1.2508)	loss-cls 10.0640 (10.0066)	loss-aux 0.0000 (0.0000)	grad_norm inf (inf)	loss_scale 32768.0000 (65482.8052)	mem 7531MB	batch_time 6.3820
[32m[2023-01-06 13:08:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][623/27342]	eta 6:11:01 lr 0.0009999998	 wd 0.1000	time 0.9010 (0.8332)	loss 1.2580 (1.2508)	loss-cls 10.0640 (10.0064)	loss-aux 0.0000 (0.0000)	grad_norm 30.6273 (inf)	loss_scale 32768.0000 (65063.3846)	mem 7531MB	batch_time 6.4834
[32m[2023-01-06 13:08:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][631/27342]	eta 6:10:47 lr 0.0009999998	 wd 0.1000	time 0.8893 (0.8329)	loss 1.2573 (1.2508)	loss-cls 10.0586 (10.0061)	loss-aux 0.0000 (0.0000)	grad_norm 60.8071 (inf)	loss_scale 32768.0000 (64654.5823)	mem 7531MB	batch_time 6.4970
[32m[2023-01-06 13:08:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][639/27342]	eta 6:10:31 lr 0.0009999998	 wd 0.1000	time 0.8730 (0.8325)	loss 1.2353 (1.2507)	loss-cls 9.8822 (10.0055)	loss-aux 0.0000 (0.0000)	grad_norm 0.7153 (inf)	loss_scale 32768.0000 (64256.0000)	mem 7531MB	batch_time 6.4339
[32m[2023-01-06 13:08:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][647/27342]	eta 6:10:13 lr 0.0009999998	 wd 0.1000	time 0.8582 (0.8321)	loss 1.2119 (1.2505)	loss-cls 9.6954 (10.0041)	loss-aux 0.0000 (0.0000)	grad_norm 0.9981 (inf)	loss_scale 32768.0000 (63867.2593)	mem 7531MB	batch_time 6.4003
[32m[2023-01-06 13:08:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][655/27342]	eta 6:09:58 lr 0.0009999998	 wd 0.1000	time 0.8722 (0.8318)	loss 1.2544 (1.2504)	loss-cls 10.0349 (10.0035)	loss-aux 0.0000 (0.0000)	grad_norm 0.5817 (inf)	loss_scale 32768.0000 (63488.0000)	mem 7531MB	batch_time 6.4395
[32m[2023-01-06 13:08:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][663/27342]	eta 6:09:42 lr 0.0009999998	 wd 0.1000	time 0.8754 (0.8314)	loss 1.2216 (1.2503)	loss-cls 9.7729 (10.0026)	loss-aux 0.0000 (0.0000)	grad_norm 0.4696 (inf)	loss_scale 32768.0000 (63117.8795)	mem 7531MB	batch_time 6.4165
[32m[2023-01-06 13:08:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][671/27342]	eta 6:09:25 lr 0.0009999998	 wd 0.1000	time 0.8705 (0.8311)	loss 1.2158 (1.2502)	loss-cls 9.7265 (10.0017)	loss-aux 0.0000 (0.0000)	grad_norm 0.4284 (inf)	loss_scale 32768.0000 (62756.5714)	mem 7531MB	batch_time 6.3906
[32m[2023-01-06 13:08:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][679/27342]	eta 6:09:10 lr 0.0009999998	 wd 0.1000	time 0.8685 (0.8307)	loss 1.2275 (1.2500)	loss-cls 9.8202 (10.0001)	loss-aux 0.0000 (0.0000)	grad_norm 0.4471 (inf)	loss_scale 32768.0000 (62403.7647)	mem 7531MB	batch_time 6.4365
[32m[2023-01-06 13:09:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][687/27342]	eta 6:08:54 lr 0.0009999998	 wd 0.1000	time 0.8845 (0.8304)	loss 1.2465 (1.2499)	loss-cls 9.9718 (9.9995)	loss-aux 0.0000 (0.0000)	grad_norm 0.6487 (inf)	loss_scale 32768.0000 (62059.1628)	mem 7531MB	batch_time 6.4062
[32m[2023-01-06 13:09:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][695/27342]	eta 6:08:39 lr 0.0009999998	 wd 0.1000	time 0.8759 (0.8301)	loss 1.2356 (1.2499)	loss-cls 9.8849 (9.9988)	loss-aux 0.0000 (0.0000)	grad_norm 0.5102 (inf)	loss_scale 32768.0000 (61722.4828)	mem 7531MB	batch_time 6.4304
[32m[2023-01-06 13:09:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][703/27342]	eta 6:08:23 lr 0.0009999998	 wd 0.1000	time 0.8544 (0.8297)	loss 1.2376 (1.2498)	loss-cls 9.9009 (9.9981)	loss-aux 0.0000 (0.0000)	grad_norm 1.0274 (inf)	loss_scale 32768.0000 (61393.4545)	mem 7531MB	batch_time 6.3910
[32m[2023-01-06 13:09:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][711/27342]	eta 6:08:09 lr 0.0009999998	 wd 0.1000	time 0.8624 (0.8294)	loss 1.2435 (1.2497)	loss-cls 9.9478 (9.9974)	loss-aux 0.0000 (0.0000)	grad_norm 0.4145 (inf)	loss_scale 32768.0000 (61071.8202)	mem 7531MB	batch_time 6.4308
[32m[2023-01-06 13:09:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][719/27342]	eta 6:07:57 lr 0.0009999998	 wd 0.1000	time 0.8815 (0.8292)	loss 1.2451 (1.2497)	loss-cls 9.9605 (9.9973)	loss-aux 0.0000 (0.0000)	grad_norm 0.4186 (inf)	loss_scale 32768.0000 (60757.3333)	mem 7531MB	batch_time 6.4900
[32m[2023-01-06 13:09:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][727/27342]	eta 6:07:41 lr 0.0009999998	 wd 0.1000	time 0.8335 (0.8289)	loss 1.2391 (1.2496)	loss-cls 9.9129 (9.9965)	loss-aux 0.0000 (0.0000)	grad_norm 0.3565 (inf)	loss_scale 32768.0000 (60449.7582)	mem 7531MB	batch_time 6.3854
[32m[2023-01-06 13:09:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][735/27342]	eta 6:07:25 lr 0.0009999998	 wd 0.1000	time 0.8737 (0.8286)	loss 1.2472 (1.2495)	loss-cls 9.9776 (9.9960)	loss-aux 0.0000 (0.0000)	grad_norm 0.5865 (inf)	loss_scale 32768.0000 (60148.8696)	mem 7531MB	batch_time 6.3900
[32m[2023-01-06 13:09:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][743/27342]	eta 6:07:19 lr 0.0009999998	 wd 0.1000	time 1.1150 (0.8286)	loss 1.2419 (1.2494)	loss-cls 9.9354 (9.9954)	loss-aux 0.0000 (0.0000)	grad_norm 0.4067 (inf)	loss_scale 32768.0000 (59854.4516)	mem 7531MB	batch_time 6.6238
[32m[2023-01-06 13:09:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][751/27342]	eta 6:07:05 lr 0.0009999998	 wd 0.1000	time 0.8739 (0.8283)	loss 1.2668 (1.2494)	loss-cls 10.1347 (9.9949)	loss-aux 0.0000 (0.0000)	grad_norm 0.4914 (inf)	loss_scale 32768.0000 (59566.2979)	mem 7531MB	batch_time 6.4355
[32m[2023-01-06 13:10:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][759/27342]	eta 6:06:53 lr 0.0009999998	 wd 0.1000	time 0.8588 (0.8281)	loss 1.2506 (1.2492)	loss-cls 10.0049 (9.9938)	loss-aux 0.0000 (0.0000)	grad_norm 0.4317 (inf)	loss_scale 32768.0000 (59284.2105)	mem 7531MB	batch_time 6.4540
[32m[2023-01-06 13:10:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][767/27342]	eta 6:06:39 lr 0.0009999998	 wd 0.1000	time 0.8671 (0.8278)	loss 1.2278 (1.2491)	loss-cls 9.8220 (9.9925)	loss-aux 0.0000 (0.0000)	grad_norm 0.3401 (inf)	loss_scale 32768.0000 (59008.0000)	mem 7531MB	batch_time 6.4355
[32m[2023-01-06 13:10:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][775/27342]	eta 6:06:25 lr 0.0009999998	 wd 0.1000	time 0.8395 (0.8275)	loss 1.2448 (1.2490)	loss-cls 9.9587 (9.9916)	loss-aux 0.0000 (0.0000)	grad_norm 0.3483 (inf)	loss_scale 32768.0000 (58737.4845)	mem 7531MB	batch_time 6.3900
[32m[2023-01-06 13:10:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][783/27342]	eta 6:06:15 lr 0.0009999998	 wd 0.1000	time 0.8971 (0.8274)	loss 1.2430 (1.2489)	loss-cls 9.9442 (9.9914)	loss-aux 0.0000 (0.0000)	grad_norm 0.3928 (inf)	loss_scale 32768.0000 (58472.4898)	mem 7531MB	batch_time 6.5126
[32m[2023-01-06 13:10:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][791/27342]	eta 6:06:01 lr 0.0009999998	 wd 0.1000	time 0.8539 (0.8272)	loss 1.2389 (1.2489)	loss-cls 9.9110 (9.9912)	loss-aux 0.0000 (0.0000)	grad_norm 0.3517 (inf)	loss_scale 32768.0000 (58212.8485)	mem 7531MB	batch_time 6.4251
[32m[2023-01-06 13:10:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][799/27342]	eta 6:05:48 lr 0.0009999997	 wd 0.1000	time 0.8724 (0.8269)	loss 1.2359 (1.2488)	loss-cls 9.8872 (9.9904)	loss-aux 0.0000 (0.0000)	grad_norm 0.4108 (inf)	loss_scale 32768.0000 (57958.4000)	mem 7531MB	batch_time 6.4218
[32m[2023-01-06 13:10:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][807/27342]	eta 6:05:34 lr 0.0009999997	 wd 0.1000	time 0.8754 (0.8266)	loss 1.2422 (1.2487)	loss-cls 9.9380 (9.9899)	loss-aux 0.0000 (0.0000)	grad_norm 2.4098 (inf)	loss_scale 32768.0000 (57708.9901)	mem 7531MB	batch_time 6.3692
[32m[2023-01-06 13:10:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][815/27342]	eta 6:05:20 lr 0.0009999997	 wd 0.1000	time 0.8878 (0.8263)	loss 1.2395 (1.2486)	loss-cls 9.9162 (9.9892)	loss-aux 0.0000 (0.0000)	grad_norm 0.3370 (inf)	loss_scale 32768.0000 (57464.4706)	mem 7531MB	batch_time 6.3903
[32m[2023-01-06 13:10:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][823/27342]	eta 6:05:06 lr 0.0009999997	 wd 0.1000	time 0.8665 (0.8261)	loss 1.2449 (1.2486)	loss-cls 9.9590 (9.9888)	loss-aux 0.0000 (0.0000)	grad_norm 0.8995 (inf)	loss_scale 32768.0000 (57224.6990)	mem 7531MB	batch_time 6.3973
[32m[2023-01-06 13:10:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][831/27342]	eta 6:04:52 lr 0.0009999997	 wd 0.1000	time 0.8767 (0.8258)	loss 1.2277 (1.2485)	loss-cls 9.8213 (9.9881)	loss-aux 0.0000 (0.0000)	grad_norm 0.3404 (inf)	loss_scale 32768.0000 (56989.5385)	mem 7536MB	batch_time 6.3763
[32m[2023-01-06 13:11:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][839/27342]	eta 6:04:39 lr 0.0009999997	 wd 0.1000	time 0.8769 (0.8256)	loss 1.2394 (1.2484)	loss-cls 9.9155 (9.9873)	loss-aux 0.0000 (0.0000)	grad_norm 0.4154 (inf)	loss_scale 32768.0000 (56758.8571)	mem 7550MB	batch_time 6.3975
[32m[2023-01-06 13:11:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][847/27342]	eta 6:04:26 lr 0.0009999997	 wd 0.1000	time 0.8477 (0.8253)	loss 1.2403 (1.2483)	loss-cls 9.9222 (9.9865)	loss-aux 0.0000 (0.0000)	grad_norm 4.0888 (inf)	loss_scale 32768.0000 (56532.5283)	mem 7559MB	batch_time 6.3874
[32m[2023-01-06 13:11:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][855/27342]	eta 6:04:13 lr 0.0009999997	 wd 0.1000	time 0.8612 (0.8251)	loss 1.2477 (1.2482)	loss-cls 9.9819 (9.9859)	loss-aux 0.0000 (0.0000)	grad_norm 1.3962 (inf)	loss_scale 32768.0000 (56310.4299)	mem 7578MB	batch_time 6.4188
[32m[2023-01-06 13:11:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][863/27342]	eta 6:04:01 lr 0.0009999997	 wd 0.1000	time 0.8614 (0.8249)	loss 1.2295 (1.2481)	loss-cls 9.8357 (9.9848)	loss-aux 0.0000 (0.0000)	grad_norm 0.3197 (inf)	loss_scale 32768.0000 (56092.4444)	mem 7578MB	batch_time 6.4130
[32m[2023-01-06 13:11:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][871/27342]	eta 6:03:49 lr 0.0009999997	 wd 0.1000	time 0.8607 (0.8246)	loss 1.2223 (1.2480)	loss-cls 9.7786 (9.9840)	loss-aux 0.0000 (0.0000)	grad_norm 0.3417 (inf)	loss_scale 32768.0000 (55878.4587)	mem 7601MB	batch_time 6.4112
[32m[2023-01-06 13:11:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][879/27342]	eta 6:03:36 lr 0.0009999997	 wd 0.1000	time 0.8714 (0.8244)	loss 1.2259 (1.2479)	loss-cls 9.8071 (9.9836)	loss-aux 0.0000 (0.0000)	grad_norm 0.3801 (inf)	loss_scale 32768.0000 (55668.3636)	mem 7601MB	batch_time 6.3819
[32m[2023-01-06 13:11:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][887/27342]	eta 6:03:22 lr 0.0009999997	 wd 0.1000	time 0.8314 (0.8241)	loss 1.2190 (1.2479)	loss-cls 9.7517 (9.9829)	loss-aux 0.0000 (0.0000)	grad_norm 0.3574 (inf)	loss_scale 32768.0000 (55462.0541)	mem 7601MB	batch_time 6.3436
[32m[2023-01-06 13:11:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][895/27342]	eta 6:03:10 lr 0.0009999997	 wd 0.1000	time 0.8484 (0.8239)	loss 1.2518 (1.2478)	loss-cls 10.0147 (9.9825)	loss-aux 0.0000 (0.0000)	grad_norm 0.3483 (inf)	loss_scale 32768.0000 (55259.4286)	mem 7601MB	batch_time 6.4255
[32m[2023-01-06 13:11:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][903/27342]	eta 6:02:56 lr 0.0009999997	 wd 0.1000	time 0.8568 (0.8237)	loss 1.2287 (1.2477)	loss-cls 9.8297 (9.9818)	loss-aux 0.0000 (0.0000)	grad_norm 0.5972 (inf)	loss_scale 32768.0000 (55060.3894)	mem 7601MB	batch_time 6.3458
[32m[2023-01-06 13:12:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][911/27342]	eta 6:02:45 lr 0.0009999997	 wd 0.1000	time 0.9046 (0.8235)	loss 1.2288 (1.2477)	loss-cls 9.8307 (9.9813)	loss-aux 0.0000 (0.0000)	grad_norm 0.3600 (inf)	loss_scale 32768.0000 (54864.8421)	mem 7601MB	batch_time 6.4371
[32m[2023-01-06 13:12:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][919/27342]	eta 6:02:30 lr 0.0009999997	 wd 0.1000	time 0.8530 (0.8232)	loss 1.2284 (1.2476)	loss-cls 9.8273 (9.9808)	loss-aux 0.0000 (0.0000)	grad_norm 0.3614 (inf)	loss_scale 32768.0000 (54672.6957)	mem 7601MB	batch_time 6.2912
[32m[2023-01-06 13:12:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][927/27342]	eta 6:02:17 lr 0.0009999997	 wd 0.1000	time 0.8687 (0.8229)	loss 1.2361 (1.2475)	loss-cls 9.8890 (9.9802)	loss-aux 0.0000 (0.0000)	grad_norm 0.3623 (inf)	loss_scale 32768.0000 (54483.8621)	mem 7601MB	batch_time 6.3710
[32m[2023-01-06 13:12:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][935/27342]	eta 6:02:03 lr 0.0009999997	 wd 0.1000	time 0.8477 (0.8226)	loss 1.2448 (1.2474)	loss-cls 9.9585 (9.9795)	loss-aux 0.0000 (0.0000)	grad_norm 0.3824 (inf)	loss_scale 32768.0000 (54298.2564)	mem 7601MB	batch_time 6.2880
[32m[2023-01-06 13:12:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][943/27342]	eta 6:01:49 lr 0.0009999996	 wd 0.1000	time 0.8608 (0.8224)	loss 1.2431 (1.2474)	loss-cls 9.9447 (9.9794)	loss-aux 0.0000 (0.0000)	grad_norm 0.4711 (inf)	loss_scale 32768.0000 (54115.7966)	mem 7601MB	batch_time 6.3232
[32m[2023-01-06 13:12:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][951/27342]	eta 6:01:36 lr 0.0009999996	 wd 0.1000	time 0.8398 (0.8221)	loss 1.2349 (1.2473)	loss-cls 9.8791 (9.9787)	loss-aux 0.0000 (0.0000)	grad_norm 0.3912 (inf)	loss_scale 32768.0000 (53936.4034)	mem 7601MB	batch_time 6.3490
[32m[2023-01-06 13:12:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][959/27342]	eta 6:01:22 lr 0.0009999996	 wd 0.1000	time 0.8447 (0.8218)	loss 1.2622 (1.2472)	loss-cls 10.0973 (9.9780)	loss-aux 0.0000 (0.0000)	grad_norm 0.3566 (inf)	loss_scale 32768.0000 (53760.0000)	mem 7601MB	batch_time 6.3246
[32m[2023-01-06 13:12:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][967/27342]	eta 6:01:10 lr 0.0009999996	 wd 0.1000	time 0.8626 (0.8216)	loss 1.2346 (1.2471)	loss-cls 9.8769 (9.9770)	loss-aux 0.0000 (0.0000)	grad_norm 0.3652 (inf)	loss_scale 32768.0000 (53586.5124)	mem 7601MB	batch_time 6.3571
[32m[2023-01-06 13:12:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][975/27342]	eta 6:00:56 lr 0.0009999996	 wd 0.1000	time 0.8531 (0.8213)	loss 1.2480 (1.2470)	loss-cls 9.9837 (9.9763)	loss-aux 0.0000 (0.0000)	grad_norm 0.4068 (inf)	loss_scale 32768.0000 (53415.8689)	mem 7601MB	batch_time 6.3032
[32m[2023-01-06 13:12:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][983/27342]	eta 6:00:42 lr 0.0009999996	 wd 0.1000	time 0.8570 (0.8211)	loss 1.2431 (1.2470)	loss-cls 9.9444 (9.9757)	loss-aux 0.0000 (0.0000)	grad_norm 0.6557 (inf)	loss_scale 32768.0000 (53248.0000)	mem 7601MB	batch_time 6.3068
[32m[2023-01-06 13:13:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][991/27342]	eta 6:00:29 lr 0.0009999996	 wd 0.1000	time 0.8439 (0.8208)	loss 1.2252 (1.2469)	loss-cls 9.8018 (9.9750)	loss-aux 0.0000 (0.0000)	grad_norm 2.1978 (inf)	loss_scale 32768.0000 (53082.8387)	mem 7601MB	batch_time 6.3039
[32m[2023-01-06 13:13:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][999/27342]	eta 6:00:15 lr 0.0009999996	 wd 0.1000	time 0.8619 (0.8206)	loss 1.2310 (1.2468)	loss-cls 9.8479 (9.9743)	loss-aux 0.0000 (0.0000)	grad_norm 0.3747 (inf)	loss_scale 32768.0000 (52920.3200)	mem 7601MB	batch_time 6.3127
[32m[2023-01-06 13:13:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1007/27342]	eta 6:00:03 lr 0.0009999996	 wd 0.1000	time 0.8688 (0.8203)	loss 1.2499 (1.2467)	loss-cls 9.9995 (9.9740)	loss-aux 0.0000 (0.0000)	grad_norm 0.3157 (inf)	loss_scale 32768.0000 (52760.3810)	mem 7601MB	batch_time 6.3348
[32m[2023-01-06 13:13:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1015/27342]	eta 5:59:51 lr 0.0009999996	 wd 0.1000	time 0.8346 (0.8201)	loss 1.2361 (1.2467)	loss-cls 9.8885 (9.9735)	loss-aux 0.0000 (0.0000)	grad_norm 0.3030 (inf)	loss_scale 32768.0000 (52602.9606)	mem 7601MB	batch_time 6.3658
[32m[2023-01-06 13:13:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1023/27342]	eta 5:59:39 lr 0.0009999996	 wd 0.1000	time 0.8733 (0.8199)	loss 1.2461 (1.2467)	loss-cls 9.9688 (9.9734)	loss-aux 0.0000 (0.0000)	grad_norm 0.3390 (inf)	loss_scale 32768.0000 (52448.0000)	mem 7601MB	batch_time 6.3496
[32m[2023-01-06 13:13:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1031/27342]	eta 5:59:26 lr 0.0009999996	 wd 0.1000	time 0.8536 (0.8197)	loss 1.2399 (1.2467)	loss-cls 9.9191 (9.9732)	loss-aux 0.0000 (0.0000)	grad_norm 0.3197 (inf)	loss_scale 32768.0000 (52295.4419)	mem 7601MB	batch_time 6.3048
[32m[2023-01-06 13:13:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1039/27342]	eta 5:59:15 lr 0.0009999996	 wd 0.1000	time 0.8531 (0.8195)	loss 1.2226 (1.2466)	loss-cls 9.7808 (9.9726)	loss-aux 0.0000 (0.0000)	grad_norm 0.5974 (inf)	loss_scale 32768.0000 (52145.2308)	mem 7601MB	batch_time 6.3656
[32m[2023-01-06 13:13:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1047/27342]	eta 5:59:03 lr 0.0009999996	 wd 0.1000	time 0.8552 (0.8193)	loss 1.2425 (1.2465)	loss-cls 9.9398 (9.9720)	loss-aux 0.0000 (0.0000)	grad_norm 0.2881 (inf)	loss_scale 32768.0000 (51997.3130)	mem 7601MB	batch_time 6.3421
[32m[2023-01-06 13:13:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1055/27342]	eta 5:58:50 lr 0.0009999996	 wd 0.1000	time 0.8331 (0.8191)	loss 1.2346 (1.2465)	loss-cls 9.8769 (9.9716)	loss-aux 0.0000 (0.0000)	grad_norm 0.4528 (inf)	loss_scale 32768.0000 (51851.6364)	mem 7601MB	batch_time 6.2992
[32m[2023-01-06 13:14:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1063/27342]	eta 5:58:38 lr 0.0009999995	 wd 0.1000	time 0.8501 (0.8189)	loss 1.2346 (1.2464)	loss-cls 9.8772 (9.9710)	loss-aux 0.0000 (0.0000)	grad_norm 0.2856 (inf)	loss_scale 32768.0000 (51708.1504)	mem 7601MB	batch_time 6.3412
[32m[2023-01-06 13:14:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1071/27342]	eta 5:58:25 lr 0.0009999995	 wd 0.1000	time 0.8456 (0.8186)	loss 1.2321 (1.2463)	loss-cls 9.8564 (9.9705)	loss-aux 0.0000 (0.0000)	grad_norm 0.3563 (inf)	loss_scale 32768.0000 (51566.8060)	mem 7601MB	batch_time 6.2984
[32m[2023-01-06 13:14:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1079/27342]	eta 5:58:14 lr 0.0009999995	 wd 0.1000	time 0.8378 (0.8184)	loss 1.2362 (1.2463)	loss-cls 9.8895 (9.9702)	loss-aux 0.0000 (0.0000)	grad_norm 0.3692 (inf)	loss_scale 32768.0000 (51427.5556)	mem 7601MB	batch_time 6.3498
[32m[2023-01-06 13:14:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1087/27342]	eta 5:58:02 lr 0.0009999995	 wd 0.1000	time 0.8500 (0.8182)	loss 1.2439 (1.2462)	loss-cls 9.9510 (9.9700)	loss-aux 0.0000 (0.0000)	grad_norm 0.3294 (inf)	loss_scale 32768.0000 (51290.3529)	mem 7601MB	batch_time 6.3134
[32m[2023-01-06 13:14:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1095/27342]	eta 5:57:52 lr 0.0009999995	 wd 0.1000	time 0.8512 (0.8181)	loss 1.2533 (1.2462)	loss-cls 10.0267 (9.9696)	loss-aux 0.0000 (0.0000)	grad_norm 0.3893 (inf)	loss_scale 32768.0000 (51155.1533)	mem 7601MB	batch_time 6.3927
[32m[2023-01-06 13:14:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1103/27342]	eta 5:57:42 lr 0.0009999995	 wd 0.1000	time 0.8565 (0.8179)	loss 1.2426 (1.2461)	loss-cls 9.9407 (9.9686)	loss-aux 0.0000 (0.0000)	grad_norm 0.5015 (inf)	loss_scale 32768.0000 (51021.9130)	mem 7601MB	batch_time 6.3973
[32m[2023-01-06 13:14:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1111/27342]	eta 5:57:31 lr 0.0009999995	 wd 0.1000	time 0.8521 (0.8178)	loss 1.2192 (1.2460)	loss-cls 9.7540 (9.9678)	loss-aux 0.0000 (0.0000)	grad_norm 0.3288 (inf)	loss_scale 32768.0000 (50890.5899)	mem 7601MB	batch_time 6.3677
[32m[2023-01-06 13:14:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1119/27342]	eta 5:57:18 lr 0.0009999995	 wd 0.1000	time 0.8161 (0.8175)	loss 1.2291 (1.2459)	loss-cls 9.8328 (9.9669)	loss-aux 0.0000 (0.0000)	grad_norm 0.3642 (inf)	loss_scale 32768.0000 (50761.1429)	mem 7601MB	batch_time 6.2674
[32m[2023-01-06 13:14:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1127/27342]	eta 5:57:08 lr 0.0009999995	 wd 0.1000	time 0.8819 (0.8174)	loss 1.2327 (1.2458)	loss-cls 9.8620 (9.9666)	loss-aux 0.0000 (0.0000)	grad_norm 0.3400 (inf)	loss_scale 32768.0000 (50633.5319)	mem 7601MB	batch_time 6.3893
[32m[2023-01-06 13:14:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1135/27342]	eta 5:56:57 lr 0.0009999995	 wd 0.1000	time 0.8476 (0.8172)	loss 1.2160 (1.2458)	loss-cls 9.7278 (9.9661)	loss-aux 0.0000 (0.0000)	grad_norm 0.3922 (inf)	loss_scale 32768.0000 (50507.7183)	mem 7601MB	batch_time 6.3343
[32m[2023-01-06 13:15:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1143/27342]	eta 5:56:48 lr 0.0009999995	 wd 0.1000	time 0.8553 (0.8171)	loss 1.2794 (1.2457)	loss-cls 10.2353 (9.9655)	loss-aux 0.0000 (0.0000)	grad_norm 0.4084 (inf)	loss_scale 32768.0000 (50383.6643)	mem 7601MB	batch_time 6.4314
[32m[2023-01-06 13:15:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1151/27342]	eta 5:56:38 lr 0.0009999995	 wd 0.1000	time 0.8567 (0.8170)	loss 1.2400 (1.2456)	loss-cls 9.9200 (9.9650)	loss-aux 0.0000 (0.0000)	grad_norm 0.3435 (inf)	loss_scale 32768.0000 (50261.3333)	mem 7601MB	batch_time 6.4094
[32m[2023-01-06 13:15:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1159/27342]	eta 5:56:28 lr 0.0009999995	 wd 0.1000	time 0.8430 (0.8169)	loss 1.2381 (1.2456)	loss-cls 9.9045 (9.9646)	loss-aux 0.0000 (0.0000)	grad_norm 0.3226 (inf)	loss_scale 32768.0000 (50140.6897)	mem 7601MB	batch_time 6.3944
[32m[2023-01-06 13:15:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1167/27342]	eta 5:56:19 lr 0.0009999995	 wd 0.1000	time 0.8862 (0.8168)	loss 1.2513 (1.2455)	loss-cls 10.0105 (9.9641)	loss-aux 0.0000 (0.0000)	grad_norm 0.3385 (inf)	loss_scale 32768.0000 (50021.6986)	mem 7601MB	batch_time 6.4088
[32m[2023-01-06 13:15:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1175/27342]	eta 5:56:10 lr 0.0009999994	 wd 0.1000	time 0.8430 (0.8167)	loss 1.2449 (1.2455)	loss-cls 9.9589 (9.9636)	loss-aux 0.0000 (0.0000)	grad_norm 0.3335 (inf)	loss_scale 32768.0000 (49904.3265)	mem 7601MB	batch_time 6.4408
[32m[2023-01-06 13:15:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1183/27342]	eta 5:56:04 lr 0.0009999994	 wd 0.1000	time 0.8489 (0.8167)	loss 1.2322 (1.2454)	loss-cls 9.8575 (9.9631)	loss-aux 0.0000 (0.0000)	grad_norm 0.3084 (inf)	loss_scale 32768.0000 (49788.5405)	mem 7601MB	batch_time 6.5320
[32m[2023-01-06 13:15:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1191/27342]	eta 5:55:55 lr 0.0009999994	 wd 0.1000	time 0.8601 (0.8166)	loss 1.2290 (1.2453)	loss-cls 9.8319 (9.9625)	loss-aux 0.0000 (0.0000)	grad_norm 0.2942 (inf)	loss_scale 32768.0000 (49674.3087)	mem 7601MB	batch_time 6.4326
[32m[2023-01-06 13:15:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1199/27342]	eta 5:55:46 lr 0.0009999994	 wd 0.1000	time 0.8600 (0.8165)	loss 1.2405 (1.2453)	loss-cls 9.9236 (9.9622)	loss-aux 0.0000 (0.0000)	grad_norm 0.2776 (inf)	loss_scale 32768.0000 (49561.6000)	mem 7601MB	batch_time 6.4316
[32m[2023-01-06 13:15:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1207/27342]	eta 5:55:37 lr 0.0009999994	 wd 0.1000	time 0.8396 (0.8164)	loss 1.2436 (1.2452)	loss-cls 9.9488 (9.9618)	loss-aux 0.0000 (0.0000)	grad_norm 0.2988 (inf)	loss_scale 32768.0000 (49450.3841)	mem 7601MB	batch_time 6.4000
[32m[2023-01-06 13:16:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1215/27342]	eta 5:55:28 lr 0.0009999994	 wd 0.1000	time 0.8698 (0.8163)	loss 1.2321 (1.2451)	loss-cls 9.8567 (9.9610)	loss-aux 0.0000 (0.0000)	grad_norm 0.2632 (inf)	loss_scale 32768.0000 (49340.6316)	mem 7601MB	batch_time 6.4214
[32m[2023-01-06 13:16:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1223/27342]	eta 5:55:17 lr 0.0009999994	 wd 0.1000	time 0.8601 (0.8162)	loss 1.2541 (1.2451)	loss-cls 10.0325 (9.9608)	loss-aux 0.0000 (0.0000)	grad_norm 0.2584 (inf)	loss_scale 32768.0000 (49232.3137)	mem 7601MB	batch_time 6.3369
[32m[2023-01-06 13:16:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1231/27342]	eta 5:55:07 lr 0.0009999994	 wd 0.1000	time 0.8768 (0.8161)	loss 1.2446 (1.2451)	loss-cls 9.9568 (9.9607)	loss-aux 0.0000 (0.0000)	grad_norm 0.3020 (inf)	loss_scale 32768.0000 (49125.4026)	mem 7601MB	batch_time 6.3649
[32m[2023-01-06 13:16:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1239/27342]	eta 5:54:56 lr 0.0009999994	 wd 0.1000	time 0.8374 (0.8158)	loss 1.2425 (1.2451)	loss-cls 9.9399 (9.9606)	loss-aux 0.0000 (0.0000)	grad_norm 0.5474 (inf)	loss_scale 32768.0000 (49019.8710)	mem 7601MB	batch_time 6.2742
[32m[2023-01-06 13:16:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1247/27342]	eta 5:54:45 lr 0.0009999994	 wd 0.1000	time 0.8612 (0.8157)	loss 1.2409 (1.2451)	loss-cls 9.9269 (9.9604)	loss-aux 0.0000 (0.0000)	grad_norm 0.3142 (inf)	loss_scale 32768.0000 (48915.6923)	mem 7601MB	batch_time 6.3363
[32m[2023-01-06 13:16:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1255/27342]	eta 5:54:34 lr 0.0009999994	 wd 0.1000	time 0.8468 (0.8155)	loss 1.2451 (1.2450)	loss-cls 9.9607 (9.9600)	loss-aux 0.0000 (0.0000)	grad_norm 0.2336 (inf)	loss_scale 32768.0000 (48812.8408)	mem 7601MB	batch_time 6.3160
[32m[2023-01-06 13:16:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1263/27342]	eta 5:54:23 lr 0.0009999994	 wd 0.1000	time 0.8461 (0.8153)	loss 1.2443 (1.2450)	loss-cls 9.9543 (9.9600)	loss-aux 0.0000 (0.0000)	grad_norm 0.2453 (inf)	loss_scale 32768.0000 (48711.2911)	mem 7601MB	batch_time 6.2834
[32m[2023-01-06 13:16:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1271/27342]	eta 5:54:11 lr 0.0009999994	 wd 0.1000	time 0.8230 (0.8151)	loss 1.2452 (1.2450)	loss-cls 9.9617 (9.9599)	loss-aux 0.0000 (0.0000)	grad_norm 0.2423 (inf)	loss_scale 32768.0000 (48611.0189)	mem 7601MB	batch_time 6.2554
[32m[2023-01-06 13:16:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1279/27342]	eta 5:54:00 lr 0.0009999993	 wd 0.1000	time 0.8648 (0.8150)	loss 1.2496 (1.2450)	loss-cls 9.9969 (9.9598)	loss-aux 0.0000 (0.0000)	grad_norm 0.2750 (inf)	loss_scale 32768.0000 (48512.0000)	mem 7601MB	batch_time 6.3393
[32m[2023-01-06 13:17:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1287/27342]	eta 5:53:48 lr 0.0009999993	 wd 0.1000	time 0.8704 (0.8148)	loss 1.2518 (1.2449)	loss-cls 10.0142 (9.9596)	loss-aux 0.0000 (0.0000)	grad_norm 0.2347 (inf)	loss_scale 32768.0000 (48414.2112)	mem 7601MB	batch_time 6.2472
[32m[2023-01-06 13:17:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1295/27342]	eta 5:53:37 lr 0.0009999993	 wd 0.1000	time 0.8542 (0.8146)	loss 1.2359 (1.2449)	loss-cls 9.8873 (9.9592)	loss-aux 0.0000 (0.0000)	grad_norm 0.2370 (inf)	loss_scale 32768.0000 (48317.6296)	mem 7601MB	batch_time 6.2586
[32m[2023-01-06 13:17:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1303/27342]	eta 5:53:25 lr 0.0009999993	 wd 0.1000	time 0.8376 (0.8144)	loss 1.2447 (1.2448)	loss-cls 9.9575 (9.9586)	loss-aux 0.0000 (0.0000)	grad_norm 0.2483 (inf)	loss_scale 32768.0000 (48222.2331)	mem 7601MB	batch_time 6.2788
[32m[2023-01-06 13:17:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1311/27342]	eta 5:53:15 lr 0.0009999993	 wd 0.1000	time 0.8590 (0.8142)	loss 1.1953 (1.2448)	loss-cls 9.5622 (9.9581)	loss-aux 0.0000 (0.0000)	grad_norm 0.2669 (inf)	loss_scale 32768.0000 (48128.0000)	mem 7601MB	batch_time 6.3210
[32m[2023-01-06 13:17:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1319/27342]	eta 5:53:03 lr 0.0009999993	 wd 0.1000	time 0.8538 (0.8140)	loss 1.2205 (1.2447)	loss-cls 9.7640 (9.9573)	loss-aux 0.0000 (0.0000)	grad_norm 0.2641 (inf)	loss_scale 32768.0000 (48034.9091)	mem 7601MB	batch_time 6.2457
[32m[2023-01-06 13:17:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1327/27342]	eta 5:52:53 lr 0.0009999993	 wd 0.1000	time 0.8527 (0.8139)	loss 1.2368 (1.2446)	loss-cls 9.8940 (9.9568)	loss-aux 0.0000 (0.0000)	grad_norm 0.2632 (inf)	loss_scale 32768.0000 (47942.9398)	mem 7601MB	batch_time 6.3016
[32m[2023-01-06 13:17:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1335/27342]	eta 5:52:42 lr 0.0009999993	 wd 0.1000	time 0.8497 (0.8137)	loss 1.2390 (1.2445)	loss-cls 9.9122 (9.9563)	loss-aux 0.0000 (0.0000)	grad_norm 0.2668 (inf)	loss_scale 32768.0000 (47852.0719)	mem 7601MB	batch_time 6.2774
[32m[2023-01-06 13:17:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1343/27342]	eta 5:52:31 lr 0.0009999993	 wd 0.1000	time 0.8542 (0.8135)	loss 1.2469 (1.2446)	loss-cls 9.9752 (9.9564)	loss-aux 0.0000 (0.0000)	grad_norm 0.2720 (inf)	loss_scale 32768.0000 (47762.2857)	mem 7601MB	batch_time 6.2843
[32m[2023-01-06 13:17:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1351/27342]	eta 5:52:21 lr 0.0009999993	 wd 0.1000	time 0.8502 (0.8134)	loss 1.2392 (1.2445)	loss-cls 9.9134 (9.9561)	loss-aux 0.0000 (0.0000)	grad_norm 0.2483 (inf)	loss_scale 32768.0000 (47673.5621)	mem 7601MB	batch_time 6.3499
[32m[2023-01-06 13:17:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1359/27342]	eta 5:52:11 lr 0.0009999993	 wd 0.1000	time 0.8390 (0.8133)	loss 1.2484 (1.2444)	loss-cls 9.9868 (9.9555)	loss-aux 0.0000 (0.0000)	grad_norm 0.3025 (inf)	loss_scale 32768.0000 (47585.8824)	mem 7601MB	batch_time 6.3136
[32m[2023-01-06 13:18:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1367/27342]	eta 5:52:00 lr 0.0009999993	 wd 0.1000	time 0.8493 (0.8131)	loss 1.2354 (1.2444)	loss-cls 9.8830 (9.9552)	loss-aux 0.0000 (0.0000)	grad_norm 0.2526 (inf)	loss_scale 32768.0000 (47499.2281)	mem 7601MB	batch_time 6.2997
[32m[2023-01-06 13:18:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1375/27342]	eta 5:51:51 lr 0.0009999992	 wd 0.1000	time 0.8558 (0.8130)	loss 1.2386 (1.2444)	loss-cls 9.9092 (9.9549)	loss-aux 0.0000 (0.0000)	grad_norm 0.2423 (inf)	loss_scale 32768.0000 (47413.5814)	mem 7601MB	batch_time 6.3624
[32m[2023-01-06 13:18:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1383/27342]	eta 5:51:42 lr 0.0009999992	 wd 0.1000	time 0.8664 (0.8129)	loss 1.2292 (1.2443)	loss-cls 9.8332 (9.9542)	loss-aux 0.0000 (0.0000)	grad_norm 0.2550 (inf)	loss_scale 32768.0000 (47328.9249)	mem 7601MB	batch_time 6.3593
[32m[2023-01-06 13:18:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1391/27342]	eta 5:51:34 lr 0.0009999992	 wd 0.1000	time 0.8852 (0.8129)	loss 1.2288 (1.2442)	loss-cls 9.8301 (9.9535)	loss-aux 0.0000 (0.0000)	grad_norm 0.2532 (inf)	loss_scale 32768.0000 (47245.2414)	mem 7601MB	batch_time 6.4188
[32m[2023-01-06 13:18:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1399/27342]	eta 5:51:25 lr 0.0009999992	 wd 0.1000	time 0.8624 (0.8128)	loss 1.2383 (1.2441)	loss-cls 9.9065 (9.9528)	loss-aux 0.0000 (0.0000)	grad_norm 0.2557 (inf)	loss_scale 32768.0000 (47162.5143)	mem 7601MB	batch_time 6.3530
[32m[2023-01-06 13:18:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1407/27342]	eta 5:51:16 lr 0.0009999992	 wd 0.1000	time 0.8544 (0.8127)	loss 1.2033 (1.2440)	loss-cls 9.6261 (9.9519)	loss-aux 0.0000 (0.0000)	grad_norm 0.2582 (inf)	loss_scale 32768.0000 (47080.7273)	mem 7601MB	batch_time 6.4007
[32m[2023-01-06 13:18:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1415/27342]	eta 5:51:08 lr 0.0009999992	 wd 0.1000	time 0.8733 (0.8126)	loss 1.2609 (1.2440)	loss-cls 10.0869 (9.9517)	loss-aux 0.0000 (0.0000)	grad_norm 0.3006 (inf)	loss_scale 32768.0000 (46999.8644)	mem 7601MB	batch_time 6.3756
[32m[2023-01-06 13:18:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1423/27342]	eta 5:50:58 lr 0.0009999992	 wd 0.1000	time 0.8441 (0.8125)	loss 1.2240 (1.2439)	loss-cls 9.7916 (9.9514)	loss-aux 0.0000 (0.0000)	grad_norm 0.2956 (inf)	loss_scale 32768.0000 (46919.9101)	mem 7601MB	batch_time 6.3269
[32m[2023-01-06 13:18:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1431/27342]	eta 5:50:49 lr 0.0009999992	 wd 0.1000	time 0.8898 (0.8124)	loss 1.2462 (1.2439)	loss-cls 9.9698 (9.9513)	loss-aux 0.0000 (0.0000)	grad_norm 0.2918 (inf)	loss_scale 32768.0000 (46840.8492)	mem 7601MB	batch_time 6.3961
[32m[2023-01-06 13:19:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1439/27342]	eta 5:50:40 lr 0.0009999992	 wd 0.1000	time 0.8610 (0.8123)	loss 1.2554 (1.2439)	loss-cls 10.0433 (9.9513)	loss-aux 0.0000 (0.0000)	grad_norm 0.2655 (inf)	loss_scale 32768.0000 (46762.6667)	mem 7601MB	batch_time 6.3446
[32m[2023-01-06 13:19:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1447/27342]	eta 5:50:30 lr 0.0009999992	 wd 0.1000	time 0.8410 (0.8122)	loss 1.2170 (1.2438)	loss-cls 9.7358 (9.9507)	loss-aux 0.0000 (0.0000)	grad_norm 0.2304 (inf)	loss_scale 32768.0000 (46685.3481)	mem 7601MB	batch_time 6.3024
[32m[2023-01-06 13:19:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1455/27342]	eta 5:50:21 lr 0.0009999992	 wd 0.1000	time 0.8696 (0.8120)	loss 1.2383 (1.2438)	loss-cls 9.9064 (9.9503)	loss-aux 0.0000 (0.0000)	grad_norm 0.2225 (inf)	loss_scale 32768.0000 (46608.8791)	mem 7601MB	batch_time 6.3455
[32m[2023-01-06 13:19:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1463/27342]	eta 5:50:12 lr 0.0009999991	 wd 0.1000	time 0.8585 (0.8119)	loss 1.2214 (1.2438)	loss-cls 9.7709 (9.9500)	loss-aux 0.0000 (0.0000)	grad_norm 0.2169 (inf)	loss_scale 32768.0000 (46533.2459)	mem 7601MB	batch_time 6.3466
[32m[2023-01-06 13:19:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1471/27342]	eta 5:50:03 lr 0.0009999991	 wd 0.1000	time 0.8628 (0.8118)	loss 1.2321 (1.2437)	loss-cls 9.8569 (9.9499)	loss-aux 0.0000 (0.0000)	grad_norm 0.2204 (inf)	loss_scale 32768.0000 (46458.4348)	mem 7601MB	batch_time 6.3502
[32m[2023-01-06 13:19:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1479/27342]	eta 5:49:54 lr 0.0009999991	 wd 0.1000	time 0.8336 (0.8117)	loss 1.2236 (1.2437)	loss-cls 9.7888 (9.9495)	loss-aux 0.0000 (0.0000)	grad_norm 0.2161 (inf)	loss_scale 32768.0000 (46384.4324)	mem 7601MB	batch_time 6.3391
[32m[2023-01-06 13:19:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1487/27342]	eta 5:49:43 lr 0.0009999991	 wd 0.1000	time 0.8389 (0.8116)	loss 1.2547 (1.2437)	loss-cls 10.0378 (9.9493)	loss-aux 0.0000 (0.0000)	grad_norm 0.2329 (inf)	loss_scale 32768.0000 (46311.2258)	mem 7601MB	batch_time 6.2845
[32m[2023-01-06 13:19:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1495/27342]	eta 5:49:34 lr 0.0009999991	 wd 0.1000	time 0.8677 (0.8115)	loss 1.2440 (1.2436)	loss-cls 9.9520 (9.9491)	loss-aux 0.0000 (0.0000)	grad_norm 0.2327 (inf)	loss_scale 32768.0000 (46238.8021)	mem 7601MB	batch_time 6.3322
[32m[2023-01-06 13:19:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1503/27342]	eta 5:49:24 lr 0.0009999991	 wd 0.1000	time 0.8496 (0.8113)	loss 1.2377 (1.2436)	loss-cls 9.9020 (9.9488)	loss-aux 0.0000 (0.0000)	grad_norm 0.2195 (inf)	loss_scale 32768.0000 (46167.1489)	mem 7601MB	batch_time 6.2605
[32m[2023-01-06 13:19:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1511/27342]	eta 5:49:13 lr 0.0009999991	 wd 0.1000	time 0.8583 (0.8112)	loss 1.2411 (1.2435)	loss-cls 9.9286 (9.9482)	loss-aux 0.0000 (0.0000)	grad_norm 0.2295 (inf)	loss_scale 32768.0000 (46096.2540)	mem 7601MB	batch_time 6.2274
[32m[2023-01-06 13:20:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1519/27342]	eta 5:49:03 lr 0.0009999991	 wd 0.1000	time 0.8447 (0.8110)	loss 1.2471 (1.2435)	loss-cls 9.9769 (9.9481)	loss-aux 0.0000 (0.0000)	grad_norm 0.2174 (inf)	loss_scale 32768.0000 (46026.1053)	mem 7601MB	batch_time 6.3087
[32m[2023-01-06 13:20:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1527/27342]	eta 5:48:54 lr 0.0009999991	 wd 0.1000	time 0.8422 (0.8110)	loss 1.2257 (1.2435)	loss-cls 9.8058 (9.9478)	loss-aux 0.0000 (0.0000)	grad_norm 0.2539 (inf)	loss_scale 32768.0000 (45956.6911)	mem 7601MB	batch_time 6.3442
[32m[2023-01-06 13:20:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1535/27342]	eta 5:48:45 lr 0.0009999991	 wd 0.1000	time 0.8532 (0.8108)	loss 1.2340 (1.2434)	loss-cls 9.8722 (9.9474)	loss-aux 0.0000 (0.0000)	grad_norm 0.2224 (inf)	loss_scale 32768.0000 (45888.0000)	mem 7601MB	batch_time 6.3050
[32m[2023-01-06 13:20:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1543/27342]	eta 5:48:34 lr 0.0009999990	 wd 0.1000	time 0.8431 (0.8107)	loss 1.2170 (1.2434)	loss-cls 9.7356 (9.9473)	loss-aux 0.0000 (0.0000)	grad_norm 0.2646 (inf)	loss_scale 32768.0000 (45820.0207)	mem 7601MB	batch_time 6.2213
[32m[2023-01-06 13:20:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1551/27342]	eta 5:48:23 lr 0.0009999990	 wd 0.1000	time 0.8358 (0.8105)	loss 1.2221 (1.2433)	loss-cls 9.7767 (9.9467)	loss-aux 0.0000 (0.0000)	grad_norm 0.2643 (inf)	loss_scale 32768.0000 (45752.7423)	mem 7601MB	batch_time 6.2424
[32m[2023-01-06 13:20:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1559/27342]	eta 5:48:14 lr 0.0009999990	 wd 0.1000	time 0.8440 (0.8104)	loss 1.2315 (1.2433)	loss-cls 9.8517 (9.9461)	loss-aux 0.0000 (0.0000)	grad_norm 0.3007 (inf)	loss_scale 32768.0000 (45686.1538)	mem 7601MB	batch_time 6.3012
[32m[2023-01-06 13:20:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1567/27342]	eta 5:48:03 lr 0.0009999990	 wd 0.1000	time 0.8505 (0.8102)	loss 1.2386 (1.2432)	loss-cls 9.9088 (9.9457)	loss-aux 0.0000 (0.0000)	grad_norm 0.2475 (inf)	loss_scale 32768.0000 (45620.2449)	mem 7601MB	batch_time 6.2489
[32m[2023-01-06 13:20:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1575/27342]	eta 5:47:53 lr 0.0009999990	 wd 0.1000	time 0.8492 (0.8101)	loss 1.2246 (1.2432)	loss-cls 9.7965 (9.9457)	loss-aux 0.0000 (0.0000)	grad_norm 0.2874 (inf)	loss_scale 32768.0000 (45555.0051)	mem 7601MB	batch_time 6.2406
[32m[2023-01-06 13:20:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1583/27342]	eta 5:47:41 lr 0.0009999990	 wd 0.1000	time 0.8294 (0.8099)	loss 1.2276 (1.2431)	loss-cls 9.8205 (9.9450)	loss-aux 0.0000 (0.0000)	grad_norm 0.2523 (inf)	loss_scale 32768.0000 (45490.4242)	mem 7601MB	batch_time 6.1655
[32m[2023-01-06 13:21:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1591/27342]	eta 5:47:30 lr 0.0009999990	 wd 0.1000	time 0.8399 (0.8097)	loss 1.2138 (1.2431)	loss-cls 9.7100 (9.9445)	loss-aux 0.0000 (0.0000)	grad_norm 0.2303 (inf)	loss_scale 32768.0000 (45426.4925)	mem 7601MB	batch_time 6.1626
[32m[2023-01-06 13:21:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1599/27342]	eta 5:47:18 lr 0.0009999990	 wd 0.1000	time 0.7982 (0.8095)	loss 1.2279 (1.2430)	loss-cls 9.8230 (9.9442)	loss-aux 0.0000 (0.0000)	grad_norm 0.2352 (inf)	loss_scale 32768.0000 (45363.2000)	mem 7601MB	batch_time 6.1571
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_32625.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7411.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2852.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15455.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28595.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8873.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_21756.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22698.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12064.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13516.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25408.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4524.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12757.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11827.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10586.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_1796.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6247.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6710.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n06470073/n06470073_47249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_15341.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8925.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7728.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_34297.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22166.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10175248/n10175248_583.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25530.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25750.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5051.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7013.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03957420/n03957420_33553.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10675.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17471.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_135.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22980.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9819.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5664.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22719.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12740.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_16737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13244.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2040.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2322.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8812.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13641.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13103.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33259.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27578.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27412.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19281.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15697.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7953.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9068.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30043.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13306.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22995.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8783.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10242.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27627.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_16320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9215.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7996.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7072.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6567.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6236.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9041.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12364.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9031.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29695.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14020.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25717.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_4539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_9566.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15811.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_16430.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_7026.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13261.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29025.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_11746.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n02368116/n02368116_318.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9401.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22581.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8806.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20118.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6669.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13396.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4456.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11766.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33630.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15830.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27296.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30926.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6850.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19005.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8645.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27611.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_14701.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20180.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12231.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17877.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7365.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11816.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_26924.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12108.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9282.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27221.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24638.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_23316.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14718.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_1914.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24434.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18729.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24544.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_10353.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12206.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13871.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15480.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18350.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_3493.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27317.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33623.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28288.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13419.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12142.JPEG
[32m[2023-01-06 13:21:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 282)[0m: INFO EPOCH 0 training takes 0:21:36
