+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ scontrol show JobId=102156
++ scontrol show JobId=102156
++ scontrol show JobId=102156
++ grep BatchHost
++ scontrol show JobId=102156
++ grep BatchHost
++ scontrol show JobId=102156
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=102156
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=102156
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ scontrol show JobId=102156
++ scontrol show JobId=102156
++ grep BatchHost
++ scontrol show JobId=102156
++ scontrol show JobId=102156
++ scontrol show JobId=102156
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=102156
++ tr = ' '
++ scontrol show JobId=102156
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ grep BatchHost
++ scontrol show JobId=102156
++ scontrol show JobId=102156
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=102156
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=32
+ export MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ WORLD_SIZE=32
+ localrank=7
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export NNODES=4
+ NNODES=4
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export RANK=4
+ RANK=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export NNODES=4
+ NNODES=4
+ '[' nico == nico ']'
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MASTER_ADDR=nico1
+ MAX_JOBS=64
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=0
+ RANK=0
+ export NNODES=4
+ NNODES=4
+ export MASTER_ADDR=nico1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ export NODE_RANK=1
+ NODE_RANK=1
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=4
+ NNODES=4
+ true
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export RANK=14
+ RANK=14
+ export NNODES=4
+ NNODES=4
+ cd /home/zms/model_training/MoE/FastSwin
+ export NODE_RANK=0
+ NODE_RANK=0
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ MAX_JOBS=64
+ DATASET_PREFIX=/mnt/znvme/zms
+ localrank=6
+ export MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ MASTER_ADDR=nico1
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export RANK=1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export NNODES=4
+ NNODES=4
+ true
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ cd /home/zms/model_training/MoE/FastSwin
+ MAX_JOBS=64
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ export RANK=15
+ RANK=15
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ export RANK=10
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ RANK=10
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ '[' srun == srun ']'
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ RANK=1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export RANK=6
+ RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ NNODES=4
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' nico == sh-lab ']'
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
++ scontrol show JobId=102156
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
++ grep BatchHost
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=4
+ NNODES=4
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
++ tr = ' '
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ '[' EP+DP == EP+DP ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
++ awk '{print $2}'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ scontrol show JobId=102156
++ grep BatchHost
++ scontrol show JobId=102156
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=18
+ RANK=18
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=23
+ RANK=23
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=19
+ RANK=19
+ export RANK=21
+ RANK=21
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ NODE_RANK=4
+ export MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args=
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ true
+ EXEC=./main_moe.py
+ false
+ true
+ false
+ true
+ false
+ EXEC=./main_moe.py
+ EXEC=./main_moe.py
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=20
+ RANK=20
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=31
+ RANK=31
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export RANK=28
+ RANK=28
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ localrank=4
+ export RANK=29
+ RANK=29
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export NNODES=4
+ NNODES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=7
+ NODE_RANK=7
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ MAX_JOBS=64
+ DATASET_PREFIX=/mnt/znvme/zms
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ true
+ DATASET_PREFIX=/mnt/znvme/zms
+ cd /home/zms/model_training/MoE/FastSwin
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=30
+ RANK=30
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
++ scontrol show JobId=102156
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
++ grep BatchHost
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
++ tr = ' '
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
++ awk '{print $2}'
+ python_args=
+ python_args=
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ false
+ true
+ false
+ '[' OFF == ON ']'
+ EXEC=./main_moe.py
+ true
+ EXEC=./main_moe.py
+ false
+ true
+ EXEC=./main_moe.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=22
+ RANK=22
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=27
+ RANK=27
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=24
+ RANK=24
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=16
+ RANK=16
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
++ scontrol show JobId=102156
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=26
+ RANK=26
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ '[' srun == srun ']'
+ '[' nico == sh-lab ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=17
+ RANK=17
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
++ scontrol show JobId=102156
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=25
+ RANK=25
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' nico == nico ']'
+ CODE_PREFIX=/home/zms/model_training
+ DATASET_PREFIX=/mnt/znvme/zms
+ true
+ cd /home/zms/model_training/MoE/FastSwin
+ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=20520960
+ DUMP_FILE=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 1         --balance-strategy naive         --gshard-cap 4.8         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers -1         --hidden-size -1         --num-attention-heads -1         --seq-length -1         --max-position-embeddings -1         --micro-batch-size 16         --global-batch-size 4096         --train-samples 20520960 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 32                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ false
+ true
+ EXEC=./main_moe.py
+ python_args='--local_rank 0 
                --init-method-std 0.002                 --output /home/zms/model_training/Auto-Megatron/logs/test_swin 
                --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml 
                --fused_window_process 
                --data-path /mnt/znvme/dataset/imagenet22k 
                --batch-size 16
                --accumulation-steps 8
                --balance-strategy naive
                --gshard-cap 4.8
                --expert-parallel-strategy EP+DP
                --num-experts 1 
                --expert-ep-size 32
                --expert-dp-size 1 '
+ false
+ echo ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
+ USE_MEGATRON=0
+ PROFILER_LOG_PATH=/home/zms/model_training/Auto-Megatron/logs/test_swin/Swin-MoE-B-32MoE_naive4.8_EP+DP_t1_p1_d32_ep32_dp1_totalE32_localE1_gbs4096_mbs16_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2023-01-04T18:49:46+08:00.prof
+ DEBUG=OFF
+ exec python3 ./main_moe.py --local_rank 0 --init-method-std 0.002 --output /home/zms/model_training/Auto-Megatron/logs/test_swin --cfg configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml --fused_window_process --data-path /mnt/znvme/dataset/imagenet22k --batch-size 16 --accumulation-steps 8 --balance-strategy naive --gshard-cap 4.8 --expert-parallel-strategy EP+DP --num-experts 1 --expert-ep-size 32 --expert-dp-size 1
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 0/32
[32m[2023-01-04 18:50:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 403)[0m: INFO Full config saved to /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default/config.json
[32m[2023-01-04 18:50:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 406)[0m: INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 16
  CACHE_MODE: part
  DATASET: imagenet22K
  DATA_PATH: /mnt/znvme/dataset/imagenet22k
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: true
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.3
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    GATE_NOISE: 1.0
    INIT_STD: 0.005
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: false
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - 1
      - 3
      - 5
      - 7
      - 9
      - 11
      - 13
      - 15
      - 17
    - - 1
    MOE_DROP: 0.1
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_LOCAL_EXPERTS: 2
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 2
    USE_BPR: true
    WINDOW_SIZE: 12
  TYPE: swin_fastmoe
OUTPUT: /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default
PRINT_FREQ: 1
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 8
  AUTO_RESUME: true
  BASE_LR: 0.001
  CHECKPOINT_MODE: full
  CLIP_GRAD: 3.0
  EPOCHS: 90
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-05
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 1.0e-06
  WEIGHT_DECAY: 0.1

[32m[2023-01-04 18:50:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 407)[0m: INFO {"cfg": "configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml", "opts": null, "batch_size": 16, "data_path": "/mnt/znvme/dataset/imagenet22k", "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": 8, "use_checkpoint": false, "checkpoint_mode": "full", "disable_amp": false, "amp_opt_level": null, "output": "/home/zms/model_training/Auto-Megatron/logs/test_swin", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": true, "fused_layernorm": false, "optim": null, "num_experts": 1, "top_k": 2, "balance_strategy": "naive", "expert_parallel_strategy": "EP+DP", "expert_ep_size": 32, "expert_dp_size": 1, "dump": false, "dynamic_placement": false, "dynamic_freq": 10, "new_shadow": false, "gshard_cap": 4.8, "init_method_std": 0.002, "num_layers": 12}
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
local rank 0 / global rank 0 successfully build train dataset
local rank 0 / global rank 0 successfully build val dataset
[32m[2023-01-04 18:50:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 120)[0m: INFO Creating model:swin_fastmoe/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/zms/.local/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 26/32
local rank 0 / global rank 26 successfully build train dataset
local rank 0 / global rank 26 successfully build val dataset
[INFO] 26 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 26 in DP group [26]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 17/32
local rank 0 / global rank 17 successfully build train dataset
local rank 0 / global rank 17 successfully build val dataset
[INFO] 17 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 17 in DP group [17]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 0 in DP group [0]
[32m[2023-01-04 18:51:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 122)[0m: INFO SwinTransformerFastMoE(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(48, 48), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(48, 48), num_heads=4, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=128, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=4
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=4, bias=False)
            )
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(48, 48), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(24, 24), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(24, 24), num_heads=8, window_size=12, shift_size=6, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=256, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=8
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=8, bias=False)
            )
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(24, 24), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(12, 12), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(12, 12), num_heads=16, window_size=12, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=512, window_size=(12, 12), pretrained_window_size=(0, 0), num_heads=16
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=16, bias=False)
            )
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 512, hidden_features = 2048, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=2048,         out_features=4096, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=4096,         out_features=2048, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=2048, out_features=32, bias=True)
              )
            )
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(12, 12), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(6, 6), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=4096, out_features=1024, bias=False)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(6, 6), num_heads=32, window_size=6, shift_size=0, mlp_ratio=4.0
          (norm1): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (attn): WindowAttention(
            dim=1024, window_size=(6, 6), pretrained_window_size=(0, 0), num_heads=32
            (cpb_mlp): Sequential(
              (0): Linear(in_features=2, out_features=512, bias=True)
              (1): ReLU(inplace=True)
              (2): Linear(in_features=512, out_features=32, bias=False)
            )
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): TimerModule(
            (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (mlp): MoEMlp(
            [Statistics-0] param count for MoE, in_features = 1024, hidden_features = 4096, num_local_experts = 2, top_value = 2, cosine_router=False normalize_gate=False, use_bpr = True
            (_dropout): Dropout(p=0.1, inplace=False)
            (_moe_layer): MegatronMLP(
              (experts): ModuleList(
                (0): _Expert(
                  (htoh4): FMoELinear(num_expert=1, in_features=4096,         out_features=8192, bias=True, rank=0)
                  (h4toh): FMoELinear(num_expert=1, in_features=8192,         out_features=4096, bias=True, rank=0)
                  (activation): GELU(approximate=none)
                )
              )
              (gate): NaiveGate(
                (gate): Linear(in_features=4096, out_features=32, bias=True)
              )
            )
          )
        )
      )
    )
  )
  (norm): TimerModule(
    (model): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (avgpool): TimerModule(
    (model): AdaptiveAvgPool1d(output_size=1)
  )
  (head): TimerModule(
    (model): Linear(in_features=1024, out_features=21841, bias=True)
  )
)
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-04 18:51:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 156)[0m: INFO no checkpoint found in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default, ignoring auto resume
[32m[2023-01-04 18:51:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 176)[0m: INFO Start training
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 30/32
local rank 0 / global rank 30 successfully build train dataset
local rank 0 / global rank 30 successfully build val dataset
[INFO] 30 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 30 in DP group [30]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 5/32
local rank 0 / global rank 5 successfully build train dataset
local rank 0 / global rank 5 successfully build val dataset
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 5 in DP group [5]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 10/32
local rank 0 / global rank 10 successfully build train dataset
local rank 0 / global rank 10 successfully build val dataset
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 10 in DP group [10]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 29/32
local rank 0 / global rank 29 successfully build train dataset
local rank 0 / global rank 29 successfully build val dataset
[INFO] 29 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 29 in DP group [29]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 1/32
local rank 0 / global rank 1 successfully build train dataset
local rank 0 / global rank 1 successfully build val dataset
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 1 in DP group [1]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 12/32
local rank 0 / global rank 12 successfully build train dataset
local rank 0 / global rank 12 successfully build val dataset
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 12 in DP group [12]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 24/32
local rank 0 / global rank 24 successfully build train dataset
local rank 0 / global rank 24 successfully build val dataset
[INFO] 24 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 24 in DP group [24]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 20/32
local rank 0 / global rank 20 successfully build train dataset
local rank 0 / global rank 20 successfully build val dataset
[INFO] 20 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 20 in DP group [20]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 27/32
local rank 0 / global rank 27 successfully build train dataset
local rank 0 / global rank 27 successfully build val dataset
[INFO] 27 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 27 in DP group [27]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 25/32
local rank 0 / global rank 25 successfully build train dataset
local rank 0 / global rank 25 successfully build val dataset
[INFO] 25 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 25 in DP group [25]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 16/32
local rank 0 / global rank 16 successfully build train dataset
local rank 0 / global rank 16 successfully build val dataset
[INFO] 16 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 16 in DP group [16]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 4/32
local rank 0 / global rank 4 successfully build train dataset
local rank 0 / global rank 4 successfully build val dataset
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 4 in DP group [4]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 31/32
local rank 0 / global rank 31 successfully build train dataset
local rank 0 / global rank 31 successfully build val dataset
[INFO] 31 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 31 in DP group [31]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 19/32
local rank 0 / global rank 19 successfully build train dataset
local rank 0 / global rank 19 successfully build val dataset
[INFO] 19 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 19 in DP group [19]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 6/32
local rank 0 / global rank 6 successfully build train dataset
local rank 0 / global rank 6 successfully build val dataset
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 6 in DP group [6]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 28/32
local rank 0 / global rank 28 successfully build train dataset
local rank 0 / global rank 28 successfully build val dataset
[INFO] 28 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 28 in DP group [28]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 15/32
local rank 0 / global rank 15 successfully build train dataset
local rank 0 / global rank 15 successfully build val dataset
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 15 in DP group [15]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 3/32
local rank 0 / global rank 3 successfully build train dataset
local rank 0 / global rank 3 successfully build val dataset
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 3 in DP group [3]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 11/32
local rank 0 / global rank 11 successfully build train dataset
local rank 0 / global rank 11 successfully build val dataset
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 11 in DP group [11]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 21/32
local rank 0 / global rank 21 successfully build train dataset
local rank 0 / global rank 21 successfully build val dataset
[INFO] 21 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 21 in DP group [21]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 22/32
local rank 0 / global rank 22 successfully build train dataset
local rank 0 / global rank 22 successfully build val dataset
[INFO] 22 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 22 in DP group [22]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 13/32
local rank 0 / global rank 13 successfully build train dataset
local rank 0 / global rank 13 successfully build val dataset
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 13 in DP group [13]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 7/32
local rank 0 / global rank 7 successfully build train dataset
local rank 0 / global rank 7 successfully build val dataset
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 7 in DP group [7]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 14/32
local rank 0 / global rank 14 successfully build train dataset
local rank 0 / global rank 14 successfully build val dataset
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 14 in DP group [14]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 2/32
local rank 0 / global rank 2 successfully build train dataset
local rank 0 / global rank 2 successfully build val dataset
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 2 in DP group [2]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 18/32
local rank 0 / global rank 18 successfully build train dataset
local rank 0 / global rank 18 successfully build val dataset
[INFO] 18 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 18 in DP group [18]
[WARNING] world comm group not exist!
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 23/32
local rank 0 / global rank 23 successfully build train dataset
local rank 0 / global rank 23 successfully build val dataset
[INFO] 23 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 23 in DP group [23]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 9/32
local rank 0 / global rank 9 successfully build train dataset
local rank 0 / global rank 9 successfully build val dataset
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 9 in DP group [9]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
=> merge config from configs/swinmoe/swin_moe_base_patch4_window12_192_32expert_32gpu_22k.yaml
RANK and WORLD_SIZE in environ: 8/32
local rank 0 / global rank 8 successfully build train dataset
local rank 0 / global rank 8 successfully build val dataset
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 8 in DP group [8]
[WARNING] world comm group not exist!
All master checkpoints founded in /home/zms/model_training/Auto-Megatron/logs/test_swin/swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k/default: []
[32m[2023-01-04 18:51:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][7/27342]	eta 1 day, 6:57:24 lr 0.0010000000	 wd 0.1000	time 1.0095 (4.0770)	loss 1.2463 (1.2481)	loss-cls 9.9703 (9.9847)	loss-aux 0.0000 (0.0000)	grad_norm 3.0917 (3.0917)	loss_scale 65536.0000 (65536.0000)	mem 5198MB	batch_time 32.6160
[32m[2023-01-04 18:51:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][15/27342]	eta 18:34:01 lr 0.0010000000	 wd 0.1000	time 0.8979 (2.4460)	loss 1.2504 (1.2491)	loss-cls 10.0035 (9.9926)	loss-aux 0.0000 (0.0000)	grad_norm 2.2229 (2.6573)	loss_scale 65536.0000 (65536.0000)	mem 7523MB	batch_time 6.5198
[32m[2023-01-04 18:51:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][23/27342]	eta 14:24:03 lr 0.0010000000	 wd 0.1000	time 0.8780 (1.8977)	loss 1.2530 (1.2495)	loss-cls 10.0244 (9.9963)	loss-aux 0.0000 (0.0000)	grad_norm 2.0032 (2.4393)	loss_scale 65536.0000 (65536.0000)	mem 7523MB	batch_time 6.4094
[32m[2023-01-04 18:51:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][31/27342]	eta 12:19:23 lr 0.0010000000	 wd 0.1000	time 0.8326 (1.6244)	loss 1.2464 (1.2498)	loss-cls 9.9709 (9.9981)	loss-aux 0.0000 (0.0000)	grad_norm 1.8383 (2.2890)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4354
[32m[2023-01-04 18:52:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][39/27342]	eta 11:06:14 lr 0.0010000000	 wd 0.1000	time 0.9012 (1.4641)	loss 1.2533 (1.2500)	loss-cls 10.0263 (10.0003)	loss-aux 0.0000 (0.0000)	grad_norm 1.7754 (2.1863)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.5838
[32m[2023-01-04 18:52:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][47/27342]	eta 10:16:18 lr 0.0010000000	 wd 0.1000	time 0.8590 (1.3548)	loss 1.2442 (1.2498)	loss-cls 9.9536 (9.9981)	loss-aux 0.0000 (0.0000)	grad_norm 1.9809 (2.1521)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4643
[32m[2023-01-04 18:52:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][55/27342]	eta 9:41:02 lr 0.0010000000	 wd 0.1000	time 0.9011 (1.2776)	loss 1.2543 (1.2501)	loss-cls 10.0348 (10.0012)	loss-aux 0.0000 (0.0000)	grad_norm 2.2364 (2.1641)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.5186
[32m[2023-01-04 18:52:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][63/27342]	eta 9:13:50 lr 0.0010000000	 wd 0.1000	time 0.8842 (1.2182)	loss 1.2588 (1.2504)	loss-cls 10.0707 (10.0034)	loss-aux 0.0000 (0.0000)	grad_norm 2.1419 (2.1613)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4149
[32m[2023-01-04 18:52:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][71/27342]	eta 8:52:28 lr 0.0010000000	 wd 0.1000	time 0.8865 (1.1715)	loss 1.2587 (1.2507)	loss-cls 10.0696 (10.0055)	loss-aux 0.0000 (0.0000)	grad_norm 2.0256 (2.1463)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3885
[32m[2023-01-04 18:52:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][79/27342]	eta 8:35:33 lr 0.0010000000	 wd 0.1000	time 0.8481 (1.1346)	loss 1.2516 (1.2509)	loss-cls 10.0131 (10.0074)	loss-aux 0.0000 (0.0000)	grad_norm 1.8557 (2.1172)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4194
[32m[2023-01-04 18:52:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][87/27342]	eta 8:22:04 lr 0.0010000000	 wd 0.1000	time 0.8795 (1.1053)	loss 1.2517 (1.2511)	loss-cls 10.0135 (10.0090)	loss-aux 0.0000 (0.0000)	grad_norm 1.7750 (2.0861)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4933
[32m[2023-01-04 18:52:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][95/27342]	eta 8:10:40 lr 0.0010000000	 wd 0.1000	time 0.8689 (1.0805)	loss 1.2530 (1.2510)	loss-cls 10.0240 (10.0080)	loss-aux 0.0000 (0.0000)	grad_norm 1.7674 (2.0595)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4646
[32m[2023-01-04 18:52:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][103/27342]	eta 8:01:13 lr 0.0010000000	 wd 0.1000	time 0.8817 (1.0600)	loss 1.2608 (1.2510)	loss-cls 10.0861 (10.0079)	loss-aux 0.0000 (0.0000)	grad_norm 1.5734 (2.0221)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.5113
[32m[2023-01-04 18:53:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][111/27342]	eta 7:52:46 lr 0.0010000000	 wd 0.1000	time 0.8621 (1.0417)	loss 1.2401 (1.2508)	loss-cls 9.9209 (10.0061)	loss-aux 0.0000 (0.0000)	grad_norm 1.6350 (1.9945)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4317
[32m[2023-01-04 18:53:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][119/27342]	eta 7:45:28 lr 0.0010000000	 wd 0.1000	time 0.8847 (1.0259)	loss 1.2491 (1.2510)	loss-cls 9.9931 (10.0083)	loss-aux 0.0000 (0.0000)	grad_norm 1.6955 (1.9746)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4380
[32m[2023-01-04 18:53:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][127/27342]	eta 7:38:25 lr 0.0010000000	 wd 0.1000	time 0.8504 (1.0107)	loss 1.2476 (1.2508)	loss-cls 9.9811 (10.0068)	loss-aux 0.0000 (0.0000)	grad_norm 1.6906 (1.9568)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2575
[32m[2023-01-04 18:53:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][135/27342]	eta 7:32:14 lr 0.0010000000	 wd 0.1000	time 0.8317 (0.9973)	loss 1.2494 (1.2507)	loss-cls 9.9955 (10.0059)	loss-aux 0.0000 (0.0000)	grad_norm 1.7983 (1.9475)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2724
[32m[2023-01-04 18:53:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][143/27342]	eta 7:26:58 lr 0.0010000000	 wd 0.1000	time 0.8572 (0.9860)	loss 1.2515 (1.2508)	loss-cls 10.0121 (10.0061)	loss-aux 0.0000 (0.0000)	grad_norm 1.5845 (1.9273)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3454
[32m[2023-01-04 18:53:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][151/27342]	eta 7:22:10 lr 0.0010000000	 wd 0.1000	time 0.8509 (0.9757)	loss 1.2481 (1.2508)	loss-cls 9.9851 (10.0061)	loss-aux 0.0000 (0.0000)	grad_norm 1.5024 (1.9050)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3223
[32m[2023-01-04 18:53:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][159/27342]	eta 7:17:53 lr 0.0010000000	 wd 0.1000	time 0.8935 (0.9665)	loss 1.2610 (1.2508)	loss-cls 10.0876 (10.0067)	loss-aux 0.0000 (0.0000)	grad_norm 3.3636 (1.9779)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3401
[32m[2023-01-04 18:53:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][167/27342]	eta 7:14:08 lr 0.0010000000	 wd 0.1000	time 0.8666 (0.9586)	loss 1.2635 (1.2510)	loss-cls 10.1080 (10.0076)	loss-aux 0.0000 (0.0000)	grad_norm 1.7000 (1.9647)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3908
[32m[2023-01-04 18:53:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][175/27342]	eta 7:10:42 lr 0.0010000000	 wd 0.1000	time 0.8573 (0.9513)	loss 1.2547 (1.2511)	loss-cls 10.0377 (10.0087)	loss-aux 0.0000 (0.0000)	grad_norm 1.5707 (1.9467)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3826
[32m[2023-01-04 18:54:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][183/27342]	eta 7:07:30 lr 0.0010000000	 wd 0.1000	time 0.8606 (0.9444)	loss 1.2298 (1.2511)	loss-cls 9.8382 (10.0086)	loss-aux 0.0000 (0.0000)	grad_norm 1.5233 (1.9283)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3579
[32m[2023-01-04 18:54:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][191/27342]	eta 7:04:32 lr 0.0010000000	 wd 0.1000	time 0.8669 (0.9382)	loss 1.2370 (1.2511)	loss-cls 9.8959 (10.0088)	loss-aux 0.0000 (0.0000)	grad_norm 1.5073 (1.9108)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3515
[32m[2023-01-04 18:54:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][199/27342]	eta 7:01:58 lr 0.0010000000	 wd 0.1000	time 0.8833 (0.9328)	loss 1.2502 (1.2511)	loss-cls 10.0017 (10.0087)	loss-aux 0.0000 (0.0000)	grad_norm 1.6446 (1.9001)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4266
[32m[2023-01-04 18:54:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][207/27342]	eta 6:59:40 lr 0.0010000000	 wd 0.1000	time 0.8972 (0.9280)	loss 1.2665 (1.2511)	loss-cls 10.1320 (10.0091)	loss-aux 0.0000 (0.0000)	grad_norm 1.6826 (1.8918)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4627
[32m[2023-01-04 18:54:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][215/27342]	eta 6:57:15 lr 0.0010000000	 wd 0.1000	time 0.8530 (0.9229)	loss 1.2491 (1.2514)	loss-cls 9.9931 (10.0114)	loss-aux 0.0000 (0.0000)	grad_norm 1.6876 (1.8842)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3301
[32m[2023-01-04 18:54:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][223/27342]	eta 6:55:19 lr 0.0010000000	 wd 0.1000	time 0.9040 (0.9189)	loss 1.2329 (1.2514)	loss-cls 9.8629 (10.0110)	loss-aux 0.0000 (0.0000)	grad_norm 1.7338 (1.8788)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4821
[32m[2023-01-04 18:54:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][231/27342]	eta 6:53:13 lr 0.0010000000	 wd 0.1000	time 0.8676 (0.9145)	loss 1.2579 (1.2514)	loss-cls 10.0633 (10.0116)	loss-aux 0.0000 (0.0000)	grad_norm 1.7405 (1.8741)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3398
[32m[2023-01-04 18:54:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][239/27342]	eta 6:51:21 lr 0.0010000000	 wd 0.1000	time 0.8844 (0.9107)	loss 1.2506 (1.2517)	loss-cls 10.0046 (10.0134)	loss-aux 0.0000 (0.0000)	grad_norm 1.7479 (1.8699)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3901
[32m[2023-01-04 18:54:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][247/27342]	eta 6:49:39 lr 0.0010000000	 wd 0.1000	time 0.8726 (0.9072)	loss 1.2545 (1.2519)	loss-cls 10.0357 (10.0149)	loss-aux 0.0000 (0.0000)	grad_norm 1.7649 (1.8665)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4123
[32m[2023-01-04 18:54:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][255/27342]	eta 6:47:59 lr 0.0010000000	 wd 0.1000	time 0.8851 (0.9037)	loss 1.2670 (1.2519)	loss-cls 10.1361 (10.0152)	loss-aux 0.0000 (0.0000)	grad_norm 1.8238 (1.8651)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3823
[32m[2023-01-04 18:55:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][263/27342]	eta 6:46:26 lr 0.0010000000	 wd 0.1000	time 0.8854 (0.9006)	loss 1.2482 (1.2518)	loss-cls 9.9854 (10.0146)	loss-aux 0.0000 (0.0000)	grad_norm 1.8458 (1.8646)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3933
[32m[2023-01-04 18:55:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][271/27342]	eta 6:45:07 lr 0.0010000000	 wd 0.1000	time 0.8839 (0.8979)	loss 1.2378 (1.2517)	loss-cls 9.9027 (10.0140)	loss-aux 0.0000 (0.0000)	grad_norm 1.5668 (1.8558)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4806
[32m[2023-01-04 18:55:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][279/27342]	eta 6:43:41 lr 0.0010000000	 wd 0.1000	time 0.8752 (0.8950)	loss 1.2703 (1.2518)	loss-cls 10.1621 (10.0141)	loss-aux 0.0000 (0.0000)	grad_norm 1.7042 (1.8515)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3711
[32m[2023-01-04 18:55:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][287/27342]	eta 6:42:23 lr 0.0010000000	 wd 0.1000	time 0.8603 (0.8924)	loss 1.2506 (1.2518)	loss-cls 10.0045 (10.0142)	loss-aux 0.0000 (0.0000)	grad_norm 1.6481 (1.8458)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4022
[32m[2023-01-04 18:55:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][295/27342]	eta 6:41:15 lr 0.0010000000	 wd 0.1000	time 0.8596 (0.8901)	loss 1.2594 (1.2518)	loss-cls 10.0755 (10.0142)	loss-aux 0.0000 (0.0000)	grad_norm 1.5581 (1.8380)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4774
[32m[2023-01-04 18:55:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][303/27342]	eta 6:40:05 lr 0.0010000000	 wd 0.1000	time 0.8714 (0.8878)	loss 1.2603 (1.2516)	loss-cls 10.0826 (10.0131)	loss-aux 0.0000 (0.0000)	grad_norm 1.6406 (1.8329)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4194
[32m[2023-01-04 18:55:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][311/27342]	eta 6:39:05 lr 0.0010000000	 wd 0.1000	time 0.8575 (0.8858)	loss 1.2533 (1.2515)	loss-cls 10.0261 (10.0118)	loss-aux 0.0000 (0.0000)	grad_norm 1.5351 (1.8252)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4824
[32m[2023-01-04 18:55:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][319/27342]	eta 6:38:00 lr 0.0010000000	 wd 0.1000	time 0.8612 (0.8837)	loss 1.2557 (1.2515)	loss-cls 10.0456 (10.0123)	loss-aux 0.0000 (0.0000)	grad_norm 1.7487 (1.8233)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4075
[32m[2023-01-04 18:55:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][327/27342]	eta 6:37:00 lr 0.0010000000	 wd 0.1000	time 0.8542 (0.8818)	loss 1.2607 (1.2516)	loss-cls 10.0853 (10.0129)	loss-aux 0.0000 (0.0000)	grad_norm 1.7443 (1.8214)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4286
[32m[2023-01-04 18:56:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][335/27342]	eta 6:36:06 lr 0.0010000000	 wd 0.1000	time 0.8643 (0.8800)	loss 1.2557 (1.2518)	loss-cls 10.0460 (10.0143)	loss-aux 0.0000 (0.0000)	grad_norm 1.8327 (1.8216)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4718
[32m[2023-01-04 18:56:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][343/27342]	eta 6:35:15 lr 0.0010000000	 wd 0.1000	time 0.8887 (0.8784)	loss 1.2558 (1.2519)	loss-cls 10.0465 (10.0149)	loss-aux 0.0000 (0.0000)	grad_norm 1.3171 (1.8099)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4723
[32m[2023-01-04 18:56:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][351/27342]	eta 6:34:24 lr 0.0010000000	 wd 0.1000	time 0.8591 (0.8767)	loss 1.2550 (1.2518)	loss-cls 10.0397 (10.0147)	loss-aux 0.0000 (0.0000)	grad_norm 2.5457 (1.8266)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4513
[32m[2023-01-04 18:56:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][359/27342]	eta 6:33:37 lr 0.0009999999	 wd 0.1000	time 0.8996 (0.8753)	loss 1.2285 (1.2517)	loss-cls 9.8279 (10.0138)	loss-aux 0.0000 (0.0000)	grad_norm 1.1798 (1.8123)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4851
[32m[2023-01-04 18:56:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][367/27342]	eta 6:32:53 lr 0.0009999999	 wd 0.1000	time 0.8961 (0.8739)	loss 1.2644 (1.2517)	loss-cls 10.1154 (10.0132)	loss-aux 0.0000 (0.0000)	grad_norm 1.1275 (1.7974)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.5006
[32m[2023-01-04 18:56:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][375/27342]	eta 6:32:11 lr 0.0009999999	 wd 0.1000	time 0.8478 (0.8726)	loss 1.2414 (1.2515)	loss-cls 9.9310 (10.0116)	loss-aux 0.0000 (0.0000)	grad_norm 2.9693 (1.8223)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.5033
[32m[2023-01-04 18:56:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][383/27342]	eta 6:31:24 lr 0.0009999999	 wd 0.1000	time 0.8500 (0.8711)	loss 1.2479 (1.2515)	loss-cls 9.9832 (10.0116)	loss-aux 0.0000 (0.0000)	grad_norm 1.3515 (1.8125)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4091
[32m[2023-01-04 18:56:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][391/27342]	eta 6:30:42 lr 0.0009999999	 wd 0.1000	time 0.8709 (0.8698)	loss 1.2485 (1.2514)	loss-cls 9.9882 (10.0111)	loss-aux 0.0000 (0.0000)	grad_norm 1.2205 (1.8004)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4511
[32m[2023-01-04 18:56:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][399/27342]	eta 6:30:00 lr 0.0009999999	 wd 0.1000	time 0.8793 (0.8685)	loss 1.2463 (1.2513)	loss-cls 9.9705 (10.0103)	loss-aux 0.0000 (0.0000)	grad_norm 1.4100 (1.7926)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4466
[32m[2023-01-04 18:57:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][407/27342]	eta 6:29:22 lr 0.0009999999	 wd 0.1000	time 0.8672 (0.8674)	loss 1.2304 (1.2513)	loss-cls 9.8429 (10.0105)	loss-aux 0.0000 (0.0000)	grad_norm 1.4398 (1.7857)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4698
[32m[2023-01-04 18:57:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][415/27342]	eta 6:28:40 lr 0.0009999999	 wd 0.1000	time 0.8728 (0.8661)	loss 1.2516 (1.2514)	loss-cls 10.0131 (10.0110)	loss-aux 0.0000 (0.0000)	grad_norm 1.5575 (1.7813)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3984
[32m[2023-01-04 18:57:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][423/27342]	eta 6:28:00 lr 0.0009999999	 wd 0.1000	time 0.8790 (0.8648)	loss 1.2570 (1.2513)	loss-cls 10.0562 (10.0107)	loss-aux 0.0000 (0.0000)	grad_norm 1.1558 (1.7695)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.4170
[32m[2023-01-04 18:57:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][431/27342]	eta 6:27:19 lr 0.0009999999	 wd 0.1000	time 0.8778 (0.8636)	loss 1.2447 (1.2514)	loss-cls 9.9574 (10.0109)	loss-aux 0.0000 (0.0000)	grad_norm 1.0487 (1.7562)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3654
[32m[2023-01-04 18:57:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][439/27342]	eta 6:26:34 lr 0.0009999999	 wd 0.1000	time 0.8388 (0.8622)	loss 1.2251 (1.2513)	loss-cls 9.8009 (10.0106)	loss-aux 0.0000 (0.0000)	grad_norm 1.2227 (1.7465)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2942
[32m[2023-01-04 18:57:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][447/27342]	eta 6:25:53 lr 0.0009999999	 wd 0.1000	time 0.8673 (0.8609)	loss 1.2581 (1.2514)	loss-cls 10.0650 (10.0114)	loss-aux 0.0000 (0.0000)	grad_norm 1.1789 (1.7363)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3166
[32m[2023-01-04 18:57:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][455/27342]	eta 6:25:12 lr 0.0009999999	 wd 0.1000	time 0.8609 (0.8596)	loss 1.2525 (1.2516)	loss-cls 10.0203 (10.0124)	loss-aux 0.0000 (0.0000)	grad_norm 1.2628 (1.7280)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3193
[32m[2023-01-04 18:57:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][463/27342]	eta 6:24:32 lr 0.0009999999	 wd 0.1000	time 0.8537 (0.8584)	loss 1.2423 (1.2516)	loss-cls 9.9387 (10.0130)	loss-aux 0.0000 (0.0000)	grad_norm 1.4221 (1.7227)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3082
[32m[2023-01-04 18:57:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][471/27342]	eta 6:23:52 lr 0.0009999999	 wd 0.1000	time 0.8516 (0.8572)	loss 1.2299 (1.2517)	loss-cls 9.8396 (10.0137)	loss-aux 0.0000 (0.0000)	grad_norm 1.6282 (1.7211)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2802
[32m[2023-01-04 18:57:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][479/27342]	eta 6:23:14 lr 0.0009999999	 wd 0.1000	time 0.8556 (0.8560)	loss 1.2684 (1.2518)	loss-cls 10.1474 (10.0141)	loss-aux 0.0000 (0.0000)	grad_norm 1.1824 (1.7122)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2983
[32m[2023-01-04 18:58:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][487/27342]	eta 6:22:35 lr 0.0009999999	 wd 0.1000	time 0.8502 (0.8548)	loss 1.2445 (1.2517)	loss-cls 9.9559 (10.0140)	loss-aux 0.0000 (0.0000)	grad_norm 1.0735 (1.7017)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2699
[32m[2023-01-04 18:58:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][495/27342]	eta 6:22:01 lr 0.0009999999	 wd 0.1000	time 0.8595 (0.8538)	loss 1.2774 (1.2518)	loss-cls 10.2191 (10.0146)	loss-aux 0.0000 (0.0000)	grad_norm 1.3375 (1.6958)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3257
[32m[2023-01-04 18:58:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][503/27342]	eta 6:21:25 lr 0.0009999999	 wd 0.1000	time 0.8403 (0.8527)	loss 1.2559 (1.2518)	loss-cls 10.0469 (10.0140)	loss-aux 0.0000 (0.0000)	grad_norm 1.1790 (1.6876)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2814
[32m[2023-01-04 18:58:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][511/27342]	eta 6:20:50 lr 0.0009999999	 wd 0.1000	time 0.8537 (0.8517)	loss 1.2520 (1.2517)	loss-cls 10.0162 (10.0136)	loss-aux 0.0000 (0.0000)	grad_norm 1.0048 (1.6769)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2981
[32m[2023-01-04 18:58:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][519/27342]	eta 6:20:15 lr 0.0009999999	 wd 0.1000	time 0.8502 (0.8506)	loss 1.2604 (1.2518)	loss-cls 10.0830 (10.0145)	loss-aux 0.0000 (0.0000)	grad_norm 0.9942 (1.6664)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2635
[32m[2023-01-04 18:58:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][527/27342]	eta 6:19:42 lr 0.0009999999	 wd 0.1000	time 0.8622 (0.8496)	loss 1.2449 (1.2517)	loss-cls 9.9591 (10.0139)	loss-aux 0.0000 (0.0000)	grad_norm 5.8048 (1.7291)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2787
[32m[2023-01-04 18:58:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][535/27342]	eta 6:19:08 lr 0.0009999999	 wd 0.1000	time 0.8379 (0.8486)	loss 1.2577 (1.2517)	loss-cls 10.0617 (10.0139)	loss-aux 0.0000 (0.0000)	grad_norm 0.9664 (1.7178)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2636
[32m[2023-01-04 18:58:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][543/27342]	eta 6:18:39 lr 0.0009999999	 wd 0.1000	time 0.8491 (0.8478)	loss 1.2486 (1.2518)	loss-cls 9.9886 (10.0142)	loss-aux 0.0000 (0.0000)	grad_norm 0.8872 (1.7055)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.3343
[32m[2023-01-04 18:58:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][551/27342]	eta 6:18:08 lr 0.0009999999	 wd 0.1000	time 0.8524 (0.8469)	loss 1.2487 (1.2517)	loss-cls 9.9900 (10.0137)	loss-aux 0.0000 (0.0000)	grad_norm 0.7665 (1.6919)	loss_scale 65536.0000 (65536.0000)	mem 7528MB	batch_time 6.2766
[32m[2023-01-04 18:59:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][559/27342]	eta 6:17:35 lr 0.0009999999	 wd 0.1000	time 0.7993 (0.8459)	loss 1.2508 (1.2517)	loss-cls 10.0067 (10.0134)	loss-aux 0.0000 (0.0000)	grad_norm nan (nan)	loss_scale 32768.0000 (65477.4857)	mem 7528MB	batch_time 6.2290
[32m[2023-01-04 18:59:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][567/27342]	eta 6:17:07 lr 0.0009999999	 wd 0.1000	time 0.8500 (0.8451)	loss 1.2508 (1.2516)	loss-cls 10.0062 (10.0129)	loss-aux 0.0000 (0.0000)	grad_norm 0.7867 (nan)	loss_scale 32768.0000 (65016.7887)	mem 7528MB	batch_time 6.3135
[32m[2023-01-04 18:59:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][575/27342]	eta 6:16:34 lr 0.0009999999	 wd 0.1000	time 0.8389 (0.8441)	loss 1.2533 (1.2516)	loss-cls 10.0262 (10.0130)	loss-aux 0.0000 (0.0000)	grad_norm 1.8626 (nan)	loss_scale 32768.0000 (64568.8889)	mem 7528MB	batch_time 6.1947
[32m[2023-01-04 18:59:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][583/27342]	eta 6:16:03 lr 0.0009999999	 wd 0.1000	time 0.8599 (0.8432)	loss 1.2416 (1.2515)	loss-cls 9.9329 (10.0123)	loss-aux 0.0000 (0.0000)	grad_norm 0.6481 (nan)	loss_scale 32768.0000 (64133.2603)	mem 7528MB	batch_time 6.2339
[32m[2023-01-04 18:59:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][591/27342]	eta 6:15:32 lr 0.0009999999	 wd 0.1000	time 0.8518 (0.8423)	loss 1.2387 (1.2515)	loss-cls 9.9098 (10.0116)	loss-aux 0.0000 (0.0000)	grad_norm 1.0338 (nan)	loss_scale 32768.0000 (63709.4054)	mem 7528MB	batch_time 6.2009
[32m[2023-01-04 18:59:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][599/27342]	eta 6:15:00 lr 0.0009999999	 wd 0.1000	time 0.8594 (0.8414)	loss 1.2416 (1.2514)	loss-cls 9.9330 (10.0111)	loss-aux 0.0000 (0.0000)	grad_norm 0.4940 (nan)	loss_scale 32768.0000 (63296.8533)	mem 7528MB	batch_time 6.1852
[32m[2023-01-04 18:59:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][607/27342]	eta 6:14:35 lr 0.0009999999	 wd 0.1000	time 0.8407 (0.8407)	loss 1.2540 (1.2513)	loss-cls 10.0317 (10.0100)	loss-aux 0.0000 (0.0000)	grad_norm 7.6093 (nan)	loss_scale 32768.0000 (62895.1579)	mem 7528MB	batch_time 6.2967
[32m[2023-01-04 18:59:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][615/27342]	eta 6:14:07 lr 0.0009999999	 wd 0.1000	time 0.8469 (0.8399)	loss 1.2625 (1.2513)	loss-cls 10.1000 (10.0100)	loss-aux 0.0000 (0.0000)	grad_norm 0.5359 (nan)	loss_scale 32768.0000 (62503.8961)	mem 7528MB	batch_time 6.2507
[32m[2023-01-04 18:59:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][623/27342]	eta 6:13:42 lr 0.0009999998	 wd 0.1000	time 0.8584 (0.8392)	loss 1.2570 (1.2512)	loss-cls 10.0562 (10.0097)	loss-aux 0.0000 (0.0000)	grad_norm 1.0337 (nan)	loss_scale 32768.0000 (62122.6667)	mem 7528MB	batch_time 6.2849
[32m[2023-01-04 18:59:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][631/27342]	eta 6:13:17 lr 0.0009999998	 wd 0.1000	time 0.8299 (0.8385)	loss 1.2533 (1.2512)	loss-cls 10.0261 (10.0095)	loss-aux 0.0000 (0.0000)	grad_norm 1.2014 (nan)	loss_scale 32768.0000 (61751.0886)	mem 7528MB	batch_time 6.2811
[32m[2023-01-04 19:00:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][639/27342]	eta 6:12:55 lr 0.0009999998	 wd 0.1000	time 0.8431 (0.8379)	loss 1.2483 (1.2511)	loss-cls 9.9862 (10.0092)	loss-aux 0.0000 (0.0000)	grad_norm 0.4694 (nan)	loss_scale 32768.0000 (61388.8000)	mem 7528MB	batch_time 6.3413
[32m[2023-01-04 19:00:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][647/27342]	eta 6:12:32 lr 0.0009999998	 wd 0.1000	time 0.8540 (0.8373)	loss 1.2187 (1.2510)	loss-cls 9.7492 (10.0079)	loss-aux 0.0000 (0.0000)	grad_norm 0.6527 (nan)	loss_scale 32768.0000 (61035.4568)	mem 7528MB	batch_time 6.2995
[32m[2023-01-04 19:00:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][655/27342]	eta 6:12:09 lr 0.0009999998	 wd 0.1000	time 0.8386 (0.8367)	loss 1.2583 (1.2509)	loss-cls 10.0667 (10.0076)	loss-aux 0.0000 (0.0000)	grad_norm 0.9609 (nan)	loss_scale 32768.0000 (60690.7317)	mem 7528MB	batch_time 6.3005
[32m[2023-01-04 19:00:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][663/27342]	eta 6:11:46 lr 0.0009999998	 wd 0.1000	time 0.8880 (0.8361)	loss 1.2112 (1.2508)	loss-cls 9.6898 (10.0065)	loss-aux 0.0000 (0.0000)	grad_norm 0.5518 (nan)	loss_scale 32768.0000 (60354.3133)	mem 7528MB	batch_time 6.3049
[32m[2023-01-04 19:00:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][671/27342]	eta 6:11:22 lr 0.0009999998	 wd 0.1000	time 0.8409 (0.8355)	loss 1.2171 (1.2507)	loss-cls 9.7369 (10.0059)	loss-aux 0.0000 (0.0000)	grad_norm 0.4634 (nan)	loss_scale 32768.0000 (60025.9048)	mem 7528MB	batch_time 6.2456
[32m[2023-01-04 19:00:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][679/27342]	eta 6:11:00 lr 0.0009999998	 wd 0.1000	time 0.8710 (0.8349)	loss 1.2298 (1.2505)	loss-cls 9.8385 (10.0043)	loss-aux 0.0000 (0.0000)	grad_norm 0.5355 (nan)	loss_scale 32768.0000 (59705.2235)	mem 7528MB	batch_time 6.3008
[32m[2023-01-04 19:00:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][687/27342]	eta 6:10:43 lr 0.0009999998	 wd 0.1000	time 0.8791 (0.8345)	loss 1.2432 (1.2504)	loss-cls 9.9456 (10.0035)	loss-aux 0.0000 (0.0000)	grad_norm 0.6890 (nan)	loss_scale 32768.0000 (59392.0000)	mem 7528MB	batch_time 6.3893
[32m[2023-01-04 19:00:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][695/27342]	eta 6:10:21 lr 0.0009999998	 wd 0.1000	time 0.8600 (0.8339)	loss 1.2434 (1.2504)	loss-cls 9.9469 (10.0030)	loss-aux 0.0000 (0.0000)	grad_norm 1.9433 (nan)	loss_scale 32768.0000 (59085.9770)	mem 7528MB	batch_time 6.2964
[32m[2023-01-04 19:00:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][703/27342]	eta 6:10:02 lr 0.0009999998	 wd 0.1000	time 0.8644 (0.8335)	loss 1.2391 (1.2503)	loss-cls 9.9124 (10.0025)	loss-aux 0.0000 (0.0000)	grad_norm 23.1398 (nan)	loss_scale 32768.0000 (58786.9091)	mem 7528MB	batch_time 6.3374
[32m[2023-01-04 19:01:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][711/27342]	eta 6:09:43 lr 0.0009999998	 wd 0.1000	time 0.8650 (0.8330)	loss 1.2438 (1.2503)	loss-cls 9.9505 (10.0021)	loss-aux 0.0000 (0.0000)	grad_norm 0.7415 (nan)	loss_scale 32768.0000 (58494.5618)	mem 7528MB	batch_time 6.3412
[32m[2023-01-04 19:01:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][719/27342]	eta 6:09:28 lr 0.0009999998	 wd 0.1000	time 0.8875 (0.8327)	loss 1.2443 (1.2502)	loss-cls 9.9548 (10.0017)	loss-aux 0.0000 (0.0000)	grad_norm 0.6373 (nan)	loss_scale 32768.0000 (58208.7111)	mem 7528MB	batch_time 6.4244
[32m[2023-01-04 19:01:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][727/27342]	eta 6:09:08 lr 0.0009999998	 wd 0.1000	time 0.8582 (0.8322)	loss 1.2452 (1.2501)	loss-cls 9.9619 (10.0008)	loss-aux 0.0000 (0.0000)	grad_norm 0.4844 (nan)	loss_scale 32768.0000 (57929.1429)	mem 7528MB	batch_time 6.3063
[32m[2023-01-04 19:01:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][735/27342]	eta 6:08:49 lr 0.0009999998	 wd 0.1000	time 0.8639 (0.8317)	loss 1.2471 (1.2500)	loss-cls 9.9765 (10.0003)	loss-aux 0.0000 (0.0000)	grad_norm 0.4939 (nan)	loss_scale 32768.0000 (57655.6522)	mem 7528MB	batch_time 6.3189
[32m[2023-01-04 19:01:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][743/27342]	eta 6:08:29 lr 0.0009999998	 wd 0.1000	time 0.8545 (0.8312)	loss 1.2403 (1.2499)	loss-cls 9.9226 (9.9996)	loss-aux 0.0000 (0.0000)	grad_norm 0.5434 (nan)	loss_scale 32768.0000 (57388.0430)	mem 7528MB	batch_time 6.2832
[32m[2023-01-04 19:01:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][751/27342]	eta 6:08:09 lr 0.0009999998	 wd 0.1000	time 0.8495 (0.8307)	loss 1.2656 (1.2499)	loss-cls 10.1249 (9.9992)	loss-aux 0.0000 (0.0000)	grad_norm 0.3392 (nan)	loss_scale 32768.0000 (57126.1277)	mem 7528MB	batch_time 6.2609
[32m[2023-01-04 19:01:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][759/27342]	eta 6:07:50 lr 0.0009999998	 wd 0.1000	time 0.8511 (0.8302)	loss 1.2536 (1.2498)	loss-cls 10.0291 (9.9982)	loss-aux 0.0000 (0.0000)	grad_norm 0.3404 (nan)	loss_scale 32768.0000 (56869.7263)	mem 7528MB	batch_time 6.2955
[32m[2023-01-04 19:01:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][767/27342]	eta 6:07:34 lr 0.0009999998	 wd 0.1000	time 0.8799 (0.8299)	loss 1.2288 (1.2496)	loss-cls 9.8304 (9.9971)	loss-aux 0.0000 (0.0000)	grad_norm 0.3141 (nan)	loss_scale 32768.0000 (56618.6667)	mem 7528MB	batch_time 6.3773
[32m[2023-01-04 19:01:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][775/27342]	eta 6:07:16 lr 0.0009999998	 wd 0.1000	time 0.8699 (0.8295)	loss 1.2468 (1.2495)	loss-cls 9.9740 (9.9961)	loss-aux 0.0000 (0.0000)	grad_norm 0.3251 (nan)	loss_scale 32768.0000 (56372.7835)	mem 7528MB	batch_time 6.3161
[32m[2023-01-04 19:01:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][783/27342]	eta 6:07:00 lr 0.0009999998	 wd 0.1000	time 0.8874 (0.8291)	loss 1.2342 (1.2495)	loss-cls 9.8739 (9.9958)	loss-aux 0.0000 (0.0000)	grad_norm 0.3964 (nan)	loss_scale 32768.0000 (56131.9184)	mem 7528MB	batch_time 6.3390
[32m[2023-01-04 19:02:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][791/27342]	eta 6:06:41 lr 0.0009999998	 wd 0.1000	time 0.8578 (0.8287)	loss 1.2334 (1.2494)	loss-cls 9.8668 (9.9955)	loss-aux 0.0000 (0.0000)	grad_norm 0.3453 (nan)	loss_scale 32768.0000 (55895.9192)	mem 7528MB	batch_time 6.2855
[32m[2023-01-04 19:02:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][799/27342]	eta 6:06:25 lr 0.0009999997	 wd 0.1000	time 0.8539 (0.8283)	loss 1.2385 (1.2494)	loss-cls 9.9079 (9.9949)	loss-aux 0.0000 (0.0000)	grad_norm 0.3167 (nan)	loss_scale 32768.0000 (55664.6400)	mem 7528MB	batch_time 6.3221
[32m[2023-01-04 19:02:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][807/27342]	eta 6:06:06 lr 0.0009999997	 wd 0.1000	time 0.8525 (0.8278)	loss 1.2265 (1.2493)	loss-cls 9.8118 (9.9941)	loss-aux 0.0000 (0.0000)	grad_norm 0.3369 (nan)	loss_scale 32768.0000 (55437.9406)	mem 7528MB	batch_time 6.2628
[32m[2023-01-04 19:02:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][815/27342]	eta 6:05:48 lr 0.0009999997	 wd 0.1000	time 0.8661 (0.8274)	loss 1.2376 (1.2492)	loss-cls 9.9005 (9.9934)	loss-aux 0.0000 (0.0000)	grad_norm 0.3032 (nan)	loss_scale 32768.0000 (55215.6863)	mem 7528MB	batch_time 6.2735
[32m[2023-01-04 19:02:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][823/27342]	eta 6:05:30 lr 0.0009999997	 wd 0.1000	time 0.8671 (0.8270)	loss 1.2481 (1.2491)	loss-cls 9.9847 (9.9929)	loss-aux 0.0000 (0.0000)	grad_norm 0.8570 (nan)	loss_scale 32768.0000 (54997.7476)	mem 7528MB	batch_time 6.2767
[32m[2023-01-04 19:02:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][831/27342]	eta 6:05:15 lr 0.0009999997	 wd 0.1000	time 0.8693 (0.8267)	loss 1.2257 (1.2490)	loss-cls 9.8058 (9.9922)	loss-aux 0.0000 (0.0000)	grad_norm 0.3225 (nan)	loss_scale 32768.0000 (54784.0000)	mem 7528MB	batch_time 6.3429
[32m[2023-01-04 19:02:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][839/27342]	eta 6:05:00 lr 0.0009999997	 wd 0.1000	time 0.8808 (0.8264)	loss 1.2381 (1.2489)	loss-cls 9.9049 (9.9914)	loss-aux 0.0000 (0.0000)	grad_norm 0.3042 (nan)	loss_scale 32768.0000 (54574.3238)	mem 7528MB	batch_time 6.3589
[32m[2023-01-04 19:02:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][847/27342]	eta 6:04:47 lr 0.0009999997	 wd 0.1000	time 0.8783 (0.8261)	loss 1.2402 (1.2488)	loss-cls 9.9219 (9.9903)	loss-aux 0.0000 (0.0000)	grad_norm 0.3157 (nan)	loss_scale 32768.0000 (54368.6038)	mem 7528MB	batch_time 6.3811
[32m[2023-01-04 19:02:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][855/27342]	eta 6:04:30 lr 0.0009999997	 wd 0.1000	time 0.8568 (0.8257)	loss 1.2481 (1.2487)	loss-cls 9.9847 (9.9897)	loss-aux 0.0000 (0.0000)	grad_norm 0.3212 (nan)	loss_scale 32768.0000 (54166.7290)	mem 7528MB	batch_time 6.2816
[32m[2023-01-04 19:03:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][863/27342]	eta 6:04:15 lr 0.0009999997	 wd 0.1000	time 0.8465 (0.8254)	loss 1.2274 (1.2486)	loss-cls 9.8195 (9.9885)	loss-aux 0.0000 (0.0000)	grad_norm 0.3128 (nan)	loss_scale 32768.0000 (53968.5926)	mem 7528MB	batch_time 6.3362
[32m[2023-01-04 19:03:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][871/27342]	eta 6:04:01 lr 0.0009999997	 wd 0.1000	time 0.8658 (0.8251)	loss 1.2262 (1.2484)	loss-cls 9.8095 (9.9875)	loss-aux 0.0000 (0.0000)	grad_norm 0.3242 (nan)	loss_scale 32768.0000 (53774.0917)	mem 7528MB	batch_time 6.3497
[32m[2023-01-04 19:03:13 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][879/27342]	eta 6:03:45 lr 0.0009999997	 wd 0.1000	time 0.8288 (0.8247)	loss 1.2293 (1.2484)	loss-cls 9.8341 (9.9869)	loss-aux 0.0000 (0.0000)	grad_norm 0.3400 (nan)	loss_scale 32768.0000 (53583.1273)	mem 7528MB	batch_time 6.2835
[32m[2023-01-04 19:03:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][887/27342]	eta 6:03:31 lr 0.0009999997	 wd 0.1000	time 0.8730 (0.8245)	loss 1.2167 (1.2483)	loss-cls 9.7336 (9.9860)	loss-aux 0.0000 (0.0000)	grad_norm 0.3102 (nan)	loss_scale 32768.0000 (53395.6036)	mem 7528MB	batch_time 6.3584
[32m[2023-01-04 19:03:26 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][895/27342]	eta 6:03:14 lr 0.0009999997	 wd 0.1000	time 0.8433 (0.8241)	loss 1.2473 (1.2481)	loss-cls 9.9781 (9.9852)	loss-aux 0.0000 (0.0000)	grad_norm 0.3115 (nan)	loss_scale 32768.0000 (53211.4286)	mem 7528MB	batch_time 6.2629
[32m[2023-01-04 19:03:32 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][903/27342]	eta 6:03:00 lr 0.0009999997	 wd 0.1000	time 0.8372 (0.8238)	loss 1.2352 (1.2481)	loss-cls 9.8819 (9.9847)	loss-aux 0.0000 (0.0000)	grad_norm 0.3177 (nan)	loss_scale 32768.0000 (53030.5133)	mem 7528MB	batch_time 6.3374
[32m[2023-01-04 19:03:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][911/27342]	eta 6:02:48 lr 0.0009999997	 wd 0.1000	time 0.8656 (0.8236)	loss 1.2244 (1.2481)	loss-cls 9.7950 (9.9845)	loss-aux 0.0000 (0.0000)	grad_norm 2.5757 (nan)	loss_scale 32768.0000 (52852.7719)	mem 7528MB	batch_time 6.3886
[32m[2023-01-04 19:03:45 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][919/27342]	eta 6:02:38 lr 0.0009999997	 wd 0.1000	time 0.8959 (0.8235)	loss 1.2287 (1.2480)	loss-cls 9.8295 (9.9840)	loss-aux 0.0000 (0.0000)	grad_norm 1.1370 (nan)	loss_scale 32768.0000 (52678.1217)	mem 7528MB	batch_time 6.4577
[32m[2023-01-04 19:03:51 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][927/27342]	eta 6:02:26 lr 0.0009999997	 wd 0.1000	time 0.8799 (0.8232)	loss 1.2314 (1.2479)	loss-cls 9.8515 (9.9836)	loss-aux 0.0000 (0.0000)	grad_norm 7.9891 (nan)	loss_scale 32768.0000 (52506.4828)	mem 7528MB	batch_time 6.3972
[32m[2023-01-04 19:03:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][935/27342]	eta 6:02:14 lr 0.0009999997	 wd 0.1000	time 0.8538 (0.8231)	loss 1.2423 (1.2479)	loss-cls 9.9383 (9.9830)	loss-aux 0.0000 (0.0000)	grad_norm 0.3273 (nan)	loss_scale 32768.0000 (52337.7778)	mem 7528MB	batch_time 6.4176
[32m[2023-01-04 19:04:04 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][943/27342]	eta 6:02:03 lr 0.0009999996	 wd 0.1000	time 0.8737 (0.8229)	loss 1.2356 (1.2478)	loss-cls 9.8845 (9.9828)	loss-aux 0.0000 (0.0000)	grad_norm 0.4604 (nan)	loss_scale 32768.0000 (52171.9322)	mem 7528MB	batch_time 6.4232
[32m[2023-01-04 19:04:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][951/27342]	eta 6:01:53 lr 0.0009999996	 wd 0.1000	time 0.8930 (0.8228)	loss 1.2352 (1.2478)	loss-cls 9.8815 (9.9821)	loss-aux 0.0000 (0.0000)	grad_norm 0.3244 (nan)	loss_scale 32768.0000 (52008.8739)	mem 7528MB	batch_time 6.4439
[32m[2023-01-04 19:04:17 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][959/27342]	eta 6:01:40 lr 0.0009999996	 wd 0.1000	time 0.8648 (0.8225)	loss 1.2539 (1.2476)	loss-cls 10.0308 (9.9811)	loss-aux 0.0000 (0.0000)	grad_norm 0.3114 (nan)	loss_scale 32768.0000 (51848.5333)	mem 7528MB	batch_time 6.3647
[32m[2023-01-04 19:04:23 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][967/27342]	eta 6:01:30 lr 0.0009999996	 wd 0.1000	time 0.8707 (0.8224)	loss 1.2380 (1.2475)	loss-cls 9.9037 (9.9800)	loss-aux 0.0000 (0.0000)	grad_norm 0.3072 (nan)	loss_scale 32768.0000 (51690.8430)	mem 7528MB	batch_time 6.4665
[32m[2023-01-04 19:04:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][975/27342]	eta 6:01:18 lr 0.0009999996	 wd 0.1000	time 0.8525 (0.8222)	loss 1.2475 (1.2474)	loss-cls 9.9802 (9.9794)	loss-aux 0.0000 (0.0000)	grad_norm 0.2894 (nan)	loss_scale 32768.0000 (51535.7377)	mem 7528MB	batch_time 6.3606
[32m[2023-01-04 19:04:36 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][983/27342]	eta 6:01:05 lr 0.0009999996	 wd 0.1000	time 0.8675 (0.8219)	loss 1.2388 (1.2474)	loss-cls 9.9101 (9.9788)	loss-aux 0.0000 (0.0000)	grad_norm 0.3786 (nan)	loss_scale 32768.0000 (51383.1545)	mem 7528MB	batch_time 6.3473
[32m[2023-01-04 19:04:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][991/27342]	eta 6:00:52 lr 0.0009999996	 wd 0.1000	time 0.8588 (0.8217)	loss 1.2293 (1.2473)	loss-cls 9.8342 (9.9782)	loss-aux 0.0000 (0.0000)	grad_norm 0.2938 (nan)	loss_scale 32768.0000 (51233.0323)	mem 7528MB	batch_time 6.3339
[32m[2023-01-04 19:04:49 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][999/27342]	eta 6:00:40 lr 0.0009999996	 wd 0.1000	time 0.8717 (0.8215)	loss 1.2316 (1.2472)	loss-cls 9.8525 (9.9775)	loss-aux 0.0000 (0.0000)	grad_norm 1.2686 (nan)	loss_scale 32768.0000 (51085.3120)	mem 7528MB	batch_time 6.3509
[32m[2023-01-04 19:04:55 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1007/27342]	eta 6:00:27 lr 0.0009999996	 wd 0.1000	time 0.8470 (0.8213)	loss 1.2539 (1.2471)	loss-cls 10.0311 (9.9771)	loss-aux 0.0000 (0.0000)	grad_norm 0.3120 (nan)	loss_scale 32768.0000 (50939.9365)	mem 7528MB	batch_time 6.3381
[32m[2023-01-04 19:05:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1015/27342]	eta 6:00:17 lr 0.0009999996	 wd 0.1000	time 0.8434 (0.8211)	loss 1.2482 (1.2471)	loss-cls 9.9854 (9.9767)	loss-aux 0.0000 (0.0000)	grad_norm 0.3116 (nan)	loss_scale 32768.0000 (50796.8504)	mem 7528MB	batch_time 6.4186
[32m[2023-01-04 19:05:08 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1023/27342]	eta 6:00:05 lr 0.0009999996	 wd 0.1000	time 0.8446 (0.8209)	loss 1.2496 (1.2471)	loss-cls 9.9969 (9.9765)	loss-aux 0.0000 (0.0000)	grad_norm 1.3352 (nan)	loss_scale 32768.0000 (50656.0000)	mem 7528MB	batch_time 6.3799
[32m[2023-01-04 19:05:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1031/27342]	eta 5:59:53 lr 0.0009999996	 wd 0.1000	time 0.8560 (0.8207)	loss 1.2359 (1.2470)	loss-cls 9.8876 (9.9762)	loss-aux 0.0000 (0.0000)	grad_norm 1.5529 (nan)	loss_scale 32768.0000 (50517.3333)	mem 7528MB	batch_time 6.3346
[32m[2023-01-04 19:05:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1039/27342]	eta 5:59:41 lr 0.0009999996	 wd 0.1000	time 0.8653 (0.8205)	loss 1.2191 (1.2469)	loss-cls 9.7529 (9.9753)	loss-aux 0.0000 (0.0000)	grad_norm 0.3077 (nan)	loss_scale 32768.0000 (50380.8000)	mem 7528MB	batch_time 6.3561
[32m[2023-01-04 19:05:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1047/27342]	eta 5:59:27 lr 0.0009999996	 wd 0.1000	time 0.8272 (0.8202)	loss 1.2427 (1.2468)	loss-cls 9.9414 (9.9746)	loss-aux 0.0000 (0.0000)	grad_norm 0.2798 (nan)	loss_scale 32768.0000 (50246.3511)	mem 7528MB	batch_time 6.2895
[32m[2023-01-04 19:05:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1055/27342]	eta 5:59:15 lr 0.0009999996	 wd 0.1000	time 0.8608 (0.8200)	loss 1.2413 (1.2468)	loss-cls 9.9306 (9.9744)	loss-aux 0.0000 (0.0000)	grad_norm 0.3079 (nan)	loss_scale 32768.0000 (50113.9394)	mem 7528MB	batch_time 6.3242
[32m[2023-01-04 19:05:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1063/27342]	eta 5:59:02 lr 0.0009999995	 wd 0.1000	time 0.8634 (0.8198)	loss 1.2406 (1.2467)	loss-cls 9.9244 (9.9738)	loss-aux 0.0000 (0.0000)	grad_norm 0.3029 (nan)	loss_scale 32768.0000 (49983.5188)	mem 7528MB	batch_time 6.3172
[32m[2023-01-04 19:05:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1071/27342]	eta 5:58:51 lr 0.0009999995	 wd 0.1000	time 0.8553 (0.8196)	loss 1.2284 (1.2467)	loss-cls 9.8272 (9.9733)	loss-aux 0.0000 (0.0000)	grad_norm 0.3520 (nan)	loss_scale 32768.0000 (49855.0448)	mem 7528MB	batch_time 6.3821
[32m[2023-01-04 19:05:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1079/27342]	eta 5:58:40 lr 0.0009999995	 wd 0.1000	time 0.8563 (0.8194)	loss 1.2334 (1.2466)	loss-cls 9.8671 (9.9729)	loss-aux 0.0000 (0.0000)	grad_norm 0.4147 (nan)	loss_scale 32768.0000 (49728.4741)	mem 7528MB	batch_time 6.3654
[32m[2023-01-04 19:05:59 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1087/27342]	eta 5:58:30 lr 0.0009999995	 wd 0.1000	time 0.8748 (0.8193)	loss 1.2478 (1.2466)	loss-cls 9.9825 (9.9729)	loss-aux 0.0000 (0.0000)	grad_norm 0.9092 (nan)	loss_scale 32768.0000 (49603.7647)	mem 7528MB	batch_time 6.3842
[32m[2023-01-04 19:06:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1095/27342]	eta 5:58:19 lr 0.0009999995	 wd 0.1000	time 0.8699 (0.8191)	loss 1.2494 (1.2465)	loss-cls 9.9955 (9.9723)	loss-aux 0.0000 (0.0000)	grad_norm 0.3451 (nan)	loss_scale 32768.0000 (49480.8759)	mem 7528MB	batch_time 6.3851
[32m[2023-01-04 19:06:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1103/27342]	eta 5:58:11 lr 0.0009999995	 wd 0.1000	time 0.9039 (0.8191)	loss 1.2352 (1.2464)	loss-cls 9.8818 (9.9711)	loss-aux 0.0000 (0.0000)	grad_norm 0.4556 (nan)	loss_scale 32768.0000 (49359.7681)	mem 7528MB	batch_time 6.4772
[32m[2023-01-04 19:06:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1111/27342]	eta 5:57:59 lr 0.0009999995	 wd 0.1000	time 0.8909 (0.8189)	loss 1.2290 (1.2463)	loss-cls 9.8321 (9.9706)	loss-aux 0.0000 (0.0000)	grad_norm 0.3980 (nan)	loss_scale 32768.0000 (49240.4029)	mem 7528MB	batch_time 6.3387
[32m[2023-01-04 19:06:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1119/27342]	eta 5:57:48 lr 0.0009999995	 wd 0.1000	time 0.8436 (0.8187)	loss 1.2276 (1.2462)	loss-cls 9.8209 (9.9698)	loss-aux 0.0000 (0.0000)	grad_norm 0.4291 (nan)	loss_scale 32768.0000 (49122.7429)	mem 7528MB	batch_time 6.3442
[32m[2023-01-04 19:06:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1127/27342]	eta 5:57:37 lr 0.0009999995	 wd 0.1000	time 0.8704 (0.8185)	loss 1.2308 (1.2462)	loss-cls 9.8465 (9.9696)	loss-aux 0.0000 (0.0000)	grad_norm 0.4841 (nan)	loss_scale 32768.0000 (49006.7518)	mem 7528MB	batch_time 6.3558
[32m[2023-01-04 19:06:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1135/27342]	eta 5:57:25 lr 0.0009999995	 wd 0.1000	time 0.8693 (0.8183)	loss 1.2128 (1.2462)	loss-cls 9.7022 (9.9694)	loss-aux 0.0000 (0.0000)	grad_norm 0.4334 (nan)	loss_scale 32768.0000 (48892.3944)	mem 7528MB	batch_time 6.3215
[32m[2023-01-04 19:06:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1143/27342]	eta 5:57:13 lr 0.0009999995	 wd 0.1000	time 0.8637 (0.8181)	loss 1.2758 (1.2461)	loss-cls 10.2064 (9.9688)	loss-aux 0.0000 (0.0000)	grad_norm 0.5416 (nan)	loss_scale 32768.0000 (48779.6364)	mem 7528MB	batch_time 6.3229
[32m[2023-01-04 19:06:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1151/27342]	eta 5:57:01 lr 0.0009999995	 wd 0.1000	time 0.8722 (0.8179)	loss 1.2496 (1.2460)	loss-cls 9.9970 (9.9682)	loss-aux 0.0000 (0.0000)	grad_norm 0.3491 (nan)	loss_scale 32768.0000 (48668.4444)	mem 7528MB	batch_time 6.3160
[32m[2023-01-04 19:06:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1159/27342]	eta 5:56:51 lr 0.0009999995	 wd 0.1000	time 0.8718 (0.8178)	loss 1.2383 (1.2460)	loss-cls 9.9068 (9.9678)	loss-aux 0.0000 (0.0000)	grad_norm 0.3230 (nan)	loss_scale 32768.0000 (48558.7862)	mem 7528MB	batch_time 6.3749
[32m[2023-01-04 19:07:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1167/27342]	eta 5:56:42 lr 0.0009999995	 wd 0.1000	time 0.8945 (0.8177)	loss 1.2461 (1.2459)	loss-cls 9.9688 (9.9673)	loss-aux 0.0000 (0.0000)	grad_norm 1.3310 (nan)	loss_scale 32768.0000 (48450.6301)	mem 7528MB	batch_time 6.4143
[32m[2023-01-04 19:07:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1175/27342]	eta 5:56:32 lr 0.0009999994	 wd 0.1000	time 0.8715 (0.8175)	loss 1.2425 (1.2459)	loss-cls 9.9404 (9.9669)	loss-aux 0.0000 (0.0000)	grad_norm 3.2949 (nan)	loss_scale 32768.0000 (48343.9456)	mem 7528MB	batch_time 6.3837
[32m[2023-01-04 19:07:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1183/27342]	eta 5:56:23 lr 0.0009999994	 wd 0.1000	time 0.8966 (0.8174)	loss 1.2347 (1.2458)	loss-cls 9.8779 (9.9664)	loss-aux 0.0000 (0.0000)	grad_norm 3.3024 (nan)	loss_scale 32768.0000 (48238.7027)	mem 7528MB	batch_time 6.4265
[32m[2023-01-04 19:07:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1191/27342]	eta 5:56:12 lr 0.0009999994	 wd 0.1000	time 0.8694 (0.8173)	loss 1.2306 (1.2457)	loss-cls 9.8445 (9.9657)	loss-aux 0.0000 (0.0000)	grad_norm 0.3597 (nan)	loss_scale 32768.0000 (48134.8725)	mem 7528MB	batch_time 6.3628
[32m[2023-01-04 19:07:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1199/27342]	eta 5:56:04 lr 0.0009999994	 wd 0.1000	time 0.8535 (0.8172)	loss 1.2430 (1.2457)	loss-cls 9.9438 (9.9654)	loss-aux 0.0000 (0.0000)	grad_norm 1.2297 (nan)	loss_scale 32768.0000 (48032.4267)	mem 7528MB	batch_time 6.4531
[32m[2023-01-04 19:07:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1207/27342]	eta 5:55:54 lr 0.0009999994	 wd 0.1000	time 0.8581 (0.8171)	loss 1.2452 (1.2456)	loss-cls 9.9616 (9.9650)	loss-aux 0.0000 (0.0000)	grad_norm 1.0475 (nan)	loss_scale 32768.0000 (47931.3377)	mem 7528MB	batch_time 6.3732
[32m[2023-01-04 19:07:41 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1215/27342]	eta 5:55:42 lr 0.0009999994	 wd 0.1000	time 0.8601 (0.8169)	loss 1.2336 (1.2455)	loss-cls 9.8686 (9.9644)	loss-aux 0.0000 (0.0000)	grad_norm 0.3797 (nan)	loss_scale 32768.0000 (47831.5789)	mem 7528MB	batch_time 6.3097
[32m[2023-01-04 19:07:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1223/27342]	eta 5:55:31 lr 0.0009999994	 wd 0.1000	time 0.8569 (0.8167)	loss 1.2537 (1.2455)	loss-cls 10.0295 (9.9643)	loss-aux 0.0000 (0.0000)	grad_norm 1.5631 (nan)	loss_scale 32768.0000 (47733.1242)	mem 7528MB	batch_time 6.3025
[32m[2023-01-04 19:07:53 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1231/27342]	eta 5:55:22 lr 0.0009999994	 wd 0.1000	time 0.8624 (0.8166)	loss 1.2423 (1.2455)	loss-cls 9.9387 (9.9640)	loss-aux 0.0000 (0.0000)	grad_norm 0.4321 (nan)	loss_scale 32768.0000 (47635.9481)	mem 7528MB	batch_time 6.4149
[32m[2023-01-04 19:08:00 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1239/27342]	eta 5:55:13 lr 0.0009999994	 wd 0.1000	time 0.8883 (0.8165)	loss 1.2448 (1.2455)	loss-cls 9.9586 (9.9639)	loss-aux 0.0000 (0.0000)	grad_norm 0.3152 (nan)	loss_scale 32768.0000 (47540.0258)	mem 7528MB	batch_time 6.4260
[32m[2023-01-04 19:08:06 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1247/27342]	eta 5:55:02 lr 0.0009999994	 wd 0.1000	time 0.8868 (0.8164)	loss 1.2387 (1.2455)	loss-cls 9.9096 (9.9638)	loss-aux 0.0000 (0.0000)	grad_norm 1.3626 (nan)	loss_scale 32768.0000 (47445.3333)	mem 7528MB	batch_time 6.3406
[32m[2023-01-04 19:08:12 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1255/27342]	eta 5:54:52 lr 0.0009999994	 wd 0.1000	time 0.8709 (0.8162)	loss 1.2493 (1.2454)	loss-cls 9.9945 (9.9635)	loss-aux 0.0000 (0.0000)	grad_norm 4.8253 (nan)	loss_scale 32768.0000 (47351.8471)	mem 7528MB	batch_time 6.3400
[32m[2023-01-04 19:08:19 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1263/27342]	eta 5:54:40 lr 0.0009999994	 wd 0.1000	time 0.8543 (0.8160)	loss 1.2468 (1.2454)	loss-cls 9.9744 (9.9635)	loss-aux 0.0000 (0.0000)	grad_norm 8.5756 (nan)	loss_scale 32768.0000 (47259.5443)	mem 7528MB	batch_time 6.2776
[32m[2023-01-04 19:08:25 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1271/27342]	eta 5:54:29 lr 0.0009999994	 wd 0.1000	time 0.8475 (0.8158)	loss 1.2439 (1.2454)	loss-cls 9.9513 (9.9634)	loss-aux 0.0000 (0.0000)	grad_norm 4.1273 (nan)	loss_scale 32768.0000 (47168.4025)	mem 7528MB	batch_time 6.2817
[32m[2023-01-04 19:08:31 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1279/27342]	eta 5:54:17 lr 0.0009999993	 wd 0.1000	time 0.8556 (0.8156)	loss 1.2487 (1.2454)	loss-cls 9.9897 (9.9634)	loss-aux 0.0000 (0.0000)	grad_norm 0.3158 (nan)	loss_scale 32768.0000 (47078.4000)	mem 7528MB	batch_time 6.2821
[32m[2023-01-04 19:08:38 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1287/27342]	eta 5:54:06 lr 0.0009999993	 wd 0.1000	time 0.8400 (0.8154)	loss 1.2523 (1.2454)	loss-cls 10.0186 (9.9631)	loss-aux 0.0000 (0.0000)	grad_norm 1.0873 (nan)	loss_scale 32768.0000 (46989.5155)	mem 7528MB	batch_time 6.2915
[32m[2023-01-04 19:08:44 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1295/27342]	eta 5:53:55 lr 0.0009999993	 wd 0.1000	time 0.8560 (0.8153)	loss 1.2389 (1.2453)	loss-cls 9.9111 (9.9626)	loss-aux 0.0000 (0.0000)	grad_norm 0.3229 (nan)	loss_scale 32768.0000 (46901.7284)	mem 7528MB	batch_time 6.3104
[32m[2023-01-04 19:08:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1303/27342]	eta 5:53:46 lr 0.0009999993	 wd 0.1000	time 0.8920 (0.8152)	loss 1.2494 (1.2452)	loss-cls 9.9948 (9.9620)	loss-aux 0.0000 (0.0000)	grad_norm 0.2625 (nan)	loss_scale 32768.0000 (46815.0184)	mem 7528MB	batch_time 6.4031
[32m[2023-01-04 19:08:57 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1311/27342]	eta 5:53:37 lr 0.0009999993	 wd 0.1000	time 0.8581 (0.8151)	loss 1.1965 (1.2452)	loss-cls 9.5718 (9.9615)	loss-aux 0.0000 (0.0000)	grad_norm 0.6949 (nan)	loss_scale 32768.0000 (46729.3659)	mem 7528MB	batch_time 6.4058
[32m[2023-01-04 19:09:03 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1319/27342]	eta 5:53:28 lr 0.0009999993	 wd 0.1000	time 0.8167 (0.8150)	loss 1.2224 (1.2451)	loss-cls 9.7790 (9.9607)	loss-aux 0.0000 (0.0000)	grad_norm nan (nan)	loss_scale 16384.0000 (46632.3394)	mem 7528MB	batch_time 6.3821
[32m[2023-01-04 19:09:10 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1327/27342]	eta 5:53:21 lr 0.0009999993	 wd 0.1000	time 0.8858 (0.8150)	loss 1.2388 (1.2450)	loss-cls 9.9101 (9.9600)	loss-aux 0.0000 (0.0000)	grad_norm 0.4346 (nan)	loss_scale 16384.0000 (46450.1205)	mem 7528MB	batch_time 6.5003
[32m[2023-01-04 19:09:16 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1335/27342]	eta 5:53:13 lr 0.0009999993	 wd 0.1000	time 0.8719 (0.8149)	loss 1.2362 (1.2450)	loss-cls 9.8895 (9.9597)	loss-aux 0.0000 (0.0000)	grad_norm 1.1546 (nan)	loss_scale 16384.0000 (46270.0838)	mem 7528MB	batch_time 6.4284
[32m[2023-01-04 19:09:22 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1343/27342]	eta 5:53:05 lr 0.0009999993	 wd 0.1000	time 0.8821 (0.8148)	loss 1.2437 (1.2450)	loss-cls 9.9500 (9.9598)	loss-aux 0.0000 (0.0000)	grad_norm 0.4074 (nan)	loss_scale 16384.0000 (46092.1905)	mem 7528MB	batch_time 6.4369
[32m[2023-01-04 19:09:29 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1351/27342]	eta 5:52:55 lr 0.0009999993	 wd 0.1000	time 0.8421 (0.8147)	loss 1.2482 (1.2449)	loss-cls 9.9857 (9.9594)	loss-aux 0.0000 (0.0000)	grad_norm 0.3969 (nan)	loss_scale 16384.0000 (45916.4024)	mem 7528MB	batch_time 6.3283
[32m[2023-01-04 19:09:35 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1359/27342]	eta 5:52:47 lr 0.0009999993	 wd 0.1000	time 0.8828 (0.8147)	loss 1.2478 (1.2449)	loss-cls 9.9828 (9.9590)	loss-aux 0.0000 (0.0000)	grad_norm 0.2684 (nan)	loss_scale 16384.0000 (45742.6824)	mem 7528MB	batch_time 6.4767
[32m[2023-01-04 19:09:42 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1367/27342]	eta 5:52:39 lr 0.0009999993	 wd 0.1000	time 0.8644 (0.8146)	loss 1.2371 (1.2448)	loss-cls 9.8965 (9.9586)	loss-aux 0.0000 (0.0000)	grad_norm 0.5218 (nan)	loss_scale 16384.0000 (45570.9942)	mem 7528MB	batch_time 6.4199
[32m[2023-01-04 19:09:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1375/27342]	eta 5:52:29 lr 0.0009999992	 wd 0.1000	time 0.8812 (0.8145)	loss 1.2373 (1.2448)	loss-cls 9.8984 (9.9583)	loss-aux 0.0000 (0.0000)	grad_norm 0.3529 (nan)	loss_scale 16384.0000 (45401.3023)	mem 7528MB	batch_time 6.3499
[32m[2023-01-04 19:09:54 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1383/27342]	eta 5:52:21 lr 0.0009999992	 wd 0.1000	time 0.8927 (0.8144)	loss 1.2372 (1.2447)	loss-cls 9.8976 (9.9575)	loss-aux 0.0000 (0.0000)	grad_norm 0.2815 (nan)	loss_scale 16384.0000 (45233.5723)	mem 7528MB	batch_time 6.3994
[32m[2023-01-04 19:10:01 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1391/27342]	eta 5:52:12 lr 0.0009999992	 wd 0.1000	time 0.8863 (0.8143)	loss 1.2298 (1.2446)	loss-cls 9.8386 (9.9567)	loss-aux 0.0000 (0.0000)	grad_norm 2.7579 (nan)	loss_scale 16384.0000 (45067.7701)	mem 7528MB	batch_time 6.3822
[32m[2023-01-04 19:10:07 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1399/27342]	eta 5:52:05 lr 0.0009999992	 wd 0.1000	time 0.8693 (0.8143)	loss 1.2365 (1.2445)	loss-cls 9.8921 (9.9560)	loss-aux 0.0000 (0.0000)	grad_norm 0.4917 (nan)	loss_scale 16384.0000 (44903.8629)	mem 7528MB	batch_time 6.5225
[32m[2023-01-04 19:10:14 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1407/27342]	eta 5:51:57 lr 0.0009999992	 wd 0.1000	time 0.8628 (0.8142)	loss 1.2016 (1.2444)	loss-cls 9.6127 (9.9551)	loss-aux 0.0000 (0.0000)	grad_norm 0.3647 (nan)	loss_scale 16384.0000 (44741.8182)	mem 7528MB	batch_time 6.4142
[32m[2023-01-04 19:10:20 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1415/27342]	eta 5:51:49 lr 0.0009999992	 wd 0.1000	time 0.8768 (0.8142)	loss 1.2631 (1.2444)	loss-cls 10.1045 (9.9549)	loss-aux 0.0000 (0.0000)	grad_norm 5.9701 (nan)	loss_scale 16384.0000 (44581.6045)	mem 7528MB	batch_time 6.4287
[32m[2023-01-04 19:10:27 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1423/27342]	eta 5:51:41 lr 0.0009999992	 wd 0.1000	time 0.9038 (0.8141)	loss 1.2267 (1.2443)	loss-cls 9.8134 (9.9545)	loss-aux 0.0000 (0.0000)	grad_norm 0.3505 (nan)	loss_scale 16384.0000 (44423.1910)	mem 7528MB	batch_time 6.4449
[32m[2023-01-04 19:10:33 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1431/27342]	eta 5:51:31 lr 0.0009999992	 wd 0.1000	time 0.8659 (0.8140)	loss 1.2398 (1.2443)	loss-cls 9.9184 (9.9541)	loss-aux 0.0000 (0.0000)	grad_norm 0.2596 (nan)	loss_scale 16384.0000 (44266.5475)	mem 7528MB	batch_time 6.3246
[32m[2023-01-04 19:10:39 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1439/27342]	eta 5:51:22 lr 0.0009999992	 wd 0.1000	time 0.8777 (0.8139)	loss 1.2453 (1.2442)	loss-cls 9.9627 (9.9540)	loss-aux 0.0000 (0.0000)	grad_norm 2.5982 (nan)	loss_scale 16384.0000 (44111.6444)	mem 7528MB	batch_time 6.3783
[32m[2023-01-04 19:10:46 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1447/27342]	eta 5:51:13 lr 0.0009999992	 wd 0.1000	time 0.8682 (0.8138)	loss 1.2229 (1.2442)	loss-cls 9.7834 (9.9535)	loss-aux 0.0000 (0.0000)	grad_norm 0.2904 (nan)	loss_scale 16384.0000 (43958.4530)	mem 7528MB	batch_time 6.3594
[32m[2023-01-04 19:10:52 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1455/27342]	eta 5:51:05 lr 0.0009999992	 wd 0.1000	time 0.9126 (0.8137)	loss 1.2437 (1.2441)	loss-cls 9.9496 (9.9532)	loss-aux 0.0000 (0.0000)	grad_norm 0.2483 (nan)	loss_scale 16384.0000 (43806.9451)	mem 7528MB	batch_time 6.4254
[32m[2023-01-04 19:10:58 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1463/27342]	eta 5:50:55 lr 0.0009999991	 wd 0.1000	time 0.8507 (0.8136)	loss 1.2167 (1.2441)	loss-cls 9.7335 (9.9529)	loss-aux 0.0000 (0.0000)	grad_norm 0.2408 (nan)	loss_scale 16384.0000 (43657.0929)	mem 7528MB	batch_time 6.3133
[32m[2023-01-04 19:11:05 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1471/27342]	eta 5:50:46 lr 0.0009999991	 wd 0.1000	time 0.8684 (0.8135)	loss 1.2325 (1.2441)	loss-cls 9.8599 (9.9528)	loss-aux 0.0000 (0.0000)	grad_norm 0.2568 (nan)	loss_scale 16384.0000 (43508.8696)	mem 7528MB	batch_time 6.3605
[32m[2023-01-04 19:11:11 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1479/27342]	eta 5:50:37 lr 0.0009999991	 wd 0.1000	time 0.8634 (0.8134)	loss 1.2183 (1.2441)	loss-cls 9.7463 (9.9525)	loss-aux 0.0000 (0.0000)	grad_norm 0.2719 (nan)	loss_scale 16384.0000 (43362.2486)	mem 7528MB	batch_time 6.4043
[32m[2023-01-04 19:11:18 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1487/27342]	eta 5:50:29 lr 0.0009999991	 wd 0.1000	time 0.8662 (0.8134)	loss 1.2652 (1.2440)	loss-cls 10.1215 (9.9523)	loss-aux 0.0000 (0.0000)	grad_norm 0.2599 (nan)	loss_scale 16384.0000 (43217.2043)	mem 7528MB	batch_time 6.4117
[32m[2023-01-04 19:11:24 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1495/27342]	eta 5:50:21 lr 0.0009999991	 wd 0.1000	time 0.8373 (0.8133)	loss 1.2449 (1.2440)	loss-cls 9.9589 (9.9520)	loss-aux 0.0000 (0.0000)	grad_norm 0.2519 (nan)	loss_scale 16384.0000 (43073.7112)	mem 7528MB	batch_time 6.3775
[32m[2023-01-04 19:11:30 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1503/27342]	eta 5:50:13 lr 0.0009999991	 wd 0.1000	time 0.9066 (0.8133)	loss 1.2363 (1.2440)	loss-cls 9.8906 (9.9517)	loss-aux 0.0000 (0.0000)	grad_norm 0.3242 (nan)	loss_scale 16384.0000 (42931.7447)	mem 7528MB	batch_time 6.4629
[32m[2023-01-04 19:11:37 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1511/27342]	eta 5:50:04 lr 0.0009999991	 wd 0.1000	time 0.8531 (0.8131)	loss 1.2380 (1.2439)	loss-cls 9.9038 (9.9512)	loss-aux 0.0000 (0.0000)	grad_norm 0.2217 (nan)	loss_scale 16384.0000 (42791.2804)	mem 7528MB	batch_time 6.3342
[32m[2023-01-04 19:11:43 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1519/27342]	eta 5:49:55 lr 0.0009999991	 wd 0.1000	time 0.8680 (0.8131)	loss 1.2425 (1.2439)	loss-cls 9.9399 (9.9510)	loss-aux 0.0000 (0.0000)	grad_norm 0.1970 (nan)	loss_scale 16384.0000 (42652.2947)	mem 7528MB	batch_time 6.3789
[32m[2023-01-04 19:11:50 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1527/27342]	eta 5:49:46 lr 0.0009999991	 wd 0.1000	time 0.8538 (0.8130)	loss 1.2357 (1.2438)	loss-cls 9.8855 (9.9507)	loss-aux 0.0000 (0.0000)	grad_norm 0.3300 (nan)	loss_scale 16384.0000 (42514.7644)	mem 7528MB	batch_time 6.3543
[32m[2023-01-04 19:11:56 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1535/27342]	eta 5:49:37 lr 0.0009999991	 wd 0.1000	time 0.8784 (0.8129)	loss 1.2348 (1.2438)	loss-cls 9.8784 (9.9503)	loss-aux 0.0000 (0.0000)	grad_norm 0.2092 (nan)	loss_scale 16384.0000 (42378.6667)	mem 7528MB	batch_time 6.3614
[32m[2023-01-04 19:12:02 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1543/27342]	eta 5:49:28 lr 0.0009999990	 wd 0.1000	time 0.8672 (0.8128)	loss 1.2191 (1.2438)	loss-cls 9.7525 (9.9502)	loss-aux 0.0000 (0.0000)	grad_norm 0.2248 (nan)	loss_scale 16384.0000 (42243.9793)	mem 7528MB	batch_time 6.3491
[32m[2023-01-04 19:12:09 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1551/27342]	eta 5:49:20 lr 0.0009999990	 wd 0.1000	time 0.8759 (0.8127)	loss 1.2214 (1.2437)	loss-cls 9.7710 (9.9494)	loss-aux 0.0000 (0.0000)	grad_norm 0.2324 (nan)	loss_scale 16384.0000 (42110.6804)	mem 7528MB	batch_time 6.3742
[32m[2023-01-04 19:12:15 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1559/27342]	eta 5:49:11 lr 0.0009999990	 wd 0.1000	time 0.8643 (0.8126)	loss 1.2305 (1.2436)	loss-cls 9.8439 (9.9489)	loss-aux 0.0000 (0.0000)	grad_norm 0.2242 (nan)	loss_scale 16384.0000 (41978.7487)	mem 7528MB	batch_time 6.3735
[32m[2023-01-04 19:12:21 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1567/27342]	eta 5:49:03 lr 0.0009999990	 wd 0.1000	time 0.8715 (0.8126)	loss 1.2353 (1.2436)	loss-cls 9.8826 (9.9485)	loss-aux 0.0000 (0.0000)	grad_norm 0.2266 (nan)	loss_scale 16384.0000 (41848.1633)	mem 7528MB	batch_time 6.4257
[32m[2023-01-04 19:12:28 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1575/27342]	eta 5:48:55 lr 0.0009999990	 wd 0.1000	time 0.8496 (0.8125)	loss 1.2289 (1.2436)	loss-cls 9.8309 (9.9485)	loss-aux 0.0000 (0.0000)	grad_norm 0.2878 (nan)	loss_scale 16384.0000 (41718.9036)	mem 7528MB	batch_time 6.3704
[32m[2023-01-04 19:12:34 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1583/27342]	eta 5:48:46 lr 0.0009999990	 wd 0.1000	time 0.8532 (0.8124)	loss 1.2286 (1.2435)	loss-cls 9.8286 (9.9479)	loss-aux 0.0000 (0.0000)	grad_norm 0.2308 (nan)	loss_scale 16384.0000 (41590.9495)	mem 7528MB	batch_time 6.3455
[32m[2023-01-04 19:12:40 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1591/27342]	eta 5:48:36 lr 0.0009999990	 wd 0.1000	time 0.8388 (0.8123)	loss 1.2185 (1.2434)	loss-cls 9.7478 (9.9473)	loss-aux 0.0000 (0.0000)	grad_norm 0.2140 (nan)	loss_scale 16384.0000 (41464.2814)	mem 7528MB	batch_time 6.3226
[32m[2023-01-04 19:12:47 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 268)[0m: INFO Train: [0/90][1599/27342]	eta 5:48:28 lr 0.0009999990	 wd 0.1000	time 0.8805 (0.8122)	loss 1.2233 (1.2434)	loss-cls 9.7860 (9.9470)	loss-aux 0.0000 (0.0000)	grad_norm 0.5131 (nan)	loss_scale 16384.0000 (41338.8800)	mem 7528MB	batch_time 6.3862
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6236.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9041.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29695.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9819.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_135.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4524.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8873.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_32625.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10175248/n10175248_583.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12740.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10242.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27627.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n06470073/n06470073_47249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_12231.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17877.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13641.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27412.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19281.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_16320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7953.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9249.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_15341.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8925.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7996.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7072.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6567.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5664.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22719.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30043.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13306.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22995.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25750.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22980.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9215.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8812.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13244.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_8806.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_11746.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n02368116/n02368116_318.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20118.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6669.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_29025.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_16430.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_7026.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7365.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11816.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_26924.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13320.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13516.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25408.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28595.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10586.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_1796.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6247.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_6710.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22698.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22166.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7411.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2852.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_16737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15455.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12064.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13396.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8737.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9401.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13261.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_6850.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24434.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_14701.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_20180.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25530.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24544.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_10353.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27221.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_19005.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9282.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12108.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12206.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13871.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7728.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_34297.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_1914.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14718.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12364.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18729.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11827.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_22581.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27611.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11766.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15830.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33630.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_21756.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33259.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27578.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_24638.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_23316.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_4539.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_8726.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_9068.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27296.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_30926.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_14020.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12757.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_11950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_7013.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03957420/n03957420_33553.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_10675.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_17471.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_5051.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_9566.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13950.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_25717.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_465.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_15697.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15811.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_13103.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8783.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_4456.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2040.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_2322.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n03001627/n03001627_8645.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_9031.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_15480.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n04132158/n04132158_3493.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18654.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_18350.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_27317.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_13419.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_33623.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n10105733/n10105733_28288.JPEG
ERROR IMG LOADED:  /mnt/znvme/dataset/imagenet22k/fall11_whole/n12245319/n12245319_12142.JPEG
[32m[2023-01-04 19:12:48 swin_fastmoe_base_patch4_window12_192_32expert_32gpu_22k][0m[33m(main_moe.py 282)[0m: INFO EPOCH 0 training takes 0:21:41
