+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 1 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 7 == 0 ']'
+ sleep 10s
+ '[' 0 == 0 ']'
+ hostname
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 5 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 4 == 0 ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 6 == 0 ']'
+ sleep 10s
+ '[' sh-lab == sh-lab ']'
+ '[' 2 == 0 ']'
+ sleep 10s
+ '[' 3 == 0 ']'
+ sleep 10s
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 10 == 0 ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 24 == 0 ']'
+ '[' 27 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' 25 == 0 ']'
+ sleep 10s
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 26 == 0 ']'
+ '[' 28 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 30 == 0 ']'
+ '[' 31 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 29 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 20 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 21 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 22 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 23 == 0 ']'
+ sleep 10s
+ '[' sh-lab == sh-lab ']'
+ '[' 19 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 18 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 16 == 0 ']'
+ '[' 17 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 12 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 9 == 0 ']'
+ sleep 10s
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 11 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 13 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 14 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 15 == 0 ']'
+ sleep 10s
+ '[' srun == srun ']'
+ '[' sh-lab == sh-lab ']'
+ '[' 8 == 0 ']'
+ sleep 10s
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export NODE_RANK=1
+ NODE_RANK=1
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=1
+ NODE_RANK=1
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
++ cat /mnt/cache/zhaishuming/master_node
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
++ cat /mnt/cache/zhaishuming/master_node
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
++ cat /mnt/cache/zhaishuming/master_node
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ python_args=
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ python_args+='
++ cat /mnt/cache/zhaishuming/master_node
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
++ cat /mnt/cache/zhaishuming/master_node
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
++ cat /mnt/cache/zhaishuming/master_node
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=28
+ RANK=28
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=19
+ RANK=19
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ localrank=4
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=30
+ RANK=30
+ export RANK=31
+ RANK=31
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=24
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=25
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=27
+ RANK=27
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=29
+ RANK=29
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ export RANK=17
+ RANK=17
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ RANK=24
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=0
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ RANK=25
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=22
+ RANK=22
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ localrank=1
+ export RANK=18
+ RANK=18
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export RANK=20
+ RANK=20
+ export WORLD_SIZE=32
+ localrank=6
+ export RANK=23
+ RANK=23
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=4
+ export RANK=21
+ RANK=21
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NNODES=4
+ NNODES=4
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export RANK=26
+ RANK=26
+ export WORLD_SIZE=32
+ export RANK=16
+ RANK=16
+ export WORLD_SIZE=32
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ WORLD_SIZE=32
+ localrank=2
+ export NNODES=4
+ NNODES=4
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ WORLD_SIZE=32
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=6
+ NODE_RANK=6
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ '[' sh-lab == nico ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == sh-lab ']'
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ export NNODES=4
+ NNODES=4
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ export NODE_RANK=6
+ NODE_RANK=6
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ '[' sh-lab == nico ']'
+ export NNODES=4
+ NNODES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export NODE_RANK=4
+ NODE_RANK=4
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export NNODES=4
+ NNODES=4
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=5
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NODE_RANK=5
+ NODE_RANK=5
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NODE_RANK=5
+ NODE_RANK=5
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ NODE_RANK=5
+ export MAX_JOBS=64
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NNODES=4
+ NNODES=4
+ '[' sh-lab == nico ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == sh-lab ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ MAX_JOBS=64
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ export NODE_RANK=4
+ NODE_RANK=4
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ python_args+='
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ '[' EP+DP == EP+DP ']'
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ '[' EP+DP == EP+DP ']'
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args=
+ python_args+='
+ python_args+='
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ true
+ EXEC=./pretrain_gpt.py
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ '[' OFF == ON ']'
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=1
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=6
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=10
+ RANK=10
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=32
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=2
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=13
+ WORLD_SIZE=32
+ localrank=0
+ RANK=13
+ export WORLD_SIZE=32
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=7
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=5
+ localrank=4
+ export MASTER_ADDR=SH-IDC1-10-140-1-93
+ MASTER_ADDR=SH-IDC1-10-140-1-93
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=32
+ WORLD_SIZE=32
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export NNODES=4
+ NNODES=4
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=2
+ NODE_RANK=2
+ export NNODES=4
+ NNODES=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NNODES=4
+ NNODES=4
+ export NODE_RANK=3
+ NODE_RANK=3
+ export NODE_RANK=3
+ NODE_RANK=3
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ '[' sh-lab == nico ']'
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ '[' sh-lab == nico ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == nico ']'
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ '[' sh-lab == nico ']'
+ '[' sh-lab == sh-lab ']'
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ CODE_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ DATASET_PREFIX=/mnt/cache/zhaishuming
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ cd /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ GPT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
+ BERT_VOCAB_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/bert-large-uncased-vocab.txt
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ MERGE_FILE=/mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ DATA_PATH=/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence
+ TRAIN_SAMPLES=102400
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ DUMP_FILE=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof/table
+ python_args=
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ python_args+='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 1.2         --expert-parallel-strategy EP+DP         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 2560         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 4         --global-batch-size 512         --train-samples 102400 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm '
+ '[' EP+DP == EP+DP ']'
+ python_args+='  --expert-ep-size 8                     --expert-dp-size 1 '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ true
+ EXEC=./pretrain_gpt.py
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ python_args+='  --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json                     --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt '
+ false
+ false
+ false
+ false
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/mnt/cache/zhaishuming/Auto-Megatron/logs/test1221_GPT-L16-H2560-32MoE/GPT-L16-H2560-32MoE_naive1.2_EP+DP_t1_p4_d8_ep8_dp1_totalE32_localE4_gbs512_mbs4_smart_OFF_smartGP0_shadowOFF_newshadowOFF_dynamicOFF_freq0_2022-12-21T23:23:04+0800.prof
+ DEBUG=OFF
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 1.2 --expert-parallel-strategy EP+DP --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 2560 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 4 --global-batch-size 512 --train-samples 102400 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
using world size: 32, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 4 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 8
  data_path ....................................... ['/mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dump ............................................ False
  dump_file ....................................... None
  dump_freq ....................................... 10
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 8
  expert_parallel_strategy ........................ EP+DP
  ffn_hidden_size ................................. 10240
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 512
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 2560
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 160
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  new_shadow ...................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 4
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 4
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 102400
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/cache/zhaishuming/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 32
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 16
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 4
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
make: Nothing to be done for `default'.
make: Leaving directory `/mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.180 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /mnt/cache/zhaishuming/FastMoe-Megatron/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in DP group [7]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in DP group [3]
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in DP group [6]
[INFO] 27 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 27 in DP group [27]
[INFO] 18 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 18 in DP group [18]
[INFO] 16 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 16 in DP group [16]
[INFO] 19 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 19 in DP group [19]
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 9.374 seconds
time to initialize megatron (seconds): -3.767
hhs=5120
[INFO] 23 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 23 in DP group [23]
hhs=5120
[INFO] 17 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 17 in DP group [17]
hhs=5120
[INFO] 21 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 21 in DP group [21]
hhs=5120
hhs=5120
[INFO] 20 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 20 in DP group [20]
hhs=5120
hhs=5120
[INFO] 22 in EP group [16, 17, 18, 19, 20, 21, 22, 23]
[INFO] 22 in DP group [22]
hhs=5120
[after megatron is initialized] datetime: 2022-12-21 23:23:40 
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in DP group [4]
hhs=5120
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in DP group [5]
hhs=5120
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in DP group [1]
hhs=5120
hhs=5120
hhs=5120
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in DP group [2]
hhs=5120
[INFO] 26 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 26 in DP group [26]
hhs=5120
hhs=5120
[INFO] 28 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 28 in DP group [28]
hhs=5120
building GPT model ...
hhs=5120
[INFO] 29 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 29 in DP group [29]
hhs=5120
[INFO] 30 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 30 in DP group [30]
hhs=5120
[INFO] 24 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 24 in DP group [24]
hhs=5120
[INFO] 31 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 31 in DP group [31]
hhs=5120
hhs=5120
[INFO] 25 in EP group [24, 25, 26, 27, 28, 29, 30, 31]
[INFO] 25 in DP group [25]
hhs=5120
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
hhs=5120
hhs=5120
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
hhs=5120
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
hhs=5120
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
hhs=5120
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
hhs=5120
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
hhs=5120
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
hhs=5120
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 653603968
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 524820608
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 524820608
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 656220288
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
setting training iterations to 200
> learning rate decay style: cosine
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[WARNING] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2022-12-21 23:23:51 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      102400
    validation: -512
    test:       -512
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.178067 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/cache/zhaishuming/fastmoe-dataset/my-bert_text_sentence_train_indexmap_102400ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.037 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2022-12-21 23:24:07 
done with setup ...
training ...
[before the start of training step] datetime: 2022-12-21 23:24:07 
[Rank 8] (after 1 iterations) memory (MB) | allocated: 10061.5751953125 | max allocated: 13295.67919921875 | reserved: 15174.0 | max reserved: 15174.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 12533.6298828125 | max allocated: 17269.6630859375 | reserved: 20694.0 | max reserved: 20694.0
[Rank 16] (after 1 iterations) memory (MB) | allocated: 10067.984375 | max allocated: 10107.9150390625 | reserved: 12690.0 | max reserved: 12690.0
 iteration        1/     200 | consumed samples:          512 | elapsed time per iteration (ms): 39103.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.082686E+01 | loss scale: 131072.0 | grad norm: 17.307 | num zeros: 42992.0 | params norm: 299.877 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 24] (after 1 iterations) memory (MB) | allocated: 12520.396484375 | max allocated: 12520.41162109375 | reserved: 13658.0 | max reserved: 13658.0
 iteration        2/     200 | consumed samples:         1024 | elapsed time per iteration (ms): 7100.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.037290E+01 | loss scale: 131072.0 | grad norm: 6.942 | num zeros: 973943488.0 | params norm: 299.928 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     200 | consumed samples:         1536 | elapsed time per iteration (ms): 7999.2 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 131072.0 | params norm: 299.928 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        4/     200 | consumed samples:         2048 | elapsed time per iteration (ms): 6490.7 | learning rate: 1.000E-04 | global batch size:   512 | loss scale: 65536.0 | params norm: 299.928 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/     200 | consumed samples:         2560 | elapsed time per iteration (ms): 7008.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 1.152649E+01 | loss scale: 65536.0 | grad norm: 76.891 | num zeros: 54035536.0 | params norm: 299.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     200 | consumed samples:         3072 | elapsed time per iteration (ms): 8323.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.954441E+00 | loss scale: 65536.0 | grad norm: 5.147 | num zeros: 1155759616.0 | params norm: 300.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     200 | consumed samples:         3584 | elapsed time per iteration (ms): 6043.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.699452E+00 | loss scale: 65536.0 | grad norm: 5.039 | num zeros: 926082688.0 | params norm: 300.083 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     200 | consumed samples:         4096 | elapsed time per iteration (ms): 6222.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.465760E+00 | loss scale: 65536.0 | grad norm: 5.021 | num zeros: 1071092224.0 | params norm: 300.149 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     200 | consumed samples:         4608 | elapsed time per iteration (ms): 5083.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.248682E+00 | loss scale: 65536.0 | grad norm: 5.017 | num zeros: 1240407680.0 | params norm: 300.221 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/     200 | consumed samples:         5120 | elapsed time per iteration (ms): 5531.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 9.032261E+00 | loss scale: 65536.0 | grad norm: 4.996 | num zeros: 1323261952.0 | params norm: 300.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     200 | consumed samples:         5632 | elapsed time per iteration (ms): 6933.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.839462E+00 | loss scale: 65536.0 | grad norm: 4.928 | num zeros: 1294424448.0 | params norm: 300.371 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     200 | consumed samples:         6144 | elapsed time per iteration (ms): 4755.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.640061E+00 | loss scale: 65536.0 | grad norm: 4.910 | num zeros: 1321199232.0 | params norm: 300.447 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     200 | consumed samples:         6656 | elapsed time per iteration (ms): 5551.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.468368E+00 | loss scale: 65536.0 | grad norm: 4.826 | num zeros: 1344568192.0 | params norm: 300.521 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     200 | consumed samples:         7168 | elapsed time per iteration (ms): 5508.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.310764E+00 | loss scale: 65536.0 | grad norm: 4.751 | num zeros: 1421100672.0 | params norm: 300.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     200 | consumed samples:         7680 | elapsed time per iteration (ms): 5628.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.161868E+00 | loss scale: 65536.0 | grad norm: 4.649 | num zeros: 1448188288.0 | params norm: 300.667 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     200 | consumed samples:         8192 | elapsed time per iteration (ms): 4307.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 8.026905E+00 | loss scale: 65536.0 | grad norm: 4.494 | num zeros: 1474468096.0 | params norm: 300.738 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     200 | consumed samples:         8704 | elapsed time per iteration (ms): 4881.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.905260E+00 | loss scale: 65536.0 | grad norm: 4.294 | num zeros: 1500887808.0 | params norm: 300.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     200 | consumed samples:         9216 | elapsed time per iteration (ms): 5252.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.788041E+00 | loss scale: 65536.0 | grad norm: 4.064 | num zeros: 1426616320.0 | params norm: 300.876 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     200 | consumed samples:         9728 | elapsed time per iteration (ms): 5258.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.690736E+00 | loss scale: 65536.0 | grad norm: 3.740 | num zeros: 1219111552.0 | params norm: 300.944 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     200 | consumed samples:        10240 | elapsed time per iteration (ms): 4173.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.601889E+00 | loss scale: 65536.0 | grad norm: 3.344 | num zeros: 1038006016.0 | params norm: 301.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     200 | consumed samples:        10752 | elapsed time per iteration (ms): 5075.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.525438E+00 | loss scale: 65536.0 | grad norm: 2.841 | num zeros: 1057894016.0 | params norm: 301.074 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     200 | consumed samples:        11264 | elapsed time per iteration (ms): 5002.5 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.456488E+00 | loss scale: 65536.0 | grad norm: 2.247 | num zeros: 1140856448.0 | params norm: 301.136 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     200 | consumed samples:        11776 | elapsed time per iteration (ms): 5382.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.403111E+00 | loss scale: 65536.0 | grad norm: 1.576 | num zeros: 1223920512.0 | params norm: 301.196 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     200 | consumed samples:        12288 | elapsed time per iteration (ms): 4323.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.367343E+00 | loss scale: 65536.0 | grad norm: 0.832 | num zeros: 1278318208.0 | params norm: 301.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     200 | consumed samples:        12800 | elapsed time per iteration (ms): 5068.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.351529E+00 | loss scale: 65536.0 | grad norm: 0.304 | num zeros: 1354529536.0 | params norm: 301.311 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     200 | consumed samples:        13312 | elapsed time per iteration (ms): 5143.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.341910E+00 | loss scale: 65536.0 | grad norm: 0.621 | num zeros: 1228252032.0 | params norm: 301.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     200 | consumed samples:        13824 | elapsed time per iteration (ms): 5083.4 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.350627E+00 | loss scale: 65536.0 | grad norm: 0.829 | num zeros: 1198526592.0 | params norm: 301.421 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     200 | consumed samples:        14336 | elapsed time per iteration (ms): 4282.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.358063E+00 | loss scale: 65536.0 | grad norm: 0.783 | num zeros: 772874752.0 | params norm: 301.474 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     200 | consumed samples:        14848 | elapsed time per iteration (ms): 5133.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.347204E+00 | loss scale: 65536.0 | grad norm: 0.722 | num zeros: 1255144576.0 | params norm: 301.527 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     200 | consumed samples:        15360 | elapsed time per iteration (ms): 4829.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.339835E+00 | loss scale: 65536.0 | grad norm: 0.475 | num zeros: 1365842176.0 | params norm: 301.578 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     200 | consumed samples:        15872 | elapsed time per iteration (ms): 5786.8 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.344651E+00 | loss scale: 65536.0 | grad norm: 0.600 | num zeros: 1246693760.0 | params norm: 301.628 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     200 | consumed samples:        16384 | elapsed time per iteration (ms): 4645.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.322389E+00 | loss scale: 65536.0 | grad norm: 0.302 | num zeros: 1042410368.0 | params norm: 301.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     200 | consumed samples:        16896 | elapsed time per iteration (ms): 5088.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.321554E+00 | loss scale: 65536.0 | grad norm: 0.220 | num zeros: 1262109824.0 | params norm: 301.726 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     200 | consumed samples:        17408 | elapsed time per iteration (ms): 4916.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.317490E+00 | loss scale: 65536.0 | grad norm: 0.275 | num zeros: 1104727296.0 | params norm: 301.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     200 | consumed samples:        17920 | elapsed time per iteration (ms): 5711.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.312154E+00 | loss scale: 65536.0 | grad norm: 0.460 | num zeros: 1349854720.0 | params norm: 301.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     200 | consumed samples:        18432 | elapsed time per iteration (ms): 4574.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.316340E+00 | loss scale: 65536.0 | grad norm: 0.311 | num zeros: 1347020416.0 | params norm: 301.857 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     200 | consumed samples:        18944 | elapsed time per iteration (ms): 5273.0 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.317851E+00 | loss scale: 65536.0 | grad norm: 0.229 | num zeros: 1207810432.0 | params norm: 301.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     200 | consumed samples:        19456 | elapsed time per iteration (ms): 5508.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.311343E+00 | loss scale: 65536.0 | grad norm: 0.235 | num zeros: 1178761856.0 | params norm: 301.933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     200 | consumed samples:        19968 | elapsed time per iteration (ms): 5495.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.319816E+00 | loss scale: 65536.0 | grad norm: 0.302 | num zeros: 1115476096.0 | params norm: 301.969 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     200 | consumed samples:        20480 | elapsed time per iteration (ms): 6614.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.312723E+00 | loss scale: 65536.0 | grad norm: 0.842 | num zeros: 659224768.0 | params norm: 302.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     200 | consumed samples:        20992 | elapsed time per iteration (ms): 5929.6 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.444390E+00 | loss scale: 65536.0 | grad norm: 14.275 | num zeros: 458158400.0 | params norm: 302.034 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     200 | consumed samples:        21504 | elapsed time per iteration (ms): 5457.1 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.316005E+00 | loss scale: 65536.0 | grad norm: 0.673 | num zeros: 484408032.0 | params norm: 302.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     200 | consumed samples:        22016 | elapsed time per iteration (ms): 4639.9 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.302865E+00 | loss scale: 65536.0 | grad norm: 0.538 | num zeros: 948106048.0 | params norm: 302.099 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     200 | consumed samples:        22528 | elapsed time per iteration (ms): 5654.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.293959E+00 | loss scale: 65536.0 | grad norm: 0.790 | num zeros: 1015208320.0 | params norm: 302.130 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     200 | consumed samples:        23040 | elapsed time per iteration (ms): 4820.3 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.284867E+00 | loss scale: 65536.0 | grad norm: 0.533 | num zeros: 919337024.0 | params norm: 302.160 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     200 | consumed samples:        23552 | elapsed time per iteration (ms): 5270.7 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.262318E+00 | loss scale: 65536.0 | grad norm: 0.476 | num zeros: 801340864.0 | params norm: 302.191 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     200 | consumed samples:        24064 | elapsed time per iteration (ms): 4461.2 | learning rate: 1.000E-04 | global batch size:   512 | lm loss: 7.240753E+00 | loss scale: 65536.0 | grad norm: 0.677 | num zeros: 724999744.0 | params norm: 302.223 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     200 | consumed samples:        24576 | elapsed time per iteration (ms): 5214.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.252378E+00 | loss scale: 65536.0 | grad norm: 1.328 | num zeros: 741621824.0 | params norm: 302.251 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     200 | consumed samples:        25088 | elapsed time per iteration (ms): 4526.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.223237E+00 | loss scale: 65536.0 | grad norm: 0.627 | num zeros: 664621632.0 | params norm: 302.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     200 | consumed samples:        25600 | elapsed time per iteration (ms): 5388.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.219905E+00 | loss scale: 65536.0 | grad norm: 0.686 | num zeros: 721417664.0 | params norm: 302.310 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     200 | consumed samples:        26112 | elapsed time per iteration (ms): 4068.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.204256E+00 | loss scale: 65536.0 | grad norm: 0.443 | num zeros: 836399232.0 | params norm: 302.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     200 | consumed samples:        26624 | elapsed time per iteration (ms): 4685.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.193879E+00 | loss scale: 65536.0 | grad norm: 0.542 | num zeros: 851696000.0 | params norm: 302.365 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     200 | consumed samples:        27136 | elapsed time per iteration (ms): 16969.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.178706E+00 | loss scale: 65536.0 | grad norm: 0.579 | num zeros: 854231808.0 | params norm: 302.390 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     200 | consumed samples:        27648 | elapsed time per iteration (ms): 4810.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.173657E+00 | loss scale: 65536.0 | grad norm: 0.771 | num zeros: 823155648.0 | params norm: 302.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     200 | consumed samples:        28160 | elapsed time per iteration (ms): 3915.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.156533E+00 | loss scale: 65536.0 | grad norm: 0.558 | num zeros: 771620608.0 | params norm: 302.445 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     200 | consumed samples:        28672 | elapsed time per iteration (ms): 4759.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.144863E+00 | loss scale: 65536.0 | grad norm: 0.461 | num zeros: 884922304.0 | params norm: 302.471 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     200 | consumed samples:        29184 | elapsed time per iteration (ms): 4287.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.137817E+00 | loss scale: 65536.0 | grad norm: 0.606 | num zeros: 869387520.0 | params norm: 302.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     200 | consumed samples:        29696 | elapsed time per iteration (ms): 6005.5 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.126221E+00 | loss scale: 65536.0 | grad norm: 0.569 | num zeros: 831913536.0 | params norm: 302.521 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     200 | consumed samples:        30208 | elapsed time per iteration (ms): 4201.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.125952E+00 | loss scale: 65536.0 | grad norm: 0.449 | num zeros: 877056704.0 | params norm: 302.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     200 | consumed samples:        30720 | elapsed time per iteration (ms): 4733.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.115015E+00 | loss scale: 65536.0 | grad norm: 0.288 | num zeros: 891007744.0 | params norm: 302.570 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     200 | consumed samples:        31232 | elapsed time per iteration (ms): 4826.9 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.108114E+00 | loss scale: 65536.0 | grad norm: 0.332 | num zeros: 945993216.0 | params norm: 302.593 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     200 | consumed samples:        31744 | elapsed time per iteration (ms): 5300.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.098095E+00 | loss scale: 65536.0 | grad norm: 0.390 | num zeros: 969671040.0 | params norm: 302.617 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     200 | consumed samples:        32256 | elapsed time per iteration (ms): 4619.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.102887E+00 | loss scale: 65536.0 | grad norm: 0.378 | num zeros: 980283264.0 | params norm: 302.639 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     200 | consumed samples:        32768 | elapsed time per iteration (ms): 4217.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.094160E+00 | loss scale: 65536.0 | grad norm: 0.347 | num zeros: 878902272.0 | params norm: 302.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     200 | consumed samples:        33280 | elapsed time per iteration (ms): 4927.1 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.091561E+00 | loss scale: 65536.0 | grad norm: 0.349 | num zeros: 804394368.0 | params norm: 302.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     200 | consumed samples:        33792 | elapsed time per iteration (ms): 5311.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.073045E+00 | loss scale: 65536.0 | grad norm: 0.364 | num zeros: 762560256.0 | params norm: 302.703 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     200 | consumed samples:        34304 | elapsed time per iteration (ms): 4849.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.073926E+00 | loss scale: 65536.0 | grad norm: 0.376 | num zeros: 747345152.0 | params norm: 302.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     200 | consumed samples:        34816 | elapsed time per iteration (ms): 5351.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.056873E+00 | loss scale: 65536.0 | grad norm: 0.661 | num zeros: 722417472.0 | params norm: 302.742 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     200 | consumed samples:        35328 | elapsed time per iteration (ms): 15741.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.052712E+00 | loss scale: 65536.0 | grad norm: 0.610 | num zeros: 663242560.0 | params norm: 302.762 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     200 | consumed samples:        35840 | elapsed time per iteration (ms): 4041.8 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.058983E+00 | loss scale: 65536.0 | grad norm: 0.665 | num zeros: 696984064.0 | params norm: 302.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     200 | consumed samples:        36352 | elapsed time per iteration (ms): 4086.2 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.023076E+00 | loss scale: 65536.0 | grad norm: 0.425 | num zeros: 725357056.0 | params norm: 302.797 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     200 | consumed samples:        36864 | elapsed time per iteration (ms): 4126.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.030528E+00 | loss scale: 65536.0 | grad norm: 0.605 | num zeros: 636111744.0 | params norm: 302.815 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     200 | consumed samples:        37376 | elapsed time per iteration (ms): 7361.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.023005E+00 | loss scale: 65536.0 | grad norm: 0.797 | num zeros: 724648256.0 | params norm: 302.833 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     200 | consumed samples:        37888 | elapsed time per iteration (ms): 53347.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.017678E+00 | loss scale: 65536.0 | grad norm: 0.571 | num zeros: 810611968.0 | params norm: 302.850 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     200 | consumed samples:        38400 | elapsed time per iteration (ms): 8026.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.009125E+00 | loss scale: 65536.0 | grad norm: 0.552 | num zeros: 736205184.0 | params norm: 302.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     200 | consumed samples:        38912 | elapsed time per iteration (ms): 6038.3 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 6.989690E+00 | loss scale: 65536.0 | grad norm: 0.426 | num zeros: 688705984.0 | params norm: 302.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     200 | consumed samples:        39424 | elapsed time per iteration (ms): 6050.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 6.986407E+00 | loss scale: 65536.0 | grad norm: 0.942 | num zeros: 677552576.0 | params norm: 302.898 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     200 | consumed samples:        39936 | elapsed time per iteration (ms): 6680.7 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.023508E+00 | loss scale: 65536.0 | grad norm: 1.312 | num zeros: 632667008.0 | params norm: 302.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     200 | consumed samples:        40448 | elapsed time per iteration (ms): 6812.0 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 6.975141E+00 | loss scale: 65536.0 | grad norm: 0.588 | num zeros: 622864064.0 | params norm: 302.932 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     200 | consumed samples:        40960 | elapsed time per iteration (ms): 5397.6 | learning rate: 9.999E-05 | global batch size:   512 | lm loss: 7.010340E+00 | loss scale: 65536.0 | grad norm: 1.214 | num zeros: 589083392.0 | params norm: 302.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     200 | consumed samples:        41472 | elapsed time per iteration (ms): 5380.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.980076E+00 | loss scale: 65536.0 | grad norm: 0.707 | num zeros: 710748032.0 | params norm: 302.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     200 | consumed samples:        41984 | elapsed time per iteration (ms): 4825.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.982316E+00 | loss scale: 65536.0 | grad norm: 0.878 | num zeros: 679763072.0 | params norm: 302.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     200 | consumed samples:        42496 | elapsed time per iteration (ms): 5186.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.966287E+00 | loss scale: 65536.0 | grad norm: 0.772 | num zeros: 621096128.0 | params norm: 302.996 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     200 | consumed samples:        43008 | elapsed time per iteration (ms): 4735.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.945790E+00 | loss scale: 65536.0 | grad norm: 0.782 | num zeros: 660031104.0 | params norm: 303.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     200 | consumed samples:        43520 | elapsed time per iteration (ms): 4939.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.944193E+00 | loss scale: 65536.0 | grad norm: 0.589 | num zeros: 689724416.0 | params norm: 303.027 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     200 | consumed samples:        44032 | elapsed time per iteration (ms): 4350.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.947906E+00 | loss scale: 65536.0 | grad norm: 0.795 | num zeros: 613992320.0 | params norm: 303.041 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     200 | consumed samples:        44544 | elapsed time per iteration (ms): 5127.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.926854E+00 | loss scale: 65536.0 | grad norm: 1.218 | num zeros: 625595392.0 | params norm: 303.056 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     200 | consumed samples:        45056 | elapsed time per iteration (ms): 4115.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.920004E+00 | loss scale: 65536.0 | grad norm: 0.506 | num zeros: 593594688.0 | params norm: 303.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     200 | consumed samples:        45568 | elapsed time per iteration (ms): 4337.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.920175E+00 | loss scale: 65536.0 | grad norm: 0.731 | num zeros: 633143808.0 | params norm: 303.090 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     200 | consumed samples:        46080 | elapsed time per iteration (ms): 4022.4 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.907815E+00 | loss scale: 65536.0 | grad norm: 0.906 | num zeros: 677714688.0 | params norm: 303.106 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     200 | consumed samples:        46592 | elapsed time per iteration (ms): 4485.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.911109E+00 | loss scale: 65536.0 | grad norm: 0.961 | num zeros: 582374464.0 | params norm: 303.123 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     200 | consumed samples:        47104 | elapsed time per iteration (ms): 4618.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.894730E+00 | loss scale: 65536.0 | grad norm: 0.491 | num zeros: 608390336.0 | params norm: 303.140 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     200 | consumed samples:        47616 | elapsed time per iteration (ms): 4224.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.889011E+00 | loss scale: 65536.0 | grad norm: 0.840 | num zeros: 504432096.0 | params norm: 303.158 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     200 | consumed samples:        48128 | elapsed time per iteration (ms): 5053.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.888673E+00 | loss scale: 65536.0 | grad norm: 0.721 | num zeros: 582100032.0 | params norm: 303.177 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     200 | consumed samples:        48640 | elapsed time per iteration (ms): 4436.6 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.870886E+00 | loss scale: 65536.0 | grad norm: 0.605 | num zeros: 563942144.0 | params norm: 303.196 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     200 | consumed samples:        49152 | elapsed time per iteration (ms): 5232.9 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.872190E+00 | loss scale: 65536.0 | grad norm: 0.859 | num zeros: 494826112.0 | params norm: 303.216 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     200 | consumed samples:        49664 | elapsed time per iteration (ms): 4162.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.857290E+00 | loss scale: 65536.0 | grad norm: 0.952 | num zeros: 473934784.0 | params norm: 303.236 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     200 | consumed samples:        50176 | elapsed time per iteration (ms): 4634.2 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.839038E+00 | loss scale: 65536.0 | grad norm: 0.631 | num zeros: 436424704.0 | params norm: 303.256 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     200 | consumed samples:        50688 | elapsed time per iteration (ms): 3763.3 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.837536E+00 | loss scale: 65536.0 | grad norm: 0.680 | num zeros: 399546208.0 | params norm: 303.277 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     200 | consumed samples:        51200 | elapsed time per iteration (ms): 4480.8 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.832880E+00 | loss scale: 65536.0 | grad norm: 0.852 | num zeros: 416953696.0 | params norm: 303.298 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      101/     200 | consumed samples:        51712 | elapsed time per iteration (ms): 5283.7 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.837740E+00 | loss scale: 65536.0 | grad norm: 0.891 | num zeros: 404544608.0 | params norm: 303.319 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      102/     200 | consumed samples:        52224 | elapsed time per iteration (ms): 5970.5 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.809298E+00 | loss scale: 65536.0 | grad norm: 0.468 | num zeros: 350127232.0 | params norm: 303.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      103/     200 | consumed samples:        52736 | elapsed time per iteration (ms): 4204.0 | learning rate: 9.998E-05 | global batch size:   512 | lm loss: 6.803781E+00 | loss scale: 65536.0 | grad norm: 0.876 | num zeros: 399844672.0 | params norm: 303.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      104/     200 | consumed samples:        53248 | elapsed time per iteration (ms): 4341.1 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.808240E+00 | loss scale: 65536.0 | grad norm: 0.815 | num zeros: 437378240.0 | params norm: 303.385 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      105/     200 | consumed samples:        53760 | elapsed time per iteration (ms): 4402.7 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.814826E+00 | loss scale: 65536.0 | grad norm: 1.064 | num zeros: 339112256.0 | params norm: 303.407 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      106/     200 | consumed samples:        54272 | elapsed time per iteration (ms): 4006.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.791354E+00 | loss scale: 65536.0 | grad norm: 0.723 | num zeros: 299285152.0 | params norm: 303.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      107/     200 | consumed samples:        54784 | elapsed time per iteration (ms): 4567.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.808942E+00 | loss scale: 65536.0 | grad norm: 1.335 | num zeros: 284386624.0 | params norm: 303.454 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      108/     200 | consumed samples:        55296 | elapsed time per iteration (ms): 4107.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.780855E+00 | loss scale: 65536.0 | grad norm: 1.003 | num zeros: 298426912.0 | params norm: 303.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      109/     200 | consumed samples:        55808 | elapsed time per iteration (ms): 4376.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.776907E+00 | loss scale: 65536.0 | grad norm: 0.662 | num zeros: 330253728.0 | params norm: 303.503 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      110/     200 | consumed samples:        56320 | elapsed time per iteration (ms): 4772.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.769702E+00 | loss scale: 65536.0 | grad norm: 0.634 | num zeros: 332723456.0 | params norm: 303.526 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      111/     200 | consumed samples:        56832 | elapsed time per iteration (ms): 4480.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.765460E+00 | loss scale: 65536.0 | grad norm: 0.565 | num zeros: 426313792.0 | params norm: 303.550 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      112/     200 | consumed samples:        57344 | elapsed time per iteration (ms): 4103.5 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.757923E+00 | loss scale: 65536.0 | grad norm: 0.716 | num zeros: 352177504.0 | params norm: 303.576 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      113/     200 | consumed samples:        57856 | elapsed time per iteration (ms): 4096.2 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.744816E+00 | loss scale: 65536.0 | grad norm: 0.337 | num zeros: 322671360.0 | params norm: 303.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      114/     200 | consumed samples:        58368 | elapsed time per iteration (ms): 4437.6 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.753992E+00 | loss scale: 65536.0 | grad norm: 0.623 | num zeros: 339961568.0 | params norm: 303.627 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      115/     200 | consumed samples:        58880 | elapsed time per iteration (ms): 4068.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.750678E+00 | loss scale: 65536.0 | grad norm: 0.533 | num zeros: 312716608.0 | params norm: 303.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      116/     200 | consumed samples:        59392 | elapsed time per iteration (ms): 4879.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.731806E+00 | loss scale: 65536.0 | grad norm: 0.568 | num zeros: 263271392.0 | params norm: 303.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      117/     200 | consumed samples:        59904 | elapsed time per iteration (ms): 3869.4 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.728269E+00 | loss scale: 65536.0 | grad norm: 0.455 | num zeros: 419864096.0 | params norm: 303.707 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      118/     200 | consumed samples:        60416 | elapsed time per iteration (ms): 3866.3 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.727348E+00 | loss scale: 65536.0 | grad norm: 0.400 | num zeros: 348636800.0 | params norm: 303.734 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      119/     200 | consumed samples:        60928 | elapsed time per iteration (ms): 4220.0 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.715252E+00 | loss scale: 65536.0 | grad norm: 0.432 | num zeros: 405302976.0 | params norm: 303.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      120/     200 | consumed samples:        61440 | elapsed time per iteration (ms): 4135.9 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.716043E+00 | loss scale: 65536.0 | grad norm: 0.635 | num zeros: 405267392.0 | params norm: 303.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      121/     200 | consumed samples:        61952 | elapsed time per iteration (ms): 4782.8 | learning rate: 9.997E-05 | global batch size:   512 | lm loss: 6.706367E+00 | loss scale: 65536.0 | grad norm: 1.259 | num zeros: 412733312.0 | params norm: 303.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      122/     200 | consumed samples:        62464 | elapsed time per iteration (ms): 3746.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.738999E+00 | loss scale: 65536.0 | grad norm: 1.673 | num zeros: 469935296.0 | params norm: 303.840 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      123/     200 | consumed samples:        62976 | elapsed time per iteration (ms): 4038.4 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.703147E+00 | loss scale: 65536.0 | grad norm: 0.796 | num zeros: 388745600.0 | params norm: 303.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      124/     200 | consumed samples:        63488 | elapsed time per iteration (ms): 4298.8 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.746003E+00 | loss scale: 65536.0 | grad norm: 1.924 | num zeros: 450582176.0 | params norm: 303.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      125/     200 | consumed samples:        64000 | elapsed time per iteration (ms): 4370.1 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.718445E+00 | loss scale: 65536.0 | grad norm: 1.283 | num zeros: 444242624.0 | params norm: 303.914 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      126/     200 | consumed samples:        64512 | elapsed time per iteration (ms): 4656.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.711798E+00 | loss scale: 65536.0 | grad norm: 1.232 | num zeros: 420757664.0 | params norm: 303.940 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      127/     200 | consumed samples:        65024 | elapsed time per iteration (ms): 3853.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.698660E+00 | loss scale: 65536.0 | grad norm: 0.806 | num zeros: 437005440.0 | params norm: 303.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      128/     200 | consumed samples:        65536 | elapsed time per iteration (ms): 4495.0 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.707142E+00 | loss scale: 65536.0 | grad norm: 0.877 | num zeros: 438809312.0 | params norm: 303.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      129/     200 | consumed samples:        66048 | elapsed time per iteration (ms): 4023.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.685427E+00 | loss scale: 65536.0 | grad norm: 0.684 | num zeros: 466208128.0 | params norm: 304.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      130/     200 | consumed samples:        66560 | elapsed time per iteration (ms): 5478.0 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.691164E+00 | loss scale: 65536.0 | grad norm: 1.020 | num zeros: 438419360.0 | params norm: 304.038 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      131/     200 | consumed samples:        67072 | elapsed time per iteration (ms): 3794.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.689993E+00 | loss scale: 65536.0 | grad norm: 0.817 | num zeros: 416348000.0 | params norm: 304.066 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      132/     200 | consumed samples:        67584 | elapsed time per iteration (ms): 3866.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.661578E+00 | loss scale: 65536.0 | grad norm: 0.519 | num zeros: 526618912.0 | params norm: 304.093 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      133/     200 | consumed samples:        68096 | elapsed time per iteration (ms): 4527.5 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.662695E+00 | loss scale: 65536.0 | grad norm: 0.622 | num zeros: 458954112.0 | params norm: 304.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      134/     200 | consumed samples:        68608 | elapsed time per iteration (ms): 4549.2 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.663223E+00 | loss scale: 65536.0 | grad norm: 0.465 | num zeros: 430849472.0 | params norm: 304.142 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      135/     200 | consumed samples:        69120 | elapsed time per iteration (ms): 4242.6 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.670495E+00 | loss scale: 65536.0 | grad norm: 0.830 | num zeros: 401646144.0 | params norm: 304.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      136/     200 | consumed samples:        69632 | elapsed time per iteration (ms): 3776.9 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.667172E+00 | loss scale: 65536.0 | grad norm: 0.801 | num zeros: 385687424.0 | params norm: 304.196 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      137/     200 | consumed samples:        70144 | elapsed time per iteration (ms): 4089.3 | learning rate: 9.996E-05 | global batch size:   512 | lm loss: 6.650224E+00 | loss scale: 65536.0 | grad norm: 0.897 | num zeros: 384522080.0 | params norm: 304.222 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      138/     200 | consumed samples:        70656 | elapsed time per iteration (ms): 4302.7 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.646424E+00 | loss scale: 65536.0 | grad norm: 0.685 | num zeros: 422712992.0 | params norm: 304.248 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      139/     200 | consumed samples:        71168 | elapsed time per iteration (ms): 4900.9 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.637267E+00 | loss scale: 65536.0 | grad norm: 0.447 | num zeros: 435036576.0 | params norm: 304.274 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      140/     200 | consumed samples:        71680 | elapsed time per iteration (ms): 4621.2 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.636183E+00 | loss scale: 65536.0 | grad norm: 0.417 | num zeros: 438606144.0 | params norm: 304.301 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      141/     200 | consumed samples:        72192 | elapsed time per iteration (ms): 3766.6 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.617391E+00 | loss scale: 65536.0 | grad norm: 0.408 | num zeros: 449762560.0 | params norm: 304.328 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      142/     200 | consumed samples:        72704 | elapsed time per iteration (ms): 4459.4 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.623106E+00 | loss scale: 65536.0 | grad norm: 0.533 | num zeros: 449960832.0 | params norm: 304.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      143/     200 | consumed samples:        73216 | elapsed time per iteration (ms): 4341.8 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.615952E+00 | loss scale: 65536.0 | grad norm: 0.737 | num zeros: 424527040.0 | params norm: 304.381 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      144/     200 | consumed samples:        73728 | elapsed time per iteration (ms): 4731.0 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.616250E+00 | loss scale: 65536.0 | grad norm: 0.842 | num zeros: 468722560.0 | params norm: 304.406 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      145/     200 | consumed samples:        74240 | elapsed time per iteration (ms): 3744.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.611292E+00 | loss scale: 65536.0 | grad norm: 0.737 | num zeros: 440531040.0 | params norm: 304.433 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      146/     200 | consumed samples:        74752 | elapsed time per iteration (ms): 3916.7 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.606181E+00 | loss scale: 65536.0 | grad norm: 0.577 | num zeros: 461660736.0 | params norm: 304.461 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      147/     200 | consumed samples:        75264 | elapsed time per iteration (ms): 4240.6 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.614826E+00 | loss scale: 65536.0 | grad norm: 0.356 | num zeros: 385580288.0 | params norm: 304.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      148/     200 | consumed samples:        75776 | elapsed time per iteration (ms): 4263.5 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.589403E+00 | loss scale: 65536.0 | grad norm: 0.455 | num zeros: 363640320.0 | params norm: 304.515 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      149/     200 | consumed samples:        76288 | elapsed time per iteration (ms): 4609.7 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.602058E+00 | loss scale: 65536.0 | grad norm: 0.507 | num zeros: 374942720.0 | params norm: 304.542 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      150/     200 | consumed samples:        76800 | elapsed time per iteration (ms): 3857.4 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.592442E+00 | loss scale: 65536.0 | grad norm: 0.704 | num zeros: 360010624.0 | params norm: 304.569 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      151/     200 | consumed samples:        77312 | elapsed time per iteration (ms): 4210.2 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.587705E+00 | loss scale: 65536.0 | grad norm: 0.592 | num zeros: 358365600.0 | params norm: 304.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      152/     200 | consumed samples:        77824 | elapsed time per iteration (ms): 4205.8 | learning rate: 9.995E-05 | global batch size:   512 | lm loss: 6.584589E+00 | loss scale: 65536.0 | grad norm: 0.421 | num zeros: 380992768.0 | params norm: 304.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      153/     200 | consumed samples:        78336 | elapsed time per iteration (ms): 4033.7 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.589281E+00 | loss scale: 65536.0 | grad norm: 0.541 | num zeros: 362411520.0 | params norm: 304.649 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      154/     200 | consumed samples:        78848 | elapsed time per iteration (ms): 4621.3 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.576215E+00 | loss scale: 65536.0 | grad norm: 0.499 | num zeros: 282925024.0 | params norm: 304.677 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      155/     200 | consumed samples:        79360 | elapsed time per iteration (ms): 3961.9 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.570890E+00 | loss scale: 65536.0 | grad norm: 0.506 | num zeros: 314924672.0 | params norm: 304.703 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      156/     200 | consumed samples:        79872 | elapsed time per iteration (ms): 4233.7 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.572433E+00 | loss scale: 65536.0 | grad norm: 0.548 | num zeros: 298344608.0 | params norm: 304.729 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      157/     200 | consumed samples:        80384 | elapsed time per iteration (ms): 3746.7 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.564291E+00 | loss scale: 65536.0 | grad norm: 0.439 | num zeros: 281538304.0 | params norm: 304.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      158/     200 | consumed samples:        80896 | elapsed time per iteration (ms): 5568.8 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.560624E+00 | loss scale: 65536.0 | grad norm: 0.557 | num zeros: 272693600.0 | params norm: 304.783 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      159/     200 | consumed samples:        81408 | elapsed time per iteration (ms): 5446.0 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.569711E+00 | loss scale: 65536.0 | grad norm: 0.607 | num zeros: 309495392.0 | params norm: 304.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      160/     200 | consumed samples:        81920 | elapsed time per iteration (ms): 4249.4 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.567472E+00 | loss scale: 65536.0 | grad norm: 0.604 | num zeros: 314638656.0 | params norm: 304.839 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      161/     200 | consumed samples:        82432 | elapsed time per iteration (ms): 4333.2 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.555473E+00 | loss scale: 65536.0 | grad norm: 0.602 | num zeros: 316588992.0 | params norm: 304.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      162/     200 | consumed samples:        82944 | elapsed time per iteration (ms): 4192.8 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.570243E+00 | loss scale: 65536.0 | grad norm: 1.471 | num zeros: 300080448.0 | params norm: 304.892 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      163/     200 | consumed samples:        83456 | elapsed time per iteration (ms): 4823.0 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.554425E+00 | loss scale: 65536.0 | grad norm: 1.172 | num zeros: 260088336.0 | params norm: 304.919 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      164/     200 | consumed samples:        83968 | elapsed time per iteration (ms): 3949.7 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.559170E+00 | loss scale: 65536.0 | grad norm: 0.651 | num zeros: 252070704.0 | params norm: 304.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      165/     200 | consumed samples:        84480 | elapsed time per iteration (ms): 4731.5 | learning rate: 9.994E-05 | global batch size:   512 | lm loss: 6.545450E+00 | loss scale: 65536.0 | grad norm: 0.670 | num zeros: 225933888.0 | params norm: 304.971 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      166/     200 | consumed samples:        84992 | elapsed time per iteration (ms): 4073.0 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.550902E+00 | loss scale: 65536.0 | grad norm: 0.873 | num zeros: 250929472.0 | params norm: 304.996 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      167/     200 | consumed samples:        85504 | elapsed time per iteration (ms): 4436.9 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.547691E+00 | loss scale: 65536.0 | grad norm: 0.814 | num zeros: 249263936.0 | params norm: 305.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      168/     200 | consumed samples:        86016 | elapsed time per iteration (ms): 3928.6 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.535291E+00 | loss scale: 65536.0 | grad norm: 0.665 | num zeros: 237050560.0 | params norm: 305.047 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      169/     200 | consumed samples:        86528 | elapsed time per iteration (ms): 3981.3 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.537662E+00 | loss scale: 65536.0 | grad norm: 0.932 | num zeros: 192965376.0 | params norm: 305.072 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      170/     200 | consumed samples:        87040 | elapsed time per iteration (ms): 4647.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.525279E+00 | loss scale: 65536.0 | grad norm: 0.859 | num zeros: 265063104.0 | params norm: 305.096 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      171/     200 | consumed samples:        87552 | elapsed time per iteration (ms): 4307.2 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.519004E+00 | loss scale: 65536.0 | grad norm: 0.648 | num zeros: 241958400.0 | params norm: 305.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      172/     200 | consumed samples:        88064 | elapsed time per iteration (ms): 4570.7 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.528281E+00 | loss scale: 65536.0 | grad norm: 0.777 | num zeros: 219579936.0 | params norm: 305.144 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      173/     200 | consumed samples:        88576 | elapsed time per iteration (ms): 3822.6 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.539017E+00 | loss scale: 65536.0 | grad norm: 1.936 | num zeros: 235537024.0 | params norm: 305.168 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      174/     200 | consumed samples:        89088 | elapsed time per iteration (ms): 3951.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.509624E+00 | loss scale: 65536.0 | grad norm: 0.851 | num zeros: 241665056.0 | params norm: 305.192 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      175/     200 | consumed samples:        89600 | elapsed time per iteration (ms): 4349.0 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.525476E+00 | loss scale: 65536.0 | grad norm: 1.340 | num zeros: 218264768.0 | params norm: 305.216 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      176/     200 | consumed samples:        90112 | elapsed time per iteration (ms): 4612.4 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.519305E+00 | loss scale: 65536.0 | grad norm: 1.045 | num zeros: 217247184.0 | params norm: 305.240 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      177/     200 | consumed samples:        90624 | elapsed time per iteration (ms): 4352.8 | learning rate: 9.993E-05 | global batch size:   512 | lm loss: 6.511302E+00 | loss scale: 65536.0 | grad norm: 0.779 | num zeros: 256512128.0 | params norm: 305.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      178/     200 | consumed samples:        91136 | elapsed time per iteration (ms): 3919.5 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.502652E+00 | loss scale: 65536.0 | grad norm: 0.800 | num zeros: 247641776.0 | params norm: 305.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      179/     200 | consumed samples:        91648 | elapsed time per iteration (ms): 4681.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.519525E+00 | loss scale: 65536.0 | grad norm: 1.148 | num zeros: 239233568.0 | params norm: 305.305 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      180/     200 | consumed samples:        92160 | elapsed time per iteration (ms): 3976.2 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.489081E+00 | loss scale: 65536.0 | grad norm: 0.773 | num zeros: 257875088.0 | params norm: 305.329 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      181/     200 | consumed samples:        92672 | elapsed time per iteration (ms): 4357.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.496233E+00 | loss scale: 65536.0 | grad norm: 0.768 | num zeros: 242431328.0 | params norm: 305.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      182/     200 | consumed samples:        93184 | elapsed time per iteration (ms): 4008.3 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.495097E+00 | loss scale: 65536.0 | grad norm: 0.750 | num zeros: 253919312.0 | params norm: 305.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      183/     200 | consumed samples:        93696 | elapsed time per iteration (ms): 4093.1 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.497432E+00 | loss scale: 65536.0 | grad norm: 0.762 | num zeros: 272286432.0 | params norm: 305.400 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      184/     200 | consumed samples:        94208 | elapsed time per iteration (ms): 4432.4 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.480144E+00 | loss scale: 65536.0 | grad norm: 0.524 | num zeros: 238100480.0 | params norm: 305.424 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      185/     200 | consumed samples:        94720 | elapsed time per iteration (ms): 3975.3 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.480968E+00 | loss scale: 65536.0 | grad norm: 0.520 | num zeros: 264602144.0 | params norm: 305.448 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      186/     200 | consumed samples:        95232 | elapsed time per iteration (ms): 4676.9 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.478561E+00 | loss scale: 65536.0 | grad norm: 0.592 | num zeros: 249250992.0 | params norm: 305.472 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      187/     200 | consumed samples:        95744 | elapsed time per iteration (ms): 3842.8 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.476794E+00 | loss scale: 65536.0 | grad norm: 0.448 | num zeros: 228155840.0 | params norm: 305.497 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      188/     200 | consumed samples:        96256 | elapsed time per iteration (ms): 4321.0 | learning rate: 9.992E-05 | global batch size:   512 | lm loss: 6.470192E+00 | loss scale: 65536.0 | grad norm: 0.433 | num zeros: 224900432.0 | params norm: 305.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      189/     200 | consumed samples:        96768 | elapsed time per iteration (ms): 4357.4 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.470870E+00 | loss scale: 65536.0 | grad norm: 0.390 | num zeros: 259415360.0 | params norm: 305.548 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      190/     200 | consumed samples:        97280 | elapsed time per iteration (ms): 4209.2 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.467644E+00 | loss scale: 65536.0 | grad norm: 0.391 | num zeros: 241637872.0 | params norm: 305.575 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      191/     200 | consumed samples:        97792 | elapsed time per iteration (ms): 5585.7 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.461577E+00 | loss scale: 65536.0 | grad norm: 0.337 | num zeros: 213804400.0 | params norm: 305.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      192/     200 | consumed samples:        98304 | elapsed time per iteration (ms): 3897.7 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.453142E+00 | loss scale: 65536.0 | grad norm: 0.399 | num zeros: 189899360.0 | params norm: 305.627 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      193/     200 | consumed samples:        98816 | elapsed time per iteration (ms): 4566.9 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.448217E+00 | loss scale: 65536.0 | grad norm: 0.542 | num zeros: 181480224.0 | params norm: 305.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      194/     200 | consumed samples:        99328 | elapsed time per iteration (ms): 4315.3 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.453355E+00 | loss scale: 65536.0 | grad norm: 0.856 | num zeros: 157868240.0 | params norm: 305.676 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      195/     200 | consumed samples:        99840 | elapsed time per iteration (ms): 4937.2 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.445822E+00 | loss scale: 65536.0 | grad norm: 0.487 | num zeros: 163647104.0 | params norm: 305.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      196/     200 | consumed samples:       100352 | elapsed time per iteration (ms): 3838.6 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.432579E+00 | loss scale: 65536.0 | grad norm: 0.396 | num zeros: 197296560.0 | params norm: 305.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      197/     200 | consumed samples:       100864 | elapsed time per iteration (ms): 3992.5 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.460050E+00 | loss scale: 65536.0 | grad norm: 0.543 | num zeros: 177872176.0 | params norm: 305.753 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      198/     200 | consumed samples:       101376 | elapsed time per iteration (ms): 5030.7 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.431195E+00 | loss scale: 65536.0 | grad norm: 0.437 | num zeros: 182239248.0 | params norm: 305.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      199/     200 | consumed samples:       101888 | elapsed time per iteration (ms): 4554.8 | learning rate: 9.991E-05 | global batch size:   512 | lm loss: 6.432913E+00 | loss scale: 65536.0 | grad norm: 0.373 | num zeros: 183701056.0 | params norm: 305.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      200/     200 | consumed samples:       102400 | elapsed time per iteration (ms): 4515.9 | learning rate: 9.990E-05 | global batch size:   512 | lm loss: 6.435225E+00 | loss scale: 65536.0 | grad norm: 0.538 | num zeros: 142392064.0 | params norm: 305.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2022-12-21 23:41:45 
