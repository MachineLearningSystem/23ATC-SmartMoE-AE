srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w nico[1-2] -p AE
#!/bin/bash

export NUM_LAYERS=16
export HIDDEN_SIZE=1536
export NUM_ATTN_HEADS=16
export SEQ_LEN=1024

export FMOE_FASTER_GLBPLC_ALPHA="2"
export FMOE_FASTER_GLBPLC_DMODEL=${HIDDEN_SIZE}

export dense_name=GPT-L${NUM_LAYERS}-H${HIDDEN_SIZE}-ATTN${NUM_ATTN_HEADS}-SEQ${SEQ_LEN}-ALPHA${FMOE_FASTER_GLBPLC_ALPHA}#!/bin/bash

export TOT_EXPERTS=32
export GATE="naive"
export GSHARD_CAP="4.8"

if [ ${GATE} == 'gshard' ];then
    GATE_NAME=${GATE}${GSHARD_CAP}
else
    GATE_NAME=${GATE}

export sparse_name=${TOT_EXPERTS}MoE\_${GATE_NAME}#!/bin/bash

export TENSOR_PARALLEL_SIZE=1
export PIPELINE_PARALLEL_SIZE=2
export DATA_PARALLEL_SIZE=8
export EXPERT_EP_SIZE=8
export EXPERT_DP_SIZE=1
export GLOBAL_BATCH_SIZE=256
export MICRO_BATCH_SIZE=2
export FMOE_FASTER_SCHEDULE_ENABLE=OFF
export FMOE_FASTER_GROUP_SIZE=none
export FMOE_FASTER_SHADOW_ENABLE=OFF
export DYNAMIC_ENABLE=ON
export DYNAMIC_FREQ=10
export ITERATION=100

export TRAIN_SAMPLES=$(( $GLOBAL_BATCH_SIZE * $ITERATION))

export parallel_name=t$TENSOR_PARALLEL_SIZE\_p$PIPELINE_PARALLEL_SIZE\_d$DATA_PARALLEL_SIZE\_ep${EXPERT_EP_SIZE}\_dp${EXPERT_DP_SIZE}\_gbs$GLOBAL_BATCH_SIZE\_mbs$MICRO_BATCH_SIZE\_smart\_${FMOE_FASTER_SCHEDULE_ENABLE}\_smartGP${FMOE_FASTER_GROUP_SIZE}\_shadow${FMOE_FASTER_SHADOW_ENABLE}\_dynamic${DYNAMIC_ENABLE}\_freq${DYNAMIC_FREQ}\_iter${ITERATION}#!/bin/bash

export NNODES=2
export NODELIST="nico[1-2]"
export FMOE_FASTER_GLBPLC_NETBW="8e9"
export FMOE_FASTER_GLBPLC_NETBW_Bcast="2e9"
export FMOE_FASTER_GLBPLC_GPUTP="112e12"

export CODE_PREFIX=$AEROOT/src/Megatron-LM

export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
export GPT_VOCAB_FILE=${DATASET_PREFIX}/gpt2-vocab.json
export BERT_VOCAB_FILE=${DATASET_PREFIX}/bert-large-uncased-vocab.txt
export MERGE_FILE=${DATASET_PREFIX}/gpt2-merges.txt
export DATA_PATH=${DATASET_PREFIX}/my-bert_text_sentence

export cluster_name=nico++ scontrol show JobId=164092
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=164092
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ scontrol show JobId=164092
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ scontrol show JobId=164092
++ tr = ' '
++ scontrol show JobId=164092
++ scontrol show JobId=164092
++ grep BatchHost
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ scontrol show JobId=164092
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ python_args='
+ EXEC=./pretrain_gpt.py
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=7
+ NODE_RANK=7
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ USE_MEGATRON=1
+ export WORLD_SIZE=16
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ export MASTER_ADDR=nico1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ MASTER_ADDR=nico1
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
+ export MASTER_ADDR=nico1
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ MASTER_ADDR=nico1
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
using world size: 16, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 2 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 8
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dynamic_freq .................................... 10
  dynamic_placement ............................... True
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 8
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 4.8
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 4
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 2
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 25600
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 16
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 2
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.139 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in DP group [4]
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in DP group [5]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in DP group [1]
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 8.745 seconds
time to initialize megatron (seconds): 6.103
[after megatron is initialized] datetime: 2023-05-28 17:12:38 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 456966400
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
[Warning] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 455396608
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in DP group [6]
[Warning] world comm group not exist!
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in DP group [3]
[Warning] world comm group not exist!
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
[Warning] world comm group not exist!
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[Warning] world comm group not exist!
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in DP group [7]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
setting training iterations to 100
> learning rate decay style: cosine
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in DP group [2]
[Warning] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 17:12:47 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      25600
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002518 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_25600ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_25600ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_25600ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.004 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 17:12:58 
done with setup ...
training ...
[before the start of training step] datetime: 2023-05-28 17:12:58 
 iteration        1/     100 | consumed samples:          256 | elapsed time per iteration (ms): 8018.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082583E+01 | loss scale: 131072.0 | grad norm: 9.925 | num zeros: 23128.0 | params norm: 229.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 8765.15625 | max allocated: 8765.15869140625 | reserved: 9996.0 | max reserved: 9996.0
[Rank 8] (after 1 iterations) memory (MB) | allocated: 8736.6171875 | max allocated: 8736.63232421875 | reserved: 9104.0 | max reserved: 9104.0
 iteration        2/     100 | consumed samples:          512 | elapsed time per iteration (ms): 8166.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.049447E+01 | loss scale: 131072.0 | grad norm: 4.181 | num zeros: 360033344.0 | params norm: 229.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     100 | consumed samples:          768 | elapsed time per iteration (ms): 8769.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.033619E+01 | loss scale: 131072.0 | grad norm: 3.968 | num zeros: 425755072.0 | params norm: 230.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     100 | consumed samples:         1024 | elapsed time per iteration (ms): 8103.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.152107E+01 | loss scale: 131072.0 | grad norm: 3.966 | num zeros: 331580352.0 | params norm: 230.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/     100 | consumed samples:         1280 | elapsed time per iteration (ms): 8803.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.149078E+01 | loss scale: 131072.0 | grad norm: 4.015 | num zeros: 360403520.0 | params norm: 230.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     100 | consumed samples:         1536 | elapsed time per iteration (ms): 8502.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.120801E+01 | loss scale: 131072.0 | grad norm: 4.928 | num zeros: 353777248.0 | params norm: 230.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     100 | consumed samples:         1792 | elapsed time per iteration (ms): 8613.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.024643E+01 | loss scale: 131072.0 | grad norm: 3.996 | num zeros: 473425824.0 | params norm: 230.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     100 | consumed samples:         2048 | elapsed time per iteration (ms): 8567.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.019171E+01 | loss scale: 131072.0 | grad norm: 3.939 | num zeros: 435711776.0 | params norm: 230.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     100 | consumed samples:         2304 | elapsed time per iteration (ms): 8584.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.009919E+01 | loss scale: 131072.0 | grad norm: 3.929 | num zeros: 483968320.0 | params norm: 230.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       10/     100 | consumed samples:         2560 | elapsed time per iteration (ms): 9714.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.000480E+01 | loss scale: 131072.0 | grad norm: 3.937 | num zeros: 473574656.0 | params norm: 230.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     100 | consumed samples:         2816 | elapsed time per iteration (ms): 7987.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.907877E+00 | loss scale: 131072.0 | grad norm: 3.928 | num zeros: 465092352.0 | params norm: 230.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     100 | consumed samples:         3072 | elapsed time per iteration (ms): 8123.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.796535E+00 | loss scale: 131072.0 | grad norm: 3.933 | num zeros: 469427584.0 | params norm: 230.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     100 | consumed samples:         3328 | elapsed time per iteration (ms): 8391.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.682419E+00 | loss scale: 131072.0 | grad norm: 3.896 | num zeros: 479955936.0 | params norm: 230.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     100 | consumed samples:         3584 | elapsed time per iteration (ms): 8510.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.565166E+00 | loss scale: 131072.0 | grad norm: 3.909 | num zeros: 499412736.0 | params norm: 230.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     100 | consumed samples:         3840 | elapsed time per iteration (ms): 8482.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.442050E+00 | loss scale: 131072.0 | grad norm: 3.918 | num zeros: 517496512.0 | params norm: 230.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     100 | consumed samples:         4096 | elapsed time per iteration (ms): 8271.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.322581E+00 | loss scale: 131072.0 | grad norm: 3.889 | num zeros: 509442304.0 | params norm: 230.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     100 | consumed samples:         4352 | elapsed time per iteration (ms): 8199.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.200008E+00 | loss scale: 131072.0 | grad norm: 3.895 | num zeros: 537546944.0 | params norm: 230.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     100 | consumed samples:         4608 | elapsed time per iteration (ms): 8439.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.082571E+00 | loss scale: 131072.0 | grad norm: 3.906 | num zeros: 519764256.0 | params norm: 230.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     100 | consumed samples:         4864 | elapsed time per iteration (ms): 8430.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.966105E+00 | loss scale: 131072.0 | grad norm: 3.900 | num zeros: 542683008.0 | params norm: 230.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       20/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 8129.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.854991E+00 | loss scale: 131072.0 | grad norm: 3.873 | num zeros: 542760192.0 | params norm: 230.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     100 | consumed samples:         5376 | elapsed time per iteration (ms): 7975.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.755208E+00 | loss scale: 131072.0 | grad norm: 3.857 | num zeros: 556277632.0 | params norm: 230.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     100 | consumed samples:         5632 | elapsed time per iteration (ms): 7992.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.651165E+00 | loss scale: 131072.0 | grad norm: 3.805 | num zeros: 528603392.0 | params norm: 230.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     100 | consumed samples:         5888 | elapsed time per iteration (ms): 7933.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.546199E+00 | loss scale: 131072.0 | grad norm: 3.832 | num zeros: 546014400.0 | params norm: 230.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     100 | consumed samples:         6144 | elapsed time per iteration (ms): 7871.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.452353E+00 | loss scale: 131072.0 | grad norm: 3.800 | num zeros: 541892160.0 | params norm: 230.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     100 | consumed samples:         6400 | elapsed time per iteration (ms): 7858.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.363504E+00 | loss scale: 131072.0 | grad norm: 3.776 | num zeros: 492272768.0 | params norm: 230.693 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     100 | consumed samples:         6656 | elapsed time per iteration (ms): 7940.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.279668E+00 | loss scale: 131072.0 | grad norm: 3.726 | num zeros: 515007488.0 | params norm: 230.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     100 | consumed samples:         6912 | elapsed time per iteration (ms): 7859.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.196972E+00 | loss scale: 131072.0 | grad norm: 3.702 | num zeros: 556115136.0 | params norm: 230.769 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     100 | consumed samples:         7168 | elapsed time per iteration (ms): 7736.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.119210E+00 | loss scale: 131072.0 | grad norm: 3.657 | num zeros: 574956672.0 | params norm: 230.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     100 | consumed samples:         7424 | elapsed time per iteration (ms): 7737.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.042410E+00 | loss scale: 131072.0 | grad norm: 3.604 | num zeros: 573968256.0 | params norm: 230.845 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       30/     100 | consumed samples:         7680 | elapsed time per iteration (ms): 7873.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.963447E+00 | loss scale: 131072.0 | grad norm: 3.548 | num zeros: 559567552.0 | params norm: 230.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     100 | consumed samples:         7936 | elapsed time per iteration (ms): 7659.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.900511E+00 | loss scale: 131072.0 | grad norm: 3.454 | num zeros: 568742016.0 | params norm: 230.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     100 | consumed samples:         8192 | elapsed time per iteration (ms): 7652.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.823651E+00 | loss scale: 131072.0 | grad norm: 3.380 | num zeros: 561046272.0 | params norm: 230.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     100 | consumed samples:         8448 | elapsed time per iteration (ms): 7623.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.768705E+00 | loss scale: 131072.0 | grad norm: 3.250 | num zeros: 559487104.0 | params norm: 230.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     100 | consumed samples:         8704 | elapsed time per iteration (ms): 7633.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.704644E+00 | loss scale: 131072.0 | grad norm: 3.151 | num zeros: 542312960.0 | params norm: 231.035 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     100 | consumed samples:         8960 | elapsed time per iteration (ms): 7780.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.644390E+00 | loss scale: 131072.0 | grad norm: 2.981 | num zeros: 540694336.0 | params norm: 231.074 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     100 | consumed samples:         9216 | elapsed time per iteration (ms): 7792.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.587122E+00 | loss scale: 131072.0 | grad norm: 2.853 | num zeros: 540959296.0 | params norm: 231.112 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     100 | consumed samples:         9472 | elapsed time per iteration (ms): 7847.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.547141E+00 | loss scale: 131072.0 | grad norm: 2.627 | num zeros: 521952256.0 | params norm: 231.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     100 | consumed samples:         9728 | elapsed time per iteration (ms): 7894.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.504226E+00 | loss scale: 131072.0 | grad norm: 2.399 | num zeros: 456293568.0 | params norm: 231.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     100 | consumed samples:         9984 | elapsed time per iteration (ms): 7891.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.478713E+00 | loss scale: 131072.0 | grad norm: 2.146 | num zeros: 333667136.0 | params norm: 231.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       40/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 8333.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.428458E+00 | loss scale: 131072.0 | grad norm: 1.920 | num zeros: 297058624.0 | params norm: 231.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     100 | consumed samples:        10496 | elapsed time per iteration (ms): 7799.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.402873E+00 | loss scale: 131072.0 | grad norm: 1.606 | num zeros: 522197248.0 | params norm: 231.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     100 | consumed samples:        10752 | elapsed time per iteration (ms): 7800.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.407605E+00 | loss scale: 131072.0 | grad norm: 1.335 | num zeros: 493904128.0 | params norm: 231.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     100 | consumed samples:        11008 | elapsed time per iteration (ms): 7784.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.368039E+00 | loss scale: 131072.0 | grad norm: 1.079 | num zeros: 484392192.0 | params norm: 231.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     100 | consumed samples:        11264 | elapsed time per iteration (ms): 7889.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.351244E+00 | loss scale: 131072.0 | grad norm: 0.751 | num zeros: 449012864.0 | params norm: 231.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     100 | consumed samples:        11520 | elapsed time per iteration (ms): 7875.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.329133E+00 | loss scale: 131072.0 | grad norm: 0.689 | num zeros: 391627168.0 | params norm: 231.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     100 | consumed samples:        11776 | elapsed time per iteration (ms): 7870.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.308473E+00 | loss scale: 131072.0 | grad norm: 0.776 | num zeros: 346728512.0 | params norm: 231.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     100 | consumed samples:        12032 | elapsed time per iteration (ms): 7911.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.297517E+00 | loss scale: 131072.0 | grad norm: 0.615 | num zeros: 317402208.0 | params norm: 231.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     100 | consumed samples:        12288 | elapsed time per iteration (ms): 7899.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.288738E+00 | loss scale: 131072.0 | grad norm: 0.551 | num zeros: 327274720.0 | params norm: 231.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     100 | consumed samples:        12544 | elapsed time per iteration (ms): 7825.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.281242E+00 | loss scale: 131072.0 | grad norm: 0.465 | num zeros: 327508928.0 | params norm: 231.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       50/     100 | consumed samples:        12800 | elapsed time per iteration (ms): 7760.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.263362E+00 | loss scale: 131072.0 | grad norm: 0.487 | num zeros: 400844864.0 | params norm: 231.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     100 | consumed samples:        13056 | elapsed time per iteration (ms): 7208.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.260896E+00 | loss scale: 131072.0 | grad norm: 0.457 | num zeros: 278800064.0 | params norm: 231.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     100 | consumed samples:        13312 | elapsed time per iteration (ms): 7216.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.240244E+00 | loss scale: 131072.0 | grad norm: 0.548 | num zeros: 295072448.0 | params norm: 231.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     100 | consumed samples:        13568 | elapsed time per iteration (ms): 7099.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.242093E+00 | loss scale: 131072.0 | grad norm: 0.405 | num zeros: 371713600.0 | params norm: 231.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     100 | consumed samples:        13824 | elapsed time per iteration (ms): 6948.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.223278E+00 | loss scale: 131072.0 | grad norm: 0.378 | num zeros: 420016384.0 | params norm: 231.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     100 | consumed samples:        14080 | elapsed time per iteration (ms): 6958.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.213781E+00 | loss scale: 131072.0 | grad norm: 0.348 | num zeros: 420322944.0 | params norm: 231.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     100 | consumed samples:        14336 | elapsed time per iteration (ms): 6904.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.226130E+00 | loss scale: 131072.0 | grad norm: 0.431 | num zeros: 428190016.0 | params norm: 231.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     100 | consumed samples:        14592 | elapsed time per iteration (ms): 6818.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.198305E+00 | loss scale: 131072.0 | grad norm: 0.411 | num zeros: 417928160.0 | params norm: 231.982 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     100 | consumed samples:        14848 | elapsed time per iteration (ms): 6812.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.187392E+00 | loss scale: 131072.0 | grad norm: 0.301 | num zeros: 427298688.0 | params norm: 232.027 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     100 | consumed samples:        15104 | elapsed time per iteration (ms): 6822.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.180792E+00 | loss scale: 131072.0 | grad norm: 0.341 | num zeros: 429448736.0 | params norm: 232.071 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       60/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6949.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.171424E+00 | loss scale: 131072.0 | grad norm: 0.264 | num zeros: 437555712.0 | params norm: 232.114 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     100 | consumed samples:        15616 | elapsed time per iteration (ms): 6474.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.184900E+00 | loss scale: 131072.0 | grad norm: 0.423 | num zeros: 374059616.0 | params norm: 232.160 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     100 | consumed samples:        15872 | elapsed time per iteration (ms): 6774.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.168732E+00 | loss scale: 131072.0 | grad norm: 0.447 | num zeros: 392319360.0 | params norm: 232.202 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     100 | consumed samples:        16128 | elapsed time per iteration (ms): 6909.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.149908E+00 | loss scale: 131072.0 | grad norm: 0.430 | num zeros: 399140864.0 | params norm: 232.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     100 | consumed samples:        16384 | elapsed time per iteration (ms): 6880.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.145298E+00 | loss scale: 131072.0 | grad norm: 0.268 | num zeros: 401028672.0 | params norm: 232.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     100 | consumed samples:        16640 | elapsed time per iteration (ms): 6920.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.150368E+00 | loss scale: 131072.0 | grad norm: 0.373 | num zeros: 382904192.0 | params norm: 232.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     100 | consumed samples:        16896 | elapsed time per iteration (ms): 6945.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.132328E+00 | loss scale: 131072.0 | grad norm: 0.388 | num zeros: 392306336.0 | params norm: 232.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     100 | consumed samples:        17152 | elapsed time per iteration (ms): 7025.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.133253E+00 | loss scale: 131072.0 | grad norm: 0.227 | num zeros: 377924864.0 | params norm: 232.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     100 | consumed samples:        17408 | elapsed time per iteration (ms): 7049.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.130468E+00 | loss scale: 131072.0 | grad norm: 0.219 | num zeros: 412094016.0 | params norm: 232.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     100 | consumed samples:        17664 | elapsed time per iteration (ms): 7056.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.125593E+00 | loss scale: 131072.0 | grad norm: 0.281 | num zeros: 374050752.0 | params norm: 232.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       70/     100 | consumed samples:        17920 | elapsed time per iteration (ms): 6836.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.121429E+00 | loss scale: 131072.0 | grad norm: 0.206 | num zeros: 310189568.0 | params norm: 232.510 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     100 | consumed samples:        18176 | elapsed time per iteration (ms): 6328.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.113387E+00 | loss scale: 131072.0 | grad norm: 0.224 | num zeros: 311623968.0 | params norm: 232.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     100 | consumed samples:        18432 | elapsed time per iteration (ms): 6332.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.131769E+00 | loss scale: 131072.0 | grad norm: 0.296 | num zeros: 317678208.0 | params norm: 232.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     100 | consumed samples:        18688 | elapsed time per iteration (ms): 6564.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.124979E+00 | loss scale: 131072.0 | grad norm: 0.364 | num zeros: 321535232.0 | params norm: 232.610 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     100 | consumed samples:        18944 | elapsed time per iteration (ms): 6679.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.110984E+00 | loss scale: 131072.0 | grad norm: 0.678 | num zeros: 314141152.0 | params norm: 232.641 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     100 | consumed samples:        19200 | elapsed time per iteration (ms): 6397.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.121051E+00 | loss scale: 131072.0 | grad norm: 0.615 | num zeros: 312183392.0 | params norm: 232.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     100 | consumed samples:        19456 | elapsed time per iteration (ms): 6696.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.095195E+00 | loss scale: 131072.0 | grad norm: 0.259 | num zeros: 331487040.0 | params norm: 232.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     100 | consumed samples:        19712 | elapsed time per iteration (ms): 6685.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.119187E+00 | loss scale: 131072.0 | grad norm: 0.408 | num zeros: 272801472.0 | params norm: 232.730 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     100 | consumed samples:        19968 | elapsed time per iteration (ms): 6536.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.103200E+00 | loss scale: 131072.0 | grad norm: 0.295 | num zeros: 253443072.0 | params norm: 232.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     100 | consumed samples:        20224 | elapsed time per iteration (ms): 6594.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.094181E+00 | loss scale: 131072.0 | grad norm: 0.325 | num zeros: 298725024.0 | params norm: 232.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       80/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 7501.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.103314E+00 | loss scale: 131072.0 | grad norm: 0.346 | num zeros: 180159296.0 | params norm: 232.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     100 | consumed samples:        20736 | elapsed time per iteration (ms): 6739.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.098647E+00 | loss scale: 131072.0 | grad norm: 0.378 | num zeros: 170549152.0 | params norm: 232.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     100 | consumed samples:        20992 | elapsed time per iteration (ms): 6425.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.076911E+00 | loss scale: 131072.0 | grad norm: 0.361 | num zeros: 135593920.0 | params norm: 232.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     100 | consumed samples:        21248 | elapsed time per iteration (ms): 6614.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.095897E+00 | loss scale: 131072.0 | grad norm: 0.318 | num zeros: 141928912.0 | params norm: 232.895 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     100 | consumed samples:        21504 | elapsed time per iteration (ms): 7057.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.084781E+00 | loss scale: 131072.0 | grad norm: 2.713 | num zeros: 172094944.0 | params norm: 232.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     100 | consumed samples:        21760 | elapsed time per iteration (ms): 6432.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.071389E+00 | loss scale: 131072.0 | grad norm: 0.915 | num zeros: 178908416.0 | params norm: 232.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     100 | consumed samples:        22016 | elapsed time per iteration (ms): 6699.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.083826E+00 | loss scale: 131072.0 | grad norm: 0.780 | num zeros: 164097280.0 | params norm: 232.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     100 | consumed samples:        22272 | elapsed time per iteration (ms): 6906.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.069844E+00 | loss scale: 131072.0 | grad norm: 0.697 | num zeros: 170550944.0 | params norm: 232.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     100 | consumed samples:        22528 | elapsed time per iteration (ms): 7028.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.063616E+00 | loss scale: 131072.0 | grad norm: 0.709 | num zeros: 172540016.0 | params norm: 233.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     100 | consumed samples:        22784 | elapsed time per iteration (ms): 6794.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.044395E+00 | loss scale: 131072.0 | grad norm: 0.365 | num zeros: 213718272.0 | params norm: 233.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       90/     100 | consumed samples:        23040 | elapsed time per iteration (ms): 6959.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.054244E+00 | loss scale: 131072.0 | grad norm: 0.564 | num zeros: 204094752.0 | params norm: 233.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     100 | consumed samples:        23296 | elapsed time per iteration (ms): 6497.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.032516E+00 | loss scale: 131072.0 | grad norm: 0.701 | num zeros: 198556864.0 | params norm: 233.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     100 | consumed samples:        23552 | elapsed time per iteration (ms): 6664.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.014774E+00 | loss scale: 131072.0 | grad norm: 0.335 | num zeros: 179286976.0 | params norm: 233.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     100 | consumed samples:        23808 | elapsed time per iteration (ms): 6971.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.015360E+00 | loss scale: 131072.0 | grad norm: 0.914 | num zeros: 201062480.0 | params norm: 233.141 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     100 | consumed samples:        24064 | elapsed time per iteration (ms): 6743.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.992983E+00 | loss scale: 131072.0 | grad norm: 0.315 | num zeros: 214308160.0 | params norm: 233.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     100 | consumed samples:        24320 | elapsed time per iteration (ms): 6716.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.000073E+00 | loss scale: 131072.0 | grad norm: 0.721 | num zeros: 192720144.0 | params norm: 233.191 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     100 | consumed samples:        24576 | elapsed time per iteration (ms): 6764.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.984873E+00 | loss scale: 131072.0 | grad norm: 0.527 | num zeros: 180701536.0 | params norm: 233.216 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     100 | consumed samples:        24832 | elapsed time per iteration (ms): 6926.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.994129E+00 | loss scale: 131072.0 | grad norm: 0.685 | num zeros: 184647872.0 | params norm: 233.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     100 | consumed samples:        25088 | elapsed time per iteration (ms): 6859.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.964220E+00 | loss scale: 131072.0 | grad norm: 0.458 | num zeros: 204610528.0 | params norm: 233.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     100 | consumed samples:        25344 | elapsed time per iteration (ms): 6763.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.963180E+00 | loss scale: 131072.0 | grad norm: 0.654 | num zeros: 212301264.0 | params norm: 233.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration      100/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7062.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.960465E+00 | loss scale: 131072.0 | grad norm: 0.477 | num zeros: 219643840.0 | params norm: 233.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-05-28 17:25:23 
