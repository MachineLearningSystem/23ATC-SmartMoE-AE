++ pwd
+ export AEROOT=/home/atc23_ae/SmartMoE-AE
+ AEROOT=/home/atc23_ae/SmartMoE-AE
+ ./clean.sh
rm: cannot remove './plotting/from_exec/fig13/freq*': No such file or directory
++ date -Iseconds
+ output_dir=/home/atc23_ae/SmartMoE-AE/outputs_from_exec_2023-05-28T16:36:45+08:00
+ mkdir /home/atc23_ae/SmartMoE-AE/outputs_from_exec_2023-05-28T16:36:45+08:00
+ cd ./plotting/from_exec/fig8
+ ./fig8.sh
+ '[' 2 -ne '2]'
./run_moe.sh: line 7: [: missing `]'
++ expr 18177 % 10000 + 10000
+ export MASTER_PORT=18177
+ MASTER_PORT=18177
+ export EXP_NAME=fig8_2023-05-28T16:36:48+08:00
+ EXP_NAME=fig8_2023-05-28T16:36:48+08:00
+ export DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/dense.sh
+ DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/dense.sh
+ export SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/sparse.sh
+ SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/sparse.sh
+ export PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/parallel.sh
+ PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/parallel.sh
+ export CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/cluster.sh
+ CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/cluster.sh
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/dense.sh
++ export NUM_LAYERS=16
++ NUM_LAYERS=16
++ export HIDDEN_SIZE=1536
++ HIDDEN_SIZE=1536
++ export NUM_ATTN_HEADS=16
++ NUM_ATTN_HEADS=16
++ export SEQ_LEN=1024
++ SEQ_LEN=1024
++ export FMOE_FASTER_GLBPLC_ALPHA=2
++ FMOE_FASTER_GLBPLC_ALPHA=2
++ export FMOE_FASTER_GLBPLC_DMODEL=1536
++ FMOE_FASTER_GLBPLC_DMODEL=1536
++ export dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
++ dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/sparse.sh
++ export TOT_EXPERTS=32
++ TOT_EXPERTS=32
++ export GATE=gshard
++ GATE=gshard
++ export GSHARD_CAP=1.2
++ GSHARD_CAP=1.2
++ '[' gshard == gshard ']'
++ GATE_NAME=gshard1.2
++ export sparse_name=32MoE_gshard1.2
++ sparse_name=32MoE_gshard1.2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/parallel.sh
++ export TENSOR_PARALLEL_SIZE=1
++ TENSOR_PARALLEL_SIZE=1
++ export PIPELINE_PARALLEL_SIZE=1
++ PIPELINE_PARALLEL_SIZE=1
++ export DATA_PARALLEL_SIZE=16
++ DATA_PARALLEL_SIZE=16
++ export EXPERT_EP_SIZE=16
++ EXPERT_EP_SIZE=16
++ export EXPERT_DP_SIZE=1
++ EXPERT_DP_SIZE=1
++ export GLOBAL_BATCH_SIZE=256
++ GLOBAL_BATCH_SIZE=256
++ export MICRO_BATCH_SIZE=2
++ MICRO_BATCH_SIZE=2
++ export FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ export FMOE_FASTER_GROUP_SIZE=none
++ FMOE_FASTER_GROUP_SIZE=none
++ export FMOE_FASTER_SHADOW_ENABLE=OFF
++ FMOE_FASTER_SHADOW_ENABLE=OFF
++ export DYNAMIC_ENABLE=OFF
++ DYNAMIC_ENABLE=OFF
++ export DYNAMIC_FREQ=0
++ DYNAMIC_FREQ=0
++ export ITERATION=20
++ ITERATION=20
++ export TRAIN_SAMPLES=5120
++ TRAIN_SAMPLES=5120
++ export parallel_name=t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
++ parallel_name=t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/cluster.sh
++ export NNODES=2
++ NNODES=2
++ export 'NODELIST=nico[1-2]'
++ NODELIST='nico[1-2]'
++ export FMOE_FASTER_GLBPLC_NETBW=8e9
++ FMOE_FASTER_GLBPLC_NETBW=8e9
++ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ export FMOE_FASTER_GLBPLC_GPUTP=112e12
++ FMOE_FASTER_GLBPLC_GPUTP=112e12
++ export CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ export GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ export BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ export MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ export DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ export cluster_name=nico
++ cluster_name=nico
+ export NUM_EXPERTS=2
+ NUM_EXPERTS=2
+ mkdir -p ./logs
+ mkdir -p ./logs/fig8_2023-05-28T16:36:48+08:00
++ pwd
+ export LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00
++ date -Iseconds
+ LOG_PREFIX=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00
+ LOG_NAME=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.log
+ export PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ mkdir -p /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 2'
+ '[' 'nico[1-2]' '!=' None ']'
++ scontrol show hostnames 'nico[1-2]'
++ wc -l
+ tmp=2
+ '[' 2 '!=' 2 ']'
+ exec_args+=' -w nico[1-2]'
+ exec_args+=' -p AE'
+ LOG_FILE=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.log
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.log
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w nico[1-2] -p AE
+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/dense.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.log
#!/bin/bash

export NUM_LAYERS=16
export HIDDEN_SIZE=1536
export NUM_ATTN_HEADS=16
export SEQ_LEN=1024

export FMOE_FASTER_GLBPLC_ALPHA="2"
export FMOE_FASTER_GLBPLC_DMODEL=${HIDDEN_SIZE}

export dense_name=GPT-L${NUM_LAYERS}-H${HIDDEN_SIZE}-ATTN${NUM_ATTN_HEADS}-SEQ${SEQ_LEN}-ALPHA${FMOE_FASTER_GLBPLC_ALPHA}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/sparse.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.log
#!/bin/bash

export TOT_EXPERTS=32
export GATE="gshard"
export GSHARD_CAP="1.2"

if [ ${GATE} == 'gshard' ];then
    GATE_NAME=${GATE}${GSHARD_CAP}
else
    GATE_NAME=${GATE}
fi
export sparse_name=${TOT_EXPERTS}MoE\_${GATE_NAME}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/parallel.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.log
#!/bin/bash

export TENSOR_PARALLEL_SIZE=1
export PIPELINE_PARALLEL_SIZE=1
export DATA_PARALLEL_SIZE=16
export EXPERT_EP_SIZE=16
export EXPERT_DP_SIZE=1
export GLOBAL_BATCH_SIZE=256
export MICRO_BATCH_SIZE=2
export FMOE_FASTER_SCHEDULE_ENABLE=OFF
export FMOE_FASTER_GROUP_SIZE=none
export FMOE_FASTER_SHADOW_ENABLE=OFF
export DYNAMIC_ENABLE=OFF
export DYNAMIC_FREQ=0
export ITERATION=20

export TRAIN_SAMPLES=$(( $GLOBAL_BATCH_SIZE * $ITERATION))

export parallel_name=t$TENSOR_PARALLEL_SIZE\_p$PIPELINE_PARALLEL_SIZE\_d$DATA_PARALLEL_SIZE\_ep${EXPERT_EP_SIZE}\_dp${EXPERT_DP_SIZE}\_gbs$GLOBAL_BATCH_SIZE\_mbs$MICRO_BATCH_SIZE\_smart\_${FMOE_FASTER_SCHEDULE_ENABLE}\_smartGP${FMOE_FASTER_GROUP_SIZE}\_shadow${FMOE_FASTER_SHADOW_ENABLE}\_dynamic${DYNAMIC_ENABLE}\_freq${DYNAMIC_FREQ}\_iter${ITERATION}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastmoe/cluster.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.log
#!/bin/bash

export NNODES=2
export NODELIST="nico[1-2]"
export FMOE_FASTER_GLBPLC_NETBW="8e9"
export FMOE_FASTER_GLBPLC_NETBW_Bcast="2e9"
export FMOE_FASTER_GLBPLC_GPUTP="112e12"

export CODE_PREFIX=$AEROOT/src/Megatron-LM

export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
export GPT_VOCAB_FILE=${DATASET_PREFIX}/gpt2-vocab.json
export BERT_VOCAB_FILE=${DATASET_PREFIX}/bert-large-uncased-vocab.txt
export MERGE_FILE=${DATASET_PREFIX}/gpt2-merges.txt
export DATA_PATH=${DATASET_PREFIX}/my-bert_text_sentence

export cluster_name=nico+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE ./pretrain_moe.sh
++ scontrol show JobId=164081
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=164081
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164081
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export RANK=13
+ RANK=13
+ export RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ RANK=10
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ CUDA_VISIBLE_DEVICES=5
+ localrank=2
+ export NODE_RANK=6
+ NODE_RANK=6
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164081
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164081
++ scontrol show JobId=164081
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=164081
++ grep BatchHost
++ scontrol show JobId=164081
++ tr = ' '
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164081
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MASTER_ADDR=nico1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MASTER_ADDR=nico1
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ scontrol show JobId=164081
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164081
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164081
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164081
++ grep BatchHost
++ scontrol show JobId=164081
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164081
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164081
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znv+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
me/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export RANK=11
+ RANK=11
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export WORLD_SIZE=16
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:36:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 16
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 2
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 5120
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.159 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2]
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 10.514 seconds
time to initialize megatron (seconds): 54.885
[after megatron is initialized] datetime: 2023-05-28 16:37:10 
building GPT model ...
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
[Warning] world comm group not exist!
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
[Warning] world comm group not exist!
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[Warning] world comm group not exist!
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
[Warning] world comm group not exist!
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
[Warning] world comm group not exist!
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
[Warning] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 532958720
[Warning] world comm group not exist!
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4]
[Warning] world comm group not exist!
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
setting training iterations to 20
> learning rate decay style: cosine
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3]
[Warning] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 16:37:19 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      5120
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.037235 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.058 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 16:37:38 
done with setup ...
training ...
[before the start of training step] datetime: 2023-05-28 16:37:38 
 iteration        1/      20 | consumed samples:          256 | elapsed time per iteration (ms): 24584.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082706E+01 | gshard_loss: 3.946874E-03 | loss scale: 131072.0 | grad norm: 10.796 | num zeros: 14714.0 | params norm: 228.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 10231.3720703125 | max allocated: 10231.37939453125 | reserved: 11494.0 | max reserved: 11494.0
 iteration        2/      20 | consumed samples:          512 | elapsed time per iteration (ms): 10516.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.049288E+01 | gshard_loss: 1.104513E-02 | loss scale: 131072.0 | grad norm: 4.620 | num zeros: 94536464.0 | params norm: 228.616 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/      20 | consumed samples:          768 | elapsed time per iteration (ms): 9300.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.043696E+01 | gshard_loss: 1.077302E-02 | loss scale: 131072.0 | grad norm: 6.611 | num zeros: 151072816.0 | params norm: 228.626 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/      20 | consumed samples:         1024 | elapsed time per iteration (ms): 7770.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.019604E+01 | gshard_loss: 9.188979E-03 | loss scale: 131072.0 | grad norm: 3.958 | num zeros: 161050432.0 | params norm: 228.638 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/      20 | consumed samples:         1280 | elapsed time per iteration (ms): 6793.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.003283E+01 | gshard_loss: 7.759058E-03 | loss scale: 131072.0 | grad norm: 3.945 | num zeros: 170027312.0 | params norm: 228.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/      20 | consumed samples:         1536 | elapsed time per iteration (ms): 6299.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.877900E+00 | gshard_loss: 6.719904E-03 | loss scale: 131072.0 | grad norm: 3.909 | num zeros: 160735184.0 | params norm: 228.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/      20 | consumed samples:         1792 | elapsed time per iteration (ms): 5138.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.718444E+00 | gshard_loss: 6.164939E-03 | loss scale: 131072.0 | grad norm: 3.943 | num zeros: 141985840.0 | params norm: 228.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/      20 | consumed samples:         2048 | elapsed time per iteration (ms): 5065.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.575145E+00 | gshard_loss: 5.716431E-03 | loss scale: 131072.0 | grad norm: 3.899 | num zeros: 114027648.0 | params norm: 228.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/      20 | consumed samples:         2304 | elapsed time per iteration (ms): 4204.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.424889E+00 | gshard_loss: 5.443966E-03 | loss scale: 131072.0 | grad norm: 3.902 | num zeros: 113482976.0 | params norm: 228.774 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/      20 | consumed samples:         2560 | elapsed time per iteration (ms): 4244.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.279609E+00 | gshard_loss: 5.271874E-03 | loss scale: 131072.0 | grad norm: 3.910 | num zeros: 104079152.0 | params norm: 228.809 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/      20 | consumed samples:         2816 | elapsed time per iteration (ms): 4412.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.138750E+00 | gshard_loss: 5.096472E-03 | loss scale: 131072.0 | grad norm: 3.900 | num zeros: 114462448.0 | params norm: 228.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/      20 | consumed samples:         3072 | elapsed time per iteration (ms): 4180.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.004840E+00 | gshard_loss: 4.997155E-03 | loss scale: 131072.0 | grad norm: 3.897 | num zeros: 105251088.0 | params norm: 228.880 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/      20 | consumed samples:         3328 | elapsed time per iteration (ms): 4322.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.878920E+00 | gshard_loss: 4.892082E-03 | loss scale: 131072.0 | grad norm: 3.848 | num zeros: 105162776.0 | params norm: 228.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/      20 | consumed samples:         3584 | elapsed time per iteration (ms): 4231.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.760447E+00 | gshard_loss: 4.882847E-03 | loss scale: 131072.0 | grad norm: 3.850 | num zeros: 113542424.0 | params norm: 228.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/      20 | consumed samples:         3840 | elapsed time per iteration (ms): 4440.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.641960E+00 | gshard_loss: 4.855872E-03 | loss scale: 131072.0 | grad norm: 3.846 | num zeros: 104598528.0 | params norm: 228.995 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/      20 | consumed samples:         4096 | elapsed time per iteration (ms): 4312.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.541189E+00 | gshard_loss: 4.813354E-03 | loss scale: 131072.0 | grad norm: 3.804 | num zeros: 104540184.0 | params norm: 229.037 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/      20 | consumed samples:         4352 | elapsed time per iteration (ms): 4230.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.430980E+00 | gshard_loss: 4.838032E-03 | loss scale: 131072.0 | grad norm: 3.795 | num zeros: 104041296.0 | params norm: 229.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/      20 | consumed samples:         4608 | elapsed time per iteration (ms): 4346.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.333664E+00 | gshard_loss: 4.757650E-03 | loss scale: 131072.0 | grad norm: 3.784 | num zeros: 85277528.0 | params norm: 229.120 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/      20 | consumed samples:         4864 | elapsed time per iteration (ms): 4506.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.237703E+00 | gshard_loss: 4.670389E-03 | loss scale: 131072.0 | grad norm: 3.754 | num zeros: 85688544.0 | params norm: 229.164 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/      20 | consumed samples:         5120 | elapsed time per iteration (ms): 4504.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.150252E+00 | gshard_loss: 4.660506E-03 | loss scale: 131072.0 | grad norm: 3.698 | num zeros: 85243776.0 | params norm: 229.207 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-05-28 16:39:45 
+ '[' 2 -ne '2]'
./run_moe.sh: line 7: [: missing `]'
++ expr 26467 % 10000 + 10000
+ export MASTER_PORT=16467
+ MASTER_PORT=16467
+ export EXP_NAME=fig8_2023-05-28T16:36:48+08:00
+ EXP_NAME=fig8_2023-05-28T16:36:48+08:00
+ export DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/dense.sh
+ DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/dense.sh
+ export SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/sparse.sh
+ SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/sparse.sh
+ export PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/parallel.sh
+ PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/parallel.sh
+ export CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/cluster.sh
+ CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/cluster.sh
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/dense.sh
++ export NUM_LAYERS=16
++ NUM_LAYERS=16
++ export HIDDEN_SIZE=1536
++ HIDDEN_SIZE=1536
++ export NUM_ATTN_HEADS=16
++ NUM_ATTN_HEADS=16
++ export SEQ_LEN=1024
++ SEQ_LEN=1024
++ export FMOE_FASTER_GLBPLC_ALPHA=2
++ FMOE_FASTER_GLBPLC_ALPHA=2
++ export FMOE_FASTER_GLBPLC_DMODEL=1536
++ FMOE_FASTER_GLBPLC_DMODEL=1536
++ export dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
++ dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/sparse.sh
++ export TOT_EXPERTS=32
++ TOT_EXPERTS=32
++ export GATE=gshard
++ GATE=gshard
++ export GSHARD_CAP=1.2
++ GSHARD_CAP=1.2
++ '[' gshard == gshard ']'
++ GATE_NAME=gshard1.2
++ export sparse_name=32MoE_gshard1.2
++ sparse_name=32MoE_gshard1.2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/parallel.sh
++ export TENSOR_PARALLEL_SIZE=1
++ TENSOR_PARALLEL_SIZE=1
++ export PIPELINE_PARALLEL_SIZE=1
++ PIPELINE_PARALLEL_SIZE=1
++ export DATA_PARALLEL_SIZE=16
++ DATA_PARALLEL_SIZE=16
++ export EXPERT_EP_SIZE=16
++ EXPERT_EP_SIZE=16
++ export EXPERT_DP_SIZE=1
++ EXPERT_DP_SIZE=1
++ export GLOBAL_BATCH_SIZE=256
++ GLOBAL_BATCH_SIZE=256
++ export MICRO_BATCH_SIZE=2
++ MICRO_BATCH_SIZE=2
++ export FMOE_FASTER_SCHEDULE_ENABLE=ON
++ FMOE_FASTER_SCHEDULE_ENABLE=ON
++ export FMOE_FASTER_GROUP_SIZE=16
++ FMOE_FASTER_GROUP_SIZE=16
++ export FMOE_FASTER_SHADOW_ENABLE=ON
++ FMOE_FASTER_SHADOW_ENABLE=ON
++ export DYNAMIC_ENABLE=OFF
++ DYNAMIC_ENABLE=OFF
++ export DYNAMIC_FREQ=0
++ DYNAMIC_FREQ=0
++ export ITERATION=20
++ ITERATION=20
++ export TRAIN_SAMPLES=5120
++ TRAIN_SAMPLES=5120
++ export parallel_name=t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20
++ parallel_name=t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/cluster.sh
++ export NNODES=2
++ NNODES=2
++ export 'NODELIST=nico[1-2]'
++ NODELIST='nico[1-2]'
++ export FMOE_FASTER_GLBPLC_NETBW=8e9
++ FMOE_FASTER_GLBPLC_NETBW=8e9
++ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ export FMOE_FASTER_GLBPLC_GPUTP=112e12
++ FMOE_FASTER_GLBPLC_GPUTP=112e12
++ export CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ export GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ export BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ export MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ export DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ export cluster_name=nico
++ cluster_name=nico
+ export NUM_EXPERTS=2
+ NUM_EXPERTS=2
+ mkdir -p ./logs
+ mkdir -p ./logs/fig8_2023-05-28T16:36:48+08:00
++ pwd
+ export LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00
++ date -Iseconds
+ LOG_PREFIX=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00
+ LOG_NAME=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.log
+ export PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ mkdir -p /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 2'
+ '[' 'nico[1-2]' '!=' None ']'
++ scontrol show hostnames 'nico[1-2]'
++ wc -l
+ tmp=2
+ '[' 2 '!=' 2 ']'
+ exec_args+=' -w nico[1-2]'
+ exec_args+=' -p AE'
+ LOG_FILE=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.log
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.log
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w nico[1-2] -p AE
+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/dense.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.log
#!/bin/bash

export NUM_LAYERS=16
export HIDDEN_SIZE=1536
export NUM_ATTN_HEADS=16
export SEQ_LEN=1024

export FMOE_FASTER_GLBPLC_ALPHA="2"
export FMOE_FASTER_GLBPLC_DMODEL=${HIDDEN_SIZE}

export dense_name=GPT-L${NUM_LAYERS}-H${HIDDEN_SIZE}-ATTN${NUM_ATTN_HEADS}-SEQ${SEQ_LEN}-ALPHA${FMOE_FASTER_GLBPLC_ALPHA}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/sparse.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.log
#!/bin/bash

export TOT_EXPERTS=32
export GATE="gshard"
export GSHARD_CAP="1.2"

if [ ${GATE} == 'gshard' ];then
    GATE_NAME=${GATE}${GSHARD_CAP}
else
    GATE_NAME=${GATE}
fi
export sparse_name=${TOT_EXPERTS}MoE\_${GATE_NAME}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/parallel.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.log
#!/bin/bash

export TENSOR_PARALLEL_SIZE=1
export PIPELINE_PARALLEL_SIZE=1
export DATA_PARALLEL_SIZE=16
export EXPERT_EP_SIZE=16
export EXPERT_DP_SIZE=1
export GLOBAL_BATCH_SIZE=256
export MICRO_BATCH_SIZE=2
export FMOE_FASTER_SCHEDULE_ENABLE=ON
export FMOE_FASTER_GROUP_SIZE=16
export FMOE_FASTER_SHADOW_ENABLE=ON
export DYNAMIC_ENABLE=OFF
export DYNAMIC_FREQ=0
export ITERATION=20

export TRAIN_SAMPLES=$(( $GLOBAL_BATCH_SIZE * $ITERATION))

export parallel_name=t$TENSOR_PARALLEL_SIZE\_p$PIPELINE_PARALLEL_SIZE\_d$DATA_PARALLEL_SIZE\_ep${EXPERT_EP_SIZE}\_dp${EXPERT_DP_SIZE}\_gbs$GLOBAL_BATCH_SIZE\_mbs$MICRO_BATCH_SIZE\_smart\_${FMOE_FASTER_SCHEDULE_ENABLE}\_smartGP${FMOE_FASTER_GROUP_SIZE}\_shadow${FMOE_FASTER_SHADOW_ENABLE}\_dynamic${DYNAMIC_ENABLE}\_freq${DYNAMIC_FREQ}\_iter${ITERATION}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-fastermoe/cluster.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.log
#!/bin/bash

export NNODES=2
export NODELIST="nico[1-2]"
export FMOE_FASTER_GLBPLC_NETBW="8e9"
export FMOE_FASTER_GLBPLC_NETBW_Bcast="2e9"
export FMOE_FASTER_GLBPLC_GPUTP="112e12"

export CODE_PREFIX=$AEROOT/src/Megatron-LM

export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
export GPT_VOCAB_FILE=${DATASET_PREFIX}/gpt2-vocab.json
export BERT_VOCAB_FILE=${DATASET_PREFIX}/bert-large-uncased-vocab.txt
export MERGE_FILE=${DATASET_PREFIX}/gpt2-merges.txt
export DATA_PATH=${DATASET_PREFIX}/my-bert_text_sentence

export cluster_name=nico+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE ./pretrain_moe.sh
++ scontrol show JobId=164082
++ grep BatchHost
++ scontrol show JobId=164082
++ scontrol show JobId=164082
++ grep BatchHost
++ scontrol show JobId=164082
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=164082
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=164082
++ scontrol show JobId=164082
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164082
++ awk '{print $2}'
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=13
+ RANK=13
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164082
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=164082
++ scontrol show JobId=164082
++ grep BatchHost
++ scontrol show JobId=164082
++ grep BatchHost
++ scontrol show JobId=164082
++ scontrol show JobId=164082
++ grep BatchHost
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ scontrol show JobId=164082
++ grep BatchHost
++ grep BatchHost
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164082
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_ON_smartGP16_shadowON_dynamicOFF_freq0_iter20_nico_2023-05-28T16:39:50+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 16
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 2
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 5120
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.137 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4]
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.930 seconds
time to initialize megatron (seconds): -27.296
[after megatron is initialized] datetime: 2023-05-28 16:40:04 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 532958720
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
[Warning] world comm group not exist!
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
[Warning] world comm group not exist!
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1]
[Warning] world comm group not exist!
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
setting training iterations to 20
> learning rate decay style: cosine
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6]
[Warning] world comm group not exist!
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
[Warning] world comm group not exist!
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 16:40:13 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      5120
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002750 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 16:40:24 
done with setup ...
training ...
[before the start of training step] datetime: 2023-05-28 16:40:25 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 12930.5732421875 | max allocated: 12930.58056640625 | reserved: 14098.0 | max reserved: 14098.0
 iteration        1/      20 | consumed samples:          256 | elapsed time per iteration (ms): 9072.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082707E+01 | gshard_loss: 3.946915E-03 | loss scale: 131072.0 | grad norm: 10.795 | num zeros: 14642.0 | params norm: 228.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/      20 | consumed samples:          512 | elapsed time per iteration (ms): 3180.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.049294E+01 | gshard_loss: 1.104457E-02 | loss scale: 131072.0 | grad norm: 4.631 | num zeros: 94928944.0 | params norm: 228.616 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/      20 | consumed samples:          768 | elapsed time per iteration (ms): 3140.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.043745E+01 | gshard_loss: 1.078024E-02 | loss scale: 131072.0 | grad norm: 6.671 | num zeros: 151073088.0 | params norm: 228.626 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/      20 | consumed samples:         1024 | elapsed time per iteration (ms): 3081.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.019624E+01 | gshard_loss: 9.193091E-03 | loss scale: 131072.0 | grad norm: 3.959 | num zeros: 160577824.0 | params norm: 228.637 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/      20 | consumed samples:         1280 | elapsed time per iteration (ms): 3188.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.003306E+01 | gshard_loss: 7.773211E-03 | loss scale: 131072.0 | grad norm: 3.945 | num zeros: 170028528.0 | params norm: 228.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/      20 | consumed samples:         1536 | elapsed time per iteration (ms): 3476.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.878138E+00 | gshard_loss: 6.737976E-03 | loss scale: 131072.0 | grad norm: 3.909 | num zeros: 160735712.0 | params norm: 228.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/      20 | consumed samples:         1792 | elapsed time per iteration (ms): 3713.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.718737E+00 | gshard_loss: 6.175087E-03 | loss scale: 131072.0 | grad norm: 3.943 | num zeros: 142028480.0 | params norm: 228.710 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/      20 | consumed samples:         2048 | elapsed time per iteration (ms): 3873.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.575462E+00 | gshard_loss: 5.722841E-03 | loss scale: 131072.0 | grad norm: 3.899 | num zeros: 114429816.0 | params norm: 228.740 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/      20 | consumed samples:         2304 | elapsed time per iteration (ms): 3995.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.425138E+00 | gshard_loss: 5.448314E-03 | loss scale: 131072.0 | grad norm: 3.902 | num zeros: 113524944.0 | params norm: 228.774 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/      20 | consumed samples:         2560 | elapsed time per iteration (ms): 4335.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.279893E+00 | gshard_loss: 5.271240E-03 | loss scale: 131072.0 | grad norm: 3.910 | num zeros: 95084424.0 | params norm: 228.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/      20 | consumed samples:         2816 | elapsed time per iteration (ms): 4220.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.139000E+00 | gshard_loss: 5.100288E-03 | loss scale: 131072.0 | grad norm: 3.900 | num zeros: 114516008.0 | params norm: 228.844 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/      20 | consumed samples:         3072 | elapsed time per iteration (ms): 4166.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.005022E+00 | gshard_loss: 5.001431E-03 | loss scale: 131072.0 | grad norm: 3.897 | num zeros: 114231040.0 | params norm: 228.880 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/      20 | consumed samples:         3328 | elapsed time per iteration (ms): 4242.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.879093E+00 | gshard_loss: 4.892912E-03 | loss scale: 131072.0 | grad norm: 3.848 | num zeros: 105158120.0 | params norm: 228.918 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/      20 | consumed samples:         3584 | elapsed time per iteration (ms): 4150.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.760610E+00 | gshard_loss: 4.886131E-03 | loss scale: 131072.0 | grad norm: 3.850 | num zeros: 114045920.0 | params norm: 228.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/      20 | consumed samples:         3840 | elapsed time per iteration (ms): 4181.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.642101E+00 | gshard_loss: 4.863725E-03 | loss scale: 131072.0 | grad norm: 3.846 | num zeros: 104614000.0 | params norm: 228.995 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/      20 | consumed samples:         4096 | elapsed time per iteration (ms): 4283.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.541323E+00 | gshard_loss: 4.817250E-03 | loss scale: 131072.0 | grad norm: 3.804 | num zeros: 104029680.0 | params norm: 229.037 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/      20 | consumed samples:         4352 | elapsed time per iteration (ms): 4204.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.431101E+00 | gshard_loss: 4.838301E-03 | loss scale: 131072.0 | grad norm: 3.795 | num zeros: 104039592.0 | params norm: 229.078 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/      20 | consumed samples:         4608 | elapsed time per iteration (ms): 4289.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.333813E+00 | gshard_loss: 4.767300E-03 | loss scale: 131072.0 | grad norm: 3.784 | num zeros: 104046448.0 | params norm: 229.121 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/      20 | consumed samples:         4864 | elapsed time per iteration (ms): 4448.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.237777E+00 | gshard_loss: 4.678998E-03 | loss scale: 131072.0 | grad norm: 3.754 | num zeros: 94634320.0 | params norm: 229.164 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/      20 | consumed samples:         5120 | elapsed time per iteration (ms): 4447.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.150270E+00 | gshard_loss: 4.667072E-03 | loss scale: 131072.0 | grad norm: 3.698 | num zeros: 85256944.0 | params norm: 229.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-05-28 16:41:48 
+ '[' 2 -ne '2]'
./run_moe.sh: line 7: [: missing `]'
++ expr 16594 % 10000 + 10000
+ export MASTER_PORT=16594
+ MASTER_PORT=16594
+ export EXP_NAME=fig8_2023-05-28T16:36:48+08:00
+ EXP_NAME=fig8_2023-05-28T16:36:48+08:00
+ export DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/dense.sh
+ DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/dense.sh
+ export SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/sparse.sh
+ SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/sparse.sh
+ export PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/parallel.sh
+ PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/parallel.sh
+ export CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/cluster.sh
+ CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/cluster.sh
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/dense.sh
++ export NUM_LAYERS=16
++ NUM_LAYERS=16
++ export HIDDEN_SIZE=1536
++ HIDDEN_SIZE=1536
++ export NUM_ATTN_HEADS=16
++ NUM_ATTN_HEADS=16
++ export SEQ_LEN=1024
++ SEQ_LEN=1024
++ export FMOE_FASTER_GLBPLC_ALPHA=2
++ FMOE_FASTER_GLBPLC_ALPHA=2
++ export FMOE_FASTER_GLBPLC_DMODEL=1536
++ FMOE_FASTER_GLBPLC_DMODEL=1536
++ export dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
++ dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/sparse.sh
++ export TOT_EXPERTS=32
++ TOT_EXPERTS=32
++ export GATE=gshard
++ GATE=gshard
++ export GSHARD_CAP=1.2
++ GSHARD_CAP=1.2
++ '[' gshard == gshard ']'
++ GATE_NAME=gshard1.2
++ export sparse_name=32MoE_gshard1.2
++ sparse_name=32MoE_gshard1.2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/parallel.sh
++ export TENSOR_PARALLEL_SIZE=1
++ TENSOR_PARALLEL_SIZE=1
++ export PIPELINE_PARALLEL_SIZE=1
++ PIPELINE_PARALLEL_SIZE=1
++ export DATA_PARALLEL_SIZE=16
++ DATA_PARALLEL_SIZE=16
++ export EXPERT_EP_SIZE=8
++ EXPERT_EP_SIZE=8
++ export EXPERT_DP_SIZE=2
++ EXPERT_DP_SIZE=2
++ export GLOBAL_BATCH_SIZE=256
++ GLOBAL_BATCH_SIZE=256
++ export MICRO_BATCH_SIZE=2
++ MICRO_BATCH_SIZE=2
++ export FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ export FMOE_FASTER_GROUP_SIZE=none
++ FMOE_FASTER_GROUP_SIZE=none
++ export FMOE_FASTER_SHADOW_ENABLE=OFF
++ FMOE_FASTER_SHADOW_ENABLE=OFF
++ export DYNAMIC_ENABLE=OFF
++ DYNAMIC_ENABLE=OFF
++ export DYNAMIC_FREQ=0
++ DYNAMIC_FREQ=0
++ export ITERATION=20
++ ITERATION=20
++ export TRAIN_SAMPLES=5120
++ TRAIN_SAMPLES=5120
++ export parallel_name=t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
++ parallel_name=t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/cluster.sh
++ export NNODES=2
++ NNODES=2
++ export 'NODELIST=nico[1-2]'
++ NODELIST='nico[1-2]'
++ export FMOE_FASTER_GLBPLC_NETBW=8e9
++ FMOE_FASTER_GLBPLC_NETBW=8e9
++ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ export FMOE_FASTER_GLBPLC_GPUTP=112e12
++ FMOE_FASTER_GLBPLC_GPUTP=112e12
++ export CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ export GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ export BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ export MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ export DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ export cluster_name=nico
++ cluster_name=nico
+ export NUM_EXPERTS=4
+ NUM_EXPERTS=4
+ mkdir -p ./logs
+ mkdir -p ./logs/fig8_2023-05-28T16:36:48+08:00
++ pwd
+ export LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00
++ date -Iseconds
+ LOG_PREFIX=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00
+ LOG_NAME=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.log
+ export PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ mkdir -p /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 2'
+ '[' 'nico[1-2]' '!=' None ']'
++ scontrol show hostnames 'nico[1-2]'
++ wc -l
+ tmp=2
+ '[' 2 '!=' 2 ']'
+ exec_args+=' -w nico[1-2]'
+ exec_args+=' -p AE'
+ LOG_FILE=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.log
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.log
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w nico[1-2] -p AE
+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/dense.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.log
#!/bin/bash

export NUM_LAYERS=16
export HIDDEN_SIZE=1536
export NUM_ATTN_HEADS=16
export SEQ_LEN=1024

export FMOE_FASTER_GLBPLC_ALPHA="2"
export FMOE_FASTER_GLBPLC_DMODEL=${HIDDEN_SIZE}

export dense_name=GPT-L${NUM_LAYERS}-H${HIDDEN_SIZE}-ATTN${NUM_ATTN_HEADS}-SEQ${SEQ_LEN}-ALPHA${FMOE_FASTER_GLBPLC_ALPHA}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/sparse.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.log
#!/bin/bash

export TOT_EXPERTS=32
export GATE="gshard"
export GSHARD_CAP="1.2"

if [ ${GATE} == 'gshard' ];then
    GATE_NAME=${GATE}${GSHARD_CAP}
else
    GATE_NAME=${GATE}
fi
export sparse_name=${TOT_EXPERTS}MoE\_${GATE_NAME}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/parallel.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.log
#!/bin/bash

export TENSOR_PARALLEL_SIZE=1
export PIPELINE_PARALLEL_SIZE=1
export DATA_PARALLEL_SIZE=16
export EXPERT_EP_SIZE=8
export EXPERT_DP_SIZE=2
export GLOBAL_BATCH_SIZE=256
export MICRO_BATCH_SIZE=2
export FMOE_FASTER_SCHEDULE_ENABLE=OFF
export FMOE_FASTER_GROUP_SIZE=none
export FMOE_FASTER_SHADOW_ENABLE=OFF
export DYNAMIC_ENABLE=OFF
export DYNAMIC_FREQ=0
export ITERATION=20

export TRAIN_SAMPLES=$(( $GLOBAL_BATCH_SIZE * $ITERATION))

export parallel_name=t$TENSOR_PARALLEL_SIZE\_p$PIPELINE_PARALLEL_SIZE\_d$DATA_PARALLEL_SIZE\_ep${EXPERT_EP_SIZE}\_dp${EXPERT_DP_SIZE}\_gbs$GLOBAL_BATCH_SIZE\_mbs$MICRO_BATCH_SIZE\_smart\_${FMOE_FASTER_SCHEDULE_ENABLE}\_smartGP${FMOE_FASTER_GROUP_SIZE}\_shadow${FMOE_FASTER_SHADOW_ENABLE}\_dynamic${DYNAMIC_ENABLE}\_freq${DYNAMIC_FREQ}\_iter${ITERATION}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8/config-smartmoe/cluster.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.log
#!/bin/bash

export NNODES=2
export NODELIST="nico[1-2]"
export FMOE_FASTER_GLBPLC_NETBW="8e9"
export FMOE_FASTER_GLBPLC_NETBW_Bcast="2e9"
export FMOE_FASTER_GLBPLC_GPUTP="112e12"

export CODE_PREFIX=$AEROOT/src/Megatron-LM

export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
export GPT_VOCAB_FILE=${DATASET_PREFIX}/gpt2-vocab.json
export BERT_VOCAB_FILE=${DATASET_PREFIX}/bert-large-uncased-vocab.txt
export MERGE_FILE=${DATASET_PREFIX}/gpt2-merges.txt
export DATA_PATH=${DATASET_PREFIX}/my-bert_text_sentence

export cluster_name=nico+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE ./pretrain_moe.sh
++ scontrol show JobId=164084
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164084
++ grep BatchHost
++ scontrol show JobId=164084
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=164084
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=164084
++ awk '{print $2}'
++ grep BatchHost
++ scontrol show JobId=164084
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164084
++ scontrol show JobId=164084
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164084
++ scontrol show JobId=164084
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=164084
++ grep BatchHost
++ scontrol show JobId=164084
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=164084
++ scontrol show JobId=164084
++ scontrol show JobId=164084
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=164084
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export MASTER_ADDR=nico1
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export RANK=4
+ RANK=4
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export NODE_RANK=2
+ NODE_RANK=2
+ localrank=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=0
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export NODE_RANK=4
+ NODE_RANK=4
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvm+ export MASTER_ADDR=nico1
e/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ MASTER_ADDR=nico1
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ localrank=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ export CUDA_VISIBLE_DEVICES=1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=4
+ NODE_RANK=4
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export MASTER_ADDR=nico1
+ export NODE_RANK=5
+ NODE_RANK=5
+ MASTER_ADDR=nico1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MASTER_ADDR=nico1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MASTER_ADDR=nico1
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy gshard         --gshard-cap 1.2         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 2         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig8_2023-05-28T16:36:48+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_gshard1.2_t1_p1_d16_ep8_dp2_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:41:53+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy gshard --gshard-cap 1.2 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 2 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ gshard
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 2
  expert_ep_size .................................. 8
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 1.2
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 4
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 5120
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 0 in DP group [0, 8]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.135 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [4, 12]
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [3, 11]
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [0, 8]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in DP group [1, 9]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in DP group [4, 12]
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in DP group [7, 15]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 13.124 seconds
time to initialize megatron (seconds): -8.071
[after megatron is initialized] datetime: 2023-05-28 16:42:31 
building GPT model ...
[Warning] world comm group not exist!
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [5, 13]
[Warning] world comm group not exist!
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [7, 15]
[Warning] world comm group not exist!
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [1, 9]
[Warning] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 835096064
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [6, 14]
[Warning] world comm group not exist!
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [2, 10]
[Warning] world comm group not exist!
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in DP group [6, 14]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in DP group [3, 11]
[Warning] world comm group not exist!
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in DP group [5, 13]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
setting training iterations to 20
> learning rate decay style: cosine
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in DP group [2, 10]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 16:42:49 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      5120
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.003498 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.005 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 16:43:01 
done with setup ...
training ...
[before the start of training step] datetime: 2023-05-28 16:43:01 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 16017.0869140625 | max allocated: 16017.09423828125 | reserved: 16602.0 | max reserved: 16602.0
 iteration        1/      20 | consumed samples:          256 | elapsed time per iteration (ms): 7725.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082701E+01 | gshard_loss: 1.577374E-02 | loss scale: 131072.0 | grad norm: 9.581 | num zeros: 73704.0 | params norm: 229.965 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/      20 | consumed samples:          512 | elapsed time per iteration (ms): 4377.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.048970E+01 | gshard_loss: 3.831507E-02 | loss scale: 131072.0 | grad norm: 4.546 | num zeros: 227831152.0 | params norm: 229.990 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/      20 | consumed samples:          768 | elapsed time per iteration (ms): 4373.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.040285E+01 | gshard_loss: 3.841360E-02 | loss scale: 131072.0 | grad norm: 4.711 | num zeros: 396642272.0 | params norm: 230.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/      20 | consumed samples:         1024 | elapsed time per iteration (ms): 4485.9 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 131072.0 | params norm: 230.012 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        5/      20 | consumed samples:         1280 | elapsed time per iteration (ms): 4444.5 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 65536.0 | params norm: 230.012 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        6/      20 | consumed samples:         1536 | elapsed time per iteration (ms): 4437.1 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 32768.0 | params norm: 230.012 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        7/      20 | consumed samples:         1792 | elapsed time per iteration (ms): 4510.0 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 16384.0 | params norm: 230.012 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        8/      20 | consumed samples:         2048 | elapsed time per iteration (ms): 4486.1 | learning rate: 1.000E-04 | global batch size:   256 | loss scale: 8192.0 | params norm: 230.012 | number of skipped iterations:   1 | number of nan iterations:   0 |
 iteration        9/      20 | consumed samples:         2304 | elapsed time per iteration (ms): 4590.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.091144E+01 | gshard_loss: 1.124071E-01 | loss scale: 8192.0 | grad norm: 103.515 | num zeros: 57692600.0 | params norm: 230.031 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/      20 | consumed samples:         2560 | elapsed time per iteration (ms): 4187.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.007421E+01 | gshard_loss: 3.904641E-02 | loss scale: 8192.0 | grad norm: 3.986 | num zeros: 440117792.0 | params norm: 230.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/      20 | consumed samples:         2816 | elapsed time per iteration (ms): 4369.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.931329E+00 | gshard_loss: 3.072999E-02 | loss scale: 8192.0 | grad norm: 3.939 | num zeros: 412743008.0 | params norm: 230.095 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/      20 | consumed samples:         3072 | elapsed time per iteration (ms): 4502.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.782020E+00 | gshard_loss: 2.772773E-02 | loss scale: 8192.0 | grad norm: 3.934 | num zeros: 358977568.0 | params norm: 230.134 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/      20 | consumed samples:         3328 | elapsed time per iteration (ms): 4651.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.637819E+00 | gshard_loss: 2.619729E-02 | loss scale: 8192.0 | grad norm: 3.896 | num zeros: 303726144.0 | params norm: 230.174 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/      20 | consumed samples:         3584 | elapsed time per iteration (ms): 4698.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.496222E+00 | gshard_loss: 2.446584E-02 | loss scale: 8192.0 | grad norm: 3.905 | num zeros: 259562720.0 | params norm: 230.217 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/      20 | consumed samples:         3840 | elapsed time per iteration (ms): 4788.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.353125E+00 | gshard_loss: 2.325115E-02 | loss scale: 8192.0 | grad norm: 3.911 | num zeros: 248639808.0 | params norm: 230.260 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/      20 | consumed samples:         4096 | elapsed time per iteration (ms): 4850.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.218714E+00 | gshard_loss: 2.224734E-02 | loss scale: 8192.0 | grad norm: 3.880 | num zeros: 238636624.0 | params norm: 230.304 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/      20 | consumed samples:         4352 | elapsed time per iteration (ms): 4859.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.080664E+00 | gshard_loss: 2.174693E-02 | loss scale: 8192.0 | grad norm: 3.881 | num zeros: 210755296.0 | params norm: 230.349 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/      20 | consumed samples:         4608 | elapsed time per iteration (ms): 4977.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.951399E+00 | gshard_loss: 2.136869E-02 | loss scale: 8192.0 | grad norm: 3.886 | num zeros: 191495984.0 | params norm: 230.395 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/      20 | consumed samples:         4864 | elapsed time per iteration (ms): 5135.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.827042E+00 | gshard_loss: 2.063601E-02 | loss scale: 8192.0 | grad norm: 3.877 | num zeros: 161878512.0 | params norm: 230.441 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/      20 | consumed samples:         5120 | elapsed time per iteration (ms): 5161.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.710803E+00 | gshard_loss: 2.021152E-02 | loss scale: 8192.0 | grad norm: 3.846 | num zeros: 124712656.0 | params norm: 230.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-05-28 16:44:36 
/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning ............................. False
  data_impl ....................................... mmap
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ /home/atc23_ae/SmartMoE-AE/scripts/deepspeed/ds_config_gpt_gpt-L16_H1536_32MoE_gshard1.2-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-1-gpus-16-mp-1-pp-1-ep-32-mlc-0.01-cap-1.2-drop-true.json
  deepspeed_mpi ................................... False
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 100000
  eval_iters ...................................... 100000
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  expert_interval ................................. 2
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1536
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 16
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.2
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [32]
  num_experts_teacher ............................. [1]
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  remote_device ................................... none
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  split_transformers .............................. False
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /home/atc23_ae/SmartMoE-AE/scripts/deepspeed/output/tensorboard/gpt-L16_H1536_32MoE_gshard1.2-lr-1.0e-4-minlr-1.0e-5-gbs-256-mbs-1-gpus-16-mp-1-pp-1-ep-32-mlc-0.01-cap-1.2-drop-true_nico0_2023.05.28-16.44.41
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  tile_factor ..................................... 1
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_iters ..................................... 20
  train_samples ................................... None
  train_tokens .................................... 300000000000
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_tutel ....................................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.1
  world_size ...................................... 16
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 16
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
[2023-05-28 16:44:49,105] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.138 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.658 seconds
time to initialize megatron (seconds): 10.568
[after megatron is initialized] datetime: 2023-05-28 16:44:58 
building GPT model ...
[2023-05-28 16:44:58,637] [INFO] [utils.py:824:see_memory_usage] Before Building Model
[2023-05-28 16:44:58,638] [INFO] [utils.py:825:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2023-05-28 16:44:58,638] [INFO] [utils.py:833:see_memory_usage] CPU Virtual Memory:  used = 84.38 GB, percent = 22.4%
[2023-05-28 16:44:58,671] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 2 | expert_parallel_size: 16
[2023-05-28 16:44:58,678] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 2 | expert_parallel_size: 16
[2023-05-28 16:44:58,685] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 2 | expert_parallel_size: 16
[2023-05-28 16:44:58,692] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 2 | expert_parallel_size: 16
[2023-05-28 16:44:58,701] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 2 | expert_parallel_size: 16
[2023-05-28 16:44:58,710] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 2 | expert_parallel_size: 16
[2023-05-28 16:44:58,717] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 2 | expert_parallel_size: 16
[2023-05-28 16:44:58,723] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 32 | num_local_experts: 2 | expert_parallel_size: 16
[2023-05-28 16:44:58,809] [INFO] [utils.py:824:see_memory_usage] After Building Model
[2023-05-28 16:44:58,810] [INFO] [utils.py:825:see_memory_usage] MA 1.27 GB         Max_MA 1.31 GB         CA 1.34 GB         Max_CA 1 GB 
[2023-05-28 16:44:58,810] [INFO] [utils.py:833:see_memory_usage] CPU Virtual Memory:  used = 84.46 GB, percent = 22.4%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 683596800
> learning rate decay style: cosine
DeepSpeed is enabled.
[2023-05-28 16:44:58,825] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.1, git-hash=unknown, git-branch=unknown
No existing process group found, creating a new group named: ep_size_16
[2023-05-28 16:44:59,038] [INFO] [logging.py:69:log_dist] [Rank 0] Creating deepspeed groups with model parallel size 1, expert parallel size 16, world size 16, dp world size 16
[2023-05-28 16:44:59,604] [INFO] [engine.py:277:__init__] DeepSpeed Flops Profiler Enabled: False
[2023-05-28 16:44:59,605] [INFO] [engine.py:1050:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer
[2023-05-28 16:44:59,605] [INFO] [engine.py:1056:_configure_optimizer] Using client Optimizer as basic optimizer
[2023-05-28 16:44:59,611] [INFO] [engine.py:1072:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam
[2023-05-28 16:44:59,611] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-05-28 16:44:59,866] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-05-28 16:44:59,866] [INFO] [engine.py:786:_configure_lr_scheduler] DeepSpeed using client LR scheduler
[2023-05-28 16:44:59,866] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f487453bfa0>
[2023-05-28 16:44:59,866] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:44:59,867] [INFO] [config.py:1060:print] DeepSpeedEngine configuration:
[2023-05-28 16:44:59,867] [INFO] [config.py:1064:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-05-28 16:44:59,867] [INFO] [config.py:1064:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-05-28 16:44:59,867] [INFO] [config.py:1064:print]   amp_enabled .................. False
[2023-05-28 16:44:59,867] [INFO] [config.py:1064:print]   amp_params ................... False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   bfloat16_enabled ............. False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   checkpoint_tag_validation_enabled  True
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   checkpoint_tag_validation_fail  False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   communication_data_type ...... None
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   curriculum_enabled ........... False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   curriculum_params ............ {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 1024, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 424592, 'difficulty_step': 8}}
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   dataloader_drop_last ......... False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   disable_allgather ............ False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   dump_state ................... False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   eigenvalue_enabled ........... False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   eigenvalue_gas_boundary_resolution  1
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   eigenvalue_layer_num ......... 0
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   eigenvalue_max_iter .......... 100
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   eigenvalue_stability ......... 1e-06
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   eigenvalue_tol ............... 0.01
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   eigenvalue_verbose ........... False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   elasticity_enabled ........... False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   fp16_enabled ................. True
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   fp16_master_weights_and_gradients  False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   fp16_mixed_quantize .......... False
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   global_rank .................. 0
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   gradient_accumulation_steps .. 16
[2023-05-28 16:44:59,868] [INFO] [config.py:1064:print]   gradient_clipping ............ 1.0
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   gradient_predivide_factor .... 1.0
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   initial_dynamic_scale ........ 2048
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   loss_scale ................... 0
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   memory_breakdown ............. False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   optimizer_legacy_fusion ...... False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   optimizer_name ............... None
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   optimizer_params ............. None
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   pld_enabled .................. False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   pld_params ................... False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   prescale_gradients ........... True
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_change_rate ......... 0.001
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_groups .............. 1
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_offset .............. 1000
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_period .............. 1000
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_rounding ............ 0
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_start_bits .......... 16
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_target_bits ......... 8
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_training_enabled .... False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_type ................ 0
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   quantize_verbose ............. False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   scheduler_name ............... None
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   scheduler_params ............. None
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   sparse_attention ............. None
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   sparse_gradients_enabled ..... False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   steps_per_print .............. 1
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   tensorboard_enabled .......... False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   tensorboard_job_name ......... DeepSpeedJobName
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   tensorboard_output_path ...... 
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   train_batch_size ............. 256
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   train_micro_batch_size_per_gpu  1
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   use_quantizer_kernel ......... False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   wall_clock_breakdown ......... False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   world_size ................... 16
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   zero_allow_untested_optimizer  False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   zero_config .................. {
    "stage": 0, 
    "contiguous_gradients": true, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 5.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 5.000000e+08, 
    "overlap_comm": false, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": true, 
    "offload_param": null, 
    "offload_optimizer": null, 
    "sub_group_size": 1.000000e+09, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_16bit_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "round_robin_gradients": false, 
    "legacy_stage1": false
}
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   zero_enabled ................. False
[2023-05-28 16:44:59,869] [INFO] [config.py:1064:print]   zero_optimization_stage ...... 0
[2023-05-28 16:44:59,870] [INFO] [config.py:1066:print]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 1, 
    "steps_per_print": 1, 
    "zero_optimization": {
        "stage": 0, 
        "elastic_checkpoint": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 1.024000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 4.245920e+05, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Emitting ninja build file /home/atc23_ae/.cache/torch_extensions/py39_cu113/utils/build.ninja...
Building extension module utils...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 3.0120155811309814 seconds
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 16:45:02 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      5120
    validation: 25600000
    test:       25600000
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.001923 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 16:45:03 
done with setup ...
training ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
> setting tensorboard ...
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4275779724121094 seconds
time (ms) | model-and-optimizer-setup: 1677.32 | train/valid/test-data-iterators-setup: 878.35
[before the start of training step] datetime: 2023-05-28 16:45:05 
[2023-05-28 16:45:12,354] [INFO] [logging.py:69:log_dist] [Rank 0] step=1, skipped=0, lr=[0.0001, 0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[Rank 0] (after 1 iterations) memory (MB) | allocated: 9129.384765625 | max allocated: 13464.87890625 | reserved: 15720.0 | max reserved: 15720.0
 iteration        1/      20 | consumed samples:          256 | consumed tokens:       262144 | elapsed time per iteration (ms): 7449.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.101720E+01 | moe loss: 1.184029E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 4434.86 | backward-compute: 2714.57 | backward-embedding-all-reduce: 0.01 | optimizer: 135.32 | batch-generator: 472.90
[2023-05-28 16:45:17,106] [INFO] [logging.py:69:log_dist] [Rank 0] step=2, skipped=0, lr=[9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05, 9.999999999983044e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
 iteration        2/      20 | consumed samples:          512 | consumed tokens:       524288 | elapsed time per iteration (ms): 5011.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.748632E+00 | moe loss: 2.516424E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2193.00 | backward-compute: 2195.78 | backward-embedding-all-reduce: 0.01 | optimizer: 80.01 | batch-generator: 419.05
[2023-05-28 16:45:22,030] [INFO] [logging.py:69:log_dist] [Rank 0] step=3, skipped=0, lr=[9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05, 9.999999999932177e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:45:22,068] [INFO] [timer.py:181:stop] 0/3, SamplesPerSec=20.853787336866304, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration        3/      20 | consumed samples:          768 | consumed tokens:       786432 | elapsed time per iteration (ms): 4684.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.297160E+00 | moe loss: 2.352051E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2210.06 | backward-compute: 2191.55 | backward-embedding-all-reduce: 0.01 | optimizer: 77.80 | batch-generator: 359.20
[2023-05-28 16:45:26,593] [INFO] [logging.py:69:log_dist] [Rank 0] step=4, skipped=0, lr=[9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05, 9.999999999847399e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:45:26,630] [INFO] [timer.py:181:stop] 0/4, SamplesPerSec=20.984973335249144, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration        4/      20 | consumed samples:         1024 | consumed tokens:      1048576 | elapsed time per iteration (ms): 4579.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.214164E+00 | moe loss: 1.916004E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2098.35 | backward-compute: 2189.67 | backward-embedding-all-reduce: 0.01 | optimizer: 78.09 | batch-generator: 373.25
[2023-05-28 16:45:31,188] [INFO] [logging.py:69:log_dist] [Rank 0] step=5, skipped=0, lr=[9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05, 9.999999999728707e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:45:31,225] [INFO] [timer.py:181:stop] 0/5, SamplesPerSec=21.011411039333638, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration        5/      20 | consumed samples:         1280 | consumed tokens:      1310720 | elapsed time per iteration (ms): 4595.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.991264E+00 | moe loss: 1.786971E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2109.67 | backward-compute: 2189.93 | backward-embedding-all-reduce: 0.01 | optimizer: 77.49 | batch-generator: 385.92
[2023-05-28 16:45:35,717] [INFO] [logging.py:69:log_dist] [Rank 0] step=6, skipped=0, lr=[9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05, 9.999999999576103e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:45:35,755] [INFO] [timer.py:181:stop] 0/6, SamplesPerSec=21.049079279306973, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration        6/      20 | consumed samples:         1536 | consumed tokens:      1572864 | elapsed time per iteration (ms): 4456.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.778544E+00 | moe loss: 1.614811E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2047.07 | backward-compute: 2187.76 | backward-embedding-all-reduce: 0.01 | optimizer: 77.44 | batch-generator: 339.28
[2023-05-28 16:45:40,260] [INFO] [logging.py:69:log_dist] [Rank 0] step=7, skipped=0, lr=[9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05, 9.999999999389589e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:45:40,298] [INFO] [timer.py:181:stop] 0/7, SamplesPerSec=20.99683430836302, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration        7/      20 | consumed samples:         1792 | consumed tokens:      1835008 | elapsed time per iteration (ms): 4579.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.654326E+00 | moe loss: 1.503367E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2135.79 | backward-compute: 2188.66 | backward-embedding-all-reduce: 0.01 | optimizer: 78.01 | batch-generator: 334.36
[2023-05-28 16:45:44,731] [INFO] [logging.py:69:log_dist] [Rank 0] step=8, skipped=0, lr=[9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05, 9.999999999169163e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:45:44,768] [INFO] [timer.py:181:stop] 0/8, SamplesPerSec=20.949819478656693, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration        8/      20 | consumed samples:         2048 | consumed tokens:      2097152 | elapsed time per iteration (ms): 4476.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.514003E+00 | moe loss: 1.444962E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 2024.22 | backward-compute: 2190.39 | backward-embedding-all-reduce: 0.01 | optimizer: 77.20 | batch-generator: 310.83
[2023-05-28 16:45:49,243] [INFO] [logging.py:69:log_dist] [Rank 0] step=9, skipped=0, lr=[9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05, 9.999999998914827e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:45:49,281] [INFO] [timer.py:181:stop] 0/9, SamplesPerSec=20.941229073071582, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration        9/      20 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 4624.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.388726E+00 | moe loss: 1.423162E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1982.70 | backward-compute: 2189.44 | backward-embedding-all-reduce: 0.01 | optimizer: 77.78 | batch-generator: 276.22
[2023-05-28 16:45:53,777] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05, 9.999999998626577e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:45:53,814] [INFO] [timer.py:181:stop] 0/10, SamplesPerSec=20.96560540616237, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       10/      20 | consumed samples:         2560 | consumed tokens:      2621440 | elapsed time per iteration (ms): 4442.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.265247E+00 | moe loss: 1.380588E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1968.55 | backward-compute: 2187.71 | backward-embedding-all-reduce: 0.01 | optimizer: 78.85 | batch-generator: 253.35
[2023-05-28 16:45:58,252] [INFO] [logging.py:69:log_dist] [Rank 0] step=11, skipped=0, lr=[9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05, 9.999999998304415e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:45:58,289] [INFO] [timer.py:181:stop] 0/11, SamplesPerSec=20.969527166810572, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       11/      20 | consumed samples:         2816 | consumed tokens:      2883584 | elapsed time per iteration (ms): 4466.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.147177E+00 | moe loss: 1.331958E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1990.94 | backward-compute: 2190.97 | backward-embedding-all-reduce: 0.01 | optimizer: 77.37 | batch-generator: 306.45
[2023-05-28 16:46:02,686] [INFO] [logging.py:69:log_dist] [Rank 0] step=12, skipped=0, lr=[9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05, 9.999999997948343e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:46:02,723] [INFO] [timer.py:181:stop] 0/12, SamplesPerSec=20.936329954907666, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       12/      20 | consumed samples:         3072 | consumed tokens:      3145728 | elapsed time per iteration (ms): 4423.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.035034E+00 | moe loss: 1.318489E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1962.75 | backward-compute: 2191.39 | backward-embedding-all-reduce: 0.01 | optimizer: 76.56 | batch-generator: 283.71
[2023-05-28 16:46:07,019] [INFO] [logging.py:69:log_dist] [Rank 0] step=13, skipped=0, lr=[9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05, 9.999999997558358e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:46:07,056] [INFO] [timer.py:181:stop] 0/13, SamplesPerSec=20.939268173223077, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       13/      20 | consumed samples:         3328 | consumed tokens:      3407872 | elapsed time per iteration (ms): 4333.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.950974E+00 | moe loss: 1.320006E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1874.20 | backward-compute: 2191.89 | backward-embedding-all-reduce: 0.01 | optimizer: 77.21 | batch-generator: 242.79
[2023-05-28 16:46:11,349] [INFO] [logging.py:69:log_dist] [Rank 0] step=14, skipped=0, lr=[9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05, 9.999999997134462e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:46:11,386] [INFO] [timer.py:181:stop] 0/14, SamplesPerSec=20.951504220512252, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       14/      20 | consumed samples:         3584 | consumed tokens:      3670016 | elapsed time per iteration (ms): 4297.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.858104E+00 | moe loss: 1.322385E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1876.81 | backward-compute: 2186.69 | backward-embedding-all-reduce: 0.01 | optimizer: 80.31 | batch-generator: 260.51
[2023-05-28 16:46:15,643] [INFO] [logging.py:69:log_dist] [Rank 0] step=15, skipped=0, lr=[9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05, 9.999999996676654e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:46:15,680] [INFO] [timer.py:181:stop] 0/15, SamplesPerSec=20.962547425013224, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       15/      20 | consumed samples:         3840 | consumed tokens:      3932160 | elapsed time per iteration (ms): 4315.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.766656E+00 | moe loss: 1.336977E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1845.63 | backward-compute: 2189.69 | backward-embedding-all-reduce: 0.01 | optimizer: 78.01 | batch-generator: 235.20
[2023-05-28 16:46:19,978] [INFO] [logging.py:69:log_dist] [Rank 0] step=16, skipped=0, lr=[9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05, 9.999999996184934e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:46:20,015] [INFO] [timer.py:181:stop] 0/16, SamplesPerSec=20.96538949548235, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       16/      20 | consumed samples:         4096 | consumed tokens:      4194304 | elapsed time per iteration (ms): 4348.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.704920E+00 | moe loss: 1.308432E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1889.19 | backward-compute: 2191.21 | backward-embedding-all-reduce: 0.01 | optimizer: 77.86 | batch-generator: 205.97
[2023-05-28 16:46:24,345] [INFO] [logging.py:69:log_dist] [Rank 0] step=17, skipped=0, lr=[9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05, 9.999999995659302e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:46:24,382] [INFO] [timer.py:181:stop] 0/17, SamplesPerSec=20.96732478776538, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       17/      20 | consumed samples:         4352 | consumed tokens:      4456448 | elapsed time per iteration (ms): 4400.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.631281E+00 | moe loss: 1.266478E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1869.51 | backward-compute: 2194.08 | backward-embedding-all-reduce: 0.01 | optimizer: 77.48 | batch-generator: 224.30
[2023-05-28 16:46:28,664] [INFO] [logging.py:69:log_dist] [Rank 0] step=18, skipped=0, lr=[9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05, 9.999999995099759e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:46:28,701] [INFO] [timer.py:181:stop] 0/18, SamplesPerSec=20.97897655107962, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       18/      20 | consumed samples:         4608 | consumed tokens:      4718592 | elapsed time per iteration (ms): 4216.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.566908E+00 | moe loss: 1.293158E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1820.00 | backward-compute: 2197.58 | backward-embedding-all-reduce: 0.01 | optimizer: 78.16 | batch-generator: 201.54
[2023-05-28 16:46:32,887] [INFO] [logging.py:69:log_dist] [Rank 0] step=19, skipped=0, lr=[9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05, 9.999999994506305e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:46:32,924] [INFO] [timer.py:181:stop] 0/19, SamplesPerSec=20.972638412206503, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       19/      20 | consumed samples:         4864 | consumed tokens:      4980736 | elapsed time per iteration (ms): 4248.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.515715E+00 | moe loss: 1.279200E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1827.61 | backward-compute: 2198.22 | backward-embedding-all-reduce: 0.01 | optimizer: 77.54 | batch-generator: 224.93
[2023-05-28 16:46:37,157] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=0, lr=[9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05, 9.999999993878939e-05], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2023-05-28 16:46:37,194] [INFO] [timer.py:181:stop] 0/20, SamplesPerSec=20.977267625653962, MemAllocated=8.92GB, MaxMemAllocated=13.15GB
 iteration       20/      20 | consumed samples:         5120 | consumed tokens:      5242880 | elapsed time per iteration (ms): 4383.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.473920E+00 | moe loss: 1.259127E-01 | loss scale: 2048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward-compute: 1847.34 | backward-compute: 2197.92 | backward-embedding-all-reduce: 0.01 | optimizer: 79.78 | batch-generator: 205.25
[after training is done] datetime: 2023-05-28 16:46:37 
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 3.0141046047210693 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 3.114687919616699 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 3.019656181335449 seconds
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 3.1110658645629883 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 3.111786365509033 seconds
Loading extension module utils...
Time to load utils op: 0.4174001216888428 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4245569705963135 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.42716240882873535 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.42789602279663086 seconds
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4218764305114746 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.42756175994873047 seconds
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 3.1110482215881348 seconds
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 3.014505386352539 seconds
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [92m[OKAY][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
sparse_attn ............ [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
async_io ............... [93m[NO][0m ....... [92m[OKAY][0m
utils .................. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/home/atc23_ae/.local/lib/python3.9/site-packages/torch']
torch version .................... 1.12.1+cu113
torch cuda version ............... 11.3
torch hip version ................ None
nvcc version ..................... 11.3
deepspeed install path ........... ['/home/atc23_ae/.local/lib/python3.9/site-packages/deepspeed']
deepspeed info ................... 0.6.1, unknown, unknown
deepspeed wheel compiled w. ...... torch 1.12, cuda 11.3, hip 0.0
**** Git info for Megatron: git_hash=9543c1b git_branch=main ****
No existing process group found, creating a new group named: ep_size_16
Using /home/atc23_ae/.cache/torch_extensions/py39_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4249715805053711 seconds
/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig8
+ cd -
/home/atc23_ae/SmartMoE-AE
++ pwd
++ pwd
+ AEROOT=/home/atc23_ae/SmartMoE-AE
+ python3 ./plotting/from_exec/fig8.py /home/atc23_ae/SmartMoE-AE /home/atc23_ae/SmartMoE-AE/outputs_from_exec_2023-05-28T16:36:45+08:00
{'FastMoE': 127.404, 'DeepSpeed': 92.3315, 'FasterMoE': 83.6916, 'SmartMoE': 95.6117}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
{'FastMoE': -1.0, 'DeepSpeed': -1.0, 'FasterMoE': -1.0, 'SmartMoE': -1.0}
+ cd ./plotting/from_exec/fig10
+ ./fig10.sh
+ '[' 2 -ne '2]'
./run_moe.sh: line 7: [: missing `]'
++ expr 13756 % 10000 + 10000
+ export MASTER_PORT=13756
+ MASTER_PORT=13756
+ export EXP_NAME=fig10_2023-05-28T16:46:42+08:00
+ EXP_NAME=fig10_2023-05-28T16:46:42+08:00
+ export DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/dense.sh
+ DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/dense.sh
+ export SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/sparse.sh
+ SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/sparse.sh
+ export PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/parallel.sh
+ PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/parallel.sh
+ export CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/cluster.sh
+ CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/cluster.sh
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/dense.sh
++ export NUM_LAYERS=16
++ NUM_LAYERS=16
++ export HIDDEN_SIZE=1536
++ HIDDEN_SIZE=1536
++ export NUM_ATTN_HEADS=16
++ NUM_ATTN_HEADS=16
++ export SEQ_LEN=1024
++ SEQ_LEN=1024
++ export FMOE_FASTER_GLBPLC_ALPHA=2
++ FMOE_FASTER_GLBPLC_ALPHA=2
++ export FMOE_FASTER_GLBPLC_DMODEL=1536
++ FMOE_FASTER_GLBPLC_DMODEL=1536
++ export dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
++ dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/sparse.sh
++ export TOT_EXPERTS=32
++ TOT_EXPERTS=32
++ export GATE=naive
++ GATE=naive
++ export GSHARD_CAP=4.8
++ GSHARD_CAP=4.8
++ '[' naive == gshard ']'
++ GATE_NAME=naive
++ export sparse_name=32MoE_naive
++ sparse_name=32MoE_naive
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/parallel.sh
++ export TENSOR_PARALLEL_SIZE=1
++ TENSOR_PARALLEL_SIZE=1
++ export PIPELINE_PARALLEL_SIZE=1
++ PIPELINE_PARALLEL_SIZE=1
++ export DATA_PARALLEL_SIZE=16
++ DATA_PARALLEL_SIZE=16
++ export EXPERT_EP_SIZE=16
++ EXPERT_EP_SIZE=16
++ export EXPERT_DP_SIZE=1
++ EXPERT_DP_SIZE=1
++ export GLOBAL_BATCH_SIZE=256
++ GLOBAL_BATCH_SIZE=256
++ export MICRO_BATCH_SIZE=2
++ MICRO_BATCH_SIZE=2
++ export FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ export FMOE_FASTER_GROUP_SIZE=none
++ FMOE_FASTER_GROUP_SIZE=none
++ export FMOE_FASTER_SHADOW_ENABLE=OFF
++ FMOE_FASTER_SHADOW_ENABLE=OFF
++ export DYNAMIC_ENABLE=OFF
++ DYNAMIC_ENABLE=OFF
++ export DYNAMIC_FREQ=0
++ DYNAMIC_FREQ=0
++ export ITERATION=20
++ ITERATION=20
++ export TRAIN_SAMPLES=5120
++ TRAIN_SAMPLES=5120
++ export parallel_name=t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
++ parallel_name=t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/cluster.sh
++ export NNODES=2
++ NNODES=2
++ export 'NODELIST=nico[1-2]'
++ NODELIST='nico[1-2]'
++ export FMOE_FASTER_GLBPLC_NETBW=8e9
++ FMOE_FASTER_GLBPLC_NETBW=8e9
++ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ export FMOE_FASTER_GLBPLC_GPUTP=112e12
++ FMOE_FASTER_GLBPLC_GPUTP=112e12
++ export CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ export GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ export BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ export MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ export DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ export cluster_name=nico
++ cluster_name=nico
+ export NUM_EXPERTS=2
+ NUM_EXPERTS=2
+ mkdir -p ./logs
+ mkdir -p ./logs/fig10_2023-05-28T16:46:42+08:00
++ pwd
+ export LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00
++ date -Iseconds
+ LOG_PREFIX=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00
+ LOG_NAME=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.log
+ export PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ mkdir -p /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 2'
+ '[' 'nico[1-2]' '!=' None ']'
++ scontrol show hostnames 'nico[1-2]'
++ wc -l
+ tmp=2
+ '[' 2 '!=' 2 ']'
+ exec_args+=' -w nico[1-2]'
+ exec_args+=' -p AE'
+ LOG_FILE=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.log
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.log
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w nico[1-2] -p AE
+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/dense.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.log
#!/bin/bash

export NUM_LAYERS=16
export HIDDEN_SIZE=1536
export NUM_ATTN_HEADS=16
export SEQ_LEN=1024

export FMOE_FASTER_GLBPLC_ALPHA="2"
export FMOE_FASTER_GLBPLC_DMODEL=${HIDDEN_SIZE}

export dense_name=GPT-L${NUM_LAYERS}-H${HIDDEN_SIZE}-ATTN${NUM_ATTN_HEADS}-SEQ${SEQ_LEN}-ALPHA${FMOE_FASTER_GLBPLC_ALPHA}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/sparse.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.log
#!/bin/bash

export TOT_EXPERTS=32
export GATE="naive"
export GSHARD_CAP="4.8"

if [ ${GATE} == 'gshard' ];then
    GATE_NAME=${GATE}${GSHARD_CAP}
else
    GATE_NAME=${GATE}
fi
export sparse_name=${TOT_EXPERTS}MoE\_${GATE_NAME}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/parallel.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.log
#!/bin/bash

export TENSOR_PARALLEL_SIZE=1
export PIPELINE_PARALLEL_SIZE=1
export DATA_PARALLEL_SIZE=16
export EXPERT_EP_SIZE=16
export EXPERT_DP_SIZE=1
export GLOBAL_BATCH_SIZE=256
export MICRO_BATCH_SIZE=2
export FMOE_FASTER_SCHEDULE_ENABLE=OFF
export FMOE_FASTER_GROUP_SIZE=none
export FMOE_FASTER_SHADOW_ENABLE=OFF
export DYNAMIC_ENABLE=OFF
export DYNAMIC_FREQ=0
export ITERATION=20

export TRAIN_SAMPLES=$(( $GLOBAL_BATCH_SIZE * $ITERATION))

export parallel_name=t$TENSOR_PARALLEL_SIZE\_p$PIPELINE_PARALLEL_SIZE\_d$DATA_PARALLEL_SIZE\_ep${EXPERT_EP_SIZE}\_dp${EXPERT_DP_SIZE}\_gbs$GLOBAL_BATCH_SIZE\_mbs$MICRO_BATCH_SIZE\_smart\_${FMOE_FASTER_SCHEDULE_ENABLE}\_smartGP${FMOE_FASTER_GROUP_SIZE}\_shadow${FMOE_FASTER_SHADOW_ENABLE}\_dynamic${DYNAMIC_ENABLE}\_freq${DYNAMIC_FREQ}\_iter${ITERATION}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-fastmoe/cluster.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.log
#!/bin/bash

export NNODES=2
export NODELIST="nico[1-2]"
export FMOE_FASTER_GLBPLC_NETBW="8e9"
export FMOE_FASTER_GLBPLC_NETBW_Bcast="2e9"
export FMOE_FASTER_GLBPLC_GPUTP="112e12"

export CODE_PREFIX=$AEROOT/src/Megatron-LM

export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
export GPT_VOCAB_FILE=${DATASET_PREFIX}/gpt2-vocab.json
export BERT_VOCAB_FILE=${DATASET_PREFIX}/bert-large-uncased-vocab.txt
export MERGE_FILE=${DATASET_PREFIX}/gpt2-merges.txt
export DATA_PATH=${DATASET_PREFIX}/my-bert_text_sentence

export cluster_name=nico+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE ./pretrain_moe.sh
++ scontrol show JobId=164086
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164086
++ grep BatchHost
++ scontrol show JobId=164086
++ scontrol show JobId=164086
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=164086
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=164086
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=164086
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=164086
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ export RANK=15
+ RANK=15
+ CUDA_VISIBLE_DEVICES=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export NODE_RANK=5
+ NODE_RANK=5
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
+ python_args='
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ export MASTER_ADDR=nico1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ MASTER_ADDR=nico1
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
++ scontrol show JobId=164086
++ scontrol show JobId=164086
++ scontrol show JobId=164086
++ scontrol show JobId=164086
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ scontrol show JobId=164086
+ export RANK=9
+ RANK=9
++ scontrol show JobId=164086
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ tr = ' '
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
++ tr = ' '
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
++ scontrol show JobId=164086
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
++ tr = ' '
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ awk '{print $2}'
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
++ awk '{print $2}'
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ tr = ' '
+ python_args='
++ awk '{print $2}'
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ awk '{print $2}'
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ awk '{print $2}'
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
++ awk '{print $2}'
++ grep BatchHost
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164086
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ MASTER_ADDR=nico1
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export RANK=3
+ RANK=3
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=1
+ NODE_RANK=1
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ python_args='
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ '[' OFF == ON ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ EXEC=./pretrain_gpt.py
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ python_args='
        --fmoefy         --num-experts 2         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 1         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 16         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p1_d16_ep16_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:46:45+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 2 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 16 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 16, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 16
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 16
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 4.8
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 2
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 5120
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 8
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.135 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 6 in DP group [6]
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 2 in DP group [2]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 3 in DP group [3]
[INFO] 8 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
[INFO] 15 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.487 seconds
time to initialize megatron (seconds): 5.966
[after megatron is initialized] datetime: 2023-05-28 16:47:01 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 532958720
[Warning] world comm group not exist!
[INFO] 13 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
[Warning] world comm group not exist!
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 4 in DP group [4]
[Warning] world comm group not exist!
[INFO] 10 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
[Warning] world comm group not exist!
[INFO] 12 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
[Warning] world comm group not exist!
[INFO] 11 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
setting training iterations to 20
> learning rate decay style: cosine
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 9 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[Warning] world comm group not exist!
[INFO] 14 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
[Warning] world comm group not exist!
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 5 in DP group [5]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 1 in DP group [1]
[Warning] world comm group not exist!
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 7 in DP group [7]
[Warning] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 16:47:10 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      5120
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002208 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.004 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 16:47:22 
done with setup ...
training ...
[before the start of training step] datetime: 2023-05-28 16:47:22 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 10229.978515625 | max allocated: 10229.98583984375 | reserved: 11568.0 | max reserved: 11568.0
 iteration        1/      20 | consumed samples:          256 | elapsed time per iteration (ms): 9115.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082711E+01 | loss scale: 131072.0 | grad norm: 11.322 | num zeros: 14745.0 | params norm: 228.602 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/      20 | consumed samples:          512 | elapsed time per iteration (ms): 11598.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.051697E+01 | loss scale: 131072.0 | grad norm: 5.424 | num zeros: 133048584.0 | params norm: 228.616 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/      20 | consumed samples:          768 | elapsed time per iteration (ms): 11686.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.035071E+01 | loss scale: 131072.0 | grad norm: 4.159 | num zeros: 227716320.0 | params norm: 228.626 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/      20 | consumed samples:         1024 | elapsed time per iteration (ms): 12086.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.016249E+01 | loss scale: 131072.0 | grad norm: 3.958 | num zeros: 209959008.0 | params norm: 228.635 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/      20 | consumed samples:         1280 | elapsed time per iteration (ms): 12152.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.000442E+01 | loss scale: 131072.0 | grad norm: 3.957 | num zeros: 209563728.0 | params norm: 228.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/      20 | consumed samples:         1536 | elapsed time per iteration (ms): 12271.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.847440E+00 | loss scale: 131072.0 | grad norm: 3.919 | num zeros: 228506496.0 | params norm: 228.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/      20 | consumed samples:         1792 | elapsed time per iteration (ms): 12296.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.681252E+00 | loss scale: 131072.0 | grad norm: 3.950 | num zeros: 228587712.0 | params norm: 228.711 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/      20 | consumed samples:         2048 | elapsed time per iteration (ms): 12610.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.533571E+00 | loss scale: 131072.0 | grad norm: 3.951 | num zeros: 210412608.0 | params norm: 228.743 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/      20 | consumed samples:         2304 | elapsed time per iteration (ms): 12460.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.380404E+00 | loss scale: 131072.0 | grad norm: 3.907 | num zeros: 239405200.0 | params norm: 228.777 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/      20 | consumed samples:         2560 | elapsed time per iteration (ms): 12490.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.234404E+00 | loss scale: 131072.0 | grad norm: 3.916 | num zeros: 267034192.0 | params norm: 228.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/      20 | consumed samples:         2816 | elapsed time per iteration (ms): 12439.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.093590E+00 | loss scale: 131072.0 | grad norm: 3.906 | num zeros: 267015920.0 | params norm: 228.848 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/      20 | consumed samples:         3072 | elapsed time per iteration (ms): 12770.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.960622E+00 | loss scale: 131072.0 | grad norm: 3.902 | num zeros: 267085472.0 | params norm: 228.885 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/      20 | consumed samples:         3328 | elapsed time per iteration (ms): 12827.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.834953E+00 | loss scale: 131072.0 | grad norm: 3.853 | num zeros: 257593136.0 | params norm: 228.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/      20 | consumed samples:         3584 | elapsed time per iteration (ms): 12796.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.716868E+00 | loss scale: 131072.0 | grad norm: 3.854 | num zeros: 257622864.0 | params norm: 228.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/      20 | consumed samples:         3840 | elapsed time per iteration (ms): 12686.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.599281E+00 | loss scale: 131072.0 | grad norm: 3.849 | num zeros: 257606784.0 | params norm: 229.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/      20 | consumed samples:         4096 | elapsed time per iteration (ms): 12684.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.499996E+00 | loss scale: 131072.0 | grad norm: 3.805 | num zeros: 258022992.0 | params norm: 229.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/      20 | consumed samples:         4352 | elapsed time per iteration (ms): 12736.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.391048E+00 | loss scale: 131072.0 | grad norm: 3.793 | num zeros: 267167856.0 | params norm: 229.085 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/      20 | consumed samples:         4608 | elapsed time per iteration (ms): 12757.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.295371E+00 | loss scale: 131072.0 | grad norm: 3.780 | num zeros: 257794048.0 | params norm: 229.128 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/      20 | consumed samples:         4864 | elapsed time per iteration (ms): 12833.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.200679E+00 | loss scale: 131072.0 | grad norm: 3.747 | num zeros: 258233664.0 | params norm: 229.171 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/      20 | consumed samples:         5120 | elapsed time per iteration (ms): 12838.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.114840E+00 | loss scale: 131072.0 | grad norm: 3.687 | num zeros: 267019056.0 | params norm: 229.214 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-05-28 16:51:28 
+ '[' 2 -ne '2]'
./run_moe.sh: line 7: [: missing `]'
++ expr 22879 % 10000 + 10000
+ export MASTER_PORT=12879
+ MASTER_PORT=12879
+ export EXP_NAME=fig10_2023-05-28T16:46:42+08:00
+ EXP_NAME=fig10_2023-05-28T16:46:42+08:00
+ export DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/dense.sh
+ DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/dense.sh
+ export SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/sparse.sh
+ SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/sparse.sh
+ export PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/parallel.sh
+ PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/parallel.sh
+ export CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/cluster.sh
+ CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/cluster.sh
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/dense.sh
++ export NUM_LAYERS=16
++ NUM_LAYERS=16
++ export HIDDEN_SIZE=1536
++ HIDDEN_SIZE=1536
++ export NUM_ATTN_HEADS=16
++ NUM_ATTN_HEADS=16
++ export SEQ_LEN=1024
++ SEQ_LEN=1024
++ export FMOE_FASTER_GLBPLC_ALPHA=2
++ FMOE_FASTER_GLBPLC_ALPHA=2
++ export FMOE_FASTER_GLBPLC_DMODEL=1536
++ FMOE_FASTER_GLBPLC_DMODEL=1536
++ export dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
++ dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/sparse.sh
++ export TOT_EXPERTS=32
++ TOT_EXPERTS=32
++ export GATE=naive
++ GATE=naive
++ export GSHARD_CAP=4.8
++ GSHARD_CAP=4.8
++ '[' naive == gshard ']'
++ GATE_NAME=naive
++ export sparse_name=32MoE_naive
++ sparse_name=32MoE_naive
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/parallel.sh
++ export TENSOR_PARALLEL_SIZE=1
++ TENSOR_PARALLEL_SIZE=1
++ export PIPELINE_PARALLEL_SIZE=2
++ PIPELINE_PARALLEL_SIZE=2
++ export DATA_PARALLEL_SIZE=8
++ DATA_PARALLEL_SIZE=8
++ export EXPERT_EP_SIZE=8
++ EXPERT_EP_SIZE=8
++ export EXPERT_DP_SIZE=1
++ EXPERT_DP_SIZE=1
++ export GLOBAL_BATCH_SIZE=256
++ GLOBAL_BATCH_SIZE=256
++ export MICRO_BATCH_SIZE=2
++ MICRO_BATCH_SIZE=2
++ export FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ export FMOE_FASTER_GROUP_SIZE=none
++ FMOE_FASTER_GROUP_SIZE=none
++ export FMOE_FASTER_SHADOW_ENABLE=OFF
++ FMOE_FASTER_SHADOW_ENABLE=OFF
++ export DYNAMIC_ENABLE=OFF
++ DYNAMIC_ENABLE=OFF
++ export DYNAMIC_FREQ=0
++ DYNAMIC_FREQ=0
++ export ITERATION=20
++ ITERATION=20
++ export TRAIN_SAMPLES=5120
++ TRAIN_SAMPLES=5120
++ export parallel_name=t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
++ parallel_name=t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/cluster.sh
++ export NNODES=2
++ NNODES=2
++ export 'NODELIST=nico[1-2]'
++ NODELIST='nico[1-2]'
++ export FMOE_FASTER_GLBPLC_NETBW=8e9
++ FMOE_FASTER_GLBPLC_NETBW=8e9
++ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ export FMOE_FASTER_GLBPLC_GPUTP=112e12
++ FMOE_FASTER_GLBPLC_GPUTP=112e12
++ export CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ export GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ export BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ export MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ export DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ export cluster_name=nico
++ cluster_name=nico
+ export NUM_EXPERTS=4
+ NUM_EXPERTS=4
+ mkdir -p ./logs
+ mkdir -p ./logs/fig10_2023-05-28T16:46:42+08:00
++ pwd
+ export LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00
++ date -Iseconds
+ LOG_PREFIX=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00
+ LOG_NAME=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.log
+ export PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ mkdir -p /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 2'
+ '[' 'nico[1-2]' '!=' None ']'
++ scontrol show hostnames 'nico[1-2]'
++ wc -l
+ tmp=2
+ '[' 2 '!=' 2 ']'
+ exec_args+=' -w nico[1-2]'
+ exec_args+=' -p AE'
+ LOG_FILE=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.log
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.log
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w nico[1-2] -p AE
+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/dense.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.log
#!/bin/bash

export NUM_LAYERS=16
export HIDDEN_SIZE=1536
export NUM_ATTN_HEADS=16
export SEQ_LEN=1024

export FMOE_FASTER_GLBPLC_ALPHA="2"
export FMOE_FASTER_GLBPLC_DMODEL=${HIDDEN_SIZE}

export dense_name=GPT-L${NUM_LAYERS}-H${HIDDEN_SIZE}-ATTN${NUM_ATTN_HEADS}-SEQ${SEQ_LEN}-ALPHA${FMOE_FASTER_GLBPLC_ALPHA}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/sparse.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.log
#!/bin/bash

export TOT_EXPERTS=32
export GATE="naive"
export GSHARD_CAP="4.8"

if [ ${GATE} == 'gshard' ];then
    GATE_NAME=${GATE}${GSHARD_CAP}
else
    GATE_NAME=${GATE}
fi
export sparse_name=${TOT_EXPERTS}MoE\_${GATE_NAME}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/parallel.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.log
#!/bin/bash

export TENSOR_PARALLEL_SIZE=1
export PIPELINE_PARALLEL_SIZE=2
export DATA_PARALLEL_SIZE=8
export EXPERT_EP_SIZE=8
export EXPERT_DP_SIZE=1
export GLOBAL_BATCH_SIZE=256
export MICRO_BATCH_SIZE=2
export FMOE_FASTER_SCHEDULE_ENABLE=OFF
export FMOE_FASTER_GROUP_SIZE=none
export FMOE_FASTER_SHADOW_ENABLE=OFF
export DYNAMIC_ENABLE=OFF
export DYNAMIC_FREQ=0
export ITERATION=20

export TRAIN_SAMPLES=$(( $GLOBAL_BATCH_SIZE * $ITERATION))

export parallel_name=t$TENSOR_PARALLEL_SIZE\_p$PIPELINE_PARALLEL_SIZE\_d$DATA_PARALLEL_SIZE\_ep${EXPERT_EP_SIZE}\_dp${EXPERT_DP_SIZE}\_gbs$GLOBAL_BATCH_SIZE\_mbs$MICRO_BATCH_SIZE\_smart\_${FMOE_FASTER_SCHEDULE_ENABLE}\_smartGP${FMOE_FASTER_GROUP_SIZE}\_shadow${FMOE_FASTER_SHADOW_ENABLE}\_dynamic${DYNAMIC_ENABLE}\_freq${DYNAMIC_FREQ}\_iter${ITERATION}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-alpa/cluster.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.log
#!/bin/bash

export NNODES=2
export NODELIST="nico[1-2]"
export FMOE_FASTER_GLBPLC_NETBW="8e9"
export FMOE_FASTER_GLBPLC_NETBW_Bcast="2e9"
export FMOE_FASTER_GLBPLC_GPUTP="112e12"

export CODE_PREFIX=$AEROOT/src/Megatron-LM

export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
export GPT_VOCAB_FILE=${DATASET_PREFIX}/gpt2-vocab.json
export BERT_VOCAB_FILE=${DATASET_PREFIX}/bert-large-uncased-vocab.txt
export MERGE_FILE=${DATASET_PREFIX}/gpt2-merges.txt
export DATA_PATH=${DATASET_PREFIX}/my-bert_text_sentence

export cluster_name=nico+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE ./pretrain_moe.sh
++ scontrol show JobId=164087
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=164087
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ scontrol show JobId=164087
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164087
++ scontrol show JobId=164087
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=164087
++ tr = ' '
++ scontrol show JobId=164087
++ grep BatchHost
++ grep BatchHost
++ scontrol show JobId=164087
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164087
++ scontrol show JobId=164087
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=164087
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=164087
++ scontrol show JobId=164087
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=164087
+ export MASTER_ADDR=nico1
++ awk '{print $2}'
++ scontrol show JobId=164087
++ grep BatchHost
+ MASTER_ADDR=nico1
++ awk '{print $2}'
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
++ tr = ' '
+ localrank=3
++ scontrol show JobId=164087
+ export CUDA_VISIBLE_DEVICES=3
++ awk '{print $2}'
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ grep BatchHost
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
++ grep BatchHost
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ tr = ' '
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ EXEC=./pretrain_gpt.py
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ RANK=8
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export RANK=10
+ RANK=10
+ export NODE_RANK=4
+ NODE_RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:51:32+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 2 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 8
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 8
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 4.8
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 4
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 2
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 5120
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 16
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 2
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.136 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in DP group [1]
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in DP group [3]
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in DP group [6]
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 8.239 seconds
time to initialize megatron (seconds): 35.240
[after megatron is initialized] datetime: 2023-05-28 16:51:47 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 455396608
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 456966400
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
[Warning] world comm group not exist!
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
[Warning] world comm group not exist!
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in DP group [5]
[Warning] world comm group not exist!
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in DP group [7]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
setting training iterations to 20
> learning rate decay style: cosine
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in DP group [4]
[Warning] world comm group not exist!
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in DP group [2]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 16:51:56 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      5120
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002700 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.007 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 16:52:07 
done with setup ...
training ...
[before the start of training step] datetime: 2023-05-28 16:52:07 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 8765.15625 | max allocated: 8765.15869140625 | reserved: 9996.0 | max reserved: 9996.0
[Rank 8] (after 1 iterations) memory (MB) | allocated: 8736.6171875 | max allocated: 8736.63232421875 | reserved: 9104.0 | max reserved: 9104.0
 iteration        1/      20 | consumed samples:          256 | elapsed time per iteration (ms): 8003.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082583E+01 | loss scale: 131072.0 | grad norm: 9.925 | num zeros: 23193.0 | params norm: 229.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/      20 | consumed samples:          512 | elapsed time per iteration (ms): 7994.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.049447E+01 | loss scale: 131072.0 | grad norm: 4.181 | num zeros: 360024736.0 | params norm: 229.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/      20 | consumed samples:          768 | elapsed time per iteration (ms): 8717.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.033622E+01 | loss scale: 131072.0 | grad norm: 3.968 | num zeros: 425754688.0 | params norm: 230.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/      20 | consumed samples:         1024 | elapsed time per iteration (ms): 8041.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.152106E+01 | loss scale: 131072.0 | grad norm: 3.966 | num zeros: 331578688.0 | params norm: 230.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/      20 | consumed samples:         1280 | elapsed time per iteration (ms): 8731.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.149083E+01 | loss scale: 131072.0 | grad norm: 4.015 | num zeros: 359952448.0 | params norm: 230.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/      20 | consumed samples:         1536 | elapsed time per iteration (ms): 8355.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.120859E+01 | loss scale: 131072.0 | grad norm: 4.924 | num zeros: 354254688.0 | params norm: 230.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/      20 | consumed samples:         1792 | elapsed time per iteration (ms): 8550.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.024639E+01 | loss scale: 131072.0 | grad norm: 3.996 | num zeros: 473439648.0 | params norm: 230.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/      20 | consumed samples:         2048 | elapsed time per iteration (ms): 8465.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.019177E+01 | loss scale: 131072.0 | grad norm: 3.939 | num zeros: 436517504.0 | params norm: 230.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/      20 | consumed samples:         2304 | elapsed time per iteration (ms): 8516.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.009933E+01 | loss scale: 131072.0 | grad norm: 3.929 | num zeros: 483988928.0 | params norm: 230.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/      20 | consumed samples:         2560 | elapsed time per iteration (ms): 8741.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.000500E+01 | loss scale: 131072.0 | grad norm: 3.937 | num zeros: 482870272.0 | params norm: 230.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/      20 | consumed samples:         2816 | elapsed time per iteration (ms): 8790.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.905646E+00 | loss scale: 131072.0 | grad norm: 3.932 | num zeros: 464329216.0 | params norm: 230.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/      20 | consumed samples:         3072 | elapsed time per iteration (ms): 9038.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.797013E+00 | loss scale: 131072.0 | grad norm: 3.938 | num zeros: 493271040.0 | params norm: 230.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/      20 | consumed samples:         3328 | elapsed time per iteration (ms): 9108.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.681733E+00 | loss scale: 131072.0 | grad norm: 3.899 | num zeros: 535860064.0 | params norm: 230.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/      20 | consumed samples:         3584 | elapsed time per iteration (ms): 8817.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.565618E+00 | loss scale: 131072.0 | grad norm: 3.912 | num zeros: 540157504.0 | params norm: 230.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/      20 | consumed samples:         3840 | elapsed time per iteration (ms): 8716.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.441973E+00 | loss scale: 131072.0 | grad norm: 3.920 | num zeros: 549368576.0 | params norm: 230.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/      20 | consumed samples:         4096 | elapsed time per iteration (ms): 8575.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.323105E+00 | loss scale: 131072.0 | grad norm: 3.891 | num zeros: 550185216.0 | params norm: 230.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/      20 | consumed samples:         4352 | elapsed time per iteration (ms): 8544.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.201234E+00 | loss scale: 131072.0 | grad norm: 3.897 | num zeros: 543042240.0 | params norm: 230.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/      20 | consumed samples:         4608 | elapsed time per iteration (ms): 8734.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.083077E+00 | loss scale: 131072.0 | grad norm: 3.907 | num zeros: 542273024.0 | params norm: 230.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/      20 | consumed samples:         4864 | elapsed time per iteration (ms): 8794.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.966802E+00 | loss scale: 131072.0 | grad norm: 3.900 | num zeros: 549525504.0 | params norm: 230.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/      20 | consumed samples:         5120 | elapsed time per iteration (ms): 8854.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.856143E+00 | loss scale: 131072.0 | grad norm: 3.873 | num zeros: 568515328.0 | params norm: 230.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-05-28 16:54:59 
+ '[' 2 -ne '2]'
./run_moe.sh: line 7: [: missing `]'
++ expr 10301 % 10000 + 10000
+ export MASTER_PORT=10301
+ MASTER_PORT=10301
+ export EXP_NAME=fig10_2023-05-28T16:46:42+08:00
+ EXP_NAME=fig10_2023-05-28T16:46:42+08:00
+ export DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/dense.sh
+ DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/dense.sh
+ export SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/sparse.sh
+ SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/sparse.sh
+ export PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/parallel.sh
+ PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/parallel.sh
+ export CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/cluster.sh
+ CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/cluster.sh
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/dense.sh
++ export NUM_LAYERS=16
++ NUM_LAYERS=16
++ export HIDDEN_SIZE=1536
++ HIDDEN_SIZE=1536
++ export NUM_ATTN_HEADS=16
++ NUM_ATTN_HEADS=16
++ export SEQ_LEN=1024
++ SEQ_LEN=1024
++ export FMOE_FASTER_GLBPLC_ALPHA=2
++ FMOE_FASTER_GLBPLC_ALPHA=2
++ export FMOE_FASTER_GLBPLC_DMODEL=1536
++ FMOE_FASTER_GLBPLC_DMODEL=1536
++ export dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
++ dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/sparse.sh
++ export TOT_EXPERTS=32
++ TOT_EXPERTS=32
++ export GATE=naive
++ GATE=naive
++ export GSHARD_CAP=4.8
++ GSHARD_CAP=4.8
++ '[' naive == gshard ']'
++ GATE_NAME=naive
++ export sparse_name=32MoE_naive
++ sparse_name=32MoE_naive
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/parallel.sh
++ export TENSOR_PARALLEL_SIZE=1
++ TENSOR_PARALLEL_SIZE=1
++ export PIPELINE_PARALLEL_SIZE=4
++ PIPELINE_PARALLEL_SIZE=4
++ export DATA_PARALLEL_SIZE=4
++ DATA_PARALLEL_SIZE=4
++ export EXPERT_EP_SIZE=4
++ EXPERT_EP_SIZE=4
++ export EXPERT_DP_SIZE=1
++ EXPERT_DP_SIZE=1
++ export GLOBAL_BATCH_SIZE=256
++ GLOBAL_BATCH_SIZE=256
++ export MICRO_BATCH_SIZE=2
++ MICRO_BATCH_SIZE=2
++ export FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ export FMOE_FASTER_GROUP_SIZE=none
++ FMOE_FASTER_GROUP_SIZE=none
++ export FMOE_FASTER_SHADOW_ENABLE=OFF
++ FMOE_FASTER_SHADOW_ENABLE=OFF
++ export DYNAMIC_ENABLE=OFF
++ DYNAMIC_ENABLE=OFF
++ export DYNAMIC_FREQ=0
++ DYNAMIC_FREQ=0
++ export ITERATION=20
++ ITERATION=20
++ export TRAIN_SAMPLES=5120
++ TRAIN_SAMPLES=5120
++ export parallel_name=t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
++ parallel_name=t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/cluster.sh
++ export NNODES=2
++ NNODES=2
++ export 'NODELIST=nico[1-2]'
++ NODELIST='nico[1-2]'
++ export FMOE_FASTER_GLBPLC_NETBW=8e9
++ FMOE_FASTER_GLBPLC_NETBW=8e9
++ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ export FMOE_FASTER_GLBPLC_GPUTP=112e12
++ FMOE_FASTER_GLBPLC_GPUTP=112e12
++ export CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ export GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ export BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ export MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ export DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ export cluster_name=nico
++ cluster_name=nico
+ export NUM_EXPERTS=8
+ NUM_EXPERTS=8
+ mkdir -p ./logs
+ mkdir -p ./logs/fig10_2023-05-28T16:46:42+08:00
++ pwd
+ export LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00
++ date -Iseconds
+ LOG_PREFIX=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00
+ LOG_NAME=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.log
+ export PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ mkdir -p /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 2'
+ '[' 'nico[1-2]' '!=' None ']'
++ scontrol show hostnames 'nico[1-2]'
++ wc -l
+ tmp=2
+ '[' 2 '!=' 2 ']'
+ exec_args+=' -w nico[1-2]'
+ exec_args+=' -p AE'
+ LOG_FILE=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.log
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.log
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w nico[1-2] -p AE
+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/dense.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.log
#!/bin/bash

export NUM_LAYERS=16
export HIDDEN_SIZE=1536
export NUM_ATTN_HEADS=16
export SEQ_LEN=1024

export FMOE_FASTER_GLBPLC_ALPHA="2"
export FMOE_FASTER_GLBPLC_DMODEL=${HIDDEN_SIZE}

export dense_name=GPT-L${NUM_LAYERS}-H${HIDDEN_SIZE}-ATTN${NUM_ATTN_HEADS}-SEQ${SEQ_LEN}-ALPHA${FMOE_FASTER_GLBPLC_ALPHA}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/sparse.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.log
#!/bin/bash

export TOT_EXPERTS=32
export GATE="naive"
export GSHARD_CAP="4.8"

if [ ${GATE} == 'gshard' ];then
    GATE_NAME=${GATE}${GSHARD_CAP}
else
    GATE_NAME=${GATE}
fi
export sparse_name=${TOT_EXPERTS}MoE\_${GATE_NAME}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/parallel.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.log
#!/bin/bash

export TENSOR_PARALLEL_SIZE=1
export PIPELINE_PARALLEL_SIZE=4
export DATA_PARALLEL_SIZE=4
export EXPERT_EP_SIZE=4
export EXPERT_DP_SIZE=1
export GLOBAL_BATCH_SIZE=256
export MICRO_BATCH_SIZE=2
export FMOE_FASTER_SCHEDULE_ENABLE=OFF
export FMOE_FASTER_GROUP_SIZE=none
export FMOE_FASTER_SHADOW_ENABLE=OFF
export DYNAMIC_ENABLE=OFF
export DYNAMIC_FREQ=0
export ITERATION=20

export TRAIN_SAMPLES=$(( $GLOBAL_BATCH_SIZE * $ITERATION))

export parallel_name=t$TENSOR_PARALLEL_SIZE\_p$PIPELINE_PARALLEL_SIZE\_d$DATA_PARALLEL_SIZE\_ep${EXPERT_EP_SIZE}\_dp${EXPERT_DP_SIZE}\_gbs$GLOBAL_BATCH_SIZE\_mbs$MICRO_BATCH_SIZE\_smart\_${FMOE_FASTER_SCHEDULE_ENABLE}\_smartGP${FMOE_FASTER_GROUP_SIZE}\_shadow${FMOE_FASTER_SHADOW_ENABLE}\_dynamic${DYNAMIC_ENABLE}\_freq${DYNAMIC_FREQ}\_iter${ITERATION}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10/config-smartmoe/cluster.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.log
#!/bin/bash

export NNODES=2
export NODELIST="nico[1-2]"
export FMOE_FASTER_GLBPLC_NETBW="8e9"
export FMOE_FASTER_GLBPLC_NETBW_Bcast="2e9"
export FMOE_FASTER_GLBPLC_GPUTP="112e12"

export CODE_PREFIX=$AEROOT/src/Megatron-LM

export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
export GPT_VOCAB_FILE=${DATASET_PREFIX}/gpt2-vocab.json
export BERT_VOCAB_FILE=${DATASET_PREFIX}/bert-large-uncased-vocab.txt
export MERGE_FILE=${DATASET_PREFIX}/gpt2-merges.txt
export DATA_PATH=${DATASET_PREFIX}/my-bert_text_sentence

export cluster_name=nico+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE ./pretrain_moe.sh
++ scontrol show JobId=164089
++ scontrol show JobId=164089
++ tr = ' '
++ scontrol show JobId=164089
++ tr = ' '
++ scontrol show JobId=164089
++ grep BatchHost
++ scontrol show JobId=164089
++ awk '{print $2}'
++ grep BatchHost
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164089
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=164089
++ scontrol show JobId=164089
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164089
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=164089
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=164089
++ scontrol show JobId=164089
++ grep BatchHost
++ scontrol show JobId=164089
++ scontrol show JobId=164089
++ scontrol show JobId=164089
++ grep BatchHost
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=164089
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export RANK=6
+ RANK=6
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ EXEC=./pretrain_gpt.py
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ export MASTER_ADDR=nico1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export RANK=15
+ RANK=15
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export MASTER_ADDR=nico1
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ MASTER_ADDR=nico1
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=5
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ python_args='
+ python_args='
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 8         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 4         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 5120 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 4         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig10_2023-05-28T16:46:42+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2_32MoE_naive_t1_p4_d4_ep4_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter20_nico_2023-05-28T16:55:04+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 8 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 5120 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 4 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 4 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 4
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 4
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 4.8
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 8
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 4
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 5120
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 32
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 4
[INFO] 0 in EP group [0, 1, 2, 3]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.139 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 6 in EP group [4, 5, 6, 7]
[INFO] 6 in DP group [6]
[INFO] 2 in EP group [0, 1, 2, 3]
[INFO] 2 in DP group [2]
[INFO] 7 in EP group [4, 5, 6, 7]
[INFO] 7 in DP group [7]
[INFO] 10 in EP group [8, 9, 10, 11]
[INFO] 10 in DP group [10]
[INFO] 8 in EP group [8, 9, 10, 11]
[INFO] 8 in DP group [8]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.934 seconds
time to initialize megatron (seconds): -9.804
[after megatron is initialized] datetime: 2023-05-28 16:55:18 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 340131968
[INFO] 12 in EP group [12, 13, 14, 15]
[INFO] 12 in DP group [12]
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 417401984
[INFO] 9 in EP group [8, 9, 10, 11]
[INFO] 9 in DP group [9]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 4 in EP group [4, 5, 6, 7]
[INFO] 4 in DP group [4]
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 340131968
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 11 in EP group [8, 9, 10, 11]
[INFO] 11 in DP group [11]
[Warning] world comm group not exist!
[INFO] 14 in EP group [12, 13, 14, 15]
[INFO] 14 in DP group [14]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 13 in EP group [12, 13, 14, 15]
[INFO] 13 in DP group [13]
[Warning] world comm group not exist!
[INFO] 15 in EP group [12, 13, 14, 15]
[INFO] 15 in DP group [15]
[Warning] world comm group not exist!
[INFO] 3 in EP group [0, 1, 2, 3]
[INFO] 3 in DP group [3]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 418971776
[INFO] 5 in EP group [4, 5, 6, 7]
[INFO] 5 in DP group [5]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 1 in EP group [0, 1, 2, 3]
[INFO] 1 in DP group [1]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
setting training iterations to 20
> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 16:55:26 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      5120
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.003293 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_5120ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 16:55:38 
done with setup ...
training ...
[before the start of training step] datetime: 2023-05-28 16:55:38 
 iteration        1/      20 | consumed samples:          256 | elapsed time per iteration (ms): 10137.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082761E+01 | loss scale: 131072.0 | grad norm: 9.246 | num zeros: 29720.0 | params norm: 232.670 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 4] (after 1 iterations) memory (MB) | allocated: 6520.94921875 | max allocated: 6520.95166015625 | reserved: 7446.0 | max reserved: 7446.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 8015.2119140625 | max allocated: 8015.21435546875 | reserved: 9624.0 | max reserved: 9624.0
[Rank 12] (after 1 iterations) memory (MB) | allocated: 7997.6416015625 | max allocated: 7997.67236328125 | reserved: 8132.0 | max reserved: 8132.0
[Rank 8] (after 1 iterations) memory (MB) | allocated: 6527.9990234375 | max allocated: 6528.00146484375 | reserved: 7150.0 | max reserved: 7150.0
 iteration        2/      20 | consumed samples:          512 | elapsed time per iteration (ms): 6282.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.050807E+01 | loss scale: 131072.0 | grad norm: 4.267 | num zeros: 560101312.0 | params norm: 232.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/      20 | consumed samples:          768 | elapsed time per iteration (ms): 6320.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.034092E+01 | loss scale: 131072.0 | grad norm: 3.963 | num zeros: 758054784.0 | params norm: 232.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/      20 | consumed samples:         1024 | elapsed time per iteration (ms): 6341.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.150541E+01 | loss scale: 131072.0 | grad norm: 3.988 | num zeros: 335702784.0 | params norm: 232.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/      20 | consumed samples:         1280 | elapsed time per iteration (ms): 6154.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.013802E+01 | loss scale: 131072.0 | grad norm: 3.963 | num zeros: 549849728.0 | params norm: 232.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/      20 | consumed samples:         1536 | elapsed time per iteration (ms): 6107.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.011904E+01 | loss scale: 131072.0 | grad norm: 14.470 | num zeros: 285871168.0 | params norm: 232.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/      20 | consumed samples:         1792 | elapsed time per iteration (ms): 6365.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.980408E+00 | loss scale: 131072.0 | grad norm: 3.954 | num zeros: 719548928.0 | params norm: 232.946 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/      20 | consumed samples:         2048 | elapsed time per iteration (ms): 6345.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.885728E+00 | loss scale: 131072.0 | grad norm: 3.907 | num zeros: 729066432.0 | params norm: 233.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/      20 | consumed samples:         2304 | elapsed time per iteration (ms): 6515.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.777500E+00 | loss scale: 131072.0 | grad norm: 7.489 | num zeros: 323214048.0 | params norm: 233.058 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/      20 | consumed samples:         2560 | elapsed time per iteration (ms): 6319.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.658298E+00 | loss scale: 131072.0 | grad norm: 3.924 | num zeros: 814720640.0 | params norm: 233.113 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/      20 | consumed samples:         2816 | elapsed time per iteration (ms): 6227.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.537406E+00 | loss scale: 131072.0 | grad norm: 3.923 | num zeros: 839686080.0 | params norm: 233.169 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/      20 | consumed samples:         3072 | elapsed time per iteration (ms): 6330.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.411601E+00 | loss scale: 131072.0 | grad norm: 3.922 | num zeros: 824483840.0 | params norm: 233.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/      20 | consumed samples:         3328 | elapsed time per iteration (ms): 6336.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.286789E+00 | loss scale: 131072.0 | grad norm: 3.883 | num zeros: 842734592.0 | params norm: 233.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/      20 | consumed samples:         3584 | elapsed time per iteration (ms): 6425.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.163301E+00 | loss scale: 131072.0 | grad norm: 3.900 | num zeros: 862311296.0 | params norm: 233.339 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/      20 | consumed samples:         3840 | elapsed time per iteration (ms): 6388.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.034797E+00 | loss scale: 131072.0 | grad norm: 3.902 | num zeros: 855207040.0 | params norm: 233.396 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/      20 | consumed samples:         4096 | elapsed time per iteration (ms): 6337.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.921313E+00 | loss scale: 131072.0 | grad norm: 3.854 | num zeros: 861505216.0 | params norm: 233.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/      20 | consumed samples:         4352 | elapsed time per iteration (ms): 6380.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.799259E+00 | loss scale: 131072.0 | grad norm: 3.863 | num zeros: 785502272.0 | params norm: 233.509 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/      20 | consumed samples:         4608 | elapsed time per iteration (ms): 6376.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.686315E+00 | loss scale: 131072.0 | grad norm: 3.859 | num zeros: 805182976.0 | params norm: 233.566 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/      20 | consumed samples:         4864 | elapsed time per iteration (ms): 6341.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.577295E+00 | loss scale: 131072.0 | grad norm: 3.838 | num zeros: 854470592.0 | params norm: 233.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/      20 | consumed samples:         5120 | elapsed time per iteration (ms): 6385.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.475640E+00 | loss scale: 131072.0 | grad norm: 3.824 | num zeros: 869723520.0 | params norm: 233.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-05-28 16:57:48 
/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig10
+ cd -
/home/atc23_ae/SmartMoE-AE
++ pwd
++ pwd
+ AEROOT=/home/atc23_ae/SmartMoE-AE
+ python3 ./plotting/from_exec/fig10.py /home/atc23_ae/SmartMoE-AE /home/atc23_ae/SmartMoE-AE/outputs_from_exec_2023-05-28T16:36:45+08:00
+ cd ./plotting/from_exec/fig12
+ ./fig12.sh
+ '[' 2 -ne '2]'
./run_moe.sh: line 7: [: missing `]'
++ expr 14950 % 10000 + 10000
+ export MASTER_PORT=14950
+ MASTER_PORT=14950
+ export EXP_NAME=fig12_2023-05-28T16:57:53+08:00
+ EXP_NAME=fig12_2023-05-28T16:57:53+08:00
+ export DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/dense.sh
+ DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/dense.sh
+ export SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/sparse.sh
+ SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/sparse.sh
+ export PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/parallel.sh
+ PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/parallel.sh
+ export CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/cluster.sh
+ CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/cluster.sh
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/dense.sh
++ export NUM_LAYERS=16
++ NUM_LAYERS=16
++ export HIDDEN_SIZE=1536
++ HIDDEN_SIZE=1536
++ export NUM_ATTN_HEADS=16
++ NUM_ATTN_HEADS=16
++ export SEQ_LEN=1024
++ SEQ_LEN=1024
++ export FMOE_FASTER_GLBPLC_ALPHA=2
++ FMOE_FASTER_GLBPLC_ALPHA=2
++ export FMOE_FASTER_GLBPLC_DMODEL=1536
++ FMOE_FASTER_GLBPLC_DMODEL=1536
++ export dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
++ dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/sparse.sh
++ export TOT_EXPERTS=32
++ TOT_EXPERTS=32
++ export GATE=naive
++ GATE=naive
++ export GSHARD_CAP=4.8
++ GSHARD_CAP=4.8
/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/sparse.sh: line 13: syntax error: unexpected end of file
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/parallel.sh
++ export TENSOR_PARALLEL_SIZE=1
++ TENSOR_PARALLEL_SIZE=1
++ export PIPELINE_PARALLEL_SIZE=2
++ PIPELINE_PARALLEL_SIZE=2
++ export DATA_PARALLEL_SIZE=8
++ DATA_PARALLEL_SIZE=8
++ export EXPERT_EP_SIZE=8
++ EXPERT_EP_SIZE=8
++ export EXPERT_DP_SIZE=1
++ EXPERT_DP_SIZE=1
++ export GLOBAL_BATCH_SIZE=256
++ GLOBAL_BATCH_SIZE=256
++ export MICRO_BATCH_SIZE=2
++ MICRO_BATCH_SIZE=2
++ export FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ export FMOE_FASTER_GROUP_SIZE=none
++ FMOE_FASTER_GROUP_SIZE=none
++ export FMOE_FASTER_SHADOW_ENABLE=OFF
++ FMOE_FASTER_SHADOW_ENABLE=OFF
++ export DYNAMIC_ENABLE=OFF
++ DYNAMIC_ENABLE=OFF
++ export DYNAMIC_FREQ=0
++ DYNAMIC_FREQ=0
++ export ITERATION=100
++ ITERATION=100
++ export TRAIN_SAMPLES=25600
++ TRAIN_SAMPLES=25600
++ export parallel_name=t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100
++ parallel_name=t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/cluster.sh
++ export NNODES=2
++ NNODES=2
++ export 'NODELIST=nico[1-2]'
++ NODELIST='nico[1-2]'
++ export FMOE_FASTER_GLBPLC_NETBW=8e9
++ FMOE_FASTER_GLBPLC_NETBW=8e9
++ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ export FMOE_FASTER_GLBPLC_GPUTP=112e12
++ FMOE_FASTER_GLBPLC_GPUTP=112e12
++ export CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ export GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ export BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ export MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ export DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ export cluster_name=nico
++ cluster_name=nico
+ export NUM_EXPERTS=4
+ NUM_EXPERTS=4
+ mkdir -p ./logs
+ mkdir -p ./logs/fig12_2023-05-28T16:57:53+08:00
++ pwd
+ export LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00
++ date -Iseconds
+ LOG_PREFIX=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00
+ LOG_NAME=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.log
+ export PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ mkdir -p /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 2'
+ '[' 'nico[1-2]' '!=' None ']'
++ scontrol show hostnames 'nico[1-2]'
++ wc -l
+ tmp=2
+ '[' 2 '!=' 2 ']'
+ exec_args+=' -w nico[1-2]'
+ exec_args+=' -p AE'
+ LOG_FILE=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.log
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.log
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w nico[1-2] -p AE
+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/dense.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.log
#!/bin/bash

export NUM_LAYERS=16
export HIDDEN_SIZE=1536
export NUM_ATTN_HEADS=16
export SEQ_LEN=1024

export FMOE_FASTER_GLBPLC_ALPHA="2"
export FMOE_FASTER_GLBPLC_DMODEL=${HIDDEN_SIZE}

export dense_name=GPT-L${NUM_LAYERS}-H${HIDDEN_SIZE}-ATTN${NUM_ATTN_HEADS}-SEQ${SEQ_LEN}-ALPHA${FMOE_FASTER_GLBPLC_ALPHA}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/sparse.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.log
#!/bin/bash

export TOT_EXPERTS=32
export GATE="naive"
export GSHARD_CAP="4.8"

if [ ${GATE} == 'gshard' ];then
    GATE_NAME=${GATE}${GSHARD_CAP}
else
    GATE_NAME=${GATE}

export sparse_name=${TOT_EXPERTS}MoE\_${GATE_NAME}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/parallel.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.log
#!/bin/bash

export TENSOR_PARALLEL_SIZE=1
export PIPELINE_PARALLEL_SIZE=2
export DATA_PARALLEL_SIZE=8
export EXPERT_EP_SIZE=8
export EXPERT_DP_SIZE=1
export GLOBAL_BATCH_SIZE=256
export MICRO_BATCH_SIZE=2
export FMOE_FASTER_SCHEDULE_ENABLE=OFF
export FMOE_FASTER_GROUP_SIZE=none
export FMOE_FASTER_SHADOW_ENABLE=OFF
export DYNAMIC_ENABLE=OFF
export DYNAMIC_FREQ=0
export ITERATION=100

export TRAIN_SAMPLES=$(( $GLOBAL_BATCH_SIZE * $ITERATION))

export parallel_name=t$TENSOR_PARALLEL_SIZE\_p$PIPELINE_PARALLEL_SIZE\_d$DATA_PARALLEL_SIZE\_ep${EXPERT_EP_SIZE}\_dp${EXPERT_DP_SIZE}\_gbs$GLOBAL_BATCH_SIZE\_mbs$MICRO_BATCH_SIZE\_smart\_${FMOE_FASTER_SCHEDULE_ENABLE}\_smartGP${FMOE_FASTER_GROUP_SIZE}\_shadow${FMOE_FASTER_SHADOW_ENABLE}\_dynamic${DYNAMIC_ENABLE}\_freq${DYNAMIC_FREQ}\_iter${ITERATION}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-static/cluster.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.log
#!/bin/bash

export NNODES=2
export NODELIST="nico[1-2]"
export FMOE_FASTER_GLBPLC_NETBW="8e9"
export FMOE_FASTER_GLBPLC_NETBW_Bcast="2e9"
export FMOE_FASTER_GLBPLC_GPUTP="112e12"

export CODE_PREFIX=$AEROOT/src/Megatron-LM

export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
export GPT_VOCAB_FILE=${DATASET_PREFIX}/gpt2-vocab.json
export BERT_VOCAB_FILE=${DATASET_PREFIX}/bert-large-uncased-vocab.txt
export MERGE_FILE=${DATASET_PREFIX}/gpt2-merges.txt
export DATA_PATH=${DATASET_PREFIX}/my-bert_text_sentence

export cluster_name=nico+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE ./pretrain_moe.sh
++ grep BatchHost
++ scontrol show JobId=164090
++ scontrol show JobId=164090
++ grep BatchHost
++ scontrol show JobId=164090
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164090
++ scontrol show JobId=164090
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ scontrol show JobId=164090
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=164090
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=164090
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export RANK=3
+ RANK=3
+ export NODE_RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164090
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=164090
++ grep BatchHost
++ scontrol show JobId=164090
++ scontrol show JobId=164090
++ scontrol show JobId=164090
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ scontrol show JobId=164090
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ tr = ' '
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
++ tr = ' '
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
++ awk '{print $2}'
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ awk '{print $2}'
+ python_args='
++ awk '{print $2}'
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ scontrol show JobId=164090
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=164090
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
++ grep BatchHost
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
++ tr = ' '
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
++ awk '{print $2}'
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ export NODE_RANK=6
+ NODE_RANK=6
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ export RANK=11
+ RANK=11
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' OFF == ON ']'
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicOFF_freq0_iter100_nico_2023-05-28T16:57:56+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
using world size: 16, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 2 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 8
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dynamic_freq .................................... 10
  dynamic_placement ............................... False
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 8
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 4.8
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 4
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 2
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 25600
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 16
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 2
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.134 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in DP group [1]
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in DP group [6]
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in DP group [4]
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.885 seconds
time to initialize megatron (seconds): 34.097
[after megatron is initialized] datetime: 2023-05-28 16:58:10 
building GPT model ...
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 455396608
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 456966400
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[Warning] world comm group not exist!
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
[Warning] world comm group not exist!
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
[Warning] world comm group not exist!
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in DP group [3]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
setting training iterations to 100
> learning rate decay style: cosine
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in DP group [2]
[Warning] world comm group not exist!
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in DP group [5]
[Warning] world comm group not exist!
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in DP group [7]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 16:58:19 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      25600
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002806 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_25600ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_25600ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_25600ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.063 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 16:58:30 
done with setup ...
training ...
[before the start of training step] datetime: 2023-05-28 16:58:30 
[Rank 0] (after 1 iterations) memory (MB) | allocated: 8765.15625 | max allocated: 8765.15869140625 | reserved: 9996.0 | max reserved: 9996.0
[Rank 8] (after 1 iterations) memory (MB) | allocated: 8736.6171875 | max allocated: 8736.63232421875 | reserved: 9104.0 | max reserved: 9104.0
 iteration        1/     100 | consumed samples:          256 | elapsed time per iteration (ms): 7952.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082583E+01 | loss scale: 131072.0 | grad norm: 9.925 | num zeros: 23212.0 | params norm: 229.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        2/     100 | consumed samples:          512 | elapsed time per iteration (ms): 8031.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.049446E+01 | loss scale: 131072.0 | grad norm: 4.181 | num zeros: 360025376.0 | params norm: 229.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     100 | consumed samples:          768 | elapsed time per iteration (ms): 8750.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.033622E+01 | loss scale: 131072.0 | grad norm: 3.968 | num zeros: 425756064.0 | params norm: 230.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     100 | consumed samples:         1024 | elapsed time per iteration (ms): 8076.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.152106E+01 | loss scale: 131072.0 | grad norm: 3.966 | num zeros: 331578048.0 | params norm: 230.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/     100 | consumed samples:         1280 | elapsed time per iteration (ms): 8756.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.149079E+01 | loss scale: 131072.0 | grad norm: 4.014 | num zeros: 360396480.0 | params norm: 230.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     100 | consumed samples:         1536 | elapsed time per iteration (ms): 8559.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.120846E+01 | loss scale: 131072.0 | grad norm: 4.930 | num zeros: 354246144.0 | params norm: 230.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     100 | consumed samples:         1792 | elapsed time per iteration (ms): 8554.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.024629E+01 | loss scale: 131072.0 | grad norm: 3.996 | num zeros: 473881184.0 | params norm: 230.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     100 | consumed samples:         2048 | elapsed time per iteration (ms): 8503.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.019170E+01 | loss scale: 131072.0 | grad norm: 3.939 | num zeros: 436527360.0 | params norm: 230.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     100 | consumed samples:         2304 | elapsed time per iteration (ms): 8594.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.009923E+01 | loss scale: 131072.0 | grad norm: 3.929 | num zeros: 483983808.0 | params norm: 230.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/     100 | consumed samples:         2560 | elapsed time per iteration (ms): 8717.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.000490E+01 | loss scale: 131072.0 | grad norm: 3.937 | num zeros: 482856576.0 | params norm: 230.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     100 | consumed samples:         2816 | elapsed time per iteration (ms): 8847.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.905527E+00 | loss scale: 131072.0 | grad norm: 3.932 | num zeros: 464346368.0 | params norm: 230.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     100 | consumed samples:         3072 | elapsed time per iteration (ms): 9043.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.796884E+00 | loss scale: 131072.0 | grad norm: 3.938 | num zeros: 501665984.0 | params norm: 230.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     100 | consumed samples:         3328 | elapsed time per iteration (ms): 9061.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.681600E+00 | loss scale: 131072.0 | grad norm: 3.899 | num zeros: 535847744.0 | params norm: 230.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     100 | consumed samples:         3584 | elapsed time per iteration (ms): 8828.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.565497E+00 | loss scale: 131072.0 | grad norm: 3.912 | num zeros: 540151168.0 | params norm: 230.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     100 | consumed samples:         3840 | elapsed time per iteration (ms): 8731.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.441820E+00 | loss scale: 131072.0 | grad norm: 3.920 | num zeros: 549315584.0 | params norm: 230.346 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     100 | consumed samples:         4096 | elapsed time per iteration (ms): 8613.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.322962E+00 | loss scale: 131072.0 | grad norm: 3.891 | num zeros: 550130176.0 | params norm: 230.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     100 | consumed samples:         4352 | elapsed time per iteration (ms): 8580.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.201107E+00 | loss scale: 131072.0 | grad norm: 3.897 | num zeros: 542352960.0 | params norm: 230.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     100 | consumed samples:         4608 | elapsed time per iteration (ms): 8777.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.082898E+00 | loss scale: 131072.0 | grad norm: 3.907 | num zeros: 542155840.0 | params norm: 230.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     100 | consumed samples:         4864 | elapsed time per iteration (ms): 8887.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.966631E+00 | loss scale: 131072.0 | grad norm: 3.900 | num zeros: 549543424.0 | params norm: 230.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 8828.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.856007E+00 | loss scale: 131072.0 | grad norm: 3.873 | num zeros: 568529984.0 | params norm: 230.512 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     100 | consumed samples:         5376 | elapsed time per iteration (ms): 8891.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.755418E+00 | loss scale: 131072.0 | grad norm: 3.859 | num zeros: 568692416.0 | params norm: 230.547 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     100 | consumed samples:         5632 | elapsed time per iteration (ms): 8832.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.651546E+00 | loss scale: 131072.0 | grad norm: 3.807 | num zeros: 560604608.0 | params norm: 230.583 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     100 | consumed samples:         5888 | elapsed time per iteration (ms): 8827.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.547131E+00 | loss scale: 131072.0 | grad norm: 3.836 | num zeros: 458082240.0 | params norm: 230.619 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     100 | consumed samples:         6144 | elapsed time per iteration (ms): 8823.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.453630E+00 | loss scale: 131072.0 | grad norm: 3.801 | num zeros: 577839104.0 | params norm: 230.656 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     100 | consumed samples:         6400 | elapsed time per iteration (ms): 8840.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.364460E+00 | loss scale: 131072.0 | grad norm: 3.775 | num zeros: 560537600.0 | params norm: 230.694 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     100 | consumed samples:         6656 | elapsed time per iteration (ms): 8832.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.280421E+00 | loss scale: 131072.0 | grad norm: 3.726 | num zeros: 564668672.0 | params norm: 230.732 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     100 | consumed samples:         6912 | elapsed time per iteration (ms): 8995.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.197970E+00 | loss scale: 131072.0 | grad norm: 3.702 | num zeros: 541214208.0 | params norm: 230.770 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     100 | consumed samples:         7168 | elapsed time per iteration (ms): 8863.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.120107E+00 | loss scale: 131072.0 | grad norm: 3.657 | num zeros: 559343680.0 | params norm: 230.809 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     100 | consumed samples:         7424 | elapsed time per iteration (ms): 8845.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.043417E+00 | loss scale: 131072.0 | grad norm: 3.606 | num zeros: 550373760.0 | params norm: 230.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/     100 | consumed samples:         7680 | elapsed time per iteration (ms): 8819.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.964484E+00 | loss scale: 131072.0 | grad norm: 3.550 | num zeros: 549348352.0 | params norm: 230.885 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     100 | consumed samples:         7936 | elapsed time per iteration (ms): 8894.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.901426E+00 | loss scale: 131072.0 | grad norm: 3.457 | num zeros: 559531648.0 | params norm: 230.923 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     100 | consumed samples:         8192 | elapsed time per iteration (ms): 8826.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.824580E+00 | loss scale: 131072.0 | grad norm: 3.381 | num zeros: 541310464.0 | params norm: 230.961 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     100 | consumed samples:         8448 | elapsed time per iteration (ms): 8820.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.769650E+00 | loss scale: 131072.0 | grad norm: 3.248 | num zeros: 567998976.0 | params norm: 231.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     100 | consumed samples:         8704 | elapsed time per iteration (ms): 8834.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.705560E+00 | loss scale: 131072.0 | grad norm: 3.153 | num zeros: 578100544.0 | params norm: 231.038 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     100 | consumed samples:         8960 | elapsed time per iteration (ms): 8825.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.644735E+00 | loss scale: 131072.0 | grad norm: 2.982 | num zeros: 559365312.0 | params norm: 231.076 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     100 | consumed samples:         9216 | elapsed time per iteration (ms): 8828.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.588873E+00 | loss scale: 131072.0 | grad norm: 2.866 | num zeros: 473818624.0 | params norm: 231.115 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     100 | consumed samples:         9472 | elapsed time per iteration (ms): 8833.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.547755E+00 | loss scale: 131072.0 | grad norm: 2.642 | num zeros: 543386240.0 | params norm: 231.152 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     100 | consumed samples:         9728 | elapsed time per iteration (ms): 8897.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.505156E+00 | loss scale: 131072.0 | grad norm: 2.420 | num zeros: 558420672.0 | params norm: 231.190 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     100 | consumed samples:         9984 | elapsed time per iteration (ms): 8820.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.482578E+00 | loss scale: 131072.0 | grad norm: 2.161 | num zeros: 531551296.0 | params norm: 231.227 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 8849.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.430230E+00 | loss scale: 131072.0 | grad norm: 1.926 | num zeros: 563735872.0 | params norm: 231.264 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     100 | consumed samples:        10496 | elapsed time per iteration (ms): 8838.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.404614E+00 | loss scale: 131072.0 | grad norm: 1.614 | num zeros: 551680896.0 | params norm: 231.302 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     100 | consumed samples:        10752 | elapsed time per iteration (ms): 8821.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.405992E+00 | loss scale: 131072.0 | grad norm: 1.346 | num zeros: 421446528.0 | params norm: 231.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     100 | consumed samples:        11008 | elapsed time per iteration (ms): 8797.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.369232E+00 | loss scale: 131072.0 | grad norm: 1.177 | num zeros: 502218688.0 | params norm: 231.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     100 | consumed samples:        11264 | elapsed time per iteration (ms): 8649.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.351146E+00 | loss scale: 131072.0 | grad norm: 0.899 | num zeros: 401534080.0 | params norm: 231.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     100 | consumed samples:        11520 | elapsed time per iteration (ms): 8345.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.332571E+00 | loss scale: 131072.0 | grad norm: 0.630 | num zeros: 352266528.0 | params norm: 231.456 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     100 | consumed samples:        11776 | elapsed time per iteration (ms): 8275.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.311354E+00 | loss scale: 131072.0 | grad norm: 0.571 | num zeros: 297923008.0 | params norm: 231.496 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     100 | consumed samples:        12032 | elapsed time per iteration (ms): 8297.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.299420E+00 | loss scale: 131072.0 | grad norm: 0.638 | num zeros: 240683008.0 | params norm: 231.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     100 | consumed samples:        12288 | elapsed time per iteration (ms): 8218.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.292116E+00 | loss scale: 131072.0 | grad norm: 0.603 | num zeros: 361816384.0 | params norm: 231.579 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     100 | consumed samples:        12544 | elapsed time per iteration (ms): 8042.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.282354E+00 | loss scale: 131072.0 | grad norm: 0.447 | num zeros: 371221760.0 | params norm: 231.621 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       50/     100 | consumed samples:        12800 | elapsed time per iteration (ms): 7905.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.265786E+00 | loss scale: 131072.0 | grad norm: 0.550 | num zeros: 351044416.0 | params norm: 231.664 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     100 | consumed samples:        13056 | elapsed time per iteration (ms): 8240.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.256692E+00 | loss scale: 131072.0 | grad norm: 0.453 | num zeros: 366509568.0 | params norm: 231.708 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     100 | consumed samples:        13312 | elapsed time per iteration (ms): 8240.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.238914E+00 | loss scale: 131072.0 | grad norm: 0.448 | num zeros: 347339072.0 | params norm: 231.750 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     100 | consumed samples:        13568 | elapsed time per iteration (ms): 7985.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.244854E+00 | loss scale: 131072.0 | grad norm: 0.432 | num zeros: 364933696.0 | params norm: 231.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     100 | consumed samples:        13824 | elapsed time per iteration (ms): 7868.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.221144E+00 | loss scale: 131072.0 | grad norm: 0.395 | num zeros: 375019584.0 | params norm: 231.836 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     100 | consumed samples:        14080 | elapsed time per iteration (ms): 7773.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.215559E+00 | loss scale: 131072.0 | grad norm: 0.397 | num zeros: 375137120.0 | params norm: 231.880 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     100 | consumed samples:        14336 | elapsed time per iteration (ms): 7790.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.226683E+00 | loss scale: 131072.0 | grad norm: 0.318 | num zeros: 245628416.0 | params norm: 231.924 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     100 | consumed samples:        14592 | elapsed time per iteration (ms): 7913.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.196174E+00 | loss scale: 131072.0 | grad norm: 0.351 | num zeros: 360505152.0 | params norm: 231.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     100 | consumed samples:        14848 | elapsed time per iteration (ms): 7859.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.186224E+00 | loss scale: 131072.0 | grad norm: 0.356 | num zeros: 378787264.0 | params norm: 232.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     100 | consumed samples:        15104 | elapsed time per iteration (ms): 7853.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.176517E+00 | loss scale: 131072.0 | grad norm: 0.265 | num zeros: 351964992.0 | params norm: 232.054 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       60/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 7879.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.168419E+00 | loss scale: 131072.0 | grad norm: 0.244 | num zeros: 349988480.0 | params norm: 232.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     100 | consumed samples:        15616 | elapsed time per iteration (ms): 7991.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.174234E+00 | loss scale: 131072.0 | grad norm: 0.231 | num zeros: 341017728.0 | params norm: 232.139 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     100 | consumed samples:        15872 | elapsed time per iteration (ms): 8007.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.159466E+00 | loss scale: 131072.0 | grad norm: 0.223 | num zeros: 211033408.0 | params norm: 232.182 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     100 | consumed samples:        16128 | elapsed time per iteration (ms): 7943.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.138670E+00 | loss scale: 131072.0 | grad norm: 0.217 | num zeros: 322241792.0 | params norm: 232.223 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     100 | consumed samples:        16384 | elapsed time per iteration (ms): 7852.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.137176E+00 | loss scale: 131072.0 | grad norm: 0.207 | num zeros: 331054464.0 | params norm: 232.264 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     100 | consumed samples:        16640 | elapsed time per iteration (ms): 7783.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.140900E+00 | loss scale: 131072.0 | grad norm: 0.233 | num zeros: 321740864.0 | params norm: 232.303 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     100 | consumed samples:        16896 | elapsed time per iteration (ms): 7733.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.119604E+00 | loss scale: 131072.0 | grad norm: 0.199 | num zeros: 313660800.0 | params norm: 232.341 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     100 | consumed samples:        17152 | elapsed time per iteration (ms): 7922.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.120054E+00 | loss scale: 131072.0 | grad norm: 0.232 | num zeros: 341704992.0 | params norm: 232.379 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     100 | consumed samples:        17408 | elapsed time per iteration (ms): 7769.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.112214E+00 | loss scale: 131072.0 | grad norm: 0.360 | num zeros: 350592960.0 | params norm: 232.417 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     100 | consumed samples:        17664 | elapsed time per iteration (ms): 7566.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.126481E+00 | loss scale: 131072.0 | grad norm: 1.002 | num zeros: 350438048.0 | params norm: 232.453 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       70/     100 | consumed samples:        17920 | elapsed time per iteration (ms): 7303.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.119424E+00 | loss scale: 131072.0 | grad norm: 0.823 | num zeros: 331498176.0 | params norm: 232.487 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     100 | consumed samples:        18176 | elapsed time per iteration (ms): 7531.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.095592E+00 | loss scale: 131072.0 | grad norm: 0.523 | num zeros: 339630912.0 | params norm: 232.521 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     100 | consumed samples:        18432 | elapsed time per iteration (ms): 7737.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.104178E+00 | loss scale: 131072.0 | grad norm: 0.423 | num zeros: 378301120.0 | params norm: 232.554 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     100 | consumed samples:        18688 | elapsed time per iteration (ms): 7800.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.108442E+00 | loss scale: 131072.0 | grad norm: 0.694 | num zeros: 369437728.0 | params norm: 232.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     100 | consumed samples:        18944 | elapsed time per iteration (ms): 7867.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.084032E+00 | loss scale: 131072.0 | grad norm: 0.531 | num zeros: 379526336.0 | params norm: 232.616 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     100 | consumed samples:        19200 | elapsed time per iteration (ms): 7856.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.082423E+00 | loss scale: 131072.0 | grad norm: 0.498 | num zeros: 368941440.0 | params norm: 232.645 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     100 | consumed samples:        19456 | elapsed time per iteration (ms): 7661.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.070970E+00 | loss scale: 131072.0 | grad norm: 0.757 | num zeros: 322738560.0 | params norm: 232.675 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     100 | consumed samples:        19712 | elapsed time per iteration (ms): 7747.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.082270E+00 | loss scale: 131072.0 | grad norm: 0.487 | num zeros: 322762720.0 | params norm: 232.706 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     100 | consumed samples:        19968 | elapsed time per iteration (ms): 7794.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.071893E+00 | loss scale: 131072.0 | grad norm: 1.073 | num zeros: 347623744.0 | params norm: 232.735 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     100 | consumed samples:        20224 | elapsed time per iteration (ms): 7852.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.054166E+00 | loss scale: 131072.0 | grad norm: 0.419 | num zeros: 350454464.0 | params norm: 232.764 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       80/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 7941.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.064458E+00 | loss scale: 131072.0 | grad norm: 0.646 | num zeros: 308526912.0 | params norm: 232.793 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     100 | consumed samples:        20736 | elapsed time per iteration (ms): 7963.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.050784E+00 | loss scale: 131072.0 | grad norm: 0.491 | num zeros: 317969088.0 | params norm: 232.821 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     100 | consumed samples:        20992 | elapsed time per iteration (ms): 8050.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.032483E+00 | loss scale: 131072.0 | grad norm: 0.765 | num zeros: 297021792.0 | params norm: 232.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     100 | consumed samples:        21248 | elapsed time per iteration (ms): 8016.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.048884E+00 | loss scale: 131072.0 | grad norm: 0.396 | num zeros: 304805568.0 | params norm: 232.874 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     100 | consumed samples:        21504 | elapsed time per iteration (ms): 7875.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.016764E+00 | loss scale: 131072.0 | grad norm: 0.639 | num zeros: 308329664.0 | params norm: 232.902 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     100 | consumed samples:        21760 | elapsed time per iteration (ms): 7848.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.013573E+00 | loss scale: 131072.0 | grad norm: 0.896 | num zeros: 308017472.0 | params norm: 232.929 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     100 | consumed samples:        22016 | elapsed time per iteration (ms): 7840.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.026202E+00 | loss scale: 131072.0 | grad norm: 0.484 | num zeros: 306679904.0 | params norm: 232.956 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     100 | consumed samples:        22272 | elapsed time per iteration (ms): 7958.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.009840E+00 | loss scale: 131072.0 | grad norm: 0.457 | num zeros: 300505344.0 | params norm: 232.984 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     100 | consumed samples:        22528 | elapsed time per iteration (ms): 7958.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.004419E+00 | loss scale: 131072.0 | grad norm: 0.422 | num zeros: 308714176.0 | params norm: 233.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     100 | consumed samples:        22784 | elapsed time per iteration (ms): 7968.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.996745E+00 | loss scale: 131072.0 | grad norm: 0.353 | num zeros: 282110592.0 | params norm: 233.038 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       90/     100 | consumed samples:        23040 | elapsed time per iteration (ms): 7863.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 6.996673E+00 | loss scale: 131072.0 | grad norm: 0.461 | num zeros: 304300064.0 | params norm: 233.065 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     100 | consumed samples:        23296 | elapsed time per iteration (ms): 7871.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.978046E+00 | loss scale: 131072.0 | grad norm: 0.314 | num zeros: 317703648.0 | params norm: 233.091 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     100 | consumed samples:        23552 | elapsed time per iteration (ms): 7883.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.971939E+00 | loss scale: 131072.0 | grad norm: 0.362 | num zeros: 320452224.0 | params norm: 233.117 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     100 | consumed samples:        23808 | elapsed time per iteration (ms): 7962.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.958370E+00 | loss scale: 131072.0 | grad norm: 0.326 | num zeros: 291067264.0 | params norm: 233.145 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     100 | consumed samples:        24064 | elapsed time per iteration (ms): 7889.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.952462E+00 | loss scale: 131072.0 | grad norm: 0.446 | num zeros: 299879584.0 | params norm: 233.172 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     100 | consumed samples:        24320 | elapsed time per iteration (ms): 7933.4 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.956160E+00 | loss scale: 131072.0 | grad norm: 0.617 | num zeros: 271704640.0 | params norm: 233.199 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     100 | consumed samples:        24576 | elapsed time per iteration (ms): 7952.2 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.952338E+00 | loss scale: 131072.0 | grad norm: 0.648 | num zeros: 266971232.0 | params norm: 233.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     100 | consumed samples:        24832 | elapsed time per iteration (ms): 8011.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.948689E+00 | loss scale: 131072.0 | grad norm: 0.396 | num zeros: 255031616.0 | params norm: 233.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     100 | consumed samples:        25088 | elapsed time per iteration (ms): 7929.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.930548E+00 | loss scale: 131072.0 | grad norm: 0.637 | num zeros: 261776192.0 | params norm: 233.280 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     100 | consumed samples:        25344 | elapsed time per iteration (ms): 8095.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.921592E+00 | loss scale: 131072.0 | grad norm: 0.436 | num zeros: 252953344.0 | params norm: 233.307 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration      100/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 8087.0 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.931823E+00 | loss scale: 131072.0 | grad norm: 0.625 | num zeros: 269002496.0 | params norm: 233.334 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-05-28 17:12:18 
+ '[' 2 -ne '2]'
./run_moe.sh: line 7: [: missing `]'
++ expr 30560 % 10000 + 10000
+ export MASTER_PORT=10560
+ MASTER_PORT=10560
+ export EXP_NAME=fig12_2023-05-28T16:57:53+08:00
+ EXP_NAME=fig12_2023-05-28T16:57:53+08:00
+ export DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/dense.sh
+ DENSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/dense.sh
+ export SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/sparse.sh
+ SPARSE_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/sparse.sh
+ export PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/parallel.sh
+ PARALLEL_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/parallel.sh
+ export CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/cluster.sh
+ CLUSTER_CONFIG=/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/cluster.sh
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/dense.sh
++ export NUM_LAYERS=16
++ NUM_LAYERS=16
++ export HIDDEN_SIZE=1536
++ HIDDEN_SIZE=1536
++ export NUM_ATTN_HEADS=16
++ NUM_ATTN_HEADS=16
++ export SEQ_LEN=1024
++ SEQ_LEN=1024
++ export FMOE_FASTER_GLBPLC_ALPHA=2
++ FMOE_FASTER_GLBPLC_ALPHA=2
++ export FMOE_FASTER_GLBPLC_DMODEL=1536
++ FMOE_FASTER_GLBPLC_DMODEL=1536
++ export dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
++ dense_name=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/sparse.sh
++ export TOT_EXPERTS=32
++ TOT_EXPERTS=32
++ export GATE=naive
++ GATE=naive
++ export GSHARD_CAP=4.8
++ GSHARD_CAP=4.8
/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/sparse.sh: line 13: syntax error: unexpected end of file
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/parallel.sh
++ export TENSOR_PARALLEL_SIZE=1
++ TENSOR_PARALLEL_SIZE=1
++ export PIPELINE_PARALLEL_SIZE=2
++ PIPELINE_PARALLEL_SIZE=2
++ export DATA_PARALLEL_SIZE=8
++ DATA_PARALLEL_SIZE=8
++ export EXPERT_EP_SIZE=8
++ EXPERT_EP_SIZE=8
++ export EXPERT_DP_SIZE=1
++ EXPERT_DP_SIZE=1
++ export GLOBAL_BATCH_SIZE=256
++ GLOBAL_BATCH_SIZE=256
++ export MICRO_BATCH_SIZE=2
++ MICRO_BATCH_SIZE=2
++ export FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ FMOE_FASTER_SCHEDULE_ENABLE=OFF
++ export FMOE_FASTER_GROUP_SIZE=none
++ FMOE_FASTER_GROUP_SIZE=none
++ export FMOE_FASTER_SHADOW_ENABLE=OFF
++ FMOE_FASTER_SHADOW_ENABLE=OFF
++ export DYNAMIC_ENABLE=ON
++ DYNAMIC_ENABLE=ON
++ export DYNAMIC_FREQ=10
++ DYNAMIC_FREQ=10
++ export ITERATION=100
++ ITERATION=100
++ export TRAIN_SAMPLES=25600
++ TRAIN_SAMPLES=25600
++ export parallel_name=t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100
++ parallel_name=t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100
+ . /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/cluster.sh
++ export NNODES=2
++ NNODES=2
++ export 'NODELIST=nico[1-2]'
++ NODELIST='nico[1-2]'
++ export FMOE_FASTER_GLBPLC_NETBW=8e9
++ FMOE_FASTER_GLBPLC_NETBW=8e9
++ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
++ export FMOE_FASTER_GLBPLC_GPUTP=112e12
++ FMOE_FASTER_GLBPLC_GPUTP=112e12
++ export CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ CODE_PREFIX=/home/atc23_ae/SmartMoE-AE/src/Megatron-LM
++ export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
++ export GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ GPT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
++ export BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ BERT_VOCAB_FILE=/mnt/znvme/zms/fastmoe-dataset/bert-large-uncased-vocab.txt
++ export MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ MERGE_FILE=/mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
++ export DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ DATA_PATH=/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence
++ export cluster_name=nico
++ cluster_name=nico
+ export NUM_EXPERTS=4
+ NUM_EXPERTS=4
+ mkdir -p ./logs
+ mkdir -p ./logs/fig12_2023-05-28T16:57:53+08:00
++ pwd
+ export LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00
++ date -Iseconds
+ LOG_PREFIX=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00
+ LOG_NAME=GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.log
+ export PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ mkdir -p /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 2'
+ '[' 'nico[1-2]' '!=' None ']'
++ scontrol show hostnames 'nico[1-2]'
++ wc -l
+ tmp=2
+ '[' 2 '!=' 2 ']'
+ exec_args+=' -w nico[1-2]'
+ exec_args+=' -p AE'
+ LOG_FILE=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.log
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.log
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w nico[1-2] -p AE
+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/dense.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.log
#!/bin/bash

export NUM_LAYERS=16
export HIDDEN_SIZE=1536
export NUM_ATTN_HEADS=16
export SEQ_LEN=1024

export FMOE_FASTER_GLBPLC_ALPHA="2"
export FMOE_FASTER_GLBPLC_DMODEL=${HIDDEN_SIZE}

export dense_name=GPT-L${NUM_LAYERS}-H${HIDDEN_SIZE}-ATTN${NUM_ATTN_HEADS}-SEQ${SEQ_LEN}-ALPHA${FMOE_FASTER_GLBPLC_ALPHA}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/sparse.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.log
#!/bin/bash

export TOT_EXPERTS=32
export GATE="naive"
export GSHARD_CAP="4.8"

if [ ${GATE} == 'gshard' ];then
    GATE_NAME=${GATE}${GSHARD_CAP}
else
    GATE_NAME=${GATE}

export sparse_name=${TOT_EXPERTS}MoE\_${GATE_NAME}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/parallel.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.log
#!/bin/bash

export TENSOR_PARALLEL_SIZE=1
export PIPELINE_PARALLEL_SIZE=2
export DATA_PARALLEL_SIZE=8
export EXPERT_EP_SIZE=8
export EXPERT_DP_SIZE=1
export GLOBAL_BATCH_SIZE=256
export MICRO_BATCH_SIZE=2
export FMOE_FASTER_SCHEDULE_ENABLE=OFF
export FMOE_FASTER_GROUP_SIZE=none
export FMOE_FASTER_SHADOW_ENABLE=OFF
export DYNAMIC_ENABLE=ON
export DYNAMIC_FREQ=10
export ITERATION=100

export TRAIN_SAMPLES=$(( $GLOBAL_BATCH_SIZE * $ITERATION))

export parallel_name=t$TENSOR_PARALLEL_SIZE\_p$PIPELINE_PARALLEL_SIZE\_d$DATA_PARALLEL_SIZE\_ep${EXPERT_EP_SIZE}\_dp${EXPERT_DP_SIZE}\_gbs$GLOBAL_BATCH_SIZE\_mbs$MICRO_BATCH_SIZE\_smart\_${FMOE_FASTER_SCHEDULE_ENABLE}\_smartGP${FMOE_FASTER_GROUP_SIZE}\_shadow${FMOE_FASTER_SHADOW_ENABLE}\_dynamic${DYNAMIC_ENABLE}\_freq${DYNAMIC_FREQ}\_iter${ITERATION}+ cat /home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12/config-dynamic/cluster.sh
+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.log
#!/bin/bash

export NNODES=2
export NODELIST="nico[1-2]"
export FMOE_FASTER_GLBPLC_NETBW="8e9"
export FMOE_FASTER_GLBPLC_NETBW_Bcast="2e9"
export FMOE_FASTER_GLBPLC_GPUTP="112e12"

export CODE_PREFIX=$AEROOT/src/Megatron-LM

export DATASET_PREFIX=/mnt/znvme/zms/fastmoe-dataset
export GPT_VOCAB_FILE=${DATASET_PREFIX}/gpt2-vocab.json
export BERT_VOCAB_FILE=${DATASET_PREFIX}/bert-large-uncased-vocab.txt
export MERGE_FILE=${DATASET_PREFIX}/gpt2-merges.txt
export DATA_PATH=${DATASET_PREFIX}/my-bert_text_sentence

export cluster_name=nico+ tee -a /home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 2 -w 'nico[1-2]' -p AE ./pretrain_moe.sh
++ scontrol show JobId=164092
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ scontrol show JobId=164092
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ awk '{print $2}'
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ scontrol show JobId=164092
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ scontrol show JobId=164092
++ tr = ' '
++ scontrol show JobId=164092
++ scontrol show JobId=164092
++ grep BatchHost
++ grep BatchHost
++ awk '{print $2}'
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ scontrol show JobId=164092
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=164092
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=11
+ RANK=11
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=10
+ RANK=10
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=5
+ NODE_RANK=5
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ python_args='
+ EXEC=./pretrain_gpt.py
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=13
+ RANK=13
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=8
+ RANK=8
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export MASTER_ADDR=nico1
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ MASTER_ADDR=nico1
+ export RANK=14
+ RANK=14
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=7
+ NODE_RANK=7
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=9
+ RANK=9
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=4
+ NODE_RANK=4
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=12
+ RANK=12
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=15
+ RANK=15
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=7
+ NODE_RANK=7
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=6
+ NODE_RANK=6
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ USE_MEGATRON=1
+ export WORLD_SIZE=16
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ WORLD_SIZE=16
+ localrank=7
+ export CUDA_VISIBLE_DEVICES=7
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ CUDA_VISIBLE_DEVICES=7
+ export NODE_RANK=3
+ NODE_RANK=3
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ export MASTER_ADDR=nico1
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ MASTER_ADDR=nico1
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ export NODE_RANK=1
+ NODE_RANK=1
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
+ export MASTER_ADDR=nico1
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ MASTER_ADDR=nico1
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export NODE_RANK=2
+ NODE_RANK=2
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=16
+ WORLD_SIZE=16
+ localrank=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ export NODE_RANK=0
+ NODE_RANK=0
+ export MAX_JOBS=64
+ MAX_JOBS=64
+ cd /home/atc23_ae/SmartMoE-AE/src/Megatron-LM
+ python_args='
        --fmoefy         --num-experts 4         --balance-strategy naive         --gshard-cap 4.8         --tensor-model-parallel-size 1         --pipeline-model-parallel-size 2         --num-layers 16         --hidden-size 1536         --num-attention-heads 16         --seq-length 1024         --max-position-embeddings 1024         --micro-batch-size 2         --global-batch-size 256         --train-samples 25600 		--lr-decay-samples 4882800         --lr 0.0001         --min-lr 0.00001         --lr-decay-style cosine         --initial-loss-scale 131072         --log-interval 1         --eval-iters -1         --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence         --split 100,0,0         --clip-grad 1.0         --weight-decay 0.01         --adam-beta1 0.9         --adam-beta2 0.95         --init-method-std 0.002         --fp16         --DDP-impl local         --log-num-zeros-in-grad         --log-params-norm         --expert-ep-size 8         --expert-dp-size 1         --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json         --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt '
+ '[' ON == ON ']'
+ python_args+='  --dynamic-placement                     --dynamic-freq 10 '
+ EXEC=./pretrain_gpt.py
+ echo ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
+ USE_MEGATRON=1
+ PROFILER_LOG_PATH=/home/atc23_ae/SmartMoE-AE/scripts/GPT/logs/fig12_2023-05-28T16:57:53+08:00/GPT-L16-H1536-ATTN16-SEQ1024-ALPHA2__t1_p2_d8_ep8_dp1_gbs256_mbs2_smart_OFF_smartGPnone_shadowOFF_dynamicON_freq10_iter100_nico_2023-05-28T17:12:23+08:00.prof
+ exec python3 ./pretrain_gpt.py --fmoefy --num-experts 4 --balance-strategy naive --gshard-cap 4.8 --tensor-model-parallel-size 1 --pipeline-model-parallel-size 2 --num-layers 16 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 2 --global-batch-size 256 --train-samples 25600 --lr-decay-samples 4882800 --lr 0.0001 --min-lr 0.00001 --lr-decay-style cosine --initial-loss-scale 131072 --log-interval 1 --eval-iters -1 --data-path /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence --split 100,0,0 --clip-grad 1.0 --weight-decay 0.01 --adam-beta1 0.9 --adam-beta2 0.95 --init-method-std 0.002 --fp16 --DDP-impl local --log-num-zeros-in-grad --log-params-norm --expert-ep-size 8 --expert-dp-size 1 --vocab-file /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json --merge-file /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt --dynamic-placement --dynamic-freq 10
using world size: 16, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 2 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  balance_loss_weight ............................. 1
  balance_strategy ................................ naive
  bert_binary_head ................................ True
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... infer
  data_parallel_size .............................. 8
  data_path ....................................... ['/mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence']
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  dynamic_freq .................................... 10
  dynamic_placement ............................... True
  embedding_path .................................. None
  encoder_seq_length .............................. 1024
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... -1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  expert_dp_size .................................. 1
  expert_ep_size .................................. 8
  ffn_hidden_size ................................. 6144
  finetune ........................................ False
  fmoefy .......................................... True
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 256
  gshard_cap ...................................... 4.8
  hidden_dropout .................................. 0.1
  hidden_hidden_size .............................. None
  hidden_size ..................................... 1536
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.002
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 131072.0
  kv_channels ..................................... 96
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... True
  log_params_norm ................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 4882800
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 1024
  merge_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-merges.txt
  micro_batch_size ................................ 2
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... 4
  num_layers ...................................... 16
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 2
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... None
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 1024
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 100,0,0
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. GPT2BPETokenizer
  top_k ........................................... 2
  train_iters ..................................... None
  train_samples ................................... 25600
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /mnt/znvme/zms/fastmoe-dataset/gpt2-vocab.json
  weight_decay .................................... 0.01
  world_size ...................................... 16
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 16
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 2
[INFO] 0 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 0 in DP group [0]
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.139 seconds
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/atc23_ae/SmartMoE-AE/src/Megatron-LM/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Using envvar MAX_JOBS (64) as the number of workers...
ninja: no work to do.
[INFO] 4 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 4 in DP group [4]
[INFO] 5 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 5 in DP group [5]
[INFO] 1 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 1 in DP group [1]
[INFO] 8 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 8 in DP group [8]
[INFO] 11 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 11 in DP group [11]
[INFO] 13 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 13 in DP group [13]
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 8.745 seconds
time to initialize megatron (seconds): 6.103
[after megatron is initialized] datetime: 2023-05-28 17:12:38 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 456966400
[INFO] 12 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 12 in DP group [12]
[Warning] world comm group not exist!
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 455396608
[INFO] 6 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 6 in DP group [6]
[Warning] world comm group not exist!
[INFO] 3 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 3 in DP group [3]
[Warning] world comm group not exist!
[INFO] 10 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 10 in DP group [10]
[Warning] world comm group not exist!
[INFO] 15 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 15 in DP group [15]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 14 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 14 in DP group [14]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 9 in EP group [8, 9, 10, 11, 12, 13, 14, 15]
[INFO] 9 in DP group [9]
[Warning] world comm group not exist!
[INFO] 7 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 7 in DP group [7]
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
setting training iterations to 100
> learning rate decay style: cosine
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[Warning] world comm group not exist!
[INFO] 2 in EP group [0, 1, 2, 3, 4, 5, 6, 7]
[INFO] 2 in DP group [2]
[Warning] world comm group not exist!
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-05-28 17:12:47 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      25600
    validation: -256
    test:       -256
> building train, validation, and test datasets for GPT ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.002518 seconds
    number of documents: 106824622
 > dataset split:
    train:
     document indices in [0, 106824622) total of 106824622 documents
    validation:
     document indices in [106824622, 106824622) total of 0 documents
    test:
     document indices in [106824622, 106824622) total of 0 documents
 > loading doc-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_25600ns_1024sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_25600ns_1024sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /mnt/znvme/zms/fastmoe-dataset/my-bert_text_sentence_train_indexmap_25600ns_1024sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.004 seconds
    total number of samples: 2899936
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-05-28 17:12:58 
done with setup ...
training ...
[before the start of training step] datetime: 2023-05-28 17:12:58 
 iteration        1/     100 | consumed samples:          256 | elapsed time per iteration (ms): 8018.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.082583E+01 | loss scale: 131072.0 | grad norm: 9.925 | num zeros: 23128.0 | params norm: 229.966 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 8765.15625 | max allocated: 8765.15869140625 | reserved: 9996.0 | max reserved: 9996.0
[Rank 8] (after 1 iterations) memory (MB) | allocated: 8736.6171875 | max allocated: 8736.63232421875 | reserved: 9104.0 | max reserved: 9104.0
 iteration        2/     100 | consumed samples:          512 | elapsed time per iteration (ms): 8166.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.049447E+01 | loss scale: 131072.0 | grad norm: 4.181 | num zeros: 360033344.0 | params norm: 229.992 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/     100 | consumed samples:          768 | elapsed time per iteration (ms): 8769.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.033619E+01 | loss scale: 131072.0 | grad norm: 3.968 | num zeros: 425755072.0 | params norm: 230.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/     100 | consumed samples:         1024 | elapsed time per iteration (ms): 8103.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.152107E+01 | loss scale: 131072.0 | grad norm: 3.966 | num zeros: 331580352.0 | params norm: 230.033 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/     100 | consumed samples:         1280 | elapsed time per iteration (ms): 8803.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.149078E+01 | loss scale: 131072.0 | grad norm: 4.015 | num zeros: 360403520.0 | params norm: 230.057 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/     100 | consumed samples:         1536 | elapsed time per iteration (ms): 8502.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.120801E+01 | loss scale: 131072.0 | grad norm: 4.928 | num zeros: 353777248.0 | params norm: 230.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/     100 | consumed samples:         1792 | elapsed time per iteration (ms): 8613.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.024643E+01 | loss scale: 131072.0 | grad norm: 3.996 | num zeros: 473425824.0 | params norm: 230.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/     100 | consumed samples:         2048 | elapsed time per iteration (ms): 8567.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.019171E+01 | loss scale: 131072.0 | grad norm: 3.939 | num zeros: 435711776.0 | params norm: 230.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/     100 | consumed samples:         2304 | elapsed time per iteration (ms): 8584.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.009919E+01 | loss scale: 131072.0 | grad norm: 3.929 | num zeros: 483968320.0 | params norm: 230.166 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       10/     100 | consumed samples:         2560 | elapsed time per iteration (ms): 9714.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 1.000480E+01 | loss scale: 131072.0 | grad norm: 3.937 | num zeros: 473574656.0 | params norm: 230.195 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/     100 | consumed samples:         2816 | elapsed time per iteration (ms): 7987.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.907877E+00 | loss scale: 131072.0 | grad norm: 3.928 | num zeros: 465092352.0 | params norm: 230.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/     100 | consumed samples:         3072 | elapsed time per iteration (ms): 8123.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.796535E+00 | loss scale: 131072.0 | grad norm: 3.933 | num zeros: 469427584.0 | params norm: 230.253 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/     100 | consumed samples:         3328 | elapsed time per iteration (ms): 8391.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.682419E+00 | loss scale: 131072.0 | grad norm: 3.896 | num zeros: 479955936.0 | params norm: 230.283 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/     100 | consumed samples:         3584 | elapsed time per iteration (ms): 8510.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.565166E+00 | loss scale: 131072.0 | grad norm: 3.909 | num zeros: 499412736.0 | params norm: 230.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/     100 | consumed samples:         3840 | elapsed time per iteration (ms): 8482.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.442050E+00 | loss scale: 131072.0 | grad norm: 3.918 | num zeros: 517496512.0 | params norm: 230.345 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/     100 | consumed samples:         4096 | elapsed time per iteration (ms): 8271.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.322581E+00 | loss scale: 131072.0 | grad norm: 3.889 | num zeros: 509442304.0 | params norm: 230.377 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/     100 | consumed samples:         4352 | elapsed time per iteration (ms): 8199.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.200008E+00 | loss scale: 131072.0 | grad norm: 3.895 | num zeros: 537546944.0 | params norm: 230.410 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/     100 | consumed samples:         4608 | elapsed time per iteration (ms): 8439.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 9.082571E+00 | loss scale: 131072.0 | grad norm: 3.906 | num zeros: 519764256.0 | params norm: 230.443 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/     100 | consumed samples:         4864 | elapsed time per iteration (ms): 8430.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.966105E+00 | loss scale: 131072.0 | grad norm: 3.900 | num zeros: 542683008.0 | params norm: 230.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       20/     100 | consumed samples:         5120 | elapsed time per iteration (ms): 8129.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.854991E+00 | loss scale: 131072.0 | grad norm: 3.873 | num zeros: 542760192.0 | params norm: 230.511 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/     100 | consumed samples:         5376 | elapsed time per iteration (ms): 7975.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.755208E+00 | loss scale: 131072.0 | grad norm: 3.857 | num zeros: 556277632.0 | params norm: 230.546 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/     100 | consumed samples:         5632 | elapsed time per iteration (ms): 7992.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.651165E+00 | loss scale: 131072.0 | grad norm: 3.805 | num zeros: 528603392.0 | params norm: 230.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/     100 | consumed samples:         5888 | elapsed time per iteration (ms): 7933.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.546199E+00 | loss scale: 131072.0 | grad norm: 3.832 | num zeros: 546014400.0 | params norm: 230.618 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/     100 | consumed samples:         6144 | elapsed time per iteration (ms): 7871.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.452353E+00 | loss scale: 131072.0 | grad norm: 3.800 | num zeros: 541892160.0 | params norm: 230.655 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/     100 | consumed samples:         6400 | elapsed time per iteration (ms): 7858.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.363504E+00 | loss scale: 131072.0 | grad norm: 3.776 | num zeros: 492272768.0 | params norm: 230.693 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/     100 | consumed samples:         6656 | elapsed time per iteration (ms): 7940.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.279668E+00 | loss scale: 131072.0 | grad norm: 3.726 | num zeros: 515007488.0 | params norm: 230.731 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/     100 | consumed samples:         6912 | elapsed time per iteration (ms): 7859.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.196972E+00 | loss scale: 131072.0 | grad norm: 3.702 | num zeros: 556115136.0 | params norm: 230.769 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/     100 | consumed samples:         7168 | elapsed time per iteration (ms): 7736.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.119210E+00 | loss scale: 131072.0 | grad norm: 3.657 | num zeros: 574956672.0 | params norm: 230.807 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/     100 | consumed samples:         7424 | elapsed time per iteration (ms): 7737.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 8.042410E+00 | loss scale: 131072.0 | grad norm: 3.604 | num zeros: 573968256.0 | params norm: 230.845 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       30/     100 | consumed samples:         7680 | elapsed time per iteration (ms): 7873.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.963447E+00 | loss scale: 131072.0 | grad norm: 3.548 | num zeros: 559567552.0 | params norm: 230.882 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/     100 | consumed samples:         7936 | elapsed time per iteration (ms): 7659.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.900511E+00 | loss scale: 131072.0 | grad norm: 3.454 | num zeros: 568742016.0 | params norm: 230.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/     100 | consumed samples:         8192 | elapsed time per iteration (ms): 7652.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.823651E+00 | loss scale: 131072.0 | grad norm: 3.380 | num zeros: 561046272.0 | params norm: 230.959 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/     100 | consumed samples:         8448 | elapsed time per iteration (ms): 7623.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.768705E+00 | loss scale: 131072.0 | grad norm: 3.250 | num zeros: 559487104.0 | params norm: 230.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/     100 | consumed samples:         8704 | elapsed time per iteration (ms): 7633.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.704644E+00 | loss scale: 131072.0 | grad norm: 3.151 | num zeros: 542312960.0 | params norm: 231.035 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/     100 | consumed samples:         8960 | elapsed time per iteration (ms): 7780.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.644390E+00 | loss scale: 131072.0 | grad norm: 2.981 | num zeros: 540694336.0 | params norm: 231.074 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/     100 | consumed samples:         9216 | elapsed time per iteration (ms): 7792.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.587122E+00 | loss scale: 131072.0 | grad norm: 2.853 | num zeros: 540959296.0 | params norm: 231.112 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/     100 | consumed samples:         9472 | elapsed time per iteration (ms): 7847.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.547141E+00 | loss scale: 131072.0 | grad norm: 2.627 | num zeros: 521952256.0 | params norm: 231.150 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/     100 | consumed samples:         9728 | elapsed time per iteration (ms): 7894.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.504226E+00 | loss scale: 131072.0 | grad norm: 2.399 | num zeros: 456293568.0 | params norm: 231.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/     100 | consumed samples:         9984 | elapsed time per iteration (ms): 7891.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.478713E+00 | loss scale: 131072.0 | grad norm: 2.146 | num zeros: 333667136.0 | params norm: 231.224 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       40/     100 | consumed samples:        10240 | elapsed time per iteration (ms): 8333.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.428458E+00 | loss scale: 131072.0 | grad norm: 1.920 | num zeros: 297058624.0 | params norm: 231.261 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       41/     100 | consumed samples:        10496 | elapsed time per iteration (ms): 7799.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.402873E+00 | loss scale: 131072.0 | grad norm: 1.606 | num zeros: 522197248.0 | params norm: 231.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       42/     100 | consumed samples:        10752 | elapsed time per iteration (ms): 7800.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.407605E+00 | loss scale: 131072.0 | grad norm: 1.335 | num zeros: 493904128.0 | params norm: 231.337 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       43/     100 | consumed samples:        11008 | elapsed time per iteration (ms): 7784.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.368039E+00 | loss scale: 131072.0 | grad norm: 1.079 | num zeros: 484392192.0 | params norm: 231.375 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       44/     100 | consumed samples:        11264 | elapsed time per iteration (ms): 7889.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.351244E+00 | loss scale: 131072.0 | grad norm: 0.751 | num zeros: 449012864.0 | params norm: 231.415 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       45/     100 | consumed samples:        11520 | elapsed time per iteration (ms): 7875.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.329133E+00 | loss scale: 131072.0 | grad norm: 0.689 | num zeros: 391627168.0 | params norm: 231.455 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       46/     100 | consumed samples:        11776 | elapsed time per iteration (ms): 7870.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.308473E+00 | loss scale: 131072.0 | grad norm: 0.776 | num zeros: 346728512.0 | params norm: 231.495 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       47/     100 | consumed samples:        12032 | elapsed time per iteration (ms): 7911.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.297517E+00 | loss scale: 131072.0 | grad norm: 0.615 | num zeros: 317402208.0 | params norm: 231.538 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       48/     100 | consumed samples:        12288 | elapsed time per iteration (ms): 7899.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.288738E+00 | loss scale: 131072.0 | grad norm: 0.551 | num zeros: 327274720.0 | params norm: 231.581 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       49/     100 | consumed samples:        12544 | elapsed time per iteration (ms): 7825.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.281242E+00 | loss scale: 131072.0 | grad norm: 0.465 | num zeros: 327508928.0 | params norm: 231.624 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       50/     100 | consumed samples:        12800 | elapsed time per iteration (ms): 7760.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.263362E+00 | loss scale: 131072.0 | grad norm: 0.487 | num zeros: 400844864.0 | params norm: 231.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       51/     100 | consumed samples:        13056 | elapsed time per iteration (ms): 7208.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.260896E+00 | loss scale: 131072.0 | grad norm: 0.457 | num zeros: 278800064.0 | params norm: 231.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       52/     100 | consumed samples:        13312 | elapsed time per iteration (ms): 7216.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.240244E+00 | loss scale: 131072.0 | grad norm: 0.548 | num zeros: 295072448.0 | params norm: 231.756 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       53/     100 | consumed samples:        13568 | elapsed time per iteration (ms): 7099.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.242093E+00 | loss scale: 131072.0 | grad norm: 0.405 | num zeros: 371713600.0 | params norm: 231.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       54/     100 | consumed samples:        13824 | elapsed time per iteration (ms): 6948.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.223278E+00 | loss scale: 131072.0 | grad norm: 0.378 | num zeros: 420016384.0 | params norm: 231.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       55/     100 | consumed samples:        14080 | elapsed time per iteration (ms): 6958.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.213781E+00 | loss scale: 131072.0 | grad norm: 0.348 | num zeros: 420322944.0 | params norm: 231.891 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       56/     100 | consumed samples:        14336 | elapsed time per iteration (ms): 6904.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.226130E+00 | loss scale: 131072.0 | grad norm: 0.431 | num zeros: 428190016.0 | params norm: 231.936 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       57/     100 | consumed samples:        14592 | elapsed time per iteration (ms): 6818.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.198305E+00 | loss scale: 131072.0 | grad norm: 0.411 | num zeros: 417928160.0 | params norm: 231.982 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       58/     100 | consumed samples:        14848 | elapsed time per iteration (ms): 6812.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.187392E+00 | loss scale: 131072.0 | grad norm: 0.301 | num zeros: 427298688.0 | params norm: 232.027 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       59/     100 | consumed samples:        15104 | elapsed time per iteration (ms): 6822.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.180792E+00 | loss scale: 131072.0 | grad norm: 0.341 | num zeros: 429448736.0 | params norm: 232.071 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       60/     100 | consumed samples:        15360 | elapsed time per iteration (ms): 6949.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.171424E+00 | loss scale: 131072.0 | grad norm: 0.264 | num zeros: 437555712.0 | params norm: 232.114 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       61/     100 | consumed samples:        15616 | elapsed time per iteration (ms): 6474.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.184900E+00 | loss scale: 131072.0 | grad norm: 0.423 | num zeros: 374059616.0 | params norm: 232.160 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       62/     100 | consumed samples:        15872 | elapsed time per iteration (ms): 6774.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.168732E+00 | loss scale: 131072.0 | grad norm: 0.447 | num zeros: 392319360.0 | params norm: 232.202 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       63/     100 | consumed samples:        16128 | elapsed time per iteration (ms): 6909.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.149908E+00 | loss scale: 131072.0 | grad norm: 0.430 | num zeros: 399140864.0 | params norm: 232.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       64/     100 | consumed samples:        16384 | elapsed time per iteration (ms): 6880.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.145298E+00 | loss scale: 131072.0 | grad norm: 0.268 | num zeros: 401028672.0 | params norm: 232.282 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       65/     100 | consumed samples:        16640 | elapsed time per iteration (ms): 6920.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.150368E+00 | loss scale: 131072.0 | grad norm: 0.373 | num zeros: 382904192.0 | params norm: 232.321 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       66/     100 | consumed samples:        16896 | elapsed time per iteration (ms): 6945.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.132328E+00 | loss scale: 131072.0 | grad norm: 0.388 | num zeros: 392306336.0 | params norm: 232.362 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       67/     100 | consumed samples:        17152 | elapsed time per iteration (ms): 7025.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.133253E+00 | loss scale: 131072.0 | grad norm: 0.227 | num zeros: 377924864.0 | params norm: 232.401 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       68/     100 | consumed samples:        17408 | elapsed time per iteration (ms): 7049.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.130468E+00 | loss scale: 131072.0 | grad norm: 0.219 | num zeros: 412094016.0 | params norm: 232.439 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       69/     100 | consumed samples:        17664 | elapsed time per iteration (ms): 7056.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.125593E+00 | loss scale: 131072.0 | grad norm: 0.281 | num zeros: 374050752.0 | params norm: 232.475 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       70/     100 | consumed samples:        17920 | elapsed time per iteration (ms): 6836.1 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.121429E+00 | loss scale: 131072.0 | grad norm: 0.206 | num zeros: 310189568.0 | params norm: 232.510 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       71/     100 | consumed samples:        18176 | elapsed time per iteration (ms): 6328.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.113387E+00 | loss scale: 131072.0 | grad norm: 0.224 | num zeros: 311623968.0 | params norm: 232.544 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       72/     100 | consumed samples:        18432 | elapsed time per iteration (ms): 6332.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.131769E+00 | loss scale: 131072.0 | grad norm: 0.296 | num zeros: 317678208.0 | params norm: 232.577 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       73/     100 | consumed samples:        18688 | elapsed time per iteration (ms): 6564.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.124979E+00 | loss scale: 131072.0 | grad norm: 0.364 | num zeros: 321535232.0 | params norm: 232.610 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       74/     100 | consumed samples:        18944 | elapsed time per iteration (ms): 6679.8 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.110984E+00 | loss scale: 131072.0 | grad norm: 0.678 | num zeros: 314141152.0 | params norm: 232.641 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       75/     100 | consumed samples:        19200 | elapsed time per iteration (ms): 6397.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.121051E+00 | loss scale: 131072.0 | grad norm: 0.615 | num zeros: 312183392.0 | params norm: 232.672 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       76/     100 | consumed samples:        19456 | elapsed time per iteration (ms): 6696.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.095195E+00 | loss scale: 131072.0 | grad norm: 0.259 | num zeros: 331487040.0 | params norm: 232.701 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       77/     100 | consumed samples:        19712 | elapsed time per iteration (ms): 6685.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.119187E+00 | loss scale: 131072.0 | grad norm: 0.408 | num zeros: 272801472.0 | params norm: 232.730 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       78/     100 | consumed samples:        19968 | elapsed time per iteration (ms): 6536.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.103200E+00 | loss scale: 131072.0 | grad norm: 0.295 | num zeros: 253443072.0 | params norm: 232.758 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       79/     100 | consumed samples:        20224 | elapsed time per iteration (ms): 6594.0 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.094181E+00 | loss scale: 131072.0 | grad norm: 0.325 | num zeros: 298725024.0 | params norm: 232.787 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       80/     100 | consumed samples:        20480 | elapsed time per iteration (ms): 7501.6 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.103314E+00 | loss scale: 131072.0 | grad norm: 0.346 | num zeros: 180159296.0 | params norm: 232.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       81/     100 | consumed samples:        20736 | elapsed time per iteration (ms): 6739.3 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.098647E+00 | loss scale: 131072.0 | grad norm: 0.378 | num zeros: 170549152.0 | params norm: 232.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       82/     100 | consumed samples:        20992 | elapsed time per iteration (ms): 6425.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.076911E+00 | loss scale: 131072.0 | grad norm: 0.361 | num zeros: 135593920.0 | params norm: 232.868 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       83/     100 | consumed samples:        21248 | elapsed time per iteration (ms): 6614.5 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.095897E+00 | loss scale: 131072.0 | grad norm: 0.318 | num zeros: 141928912.0 | params norm: 232.895 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       84/     100 | consumed samples:        21504 | elapsed time per iteration (ms): 7057.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.084781E+00 | loss scale: 131072.0 | grad norm: 2.713 | num zeros: 172094944.0 | params norm: 232.921 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       85/     100 | consumed samples:        21760 | elapsed time per iteration (ms): 6432.2 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.071389E+00 | loss scale: 131072.0 | grad norm: 0.915 | num zeros: 178908416.0 | params norm: 232.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       86/     100 | consumed samples:        22016 | elapsed time per iteration (ms): 6699.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.083826E+00 | loss scale: 131072.0 | grad norm: 0.780 | num zeros: 164097280.0 | params norm: 232.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       87/     100 | consumed samples:        22272 | elapsed time per iteration (ms): 6906.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.069844E+00 | loss scale: 131072.0 | grad norm: 0.697 | num zeros: 170550944.0 | params norm: 232.997 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       88/     100 | consumed samples:        22528 | elapsed time per iteration (ms): 7028.4 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.063616E+00 | loss scale: 131072.0 | grad norm: 0.709 | num zeros: 172540016.0 | params norm: 233.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       89/     100 | consumed samples:        22784 | elapsed time per iteration (ms): 6794.7 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.044395E+00 | loss scale: 131072.0 | grad norm: 0.365 | num zeros: 213718272.0 | params norm: 233.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration       90/     100 | consumed samples:        23040 | elapsed time per iteration (ms): 6959.9 | learning rate: 1.000E-04 | global batch size:   256 | lm loss: 7.054244E+00 | loss scale: 131072.0 | grad norm: 0.564 | num zeros: 204094752.0 | params norm: 233.067 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       91/     100 | consumed samples:        23296 | elapsed time per iteration (ms): 6497.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.032516E+00 | loss scale: 131072.0 | grad norm: 0.701 | num zeros: 198556864.0 | params norm: 233.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       92/     100 | consumed samples:        23552 | elapsed time per iteration (ms): 6664.5 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.014774E+00 | loss scale: 131072.0 | grad norm: 0.335 | num zeros: 179286976.0 | params norm: 233.116 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       93/     100 | consumed samples:        23808 | elapsed time per iteration (ms): 6971.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.015360E+00 | loss scale: 131072.0 | grad norm: 0.914 | num zeros: 201062480.0 | params norm: 233.141 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       94/     100 | consumed samples:        24064 | elapsed time per iteration (ms): 6743.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.992983E+00 | loss scale: 131072.0 | grad norm: 0.315 | num zeros: 214308160.0 | params norm: 233.165 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       95/     100 | consumed samples:        24320 | elapsed time per iteration (ms): 6716.9 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 7.000073E+00 | loss scale: 131072.0 | grad norm: 0.721 | num zeros: 192720144.0 | params norm: 233.191 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       96/     100 | consumed samples:        24576 | elapsed time per iteration (ms): 6764.8 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.984873E+00 | loss scale: 131072.0 | grad norm: 0.527 | num zeros: 180701536.0 | params norm: 233.216 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       97/     100 | consumed samples:        24832 | elapsed time per iteration (ms): 6926.6 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.994129E+00 | loss scale: 131072.0 | grad norm: 0.685 | num zeros: 184647872.0 | params norm: 233.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       98/     100 | consumed samples:        25088 | elapsed time per iteration (ms): 6859.3 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.964220E+00 | loss scale: 131072.0 | grad norm: 0.458 | num zeros: 204610528.0 | params norm: 233.268 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       99/     100 | consumed samples:        25344 | elapsed time per iteration (ms): 6763.7 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.963180E+00 | loss scale: 131072.0 | grad norm: 0.654 | num zeros: 212301264.0 | params norm: 233.295 | number of skipped iterations:   0 | number of nan iterations:   0 |
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
[INFO] mapping updated!
 iteration      100/     100 | consumed samples:        25600 | elapsed time per iteration (ms): 7062.1 | learning rate: 9.999E-05 | global batch size:   256 | lm loss: 6.960465E+00 | loss scale: 131072.0 | grad norm: 0.477 | num zeros: 219643840.0 | params norm: 233.323 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2023-05-28 17:25:23 
/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig12
+ cd -
/home/atc23_ae/SmartMoE-AE
++ pwd
++ pwd
+ AEROOT=/home/atc23_ae/SmartMoE-AE
+ python3 ./plotting/from_exec/fig12.py /home/atc23_ae/SmartMoE-AE /home/atc23_ae/SmartMoE-AE/outputs_from_exec_2023-05-28T16:36:45+08:00
+ cd ./plotting/from_exec/fig13
+ ./fig13.sh
+ '[' 9 -ne 9 ']'
++ expr 27201 % 10000 + 10000
+ export MASTER_PORT=17201
+ MASTER_PORT=17201
+ export EXP_NAME=fig13_2023-05-28T17:25:28+08:00
+ EXP_NAME=fig13_2023-05-28T17:25:28+08:00
+ export NNODES=1
+ NNODES=1
+ export NODELIST=nico1
+ NODELIST=nico1
+ export EXEC=/home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py
+ EXEC=/home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py
+ export TABLE_PREFIX=/home/atc23_ae/SmartMoE-AE/moe_trace/table
+ TABLE_PREFIX=/home/atc23_ae/SmartMoE-AE/moe_trace/table
+ export MICRO_BATCH_SIZE=8
+ MICRO_BATCH_SIZE=8
+ export HIDDEN_SIZE=1536
+ HIDDEN_SIZE=1536
+ export HISTORY_LAT=0
+ HISTORY_LAT=0
+ export UPDATE_FREQ=10
+ UPDATE_FREQ=10
+ export FMOE_FASTER_GLBPLC_NETBW=8e9
+ FMOE_FASTER_GLBPLC_NETBW=8e9
+ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
+ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
+ export FMOE_FASTER_GLBPLC_GPUTP=112e12
+ FMOE_FASTER_GLBPLC_GPUTP=112e12
+ export FMOE_FASTER_SCHEDULE_ENABLE=ON
+ FMOE_FASTER_SCHEDULE_ENABLE=ON
+ export FMOE_FASTER_GROUP_SIZE=16
+ FMOE_FASTER_GROUP_SIZE=16
+ export FMOE_FASTER_SHADOW_ENABLE=ON
+ FMOE_FASTER_SHADOW_ENABLE=ON
+ export FMOE_FASTER_GLBPLC_ALPHA=2
+ FMOE_FASTER_GLBPLC_ALPHA=2
+ export FMOE_FASTER_GLBPLC_DMODEL=1536
+ FMOE_FASTER_GLBPLC_DMODEL=1536
+ mkdir -p ./logs
+ mkdir -p ./logs/fig13_2023-05-28T17:25:28+08:00
++ pwd
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/benchmarks/logs/fig13_2023-05-28T17:25:28+08:00
+ tmp=/home/atc23_ae/SmartMoE-AE/moe_trace/table
++ echo /home/atc23_ae/SmartMoE-AE/moe_trace/table
++ sed 's/\//_/g'
+ result=_home_atc23_ae_SmartMoE-AE_moe_trace_table
++ date -Iseconds
+ LOG_PREFIX=1nodes_mbs8_H1536_LAT0_FREQ10_2023-05-28T17:25:28+08:00
+ LOG_NAME=1nodes_mbs8_H1536_LAT0_FREQ10_2023-05-28T17:25:28+08:00.log
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ '[' 1 == half ']'
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 1'
+ '[' nico1 '!=' None ']'
++ scontrol show hostnames nico1
++ wc -l
+ tmp=1
+ '[' 1 '!=' 1 ']'
+ exec_args+=' -w nico1'
+ exec_args+=' -p AE'
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 1 -w nico1 -p AE
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 1 -w nico1 -p AE
+ tee /home/atc23_ae/SmartMoE-AE/scripts/benchmarks/logs/fig13_2023-05-28T17:25:28+08:00/1nodes_mbs8_H1536_LAT0_FREQ10_2023-05-28T17:25:28+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 1 -w nico1 -p AE ./wrapper_dist_smart_exchange.sh
++ scontrol show JobId=164095
++ grep BatchHost
++ scontrol show JobId=164095
++ awk '{print $2}'
++ tr = ' '
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164095
++ scontrol show JobId=164095
++ grep BatchHost
++ tr = ' '
++ scontrol show JobId=164095
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164095
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ scontrol show JobId=164095
++ scontrol show JobId=164095
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=7
+ export NODE_RANK=7
+ NODE_RANK=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 10
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=0
+ export NODE_RANK=0
+ NODE_RANK=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=1
+ export NODE_RANK=1
+ NODE_RANK=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=5
+ export NODE_RANK=5
+ NODE_RANK=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=6
+ export NODE_RANK=6
+ NODE_RANK=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 10
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=3
+ export NODE_RANK=3
+ NODE_RANK=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 10
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1 6
[INFO] mapping updated!
[19, 30, 21, 10, 15, 25, 5, 11, 23, 8, 3, 13, 9, 26, 24, 12, 29, 0, 16, 2, 18, 1, 20, 4, 6, 7, 17, 14, 22, 31, 27, 28]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1 layer 6: round 0 FastMoE=1.244 SmartMoE=0.613
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1 layer 6: round 1 FastMoE=0.079 SmartMoE=0.068
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1 layer 6: round 2 FastMoE=0.069 SmartMoE=0.070
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1 layer 6: round 3 FastMoE=0.074 SmartMoE=0.067
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter11 6
[INFO] mapping updated!
[11, 28, 5, 8, 22, 15, 10, 23, 13, 19, 4, 14, 2, 9, 17, 12, 24, 16, 29, 0, 20, 3, 26, 7, 6, 30, 18, 1, 21, 27, 25, 31]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 11 layer 6: round 0 FastMoE=0.222 SmartMoE=0.322
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 11 layer 6: round 1 FastMoE=0.181 SmartMoE=0.147
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 11 layer 6: round 2 FastMoE=0.180 SmartMoE=0.146
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 11 layer 6: round 3 FastMoE=0.179 SmartMoE=0.145
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter21 6
[INFO] mapping updated!
[6, 28, 2, 3, 0, 26, 27, 29, 5, 16, 31, 18, 19, 13, 14, 12, 9, 21, 22, 17, 20, 10, 24, 7, 8, 4, 11, 30, 25, 1, 15, 23]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 21 layer 6: round 0 FastMoE=0.212 SmartMoE=0.237
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 21 layer 6: round 1 FastMoE=0.199 SmartMoE=0.147
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 21 layer 6: round 2 FastMoE=0.193 SmartMoE=0.154
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 21 layer 6: round 3 FastMoE=0.193 SmartMoE=0.154
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter31 6
[6, 28, 2, 3, 0, 26, 27, 29, 5, 16, 31, 18, 19, 13, 14, 12, 9, 21, 22, 17, 20, 10, 24, 7, 8, 4, 11, 30, 25, 1, 15, 23]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 31 layer 6: round 0 FastMoE=0.247 SmartMoE=0.157
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 31 layer 6: round 1 FastMoE=0.210 SmartMoE=0.158
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 31 layer 6: round 2 FastMoE=0.204 SmartMoE=0.158
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 31 layer 6: round 3 FastMoE=0.206 SmartMoE=0.157
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter41 6
[INFO] mapping updated!
[23, 28, 30, 31, 9, 1, 11, 8, 0, 19, 25, 7, 27, 18, 17, 12, 3, 14, 15, 2, 20, 22, 24, 10, 26, 4, 21, 6, 5, 29, 13, 16]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 41 layer 6: round 0 FastMoE=0.248 SmartMoE=0.257
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 41 layer 6: round 1 FastMoE=0.202 SmartMoE=0.150
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 41 layer 6: round 2 FastMoE=0.201 SmartMoE=0.150
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 41 layer 6: round 3 FastMoE=0.203 SmartMoE=0.153
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter51 6
[INFO] mapping updated!
[5, 9, 25, 28, 8, 13, 3, 19, 1, 26, 18, 29, 6, 11, 27, 12, 10, 2, 21, 0, 20, 23, 24, 30, 16, 4, 17, 22, 14, 15, 31, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 51 layer 6: round 0 FastMoE=0.217 SmartMoE=0.197
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 51 layer 6: round 1 FastMoE=0.171 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 51 layer 6: round 2 FastMoE=0.170 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 51 layer 6: round 3 FastMoE=0.168 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter61 6
[INFO] mapping updated!
[15, 17, 25, 28, 8, 7, 19, 30, 26, 16, 6, 21, 22, 27, 23, 12, 5, 10, 31, 11, 20, 1, 24, 14, 3, 0, 2, 29, 13, 18, 9, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 61 layer 6: round 0 FastMoE=0.216 SmartMoE=0.180
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 61 layer 6: round 1 FastMoE=0.157 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 61 layer 6: round 2 FastMoE=0.158 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 61 layer 6: round 3 FastMoE=0.157 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter71 6
[15, 17, 25, 28, 8, 7, 19, 30, 26, 16, 6, 21, 22, 27, 23, 12, 5, 10, 31, 11, 20, 1, 24, 14, 3, 0, 2, 29, 13, 18, 9, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 71 layer 6: round 0 FastMoE=0.198 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 71 layer 6: round 1 FastMoE=0.149 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 71 layer 6: round 2 FastMoE=0.157 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 71 layer 6: round 3 FastMoE=0.154 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter81 6
[15, 17, 25, 28, 8, 7, 19, 30, 26, 16, 6, 21, 22, 27, 23, 12, 5, 10, 31, 11, 20, 1, 24, 14, 3, 0, 2, 29, 13, 18, 9, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 81 layer 6: round 0 FastMoE=0.212 SmartMoE=0.145
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 81 layer 6: round 1 FastMoE=0.160 SmartMoE=0.145
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 81 layer 6: round 2 FastMoE=0.160 SmartMoE=0.144
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 81 layer 6: round 3 FastMoE=0.157 SmartMoE=0.142
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter91 6
[15, 17, 25, 28, 8, 7, 19, 30, 26, 16, 6, 21, 22, 27, 23, 12, 5, 10, 31, 11, 20, 1, 24, 14, 3, 0, 2, 29, 13, 18, 9, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 91 layer 6: round 0 FastMoE=0.234 SmartMoE=0.155
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 91 layer 6: round 1 FastMoE=0.179 SmartMoE=0.152
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 91 layer 6: round 2 FastMoE=0.178 SmartMoE=0.154
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 91 layer 6: round 3 FastMoE=0.180 SmartMoE=0.154
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter101 6
[15, 17, 25, 28, 8, 7, 19, 30, 26, 16, 6, 21, 22, 27, 23, 12, 5, 10, 31, 11, 20, 1, 24, 14, 3, 0, 2, 29, 13, 18, 9, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 101 layer 6: round 0 FastMoE=0.217 SmartMoE=0.168
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 101 layer 6: round 1 FastMoE=0.195 SmartMoE=0.170
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 101 layer 6: round 2 FastMoE=0.192 SmartMoE=0.166
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 101 layer 6: round 3 FastMoE=0.193 SmartMoE=0.166
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter111 6
[15, 17, 25, 28, 8, 7, 19, 30, 26, 16, 6, 21, 22, 27, 23, 12, 5, 10, 31, 11, 20, 1, 24, 14, 3, 0, 2, 29, 13, 18, 9, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 111 layer 6: round 0 FastMoE=0.216 SmartMoE=0.169
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 111 layer 6: round 1 FastMoE=0.194 SmartMoE=0.167
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 111 layer 6: round 2 FastMoE=0.193 SmartMoE=0.167
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 111 layer 6: round 3 FastMoE=0.191 SmartMoE=0.164
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter121 6
[INFO] mapping updated!
[26, 16, 3, 24, 8, 0, 1, 28, 22, 21, 15, 6, 11, 19, 18, 12, 5, 29, 30, 17, 20, 2, 9, 10, 13, 7, 27, 23, 31, 25, 14, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 121 layer 6: round 0 FastMoE=0.233 SmartMoE=0.195
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 121 layer 6: round 1 FastMoE=0.186 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 121 layer 6: round 2 FastMoE=0.188 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 121 layer 6: round 3 FastMoE=0.188 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter131 6
[INFO] mapping updated!
[6, 16, 5, 24, 8, 0, 18, 28, 26, 22, 23, 11, 29, 30, 2, 12, 17, 9, 10, 7, 20, 19, 3, 21, 1, 25, 15, 14, 13, 27, 31, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 131 layer 6: round 0 FastMoE=0.261 SmartMoE=0.198
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 131 layer 6: round 1 FastMoE=0.197 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 131 layer 6: round 2 FastMoE=0.195 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 131 layer 6: round 3 FastMoE=0.193 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter141 6
[6, 16, 5, 24, 8, 0, 18, 28, 26, 22, 23, 11, 29, 30, 2, 12, 17, 9, 10, 7, 20, 19, 3, 21, 1, 25, 15, 14, 13, 27, 31, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 141 layer 6: round 0 FastMoE=0.234 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 141 layer 6: round 1 FastMoE=0.195 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 141 layer 6: round 2 FastMoE=0.194 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 141 layer 6: round 3 FastMoE=0.190 SmartMoE=0.116
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter151 6
[INFO] mapping updated!
[11, 17, 18, 24, 8, 0, 3, 28, 26, 27, 6, 14, 13, 21, 22, 12, 29, 19, 30, 31, 20, 16, 10, 15, 5, 9, 25, 2, 1, 7, 23, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 151 layer 6: round 0 FastMoE=0.230 SmartMoE=0.180
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 151 layer 6: round 1 FastMoE=0.183 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 151 layer 6: round 2 FastMoE=0.184 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 151 layer 6: round 3 FastMoE=0.178 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter161 6
[11, 17, 18, 24, 8, 0, 3, 28, 26, 27, 6, 14, 13, 21, 22, 12, 29, 19, 30, 31, 20, 16, 10, 15, 5, 9, 25, 2, 1, 7, 23, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 161 layer 6: round 0 FastMoE=0.243 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 161 layer 6: round 1 FastMoE=0.179 SmartMoE=0.102
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 161 layer 6: round 2 FastMoE=0.179 SmartMoE=0.102
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 161 layer 6: round 3 FastMoE=0.181 SmartMoE=0.104
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter171 6
[11, 17, 18, 24, 8, 0, 3, 28, 26, 27, 6, 14, 13, 21, 22, 12, 29, 19, 30, 31, 20, 16, 10, 15, 5, 9, 25, 2, 1, 7, 23, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 171 layer 6: round 0 FastMoE=0.234 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 171 layer 6: round 1 FastMoE=0.179 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 171 layer 6: round 2 FastMoE=0.179 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 171 layer 6: round 3 FastMoE=0.183 SmartMoE=0.107
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter181 6
[11, 17, 18, 24, 8, 0, 3, 28, 26, 27, 6, 14, 13, 21, 22, 12, 29, 19, 30, 31, 20, 16, 10, 15, 5, 9, 25, 2, 1, 7, 23, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 181 layer 6: round 0 FastMoE=0.239 SmartMoE=0.104
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 181 layer 6: round 1 FastMoE=0.178 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 181 layer 6: round 2 FastMoE=0.179 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 181 layer 6: round 3 FastMoE=0.181 SmartMoE=0.102
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter191 6
[11, 17, 18, 24, 8, 0, 3, 28, 26, 27, 6, 14, 13, 21, 22, 12, 29, 19, 30, 31, 20, 16, 10, 15, 5, 9, 25, 2, 1, 7, 23, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 191 layer 6: round 0 FastMoE=0.238 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 191 layer 6: round 1 FastMoE=0.179 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 191 layer 6: round 2 FastMoE=0.174 SmartMoE=0.104
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 191 layer 6: round 3 FastMoE=0.179 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter201 6
[11, 17, 18, 24, 8, 0, 3, 28, 26, 27, 6, 14, 13, 21, 22, 12, 29, 19, 30, 31, 20, 16, 10, 15, 5, 9, 25, 2, 1, 7, 23, 4]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 201 layer 6: round 0 FastMoE=0.234 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 201 layer 6: round 1 FastMoE=0.183 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 201 layer 6: round 2 FastMoE=0.181 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 201 layer 6: round 3 FastMoE=0.176 SmartMoE=0.104
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter211 6
[INFO] mapping updated!
[11, 2, 1, 24, 8, 7, 5, 28, 0, 19, 18, 17, 16, 3, 6, 12, 27, 13, 14, 15, 20, 21, 22, 23, 29, 4, 30, 31, 9, 10, 26, 25]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 211 layer 6: round 0 FastMoE=0.237 SmartMoE=0.197
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 211 layer 6: round 1 FastMoE=0.179 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 211 layer 6: round 2 FastMoE=0.176 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 211 layer 6: round 3 FastMoE=0.184 SmartMoE=0.107
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter221 6
[11, 2, 1, 24, 8, 7, 5, 28, 0, 19, 18, 17, 16, 3, 6, 12, 27, 13, 14, 15, 20, 21, 22, 23, 29, 4, 30, 31, 9, 10, 26, 25]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 221 layer 6: round 0 FastMoE=0.246 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 221 layer 6: round 1 FastMoE=0.177 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 221 layer 6: round 2 FastMoE=0.177 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 221 layer 6: round 3 FastMoE=0.177 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter231 6
[INFO] mapping updated!
[11, 0, 2, 24, 8, 1, 6, 28, 4, 17, 5, 19, 18, 3, 7, 12, 27, 13, 14, 15, 20, 21, 22, 23, 29, 30, 31, 9, 10, 16, 26, 25]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 231 layer 6: round 0 FastMoE=0.262 SmartMoE=0.141
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 231 layer 6: round 1 FastMoE=0.182 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 231 layer 6: round 2 FastMoE=0.177 SmartMoE=0.104
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 231 layer 6: round 3 FastMoE=0.176 SmartMoE=0.103
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter241 6
[11, 0, 2, 24, 8, 1, 6, 28, 4, 17, 5, 19, 18, 3, 7, 12, 27, 13, 14, 15, 20, 21, 22, 23, 29, 30, 31, 9, 10, 16, 26, 25]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 241 layer 6: round 0 FastMoE=0.241 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 241 layer 6: round 1 FastMoE=0.186 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 241 layer 6: round 2 FastMoE=0.180 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 241 layer 6: round 3 FastMoE=0.181 SmartMoE=0.114
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter251 6
[11, 0, 2, 24, 8, 1, 6, 28, 4, 17, 5, 19, 18, 3, 7, 12, 27, 13, 14, 15, 20, 21, 22, 23, 29, 30, 31, 9, 10, 16, 26, 25]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 251 layer 6: round 0 FastMoE=0.237 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 251 layer 6: round 1 FastMoE=0.179 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 251 layer 6: round 2 FastMoE=0.178 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 251 layer 6: round 3 FastMoE=0.178 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter261 6
[INFO] mapping updated!
[6, 0, 5, 24, 8, 25, 26, 28, 16, 19, 14, 21, 17, 23, 29, 12, 31, 9, 10, 15, 20, 18, 22, 13, 27, 4, 3, 2, 1, 7, 30, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 261 layer 6: round 0 FastMoE=0.216 SmartMoE=0.193
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 261 layer 6: round 1 FastMoE=0.183 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 261 layer 6: round 2 FastMoE=0.181 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 261 layer 6: round 3 FastMoE=0.182 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter271 6
[6, 0, 5, 24, 8, 25, 26, 28, 16, 19, 14, 21, 17, 23, 29, 12, 31, 9, 10, 15, 20, 18, 22, 13, 27, 4, 3, 2, 1, 7, 30, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 271 layer 6: round 0 FastMoE=0.211 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 271 layer 6: round 1 FastMoE=0.186 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 271 layer 6: round 2 FastMoE=0.183 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 271 layer 6: round 3 FastMoE=0.185 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter281 6
[6, 0, 5, 24, 8, 25, 26, 28, 16, 19, 14, 21, 17, 23, 29, 12, 31, 9, 10, 15, 20, 18, 22, 13, 27, 4, 3, 2, 1, 7, 30, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 281 layer 6: round 0 FastMoE=0.212 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 281 layer 6: round 1 FastMoE=0.179 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 281 layer 6: round 2 FastMoE=0.181 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 281 layer 6: round 3 FastMoE=0.181 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter291 6
[INFO] mapping updated!
[11, 0, 16, 24, 8, 3, 25, 28, 27, 13, 14, 15, 21, 22, 23, 12, 30, 31, 9, 10, 20, 6, 7, 26, 4, 17, 2, 1, 19, 18, 29, 5]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 291 layer 6: round 0 FastMoE=0.189 SmartMoE=0.184
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 291 layer 6: round 1 FastMoE=0.178 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 291 layer 6: round 2 FastMoE=0.176 SmartMoE=0.101
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 291 layer 6: round 3 FastMoE=0.178 SmartMoE=0.106
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter301 6
[11, 0, 16, 24, 8, 3, 25, 28, 27, 13, 14, 15, 21, 22, 23, 12, 30, 31, 9, 10, 20, 6, 7, 26, 4, 17, 2, 1, 19, 18, 29, 5]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 301 layer 6: round 0 FastMoE=0.222 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 301 layer 6: round 1 FastMoE=0.176 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 301 layer 6: round 2 FastMoE=0.175 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 301 layer 6: round 3 FastMoE=0.174 SmartMoE=0.106
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter311 6
[INFO] mapping updated!
[7, 0, 4, 24, 8, 25, 26, 28, 13, 19, 11, 21, 22, 23, 29, 12, 31, 9, 10, 5, 20, 14, 15, 27, 6, 16, 3, 17, 2, 1, 30, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 311 layer 6: round 0 FastMoE=0.230 SmartMoE=0.178
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 311 layer 6: round 1 FastMoE=0.178 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 311 layer 6: round 2 FastMoE=0.178 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 311 layer 6: round 3 FastMoE=0.177 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter321 6
[7, 0, 4, 24, 8, 25, 26, 28, 13, 19, 11, 21, 22, 23, 29, 12, 31, 9, 10, 5, 20, 14, 15, 27, 6, 16, 3, 17, 2, 1, 30, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 321 layer 6: round 0 FastMoE=0.223 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 321 layer 6: round 1 FastMoE=0.174 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 321 layer 6: round 2 FastMoE=0.174 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 321 layer 6: round 3 FastMoE=0.172 SmartMoE=0.104
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter331 6
[INFO] mapping updated!
[1, 0, 5, 24, 8, 14, 15, 28, 19, 23, 7, 30, 31, 9, 10, 12, 3, 29, 21, 6, 20, 18, 25, 13, 4, 16, 27, 17, 26, 2, 22, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 331 layer 6: round 0 FastMoE=0.217 SmartMoE=0.183
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 331 layer 6: round 1 FastMoE=0.173 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 331 layer 6: round 2 FastMoE=0.172 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 331 layer 6: round 3 FastMoE=0.175 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter341 6
[1, 0, 5, 24, 8, 14, 15, 28, 19, 23, 7, 30, 31, 9, 10, 12, 3, 29, 21, 6, 20, 18, 25, 13, 4, 16, 27, 17, 26, 2, 22, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 341 layer 6: round 0 FastMoE=0.221 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 341 layer 6: round 1 FastMoE=0.170 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 341 layer 6: round 2 FastMoE=0.169 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 341 layer 6: round 3 FastMoE=0.170 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter351 6
[INFO] mapping updated!
[11, 0, 18, 24, 8, 26, 27, 28, 6, 21, 1, 22, 13, 14, 15, 12, 5, 30, 31, 9, 20, 23, 7, 10, 4, 19, 25, 16, 3, 2, 29, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 351 layer 6: round 0 FastMoE=0.218 SmartMoE=0.168
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 351 layer 6: round 1 FastMoE=0.178 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 351 layer 6: round 2 FastMoE=0.177 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 351 layer 6: round 3 FastMoE=0.175 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter361 6
[11, 0, 18, 24, 8, 26, 27, 28, 6, 21, 1, 22, 13, 14, 15, 12, 5, 30, 31, 9, 20, 23, 7, 10, 4, 19, 25, 16, 3, 2, 29, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 361 layer 6: round 0 FastMoE=0.225 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 361 layer 6: round 1 FastMoE=0.177 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 361 layer 6: round 2 FastMoE=0.174 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 361 layer 6: round 3 FastMoE=0.175 SmartMoE=0.106
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter371 6
[11, 0, 18, 24, 8, 26, 27, 28, 6, 21, 1, 22, 13, 14, 15, 12, 5, 30, 31, 9, 20, 23, 7, 10, 4, 19, 25, 16, 3, 2, 29, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 371 layer 6: round 0 FastMoE=0.303 SmartMoE=0.099
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 371 layer 6: round 1 FastMoE=0.169 SmartMoE=0.099
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 371 layer 6: round 2 FastMoE=0.165 SmartMoE=0.101
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 371 layer 6: round 3 FastMoE=0.170 SmartMoE=0.099
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter381 6
[11, 0, 18, 24, 8, 26, 27, 28, 6, 21, 1, 22, 13, 14, 15, 12, 5, 30, 31, 9, 20, 23, 7, 10, 4, 19, 25, 16, 3, 2, 29, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 381 layer 6: round 0 FastMoE=0.224 SmartMoE=0.104
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 381 layer 6: round 1 FastMoE=0.169 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 381 layer 6: round 2 FastMoE=0.169 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 381 layer 6: round 3 FastMoE=0.170 SmartMoE=0.104
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter391 6
[INFO] mapping updated!
[11, 0, 1, 24, 8, 21, 22, 28, 26, 13, 23, 15, 29, 30, 4, 12, 18, 31, 9, 10, 20, 25, 14, 7, 6, 19, 3, 16, 2, 27, 5, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 391 layer 6: round 0 FastMoE=0.226 SmartMoE=0.180
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 391 layer 6: round 1 FastMoE=0.166 SmartMoE=0.101
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 391 layer 6: round 2 FastMoE=0.168 SmartMoE=0.103
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 391 layer 6: round 3 FastMoE=0.168 SmartMoE=0.106
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter401 6
[11, 0, 1, 24, 8, 21, 22, 28, 26, 13, 23, 15, 29, 30, 4, 12, 18, 31, 9, 10, 20, 25, 14, 7, 6, 19, 3, 16, 2, 27, 5, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 401 layer 6: round 0 FastMoE=0.224 SmartMoE=0.101
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 401 layer 6: round 1 FastMoE=0.167 SmartMoE=0.100
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 401 layer 6: round 2 FastMoE=0.166 SmartMoE=0.101
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 401 layer 6: round 3 FastMoE=0.167 SmartMoE=0.103
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter411 6
[11, 0, 1, 24, 8, 21, 22, 28, 26, 13, 23, 15, 29, 30, 4, 12, 18, 31, 9, 10, 20, 25, 14, 7, 6, 19, 3, 16, 2, 27, 5, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 411 layer 6: round 0 FastMoE=0.221 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 411 layer 6: round 1 FastMoE=0.165 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 411 layer 6: round 2 FastMoE=0.165 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 411 layer 6: round 3 FastMoE=0.166 SmartMoE=0.104
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter421 6
[INFO] mapping updated!
[11, 0, 2, 24, 8, 22, 23, 28, 27, 14, 13, 29, 30, 31, 25, 12, 9, 17, 10, 18, 20, 26, 15, 19, 6, 7, 21, 16, 3, 1, 4, 5]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 421 layer 6: round 0 FastMoE=0.221 SmartMoE=0.162
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 421 layer 6: round 1 FastMoE=0.173 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 421 layer 6: round 2 FastMoE=0.168 SmartMoE=0.103
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 421 layer 6: round 3 FastMoE=0.169 SmartMoE=0.105
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter431 6
[INFO] mapping updated!
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 431 layer 6: round 0 FastMoE=0.245 SmartMoE=0.166
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 431 layer 6: round 1 FastMoE=0.168 SmartMoE=0.100
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 431 layer 6: round 2 FastMoE=0.167 SmartMoE=0.099
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 431 layer 6: round 3 FastMoE=0.166 SmartMoE=0.100
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter441 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 441 layer 6: round 0 FastMoE=0.225 SmartMoE=0.102
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 441 layer 6: round 1 FastMoE=0.168 SmartMoE=0.103
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 441 layer 6: round 2 FastMoE=0.166 SmartMoE=0.102
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 441 layer 6: round 3 FastMoE=0.165 SmartMoE=0.101
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter451 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 451 layer 6: round 0 FastMoE=0.214 SmartMoE=0.100
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 451 layer 6: round 1 FastMoE=0.169 SmartMoE=0.103
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 451 layer 6: round 2 FastMoE=0.167 SmartMoE=0.104
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 451 layer 6: round 3 FastMoE=0.169 SmartMoE=0.101
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter461 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 461 layer 6: round 0 FastMoE=0.210 SmartMoE=0.104
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 461 layer 6: round 1 FastMoE=0.167 SmartMoE=0.103
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 461 layer 6: round 2 FastMoE=0.172 SmartMoE=0.102
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 461 layer 6: round 3 FastMoE=0.172 SmartMoE=0.101
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter471 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 471 layer 6: round 0 FastMoE=0.173 SmartMoE=0.103
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 471 layer 6: round 1 FastMoE=0.162 SmartMoE=0.102
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 471 layer 6: round 2 FastMoE=0.165 SmartMoE=0.103
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 471 layer 6: round 3 FastMoE=0.166 SmartMoE=0.105
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter481 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 481 layer 6: round 0 FastMoE=0.212 SmartMoE=0.104
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 481 layer 6: round 1 FastMoE=0.167 SmartMoE=0.104
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 481 layer 6: round 2 FastMoE=0.171 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 481 layer 6: round 3 FastMoE=0.166 SmartMoE=0.103
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter491 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 491 layer 6: round 0 FastMoE=0.220 SmartMoE=0.102
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 491 layer 6: round 1 FastMoE=0.168 SmartMoE=0.102
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 491 layer 6: round 2 FastMoE=0.173 SmartMoE=0.102
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 491 layer 6: round 3 FastMoE=0.168 SmartMoE=0.103
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter501 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 501 layer 6: round 0 FastMoE=0.222 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 501 layer 6: round 1 FastMoE=0.171 SmartMoE=0.105
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 501 layer 6: round 2 FastMoE=0.170 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 501 layer 6: round 3 FastMoE=0.172 SmartMoE=0.105
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter511 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 511 layer 6: round 0 FastMoE=0.223 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 511 layer 6: round 1 FastMoE=0.166 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 511 layer 6: round 2 FastMoE=0.171 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 511 layer 6: round 3 FastMoE=0.167 SmartMoE=0.107
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter521 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 521 layer 6: round 0 FastMoE=0.231 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 521 layer 6: round 1 FastMoE=0.169 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 521 layer 6: round 2 FastMoE=0.167 SmartMoE=0.106
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 521 layer 6: round 3 FastMoE=0.170 SmartMoE=0.106
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter531 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 531 layer 6: round 0 FastMoE=0.228 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 531 layer 6: round 1 FastMoE=0.167 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 531 layer 6: round 2 FastMoE=0.168 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 531 layer 6: round 3 FastMoE=0.167 SmartMoE=0.107
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter541 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 541 layer 6: round 0 FastMoE=0.225 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 541 layer 6: round 1 FastMoE=0.166 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 541 layer 6: round 2 FastMoE=0.166 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 541 layer 6: round 3 FastMoE=0.168 SmartMoE=0.107
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter551 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 551 layer 6: round 0 FastMoE=0.225 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 551 layer 6: round 1 FastMoE=0.170 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 551 layer 6: round 2 FastMoE=0.172 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 551 layer 6: round 3 FastMoE=0.171 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter561 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 561 layer 6: round 0 FastMoE=0.222 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 561 layer 6: round 1 FastMoE=0.165 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 561 layer 6: round 2 FastMoE=0.165 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 561 layer 6: round 3 FastMoE=0.165 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter571 6
[11, 0, 3, 25, 8, 23, 13, 28, 18, 29, 30, 31, 5, 10, 24, 12, 6, 27, 15, 26, 20, 9, 14, 1, 17, 7, 22, 16, 21, 2, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 571 layer 6: round 0 FastMoE=0.227 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 571 layer 6: round 1 FastMoE=0.171 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 571 layer 6: round 2 FastMoE=0.167 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 571 layer 6: round 3 FastMoE=0.173 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter581 6
[INFO] mapping updated!
[11, 0, 19, 7, 8, 18, 21, 28, 2, 24, 23, 13, 14, 15, 6, 12, 26, 29, 30, 5, 20, 27, 3, 31, 25, 1, 9, 16, 10, 17, 4, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 581 layer 6: round 0 FastMoE=0.226 SmartMoE=0.194
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 581 layer 6: round 1 FastMoE=0.176 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 581 layer 6: round 2 FastMoE=0.174 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 581 layer 6: round 3 FastMoE=0.174 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter591 6
[11, 0, 19, 7, 8, 18, 21, 28, 2, 24, 23, 13, 14, 15, 6, 12, 26, 29, 30, 5, 20, 27, 3, 31, 25, 1, 9, 16, 10, 17, 4, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 591 layer 6: round 0 FastMoE=0.224 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 591 layer 6: round 1 FastMoE=0.170 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 591 layer 6: round 2 FastMoE=0.174 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 591 layer 6: round 3 FastMoE=0.172 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter601 6
[11, 0, 19, 7, 8, 18, 21, 28, 2, 24, 23, 13, 14, 15, 6, 12, 26, 29, 30, 5, 20, 27, 3, 31, 25, 1, 9, 16, 10, 17, 4, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 601 layer 6: round 0 FastMoE=0.232 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 601 layer 6: round 1 FastMoE=0.170 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 601 layer 6: round 2 FastMoE=0.171 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 601 layer 6: round 3 FastMoE=0.170 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter611 6
[11, 0, 19, 7, 8, 18, 21, 28, 2, 24, 23, 13, 14, 15, 6, 12, 26, 29, 30, 5, 20, 27, 3, 31, 25, 1, 9, 16, 10, 17, 4, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 611 layer 6: round 0 FastMoE=0.230 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 611 layer 6: round 1 FastMoE=0.169 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 611 layer 6: round 2 FastMoE=0.172 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 611 layer 6: round 3 FastMoE=0.169 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter621 6
[11, 0, 19, 7, 8, 18, 21, 28, 2, 24, 23, 13, 14, 15, 6, 12, 26, 29, 30, 5, 20, 27, 3, 31, 25, 1, 9, 16, 10, 17, 4, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 621 layer 6: round 0 FastMoE=0.233 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 621 layer 6: round 1 FastMoE=0.172 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 621 layer 6: round 2 FastMoE=0.174 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 621 layer 6: round 3 FastMoE=0.173 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter631 6
[INFO] mapping updated!
[11, 5, 13, 14, 8, 18, 19, 28, 27, 24, 21, 22, 23, 29, 7, 12, 2, 30, 31, 0, 20, 3, 25, 9, 1, 6, 10, 16, 17, 26, 4, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 631 layer 6: round 0 FastMoE=0.224 SmartMoE=0.175
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 631 layer 6: round 1 FastMoE=0.169 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 631 layer 6: round 2 FastMoE=0.172 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 631 layer 6: round 3 FastMoE=0.169 SmartMoE=0.107
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter641 6
[11, 5, 13, 14, 8, 18, 19, 28, 27, 24, 21, 22, 23, 29, 7, 12, 2, 30, 31, 0, 20, 3, 25, 9, 1, 6, 10, 16, 17, 26, 4, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 641 layer 6: round 0 FastMoE=0.236 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 641 layer 6: round 1 FastMoE=0.172 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 641 layer 6: round 2 FastMoE=0.175 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 641 layer 6: round 3 FastMoE=0.173 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter651 6
[11, 5, 13, 14, 8, 18, 19, 28, 27, 24, 21, 22, 23, 29, 7, 12, 2, 30, 31, 0, 20, 3, 25, 9, 1, 6, 10, 16, 17, 26, 4, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 651 layer 6: round 0 FastMoE=0.238 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 651 layer 6: round 1 FastMoE=0.175 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 651 layer 6: round 2 FastMoE=0.172 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 651 layer 6: round 3 FastMoE=0.175 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter661 6
[INFO] mapping updated!
[11, 25, 19, 21, 8, 22, 23, 28, 3, 24, 14, 15, 29, 30, 26, 12, 27, 31, 18, 0, 20, 7, 1, 17, 5, 2, 9, 16, 10, 6, 4, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 661 layer 6: round 0 FastMoE=0.250 SmartMoE=0.181
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 661 layer 6: round 1 FastMoE=0.180 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 661 layer 6: round 2 FastMoE=0.174 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 661 layer 6: round 3 FastMoE=0.170 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter671 6
[11, 25, 19, 21, 8, 22, 23, 28, 3, 24, 14, 15, 29, 30, 26, 12, 27, 31, 18, 0, 20, 7, 1, 17, 5, 2, 9, 16, 10, 6, 4, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 671 layer 6: round 0 FastMoE=0.335 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 671 layer 6: round 1 FastMoE=0.165 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 671 layer 6: round 2 FastMoE=0.168 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 671 layer 6: round 3 FastMoE=0.166 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter681 6
[11, 25, 19, 21, 8, 22, 23, 28, 3, 24, 14, 15, 29, 30, 26, 12, 27, 31, 18, 0, 20, 7, 1, 17, 5, 2, 9, 16, 10, 6, 4, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 681 layer 6: round 0 FastMoE=0.229 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 681 layer 6: round 1 FastMoE=0.167 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 681 layer 6: round 2 FastMoE=0.165 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 681 layer 6: round 3 FastMoE=0.162 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter691 6
[INFO] mapping updated!
[11, 1, 13, 17, 8, 18, 19, 28, 27, 24, 15, 21, 22, 23, 5, 12, 6, 29, 30, 0, 20, 31, 3, 9, 25, 7, 2, 16, 10, 26, 4, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 691 layer 6: round 0 FastMoE=0.225 SmartMoE=0.181
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 691 layer 6: round 1 FastMoE=0.171 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 691 layer 6: round 2 FastMoE=0.169 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 691 layer 6: round 3 FastMoE=0.171 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter701 6
[INFO] mapping updated!
[11, 1, 15, 13, 8, 14, 19, 28, 6, 24, 22, 23, 29, 30, 3, 12, 25, 31, 9, 0, 20, 7, 27, 18, 5, 2, 17, 16, 10, 26, 4, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 701 layer 6: round 0 FastMoE=0.225 SmartMoE=0.176
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 701 layer 6: round 1 FastMoE=0.167 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 701 layer 6: round 2 FastMoE=0.170 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 701 layer 6: round 3 FastMoE=0.167 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter711 6
[INFO] mapping updated!
[11, 5, 9, 31, 8, 29, 23, 28, 27, 24, 18, 14, 15, 21, 2, 12, 1, 22, 13, 0, 20, 17, 7, 30, 25, 6, 3, 16, 10, 26, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 711 layer 6: round 0 FastMoE=0.224 SmartMoE=0.180
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 711 layer 6: round 1 FastMoE=0.168 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 711 layer 6: round 2 FastMoE=0.165 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 711 layer 6: round 3 FastMoE=0.167 SmartMoE=0.107
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter721 6
[11, 5, 9, 31, 8, 29, 23, 28, 27, 24, 18, 14, 15, 21, 2, 12, 1, 22, 13, 0, 20, 17, 7, 30, 25, 6, 3, 16, 10, 26, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 721 layer 6: round 0 FastMoE=0.178 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 721 layer 6: round 1 FastMoE=0.169 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 721 layer 6: round 2 FastMoE=0.167 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 721 layer 6: round 3 FastMoE=0.169 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter731 6
[11, 5, 9, 31, 8, 29, 23, 28, 27, 24, 18, 14, 15, 21, 2, 12, 1, 22, 13, 0, 20, 17, 7, 30, 25, 6, 3, 16, 10, 26, 4, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 731 layer 6: round 0 FastMoE=0.243 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 731 layer 6: round 1 FastMoE=0.167 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 731 layer 6: round 2 FastMoE=0.164 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 731 layer 6: round 3 FastMoE=0.164 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter741 6
[INFO] mapping updated!
[2, 5, 9, 31, 8, 29, 23, 28, 6, 24, 14, 13, 19, 18, 26, 12, 27, 3, 17, 0, 20, 15, 21, 22, 1, 25, 30, 16, 10, 7, 4, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 741 layer 6: round 0 FastMoE=0.227 SmartMoE=0.173
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 741 layer 6: round 1 FastMoE=0.166 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 741 layer 6: round 2 FastMoE=0.161 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 741 layer 6: round 3 FastMoE=0.165 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter751 6
[2, 5, 9, 31, 8, 29, 23, 28, 6, 24, 14, 13, 19, 18, 26, 12, 27, 3, 17, 0, 20, 15, 21, 22, 1, 25, 30, 16, 10, 7, 4, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 751 layer 6: round 0 FastMoE=0.233 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 751 layer 6: round 1 FastMoE=0.164 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 751 layer 6: round 2 FastMoE=0.165 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 751 layer 6: round 3 FastMoE=0.168 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter761 6
[2, 5, 9, 31, 8, 29, 23, 28, 6, 24, 14, 13, 19, 18, 26, 12, 27, 3, 17, 0, 20, 15, 21, 22, 1, 25, 30, 16, 10, 7, 4, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 761 layer 6: round 0 FastMoE=0.231 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 761 layer 6: round 1 FastMoE=0.162 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 761 layer 6: round 2 FastMoE=0.163 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 761 layer 6: round 3 FastMoE=0.161 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter771 6
[2, 5, 9, 31, 8, 29, 23, 28, 6, 24, 14, 13, 19, 18, 26, 12, 27, 3, 17, 0, 20, 15, 21, 22, 1, 25, 30, 16, 10, 7, 4, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 771 layer 6: round 0 FastMoE=0.230 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 771 layer 6: round 1 FastMoE=0.159 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 771 layer 6: round 2 FastMoE=0.165 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 771 layer 6: round 3 FastMoE=0.163 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter781 6
[INFO] mapping updated!
[11, 25, 9, 31, 8, 29, 15, 28, 26, 24, 18, 6, 23, 22, 3, 12, 1, 13, 19, 0, 20, 21, 14, 7, 5, 2, 30, 16, 10, 27, 4, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 781 layer 6: round 0 FastMoE=0.230 SmartMoE=0.182
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 781 layer 6: round 1 FastMoE=0.165 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 781 layer 6: round 2 FastMoE=0.164 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 781 layer 6: round 3 FastMoE=0.164 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter791 6
[11, 25, 9, 31, 8, 29, 15, 28, 26, 24, 18, 6, 23, 22, 3, 12, 1, 13, 19, 0, 20, 21, 14, 7, 5, 2, 30, 16, 10, 27, 4, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 791 layer 6: round 0 FastMoE=0.680 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 791 layer 6: round 1 FastMoE=0.166 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 791 layer 6: round 2 FastMoE=0.164 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 791 layer 6: round 3 FastMoE=0.166 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter801 6
[11, 25, 9, 31, 8, 29, 15, 28, 26, 24, 18, 6, 23, 22, 3, 12, 1, 13, 19, 0, 20, 21, 14, 7, 5, 2, 30, 16, 10, 27, 4, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 801 layer 6: round 0 FastMoE=0.700 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 801 layer 6: round 1 FastMoE=0.165 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 801 layer 6: round 2 FastMoE=0.167 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 801 layer 6: round 3 FastMoE=0.167 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter811 6
[INFO] mapping updated!
[19, 1, 10, 9, 8, 30, 29, 28, 25, 24, 21, 17, 15, 14, 27, 12, 2, 13, 7, 0, 20, 22, 18, 23, 5, 26, 31, 16, 11, 3, 4, 6]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 811 layer 6: round 0 FastMoE=0.367 SmartMoE=0.176
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 811 layer 6: round 1 FastMoE=0.166 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 811 layer 6: round 2 FastMoE=0.164 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 811 layer 6: round 3 FastMoE=0.165 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter821 6
[INFO] mapping updated!
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 821 layer 6: round 0 FastMoE=0.228 SmartMoE=0.167
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 821 layer 6: round 1 FastMoE=0.166 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 821 layer 6: round 2 FastMoE=0.167 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 821 layer 6: round 3 FastMoE=0.165 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter831 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 831 layer 6: round 0 FastMoE=0.235 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 831 layer 6: round 1 FastMoE=0.164 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 831 layer 6: round 2 FastMoE=0.166 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 831 layer 6: round 3 FastMoE=0.165 SmartMoE=0.115
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter841 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 841 layer 6: round 0 FastMoE=0.229 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 841 layer 6: round 1 FastMoE=0.164 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 841 layer 6: round 2 FastMoE=0.163 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 841 layer 6: round 3 FastMoE=0.163 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter851 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 851 layer 6: round 0 FastMoE=0.222 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 851 layer 6: round 1 FastMoE=0.163 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 851 layer 6: round 2 FastMoE=0.163 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 851 layer 6: round 3 FastMoE=0.165 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter861 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 861 layer 6: round 0 FastMoE=0.211 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 861 layer 6: round 1 FastMoE=0.161 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 861 layer 6: round 2 FastMoE=0.164 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 861 layer 6: round 3 FastMoE=0.165 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter871 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 871 layer 6: round 0 FastMoE=0.217 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 871 layer 6: round 1 FastMoE=0.163 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 871 layer 6: round 2 FastMoE=0.164 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 871 layer 6: round 3 FastMoE=0.159 SmartMoE=0.107
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter881 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 881 layer 6: round 0 FastMoE=0.225 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 881 layer 6: round 1 FastMoE=0.164 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 881 layer 6: round 2 FastMoE=0.163 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 881 layer 6: round 3 FastMoE=0.161 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter891 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 891 layer 6: round 0 FastMoE=0.217 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 891 layer 6: round 1 FastMoE=0.161 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 891 layer 6: round 2 FastMoE=0.160 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 891 layer 6: round 3 FastMoE=0.157 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter901 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 901 layer 6: round 0 FastMoE=0.210 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 901 layer 6: round 1 FastMoE=0.161 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 901 layer 6: round 2 FastMoE=0.161 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 901 layer 6: round 3 FastMoE=0.160 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter911 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 911 layer 6: round 0 FastMoE=0.222 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 911 layer 6: round 1 FastMoE=0.165 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 911 layer 6: round 2 FastMoE=0.165 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 911 layer 6: round 3 FastMoE=0.165 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter921 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 921 layer 6: round 0 FastMoE=0.230 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 921 layer 6: round 1 FastMoE=0.163 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 921 layer 6: round 2 FastMoE=0.164 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 921 layer 6: round 3 FastMoE=0.163 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter931 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 931 layer 6: round 0 FastMoE=0.215 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 931 layer 6: round 1 FastMoE=0.160 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 931 layer 6: round 2 FastMoE=0.159 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 931 layer 6: round 3 FastMoE=0.158 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter941 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 941 layer 6: round 0 FastMoE=0.211 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 941 layer 6: round 1 FastMoE=0.162 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 941 layer 6: round 2 FastMoE=0.163 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 941 layer 6: round 3 FastMoE=0.163 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter951 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 951 layer 6: round 0 FastMoE=0.223 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 951 layer 6: round 1 FastMoE=0.161 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 951 layer 6: round 2 FastMoE=0.165 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 951 layer 6: round 3 FastMoE=0.160 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter961 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 961 layer 6: round 0 FastMoE=0.226 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 961 layer 6: round 1 FastMoE=0.166 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 961 layer 6: round 2 FastMoE=0.165 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 961 layer 6: round 3 FastMoE=0.164 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter971 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 971 layer 6: round 0 FastMoE=0.224 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 971 layer 6: round 1 FastMoE=0.163 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 971 layer 6: round 2 FastMoE=0.158 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 971 layer 6: round 3 FastMoE=0.159 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter981 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 981 layer 6: round 0 FastMoE=0.217 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 981 layer 6: round 1 FastMoE=0.161 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 981 layer 6: round 2 FastMoE=0.163 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 981 layer 6: round 3 FastMoE=0.161 SmartMoE=0.107
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter991 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 991 layer 6: round 0 FastMoE=0.223 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 991 layer 6: round 1 FastMoE=0.163 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 991 layer 6: round 2 FastMoE=0.162 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 991 layer 6: round 3 FastMoE=0.162 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1001 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1001 layer 6: round 0 FastMoE=0.224 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1001 layer 6: round 1 FastMoE=0.166 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1001 layer 6: round 2 FastMoE=0.165 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1001 layer 6: round 3 FastMoE=0.167 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1011 6
[27, 1, 10, 13, 8, 31, 19, 28, 25, 24, 23, 22, 21, 15, 2, 12, 6, 14, 17, 0, 20, 29, 18, 30, 5, 26, 9, 16, 11, 7, 4, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1011 layer 6: round 0 FastMoE=0.227 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1011 layer 6: round 1 FastMoE=0.165 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1011 layer 6: round 2 FastMoE=0.163 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1011 layer 6: round 3 FastMoE=0.163 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1021 6
[INFO] mapping updated!
[11, 5, 10, 9, 8, 30, 29, 28, 26, 24, 15, 14, 13, 19, 7, 12, 6, 18, 3, 0, 20, 21, 22, 23, 4, 2, 31, 16, 17, 25, 1, 27]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1021 layer 6: round 0 FastMoE=0.217 SmartMoE=0.184
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1021 layer 6: round 1 FastMoE=0.162 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1021 layer 6: round 2 FastMoE=0.161 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1021 layer 6: round 3 FastMoE=0.159 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1031 6
[INFO] mapping updated!
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1031 layer 6: round 0 FastMoE=0.217 SmartMoE=0.165
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1031 layer 6: round 1 FastMoE=0.166 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1031 layer 6: round 2 FastMoE=0.162 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1031 layer 6: round 3 FastMoE=0.160 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1041 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1041 layer 6: round 0 FastMoE=0.318 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1041 layer 6: round 1 FastMoE=0.167 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1041 layer 6: round 2 FastMoE=0.167 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1041 layer 6: round 3 FastMoE=0.166 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1051 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1051 layer 6: round 0 FastMoE=0.215 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1051 layer 6: round 1 FastMoE=0.166 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1051 layer 6: round 2 FastMoE=0.166 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1051 layer 6: round 3 FastMoE=0.167 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1061 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1061 layer 6: round 0 FastMoE=0.212 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1061 layer 6: round 1 FastMoE=0.164 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1061 layer 6: round 2 FastMoE=0.163 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1061 layer 6: round 3 FastMoE=0.162 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1071 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1071 layer 6: round 0 FastMoE=0.221 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1071 layer 6: round 1 FastMoE=0.162 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1071 layer 6: round 2 FastMoE=0.164 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1071 layer 6: round 3 FastMoE=0.163 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1081 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1081 layer 6: round 0 FastMoE=0.221 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1081 layer 6: round 1 FastMoE=0.163 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1081 layer 6: round 2 FastMoE=0.164 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1081 layer 6: round 3 FastMoE=0.163 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1091 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1091 layer 6: round 0 FastMoE=0.222 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1091 layer 6: round 1 FastMoE=0.167 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1091 layer 6: round 2 FastMoE=0.166 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1091 layer 6: round 3 FastMoE=0.165 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1101 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1101 layer 6: round 0 FastMoE=0.226 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1101 layer 6: round 1 FastMoE=0.163 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1101 layer 6: round 2 FastMoE=0.163 SmartMoE=0.107
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1101 layer 6: round 3 FastMoE=0.164 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1111 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1111 layer 6: round 0 FastMoE=0.234 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1111 layer 6: round 1 FastMoE=0.165 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1111 layer 6: round 2 FastMoE=0.167 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1111 layer 6: round 3 FastMoE=0.163 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1121 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1121 layer 6: round 0 FastMoE=0.230 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1121 layer 6: round 1 FastMoE=0.167 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1121 layer 6: round 2 FastMoE=0.167 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1121 layer 6: round 3 FastMoE=0.168 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1131 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1131 layer 6: round 0 FastMoE=0.229 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1131 layer 6: round 1 FastMoE=0.165 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1131 layer 6: round 2 FastMoE=0.162 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1131 layer 6: round 3 FastMoE=0.166 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1141 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1141 layer 6: round 0 FastMoE=0.228 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1141 layer 6: round 1 FastMoE=0.164 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1141 layer 6: round 2 FastMoE=0.163 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1141 layer 6: round 3 FastMoE=0.164 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1151 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1151 layer 6: round 0 FastMoE=0.232 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1151 layer 6: round 1 FastMoE=0.163 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1151 layer 6: round 2 FastMoE=0.164 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1151 layer 6: round 3 FastMoE=0.166 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1161 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1161 layer 6: round 0 FastMoE=0.242 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1161 layer 6: round 1 FastMoE=0.164 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1161 layer 6: round 2 FastMoE=0.164 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1161 layer 6: round 3 FastMoE=0.164 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1171 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1171 layer 6: round 0 FastMoE=0.237 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1171 layer 6: round 1 FastMoE=0.168 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1171 layer 6: round 2 FastMoE=0.170 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1171 layer 6: round 3 FastMoE=0.166 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1181 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1181 layer 6: round 0 FastMoE=0.225 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1181 layer 6: round 1 FastMoE=0.163 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1181 layer 6: round 2 FastMoE=0.165 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1181 layer 6: round 3 FastMoE=0.163 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1191 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1191 layer 6: round 0 FastMoE=0.237 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1191 layer 6: round 1 FastMoE=0.166 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1191 layer 6: round 2 FastMoE=0.167 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1191 layer 6: round 3 FastMoE=0.170 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1201 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1201 layer 6: round 0 FastMoE=0.243 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1201 layer 6: round 1 FastMoE=0.167 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1201 layer 6: round 2 FastMoE=0.162 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1201 layer 6: round 3 FastMoE=0.164 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1211 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1211 layer 6: round 0 FastMoE=0.236 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1211 layer 6: round 1 FastMoE=0.162 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1211 layer 6: round 2 FastMoE=0.164 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1211 layer 6: round 3 FastMoE=0.162 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1221 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1221 layer 6: round 0 FastMoE=0.240 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1221 layer 6: round 1 FastMoE=0.166 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1221 layer 6: round 2 FastMoE=0.167 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1221 layer 6: round 3 FastMoE=0.168 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1231 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1231 layer 6: round 0 FastMoE=0.235 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1231 layer 6: round 1 FastMoE=0.166 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1231 layer 6: round 2 FastMoE=0.164 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1231 layer 6: round 3 FastMoE=0.167 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1241 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1241 layer 6: round 0 FastMoE=0.230 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1241 layer 6: round 1 FastMoE=0.167 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1241 layer 6: round 2 FastMoE=0.169 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1241 layer 6: round 3 FastMoE=0.169 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1251 6
[11, 6, 10, 27, 8, 30, 29, 28, 1, 24, 21, 15, 14, 17, 7, 12, 25, 22, 19, 0, 20, 3, 23, 31, 4, 2, 9, 16, 13, 26, 5, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1251 layer 6: round 0 FastMoE=0.229 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1251 layer 6: round 1 FastMoE=0.168 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1251 layer 6: round 2 FastMoE=0.169 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1251 layer 6: round 3 FastMoE=0.168 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1261 6
[INFO] mapping updated!
[11, 25, 13, 3, 8, 19, 14, 28, 5, 24, 21, 2, 22, 23, 7, 12, 26, 1, 29, 0, 20, 27, 30, 31, 4, 9, 18, 16, 10, 6, 17, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1261 layer 6: round 0 FastMoE=0.222 SmartMoE=0.189
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1261 layer 6: round 1 FastMoE=0.164 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1261 layer 6: round 2 FastMoE=0.162 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1261 layer 6: round 3 FastMoE=0.164 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1271 6
[11, 25, 13, 3, 8, 19, 14, 28, 5, 24, 21, 2, 22, 23, 7, 12, 26, 1, 29, 0, 20, 27, 30, 31, 4, 9, 18, 16, 10, 6, 17, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1271 layer 6: round 0 FastMoE=0.238 SmartMoE=0.108
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1271 layer 6: round 1 FastMoE=0.167 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1271 layer 6: round 2 FastMoE=0.166 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1271 layer 6: round 3 FastMoE=0.168 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1281 6
[11, 25, 13, 3, 8, 19, 14, 28, 5, 24, 21, 2, 22, 23, 7, 12, 26, 1, 29, 0, 20, 27, 30, 31, 4, 9, 18, 16, 10, 6, 17, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1281 layer 6: round 0 FastMoE=0.227 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1281 layer 6: round 1 FastMoE=0.165 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1281 layer 6: round 2 FastMoE=0.166 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1281 layer 6: round 3 FastMoE=0.163 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1291 6
[INFO] mapping updated!
[11, 25, 13, 19, 8, 15, 14, 28, 6, 24, 22, 18, 17, 23, 7, 12, 1, 2, 29, 0, 20, 27, 30, 31, 4, 3, 9, 16, 10, 26, 5, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1291 layer 6: round 0 FastMoE=0.241 SmartMoE=0.172
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1291 layer 6: round 1 FastMoE=0.169 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1291 layer 6: round 2 FastMoE=0.166 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1291 layer 6: round 3 FastMoE=0.169 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1301 6
[INFO] mapping updated!
[11, 25, 22, 21, 8, 13, 17, 28, 5, 24, 15, 14, 19, 29, 7, 12, 6, 2, 30, 0, 20, 3, 31, 9, 4, 27, 18, 16, 10, 26, 1, 23]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1301 layer 6: round 0 FastMoE=0.221 SmartMoE=0.175
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1301 layer 6: round 1 FastMoE=0.169 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1301 layer 6: round 2 FastMoE=0.167 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1301 layer 6: round 3 FastMoE=0.169 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1311 6
[11, 25, 22, 21, 8, 13, 17, 28, 5, 24, 15, 14, 19, 29, 7, 12, 6, 2, 30, 0, 20, 3, 31, 9, 4, 27, 18, 16, 10, 26, 1, 23]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1311 layer 6: round 0 FastMoE=0.223 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1311 layer 6: round 1 FastMoE=0.166 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1311 layer 6: round 2 FastMoE=0.169 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1311 layer 6: round 3 FastMoE=0.167 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1321 6
[11, 25, 22, 21, 8, 13, 17, 28, 5, 24, 15, 14, 19, 29, 7, 12, 6, 2, 30, 0, 20, 3, 31, 9, 4, 27, 18, 16, 10, 26, 1, 23]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1321 layer 6: round 0 FastMoE=0.230 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1321 layer 6: round 1 FastMoE=0.166 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1321 layer 6: round 2 FastMoE=0.166 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1321 layer 6: round 3 FastMoE=0.167 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1331 6
[INFO] mapping updated!
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1331 layer 6: round 0 FastMoE=0.240 SmartMoE=0.192
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1331 layer 6: round 1 FastMoE=0.171 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1331 layer 6: round 2 FastMoE=0.170 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1331 layer 6: round 3 FastMoE=0.171 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1341 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1341 layer 6: round 0 FastMoE=0.627 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1341 layer 6: round 1 FastMoE=0.164 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1341 layer 6: round 2 FastMoE=0.164 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1341 layer 6: round 3 FastMoE=0.166 SmartMoE=0.108
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1351 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1351 layer 6: round 0 FastMoE=0.232 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1351 layer 6: round 1 FastMoE=0.166 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1351 layer 6: round 2 FastMoE=0.166 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1351 layer 6: round 3 FastMoE=0.166 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1361 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1361 layer 6: round 0 FastMoE=0.231 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1361 layer 6: round 1 FastMoE=0.168 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1361 layer 6: round 2 FastMoE=0.169 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1361 layer 6: round 3 FastMoE=0.168 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1371 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1371 layer 6: round 0 FastMoE=0.229 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1371 layer 6: round 1 FastMoE=0.168 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1371 layer 6: round 2 FastMoE=0.168 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1371 layer 6: round 3 FastMoE=0.165 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1381 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1381 layer 6: round 0 FastMoE=0.237 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1381 layer 6: round 1 FastMoE=0.168 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1381 layer 6: round 2 FastMoE=0.165 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1381 layer 6: round 3 FastMoE=0.165 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1391 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1391 layer 6: round 0 FastMoE=0.226 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1391 layer 6: round 1 FastMoE=0.166 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1391 layer 6: round 2 FastMoE=0.169 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1391 layer 6: round 3 FastMoE=0.167 SmartMoE=0.109
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1401 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1401 layer 6: round 0 FastMoE=0.228 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1401 layer 6: round 1 FastMoE=0.168 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1401 layer 6: round 2 FastMoE=0.165 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1401 layer 6: round 3 FastMoE=0.166 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1411 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1411 layer 6: round 0 FastMoE=0.232 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1411 layer 6: round 1 FastMoE=0.165 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1411 layer 6: round 2 FastMoE=0.166 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1411 layer 6: round 3 FastMoE=0.169 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1421 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1421 layer 6: round 0 FastMoE=0.231 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1421 layer 6: round 1 FastMoE=0.167 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1421 layer 6: round 2 FastMoE=0.168 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1421 layer 6: round 3 FastMoE=0.168 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1431 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1431 layer 6: round 0 FastMoE=0.240 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1431 layer 6: round 1 FastMoE=0.163 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1431 layer 6: round 2 FastMoE=0.165 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1431 layer 6: round 3 FastMoE=0.166 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1441 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1441 layer 6: round 0 FastMoE=0.226 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1441 layer 6: round 1 FastMoE=0.167 SmartMoE=0.109
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1441 layer 6: round 2 FastMoE=0.168 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1441 layer 6: round 3 FastMoE=0.166 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1451 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1451 layer 6: round 0 FastMoE=0.223 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1451 layer 6: round 1 FastMoE=0.167 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1451 layer 6: round 2 FastMoE=0.163 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1451 layer 6: round 3 FastMoE=0.165 SmartMoE=0.111
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1461 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1461 layer 6: round 0 FastMoE=0.218 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1461 layer 6: round 1 FastMoE=0.164 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1461 layer 6: round 2 FastMoE=0.164 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1461 layer 6: round 3 FastMoE=0.165 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1471 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1471 layer 6: round 0 FastMoE=0.224 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1471 layer 6: round 1 FastMoE=0.165 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1471 layer 6: round 2 FastMoE=0.168 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1471 layer 6: round 3 FastMoE=0.167 SmartMoE=0.115
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1481 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1481 layer 6: round 0 FastMoE=0.229 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1481 layer 6: round 1 FastMoE=0.168 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1481 layer 6: round 2 FastMoE=0.169 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1481 layer 6: round 3 FastMoE=0.166 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1491 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1491 layer 6: round 0 FastMoE=0.226 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1491 layer 6: round 1 FastMoE=0.166 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1491 layer 6: round 2 FastMoE=0.164 SmartMoE=0.110
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1491 layer 6: round 3 FastMoE=0.163 SmartMoE=0.110
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1501 6
[11, 5, 7, 19, 8, 13, 14, 28, 6, 24, 21, 22, 23, 29, 27, 12, 25, 26, 30, 0, 20, 17, 31, 9, 4, 2, 10, 16, 3, 1, 18, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1501 layer 6: round 0 FastMoE=0.229 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1501 layer 6: round 1 FastMoE=0.167 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1501 layer 6: round 2 FastMoE=0.166 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1501 layer 6: round 3 FastMoE=0.168 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1511 6
[INFO] mapping updated!
[11, 5, 13, 19, 8, 14, 15, 28, 26, 24, 22, 23, 3, 29, 2, 12, 17, 1, 30, 0, 20, 27, 18, 7, 4, 31, 9, 16, 10, 6, 25, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1511 layer 6: round 0 FastMoE=0.231 SmartMoE=0.184
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1511 layer 6: round 1 FastMoE=0.168 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1511 layer 6: round 2 FastMoE=0.166 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1511 layer 6: round 3 FastMoE=0.166 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1521 6
[11, 5, 13, 19, 8, 14, 15, 28, 26, 24, 22, 23, 3, 29, 2, 12, 17, 1, 30, 0, 20, 27, 18, 7, 4, 31, 9, 16, 10, 6, 25, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1521 layer 6: round 0 FastMoE=0.220 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1521 layer 6: round 1 FastMoE=0.168 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1521 layer 6: round 2 FastMoE=0.168 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1521 layer 6: round 3 FastMoE=0.167 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1531 6
[INFO] mapping updated!
[11, 6, 10, 27, 8, 30, 29, 28, 25, 24, 23, 22, 3, 13, 1, 12, 5, 17, 14, 0, 20, 7, 15, 31, 4, 2, 9, 16, 21, 18, 26, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1531 layer 6: round 0 FastMoE=0.222 SmartMoE=0.180
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1531 layer 6: round 1 FastMoE=0.170 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1531 layer 6: round 2 FastMoE=0.167 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1531 layer 6: round 3 FastMoE=0.167 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1541 6
[INFO] mapping updated!
[11, 25, 18, 27, 8, 13, 19, 28, 1, 24, 15, 21, 3, 22, 17, 12, 6, 7, 23, 0, 20, 2, 29, 30, 4, 31, 9, 16, 10, 26, 5, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1541 layer 6: round 0 FastMoE=0.227 SmartMoE=0.175
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1541 layer 6: round 1 FastMoE=0.169 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1541 layer 6: round 2 FastMoE=0.169 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1541 layer 6: round 3 FastMoE=0.169 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1551 6
[INFO] mapping updated!
[11, 25, 13, 18, 8, 14, 15, 28, 1, 24, 22, 23, 2, 29, 7, 12, 5, 27, 30, 0, 20, 6, 19, 31, 4, 9, 10, 16, 3, 17, 26, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1551 layer 6: round 0 FastMoE=0.230 SmartMoE=0.185
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1551 layer 6: round 1 FastMoE=0.165 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1551 layer 6: round 2 FastMoE=0.165 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1551 layer 6: round 3 FastMoE=0.166 SmartMoE=0.115
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1561 6
[11, 25, 13, 18, 8, 14, 15, 28, 1, 24, 22, 23, 2, 29, 7, 12, 5, 27, 30, 0, 20, 6, 19, 31, 4, 9, 10, 16, 3, 17, 26, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1561 layer 6: round 0 FastMoE=0.232 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1561 layer 6: round 1 FastMoE=0.171 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1561 layer 6: round 2 FastMoE=0.169 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1561 layer 6: round 3 FastMoE=0.171 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1571 6
[11, 25, 13, 18, 8, 14, 15, 28, 1, 24, 22, 23, 2, 29, 7, 12, 5, 27, 30, 0, 20, 6, 19, 31, 4, 9, 10, 16, 3, 17, 26, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1571 layer 6: round 0 FastMoE=0.224 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1571 layer 6: round 1 FastMoE=0.165 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1571 layer 6: round 2 FastMoE=0.164 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1571 layer 6: round 3 FastMoE=0.164 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1581 6
[11, 25, 13, 18, 8, 14, 15, 28, 1, 24, 22, 23, 2, 29, 7, 12, 5, 27, 30, 0, 20, 6, 19, 31, 4, 9, 10, 16, 3, 17, 26, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1581 layer 6: round 0 FastMoE=0.232 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1581 layer 6: round 1 FastMoE=0.169 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1581 layer 6: round 2 FastMoE=0.172 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1581 layer 6: round 3 FastMoE=0.170 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1591 6
[11, 25, 13, 18, 8, 14, 15, 28, 1, 24, 22, 23, 2, 29, 7, 12, 5, 27, 30, 0, 20, 6, 19, 31, 4, 9, 10, 16, 3, 17, 26, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1591 layer 6: round 0 FastMoE=0.231 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1591 layer 6: round 1 FastMoE=0.173 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1591 layer 6: round 2 FastMoE=0.173 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1591 layer 6: round 3 FastMoE=0.172 SmartMoE=0.114
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1601 6
[11, 25, 13, 18, 8, 14, 15, 28, 1, 24, 22, 23, 2, 29, 7, 12, 5, 27, 30, 0, 20, 6, 19, 31, 4, 9, 10, 16, 3, 17, 26, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1601 layer 6: round 0 FastMoE=0.230 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1601 layer 6: round 1 FastMoE=0.167 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1601 layer 6: round 2 FastMoE=0.168 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1601 layer 6: round 3 FastMoE=0.168 SmartMoE=0.112
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1611 6
[INFO] mapping updated!
[14, 5, 10, 9, 8, 13, 3, 28, 25, 24, 22, 21, 27, 15, 17, 12, 6, 2, 23, 0, 20, 7, 19, 29, 4, 30, 31, 16, 11, 1, 26, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1611 layer 6: round 0 FastMoE=0.223 SmartMoE=0.188
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1611 layer 6: round 1 FastMoE=0.172 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1611 layer 6: round 2 FastMoE=0.169 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1611 layer 6: round 3 FastMoE=0.171 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1621 6
[INFO] mapping updated!
[11, 5, 14, 21, 8, 15, 13, 28, 25, 24, 23, 3, 2, 29, 18, 12, 6, 17, 30, 0, 20, 27, 19, 31, 4, 9, 10, 16, 1, 26, 7, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1621 layer 6: round 0 FastMoE=0.229 SmartMoE=0.179
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1621 layer 6: round 1 FastMoE=0.168 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1621 layer 6: round 2 FastMoE=0.166 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1621 layer 6: round 3 FastMoE=0.169 SmartMoE=0.115
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1631 6
[11, 5, 14, 21, 8, 15, 13, 28, 25, 24, 23, 3, 2, 29, 18, 12, 6, 17, 30, 0, 20, 27, 19, 31, 4, 9, 10, 16, 1, 26, 7, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1631 layer 6: round 0 FastMoE=0.390 SmartMoE=0.112
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1631 layer 6: round 1 FastMoE=0.172 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1631 layer 6: round 2 FastMoE=0.169 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1631 layer 6: round 3 FastMoE=0.171 SmartMoE=0.114
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1641 6
[11, 5, 14, 21, 8, 15, 13, 28, 25, 24, 23, 3, 2, 29, 18, 12, 6, 17, 30, 0, 20, 27, 19, 31, 4, 9, 10, 16, 1, 26, 7, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1641 layer 6: round 0 FastMoE=0.227 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1641 layer 6: round 1 FastMoE=0.171 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1641 layer 6: round 2 FastMoE=0.170 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1641 layer 6: round 3 FastMoE=0.169 SmartMoE=0.113
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1651 6
[11, 5, 14, 21, 8, 15, 13, 28, 25, 24, 23, 3, 2, 29, 18, 12, 6, 17, 30, 0, 20, 27, 19, 31, 4, 9, 10, 16, 1, 26, 7, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1651 layer 6: round 0 FastMoE=0.227 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1651 layer 6: round 1 FastMoE=0.171 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1651 layer 6: round 2 FastMoE=0.168 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1651 layer 6: round 3 FastMoE=0.172 SmartMoE=0.114
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1661 6
[11, 5, 14, 21, 8, 15, 13, 28, 25, 24, 23, 3, 2, 29, 18, 12, 6, 17, 30, 0, 20, 27, 19, 31, 4, 9, 10, 16, 1, 26, 7, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1661 layer 6: round 0 FastMoE=0.231 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1661 layer 6: round 1 FastMoE=0.167 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1661 layer 6: round 2 FastMoE=0.166 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1661 layer 6: round 3 FastMoE=0.166 SmartMoE=0.116
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1671 6
[11, 5, 14, 21, 8, 15, 13, 28, 25, 24, 23, 3, 2, 29, 18, 12, 6, 17, 30, 0, 20, 27, 19, 31, 4, 9, 10, 16, 1, 26, 7, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1671 layer 6: round 0 FastMoE=0.235 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1671 layer 6: round 1 FastMoE=0.168 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1671 layer 6: round 2 FastMoE=0.167 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1671 layer 6: round 3 FastMoE=0.168 SmartMoE=0.116
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1681 6
[11, 5, 14, 21, 8, 15, 13, 28, 25, 24, 23, 3, 2, 29, 18, 12, 6, 17, 30, 0, 20, 27, 19, 31, 4, 9, 10, 16, 1, 26, 7, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1681 layer 6: round 0 FastMoE=0.230 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1681 layer 6: round 1 FastMoE=0.170 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1681 layer 6: round 2 FastMoE=0.171 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1681 layer 6: round 3 FastMoE=0.168 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1691 6
[11, 5, 14, 21, 8, 15, 13, 28, 25, 24, 23, 3, 2, 29, 18, 12, 6, 17, 30, 0, 20, 27, 19, 31, 4, 9, 10, 16, 1, 26, 7, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1691 layer 6: round 0 FastMoE=0.228 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1691 layer 6: round 1 FastMoE=0.168 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1691 layer 6: round 2 FastMoE=0.168 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1691 layer 6: round 3 FastMoE=0.169 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1701 6
[11, 5, 14, 21, 8, 15, 13, 28, 25, 24, 23, 3, 2, 29, 18, 12, 6, 17, 30, 0, 20, 27, 19, 31, 4, 9, 10, 16, 1, 26, 7, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1701 layer 6: round 0 FastMoE=0.243 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1701 layer 6: round 1 FastMoE=0.169 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1701 layer 6: round 2 FastMoE=0.166 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1701 layer 6: round 3 FastMoE=0.169 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1711 6
[INFO] mapping updated!
[11, 1, 9, 3, 8, 30, 29, 28, 26, 24, 21, 15, 27, 14, 17, 12, 5, 6, 22, 0, 20, 7, 19, 23, 4, 13, 31, 16, 10, 2, 25, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1711 layer 6: round 0 FastMoE=0.237 SmartMoE=0.188
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1711 layer 6: round 1 FastMoE=0.172 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1711 layer 6: round 2 FastMoE=0.174 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1711 layer 6: round 3 FastMoE=0.172 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1721 6
[INFO] mapping updated!
[21, 5, 2, 10, 8, 9, 31, 28, 26, 24, 23, 14, 1, 22, 19, 12, 25, 3, 29, 0, 20, 27, 17, 30, 4, 13, 11, 16, 15, 6, 7, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1721 layer 6: round 0 FastMoE=0.242 SmartMoE=0.189
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1721 layer 6: round 1 FastMoE=0.168 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1721 layer 6: round 2 FastMoE=0.171 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1721 layer 6: round 3 FastMoE=0.169 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1731 6
[INFO] mapping updated!
[11, 25, 9, 19, 8, 29, 23, 28, 26, 24, 15, 13, 2, 14, 3, 12, 5, 17, 21, 0, 20, 1, 7, 22, 4, 30, 31, 16, 10, 18, 6, 27]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1731 layer 6: round 0 FastMoE=0.268 SmartMoE=0.193
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1731 layer 6: round 1 FastMoE=0.176 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1731 layer 6: round 2 FastMoE=0.169 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1731 layer 6: round 3 FastMoE=0.172 SmartMoE=0.116
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1741 6
[11, 25, 9, 19, 8, 29, 23, 28, 26, 24, 15, 13, 2, 14, 3, 12, 5, 17, 21, 0, 20, 1, 7, 22, 4, 30, 31, 16, 10, 18, 6, 27]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1741 layer 6: round 0 FastMoE=0.248 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1741 layer 6: round 1 FastMoE=0.175 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1741 layer 6: round 2 FastMoE=0.174 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1741 layer 6: round 3 FastMoE=0.173 SmartMoE=0.116
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1751 6
[INFO] mapping updated!
[13, 17, 19, 23, 8, 2, 31, 28, 25, 24, 15, 14, 1, 29, 21, 12, 26, 3, 30, 0, 20, 18, 22, 9, 5, 10, 7, 16, 11, 6, 4, 27]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1751 layer 6: round 0 FastMoE=0.233 SmartMoE=0.190
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1751 layer 6: round 1 FastMoE=0.166 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1751 layer 6: round 2 FastMoE=0.168 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1751 layer 6: round 3 FastMoE=0.166 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1761 6
[INFO] mapping updated!
[11, 25, 15, 14, 8, 31, 30, 28, 26, 24, 22, 21, 27, 3, 19, 12, 6, 1, 23, 0, 20, 17, 18, 13, 4, 29, 9, 16, 10, 7, 5, 2]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1761 layer 6: round 0 FastMoE=0.235 SmartMoE=0.178
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1761 layer 6: round 1 FastMoE=0.171 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1761 layer 6: round 2 FastMoE=0.169 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1761 layer 6: round 3 FastMoE=0.169 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1771 6
[INFO] mapping updated!
[14, 25, 13, 11, 8, 9, 31, 28, 5, 24, 22, 21, 7, 15, 3, 12, 26, 27, 23, 0, 20, 2, 17, 29, 4, 30, 10, 16, 19, 1, 6, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1771 layer 6: round 0 FastMoE=0.235 SmartMoE=0.182
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1771 layer 6: round 1 FastMoE=0.174 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1771 layer 6: round 2 FastMoE=0.174 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1771 layer 6: round 3 FastMoE=0.171 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1781 6
[14, 25, 13, 11, 8, 9, 31, 28, 5, 24, 22, 21, 7, 15, 3, 12, 26, 27, 23, 0, 20, 2, 17, 29, 4, 30, 10, 16, 19, 1, 6, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1781 layer 6: round 0 FastMoE=0.232 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1781 layer 6: round 1 FastMoE=0.168 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1781 layer 6: round 2 FastMoE=0.167 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1781 layer 6: round 3 FastMoE=0.168 SmartMoE=0.114
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1791 6
[14, 25, 13, 11, 8, 9, 31, 28, 5, 24, 22, 21, 7, 15, 3, 12, 26, 27, 23, 0, 20, 2, 17, 29, 4, 30, 10, 16, 19, 1, 6, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1791 layer 6: round 0 FastMoE=0.236 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1791 layer 6: round 1 FastMoE=0.168 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1791 layer 6: round 2 FastMoE=0.171 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1791 layer 6: round 3 FastMoE=0.175 SmartMoE=0.116
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1801 6
[INFO] mapping updated!
[11, 25, 19, 9, 8, 30, 29, 28, 26, 24, 21, 2, 7, 15, 1, 12, 5, 13, 22, 0, 20, 17, 18, 23, 4, 31, 14, 16, 10, 6, 27, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1801 layer 6: round 0 FastMoE=0.225 SmartMoE=0.177
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1801 layer 6: round 1 FastMoE=0.166 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1801 layer 6: round 2 FastMoE=0.166 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1801 layer 6: round 3 FastMoE=0.166 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1811 6
[INFO] mapping updated!
[22, 5, 3, 27, 8, 21, 29, 28, 1, 24, 13, 14, 2, 15, 18, 12, 25, 7, 30, 0, 20, 19, 26, 23, 4, 31, 9, 16, 10, 17, 6, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1811 layer 6: round 0 FastMoE=0.226 SmartMoE=0.181
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1811 layer 6: round 1 FastMoE=0.174 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1811 layer 6: round 2 FastMoE=0.174 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1811 layer 6: round 3 FastMoE=0.174 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1821 6
[INFO] mapping updated!
[11, 5, 19, 10, 8, 31, 30, 28, 26, 24, 22, 21, 2, 14, 27, 12, 25, 7, 15, 0, 20, 1, 17, 23, 4, 29, 9, 16, 13, 6, 18, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1821 layer 6: round 0 FastMoE=0.231 SmartMoE=0.181
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1821 layer 6: round 1 FastMoE=0.173 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1821 layer 6: round 2 FastMoE=0.172 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1821 layer 6: round 3 FastMoE=0.170 SmartMoE=0.114
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1831 6
[INFO] mapping updated!
[11, 5, 3, 2, 8, 30, 29, 28, 27, 24, 23, 13, 7, 14, 19, 12, 25, 1, 15, 0, 20, 6, 18, 31, 4, 9, 10, 16, 22, 26, 17, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1831 layer 6: round 0 FastMoE=0.228 SmartMoE=0.173
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1831 layer 6: round 1 FastMoE=0.168 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1831 layer 6: round 2 FastMoE=0.169 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1831 layer 6: round 3 FastMoE=0.167 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1841 6
[11, 5, 3, 2, 8, 30, 29, 28, 27, 24, 23, 13, 7, 14, 19, 12, 25, 1, 15, 0, 20, 6, 18, 31, 4, 9, 10, 16, 22, 26, 17, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1841 layer 6: round 0 FastMoE=0.232 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1841 layer 6: round 1 FastMoE=0.168 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1841 layer 6: round 2 FastMoE=0.169 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1841 layer 6: round 3 FastMoE=0.168 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1851 6
[INFO] mapping updated!
[11, 25, 7, 3, 8, 14, 15, 28, 5, 24, 22, 23, 18, 29, 19, 12, 26, 17, 30, 0, 20, 27, 2, 31, 4, 9, 13, 16, 10, 1, 6, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1851 layer 6: round 0 FastMoE=0.226 SmartMoE=0.183
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1851 layer 6: round 1 FastMoE=0.169 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1851 layer 6: round 2 FastMoE=0.170 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1851 layer 6: round 3 FastMoE=0.168 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1861 6
[11, 25, 7, 3, 8, 14, 15, 28, 5, 24, 22, 23, 18, 29, 19, 12, 26, 17, 30, 0, 20, 27, 2, 31, 4, 9, 13, 16, 10, 1, 6, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1861 layer 6: round 0 FastMoE=0.247 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1861 layer 6: round 1 FastMoE=0.167 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1861 layer 6: round 2 FastMoE=0.167 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1861 layer 6: round 3 FastMoE=0.168 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1871 6
[11, 25, 7, 3, 8, 14, 15, 28, 5, 24, 22, 23, 18, 29, 19, 12, 26, 17, 30, 0, 20, 27, 2, 31, 4, 9, 13, 16, 10, 1, 6, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1871 layer 6: round 0 FastMoE=0.252 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1871 layer 6: round 1 FastMoE=0.175 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1871 layer 6: round 2 FastMoE=0.174 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1871 layer 6: round 3 FastMoE=0.173 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1881 6
[11, 25, 7, 3, 8, 14, 15, 28, 5, 24, 22, 23, 18, 29, 19, 12, 26, 17, 30, 0, 20, 27, 2, 31, 4, 9, 13, 16, 10, 1, 6, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1881 layer 6: round 0 FastMoE=0.242 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1881 layer 6: round 1 FastMoE=0.174 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1881 layer 6: round 2 FastMoE=0.168 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1881 layer 6: round 3 FastMoE=0.170 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1891 6
[INFO] mapping updated!
[19, 25, 10, 18, 8, 31, 30, 28, 26, 24, 21, 15, 6, 13, 27, 12, 5, 7, 14, 0, 20, 3, 22, 23, 4, 29, 9, 16, 11, 1, 2, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1891 layer 6: round 0 FastMoE=0.254 SmartMoE=0.191
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1891 layer 6: round 1 FastMoE=0.173 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1891 layer 6: round 2 FastMoE=0.173 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1891 layer 6: round 3 FastMoE=0.175 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1901 6
[INFO] mapping updated!
[11, 5, 10, 9, 8, 30, 29, 28, 1, 24, 17, 18, 6, 23, 27, 12, 25, 7, 13, 0, 20, 2, 3, 14, 4, 15, 31, 16, 22, 26, 21, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1901 layer 6: round 0 FastMoE=0.242 SmartMoE=0.191
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1901 layer 6: round 1 FastMoE=0.179 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1901 layer 6: round 2 FastMoE=0.177 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1901 layer 6: round 3 FastMoE=0.176 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1911 6
[INFO] mapping updated!
[11, 25, 3, 9, 8, 30, 29, 28, 17, 24, 23, 1, 26, 13, 27, 12, 6, 19, 14, 0, 20, 7, 2, 15, 4, 31, 10, 16, 22, 18, 5, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1911 layer 6: round 0 FastMoE=0.229 SmartMoE=0.173
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1911 layer 6: round 1 FastMoE=0.169 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1911 layer 6: round 2 FastMoE=0.168 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1911 layer 6: round 3 FastMoE=0.168 SmartMoE=0.115
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1921 6
[INFO] mapping updated!
[11, 5, 7, 18, 8, 14, 13, 28, 1, 24, 21, 22, 6, 23, 19, 12, 25, 27, 29, 0, 20, 3, 17, 30, 4, 31, 9, 16, 10, 2, 26, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1921 layer 6: round 0 FastMoE=0.240 SmartMoE=0.179
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1921 layer 6: round 1 FastMoE=0.170 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1921 layer 6: round 2 FastMoE=0.171 SmartMoE=0.111
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1921 layer 6: round 3 FastMoE=0.170 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1931 6
[11, 5, 7, 18, 8, 14, 13, 28, 1, 24, 21, 22, 6, 23, 19, 12, 25, 27, 29, 0, 20, 3, 17, 30, 4, 31, 9, 16, 10, 2, 26, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1931 layer 6: round 0 FastMoE=0.230 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1931 layer 6: round 1 FastMoE=0.171 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1931 layer 6: round 2 FastMoE=0.168 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1931 layer 6: round 3 FastMoE=0.167 SmartMoE=0.114
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1941 6
[INFO] mapping updated!
[11, 25, 7, 3, 8, 30, 29, 28, 1, 24, 13, 22, 19, 14, 18, 12, 5, 27, 15, 0, 20, 17, 23, 31, 4, 9, 10, 16, 2, 26, 6, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1941 layer 6: round 0 FastMoE=0.226 SmartMoE=0.187
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1941 layer 6: round 1 FastMoE=0.171 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1941 layer 6: round 2 FastMoE=0.172 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1941 layer 6: round 3 FastMoE=0.174 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1951 6
[11, 25, 7, 3, 8, 30, 29, 28, 1, 24, 13, 22, 19, 14, 18, 12, 5, 27, 15, 0, 20, 17, 23, 31, 4, 9, 10, 16, 2, 26, 6, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1951 layer 6: round 0 FastMoE=0.213 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1951 layer 6: round 1 FastMoE=0.176 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1951 layer 6: round 2 FastMoE=0.176 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1951 layer 6: round 3 FastMoE=0.176 SmartMoE=0.114
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1961 6
[11, 25, 7, 3, 8, 30, 29, 28, 1, 24, 13, 22, 19, 14, 18, 12, 5, 27, 15, 0, 20, 17, 23, 31, 4, 9, 10, 16, 2, 26, 6, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1961 layer 6: round 0 FastMoE=0.208 SmartMoE=0.113
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1961 layer 6: round 1 FastMoE=0.171 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1961 layer 6: round 2 FastMoE=0.171 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1961 layer 6: round 3 FastMoE=0.171 SmartMoE=0.116
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1971 6
[11, 25, 7, 3, 8, 30, 29, 28, 1, 24, 13, 22, 19, 14, 18, 12, 5, 27, 15, 0, 20, 17, 23, 31, 4, 9, 10, 16, 2, 26, 6, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1971 layer 6: round 0 FastMoE=0.208 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1971 layer 6: round 1 FastMoE=0.173 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1971 layer 6: round 2 FastMoE=0.174 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1971 layer 6: round 3 FastMoE=0.172 SmartMoE=0.115
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1981 6
[INFO] mapping updated!
[21, 6, 19, 10, 8, 31, 30, 28, 1, 24, 13, 3, 2, 22, 17, 12, 5, 27, 23, 0, 20, 7, 14, 15, 4, 29, 9, 16, 11, 26, 25, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1981 layer 6: round 0 FastMoE=0.216 SmartMoE=0.184
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1981 layer 6: round 1 FastMoE=0.173 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1981 layer 6: round 2 FastMoE=0.170 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1981 layer 6: round 3 FastMoE=0.175 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1991 6
[21, 6, 19, 10, 8, 31, 30, 28, 1, 24, 13, 3, 2, 22, 17, 12, 5, 27, 23, 0, 20, 7, 14, 15, 4, 29, 9, 16, 11, 26, 25, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1991 layer 6: round 0 FastMoE=0.213 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1991 layer 6: round 1 FastMoE=0.171 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1991 layer 6: round 2 FastMoE=0.170 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1991 layer 6: round 3 FastMoE=0.173 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2001 6
[21, 6, 19, 10, 8, 31, 30, 28, 1, 24, 13, 3, 2, 22, 17, 12, 5, 27, 23, 0, 20, 7, 14, 15, 4, 29, 9, 16, 11, 26, 25, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2001 layer 6: round 0 FastMoE=0.216 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2001 layer 6: round 1 FastMoE=0.171 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2001 layer 6: round 2 FastMoE=0.174 SmartMoE=0.114
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2001 layer 6: round 3 FastMoE=0.170 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2011 6
[INFO] mapping updated!
[11, 5, 18, 19, 8, 14, 13, 28, 26, 24, 21, 22, 6, 23, 3, 12, 25, 2, 29, 0, 20, 7, 17, 30, 4, 31, 9, 16, 10, 27, 1, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2011 layer 6: round 0 FastMoE=0.224 SmartMoE=0.185
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2011 layer 6: round 1 FastMoE=0.176 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2011 layer 6: round 2 FastMoE=0.175 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2011 layer 6: round 3 FastMoE=0.175 SmartMoE=0.116
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2021 6
[11, 5, 18, 19, 8, 14, 13, 28, 26, 24, 21, 22, 6, 23, 3, 12, 25, 2, 29, 0, 20, 7, 17, 30, 4, 31, 9, 16, 10, 27, 1, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2021 layer 6: round 0 FastMoE=0.243 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2021 layer 6: round 1 FastMoE=0.176 SmartMoE=0.115
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2021 layer 6: round 2 FastMoE=0.175 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2021 layer 6: round 3 FastMoE=0.174 SmartMoE=0.115
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2031 6
[11, 5, 18, 19, 8, 14, 13, 28, 26, 24, 21, 22, 6, 23, 3, 12, 25, 2, 29, 0, 20, 7, 17, 30, 4, 31, 9, 16, 10, 27, 1, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2031 layer 6: round 0 FastMoE=0.236 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2031 layer 6: round 1 FastMoE=0.169 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2031 layer 6: round 2 FastMoE=0.170 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2031 layer 6: round 3 FastMoE=0.171 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2041 6
[11, 5, 18, 19, 8, 14, 13, 28, 26, 24, 21, 22, 6, 23, 3, 12, 25, 2, 29, 0, 20, 7, 17, 30, 4, 31, 9, 16, 10, 27, 1, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2041 layer 6: round 0 FastMoE=0.244 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2041 layer 6: round 1 FastMoE=0.174 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2041 layer 6: round 2 FastMoE=0.174 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2041 layer 6: round 3 FastMoE=0.174 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2051 6
[INFO] mapping updated!
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2051 layer 6: round 0 FastMoE=0.241 SmartMoE=0.176
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2051 layer 6: round 1 FastMoE=0.175 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2051 layer 6: round 2 FastMoE=0.172 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2051 layer 6: round 3 FastMoE=0.176 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2061 6
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2061 layer 6: round 0 FastMoE=0.255 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2061 layer 6: round 1 FastMoE=0.175 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2061 layer 6: round 2 FastMoE=0.174 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2061 layer 6: round 3 FastMoE=0.175 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2071 6
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2071 layer 6: round 0 FastMoE=0.253 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2071 layer 6: round 1 FastMoE=0.173 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2071 layer 6: round 2 FastMoE=0.175 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2071 layer 6: round 3 FastMoE=0.172 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2081 6
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2081 layer 6: round 0 FastMoE=0.254 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2081 layer 6: round 1 FastMoE=0.173 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2081 layer 6: round 2 FastMoE=0.173 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2081 layer 6: round 3 FastMoE=0.173 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2091 6
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2091 layer 6: round 0 FastMoE=0.253 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2091 layer 6: round 1 FastMoE=0.177 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2091 layer 6: round 2 FastMoE=0.176 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2091 layer 6: round 3 FastMoE=0.177 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2101 6
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2101 layer 6: round 0 FastMoE=0.242 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2101 layer 6: round 1 FastMoE=0.176 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2101 layer 6: round 2 FastMoE=0.176 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2101 layer 6: round 3 FastMoE=0.177 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2111 6
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2111 layer 6: round 0 FastMoE=0.248 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2111 layer 6: round 1 FastMoE=0.174 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2111 layer 6: round 2 FastMoE=0.176 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2111 layer 6: round 3 FastMoE=0.175 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2121 6
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2121 layer 6: round 0 FastMoE=0.247 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2121 layer 6: round 1 FastMoE=0.180 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2121 layer 6: round 2 FastMoE=0.178 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2121 layer 6: round 3 FastMoE=0.175 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2131 6
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2131 layer 6: round 0 FastMoE=0.239 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2131 layer 6: round 1 FastMoE=0.174 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2131 layer 6: round 2 FastMoE=0.177 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2131 layer 6: round 3 FastMoE=0.177 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2141 6
[11, 5, 19, 23, 8, 14, 13, 28, 2, 24, 29, 18, 7, 30, 26, 12, 1, 17, 31, 0, 20, 27, 3, 9, 4, 10, 21, 16, 22, 25, 6, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2141 layer 6: round 0 FastMoE=0.253 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2141 layer 6: round 1 FastMoE=0.176 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2141 layer 6: round 2 FastMoE=0.176 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2141 layer 6: round 3 FastMoE=0.176 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2151 6
[INFO] mapping updated!
[21, 5, 10, 9, 8, 17, 29, 28, 26, 24, 13, 27, 3, 22, 7, 12, 25, 23, 14, 0, 20, 6, 19, 15, 4, 30, 31, 16, 11, 2, 1, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2151 layer 6: round 0 FastMoE=0.245 SmartMoE=0.194
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2151 layer 6: round 1 FastMoE=0.172 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2151 layer 6: round 2 FastMoE=0.171 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2151 layer 6: round 3 FastMoE=0.173 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2161 6
[21, 5, 10, 9, 8, 17, 29, 28, 26, 24, 13, 27, 3, 22, 7, 12, 25, 23, 14, 0, 20, 6, 19, 15, 4, 30, 31, 16, 11, 2, 1, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2161 layer 6: round 0 FastMoE=0.228 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2161 layer 6: round 1 FastMoE=0.175 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2161 layer 6: round 2 FastMoE=0.175 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2161 layer 6: round 3 FastMoE=0.173 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2171 6
[21, 5, 10, 9, 8, 17, 29, 28, 26, 24, 13, 27, 3, 22, 7, 12, 25, 23, 14, 0, 20, 6, 19, 15, 4, 30, 31, 16, 11, 2, 1, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2171 layer 6: round 0 FastMoE=0.233 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2171 layer 6: round 1 FastMoE=0.175 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2171 layer 6: round 2 FastMoE=0.174 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2171 layer 6: round 3 FastMoE=0.173 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2181 6
[INFO] mapping updated!
[11, 5, 10, 9, 8, 30, 29, 28, 26, 24, 21, 17, 7, 14, 3, 12, 25, 2, 15, 0, 20, 27, 19, 22, 4, 23, 31, 16, 13, 6, 1, 18]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2181 layer 6: round 0 FastMoE=0.229 SmartMoE=0.178
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2181 layer 6: round 1 FastMoE=0.172 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2181 layer 6: round 2 FastMoE=0.172 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2181 layer 6: round 3 FastMoE=0.173 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2191 6
[INFO] mapping updated!
[11, 5, 19, 18, 8, 21, 29, 28, 6, 24, 14, 23, 3, 15, 7, 12, 25, 17, 30, 0, 20, 2, 27, 31, 4, 9, 10, 16, 13, 26, 1, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2191 layer 6: round 0 FastMoE=0.250 SmartMoE=0.178
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2191 layer 6: round 1 FastMoE=0.174 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2191 layer 6: round 2 FastMoE=0.177 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2191 layer 6: round 3 FastMoE=0.177 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2201 6
[11, 5, 19, 18, 8, 21, 29, 28, 6, 24, 14, 23, 3, 15, 7, 12, 25, 17, 30, 0, 20, 2, 27, 31, 4, 9, 10, 16, 13, 26, 1, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2201 layer 6: round 0 FastMoE=0.230 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2201 layer 6: round 1 FastMoE=0.176 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2201 layer 6: round 2 FastMoE=0.175 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2201 layer 6: round 3 FastMoE=0.176 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2211 6
[INFO] mapping updated!
[14, 25, 10, 19, 8, 30, 29, 28, 5, 24, 15, 18, 26, 21, 27, 12, 1, 2, 22, 0, 20, 17, 7, 23, 4, 31, 9, 16, 11, 6, 3, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2211 layer 6: round 0 FastMoE=0.236 SmartMoE=0.186
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2211 layer 6: round 1 FastMoE=0.177 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2211 layer 6: round 2 FastMoE=0.176 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2211 layer 6: round 3 FastMoE=0.179 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2221 6
[INFO] mapping updated!
[11, 25, 3, 7, 8, 13, 23, 28, 1, 24, 21, 15, 17, 22, 19, 12, 5, 27, 29, 0, 20, 18, 2, 30, 4, 31, 9, 16, 10, 26, 6, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2221 layer 6: round 0 FastMoE=0.418 SmartMoE=0.195
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2221 layer 6: round 1 FastMoE=0.175 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2221 layer 6: round 2 FastMoE=0.174 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2221 layer 6: round 3 FastMoE=0.172 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2231 6
[11, 25, 3, 7, 8, 13, 23, 28, 1, 24, 21, 15, 17, 22, 19, 12, 5, 27, 29, 0, 20, 18, 2, 30, 4, 31, 9, 16, 10, 26, 6, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2231 layer 6: round 0 FastMoE=0.236 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2231 layer 6: round 1 FastMoE=0.174 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2231 layer 6: round 2 FastMoE=0.172 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2231 layer 6: round 3 FastMoE=0.172 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2241 6
[INFO] mapping updated!
[19, 26, 9, 18, 8, 29, 23, 28, 1, 24, 15, 3, 7, 14, 5, 12, 25, 17, 13, 0, 20, 6, 21, 22, 4, 30, 31, 16, 10, 27, 2, 11]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2241 layer 6: round 0 FastMoE=0.236 SmartMoE=0.189
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2241 layer 6: round 1 FastMoE=0.174 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2241 layer 6: round 2 FastMoE=0.174 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2241 layer 6: round 3 FastMoE=0.175 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2251 6
[INFO] mapping updated!
[11, 26, 3, 19, 8, 29, 15, 28, 6, 24, 23, 22, 17, 13, 7, 12, 25, 2, 14, 0, 20, 1, 18, 30, 4, 31, 9, 16, 10, 27, 5, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2251 layer 6: round 0 FastMoE=0.263 SmartMoE=0.189
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2251 layer 6: round 1 FastMoE=0.176 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2251 layer 6: round 2 FastMoE=0.175 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2251 layer 6: round 3 FastMoE=0.174 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2261 6
[INFO] mapping updated!
[11, 25, 13, 23, 8, 21, 22, 28, 26, 24, 15, 19, 2, 29, 3, 12, 5, 17, 30, 0, 20, 27, 7, 31, 4, 9, 10, 16, 18, 1, 6, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2261 layer 6: round 0 FastMoE=0.230 SmartMoE=0.181
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2261 layer 6: round 1 FastMoE=0.174 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2261 layer 6: round 2 FastMoE=0.174 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2261 layer 6: round 3 FastMoE=0.174 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2271 6
[11, 25, 13, 23, 8, 21, 22, 28, 26, 24, 15, 19, 2, 29, 3, 12, 5, 17, 30, 0, 20, 27, 7, 31, 4, 9, 10, 16, 18, 1, 6, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2271 layer 6: round 0 FastMoE=0.245 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2271 layer 6: round 1 FastMoE=0.180 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2271 layer 6: round 2 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2271 layer 6: round 3 FastMoE=0.178 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2281 6
[11, 25, 13, 23, 8, 21, 22, 28, 26, 24, 15, 19, 2, 29, 3, 12, 5, 17, 30, 0, 20, 27, 7, 31, 4, 9, 10, 16, 18, 1, 6, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2281 layer 6: round 0 FastMoE=0.250 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2281 layer 6: round 1 FastMoE=0.177 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2281 layer 6: round 2 FastMoE=0.178 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2281 layer 6: round 3 FastMoE=0.177 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2291 6
[11, 25, 13, 23, 8, 21, 22, 28, 26, 24, 15, 19, 2, 29, 3, 12, 5, 17, 30, 0, 20, 27, 7, 31, 4, 9, 10, 16, 18, 1, 6, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2291 layer 6: round 0 FastMoE=0.251 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2291 layer 6: round 1 FastMoE=0.175 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2291 layer 6: round 2 FastMoE=0.176 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2291 layer 6: round 3 FastMoE=0.176 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2301 6
[11, 25, 13, 23, 8, 21, 22, 28, 26, 24, 15, 19, 2, 29, 3, 12, 5, 17, 30, 0, 20, 27, 7, 31, 4, 9, 10, 16, 18, 1, 6, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2301 layer 6: round 0 FastMoE=0.244 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2301 layer 6: round 1 FastMoE=0.173 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2301 layer 6: round 2 FastMoE=0.172 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2301 layer 6: round 3 FastMoE=0.172 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2311 6
[11, 25, 13, 23, 8, 21, 22, 28, 26, 24, 15, 19, 2, 29, 3, 12, 5, 17, 30, 0, 20, 27, 7, 31, 4, 9, 10, 16, 18, 1, 6, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2311 layer 6: round 0 FastMoE=0.252 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2311 layer 6: round 1 FastMoE=0.174 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2311 layer 6: round 2 FastMoE=0.174 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2311 layer 6: round 3 FastMoE=0.174 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2321 6
[11, 25, 13, 23, 8, 21, 22, 28, 26, 24, 15, 19, 2, 29, 3, 12, 5, 17, 30, 0, 20, 27, 7, 31, 4, 9, 10, 16, 18, 1, 6, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2321 layer 6: round 0 FastMoE=0.246 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2321 layer 6: round 1 FastMoE=0.177 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2321 layer 6: round 2 FastMoE=0.179 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2321 layer 6: round 3 FastMoE=0.176 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2331 6
[INFO] mapping updated!
[11, 25, 3, 21, 8, 13, 29, 28, 26, 24, 15, 2, 18, 23, 19, 12, 5, 6, 30, 0, 20, 27, 7, 31, 4, 9, 22, 16, 10, 1, 17, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2331 layer 6: round 0 FastMoE=0.249 SmartMoE=0.183
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2331 layer 6: round 1 FastMoE=0.175 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2331 layer 6: round 2 FastMoE=0.173 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2331 layer 6: round 3 FastMoE=0.174 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2341 6
[11, 25, 3, 21, 8, 13, 29, 28, 26, 24, 15, 2, 18, 23, 19, 12, 5, 6, 30, 0, 20, 27, 7, 31, 4, 9, 22, 16, 10, 1, 17, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2341 layer 6: round 0 FastMoE=0.253 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2341 layer 6: round 1 FastMoE=0.177 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2341 layer 6: round 2 FastMoE=0.176 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2341 layer 6: round 3 FastMoE=0.176 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2351 6
[11, 25, 3, 21, 8, 13, 29, 28, 26, 24, 15, 2, 18, 23, 19, 12, 5, 6, 30, 0, 20, 27, 7, 31, 4, 9, 22, 16, 10, 1, 17, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2351 layer 6: round 0 FastMoE=0.225 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2351 layer 6: round 1 FastMoE=0.171 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2351 layer 6: round 2 FastMoE=0.173 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2351 layer 6: round 3 FastMoE=0.170 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2361 6
[INFO] mapping updated!
[11, 25, 13, 19, 8, 22, 23, 28, 1, 24, 15, 3, 2, 29, 17, 12, 26, 7, 30, 0, 20, 6, 18, 31, 4, 9, 10, 16, 21, 5, 27, 14]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2361 layer 6: round 0 FastMoE=0.237 SmartMoE=0.178
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2361 layer 6: round 1 FastMoE=0.177 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2361 layer 6: round 2 FastMoE=0.173 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2361 layer 6: round 3 FastMoE=0.175 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2371 6
[INFO] mapping updated!
[11, 5, 9, 31, 8, 3, 29, 28, 6, 24, 15, 2, 7, 21, 17, 12, 26, 1, 22, 0, 20, 19, 18, 23, 4, 30, 14, 16, 10, 27, 25, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2371 layer 6: round 0 FastMoE=0.226 SmartMoE=0.186
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2371 layer 6: round 1 FastMoE=0.173 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2371 layer 6: round 2 FastMoE=0.169 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2371 layer 6: round 3 FastMoE=0.168 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2381 6
[11, 5, 9, 31, 8, 3, 29, 28, 6, 24, 15, 2, 7, 21, 17, 12, 26, 1, 22, 0, 20, 19, 18, 23, 4, 30, 14, 16, 10, 27, 25, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2381 layer 6: round 0 FastMoE=0.231 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2381 layer 6: round 1 FastMoE=0.175 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2381 layer 6: round 2 FastMoE=0.174 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2381 layer 6: round 3 FastMoE=0.175 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2391 6
[11, 5, 9, 31, 8, 3, 29, 28, 6, 24, 15, 2, 7, 21, 17, 12, 26, 1, 22, 0, 20, 19, 18, 23, 4, 30, 14, 16, 10, 27, 25, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2391 layer 6: round 0 FastMoE=0.229 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2391 layer 6: round 1 FastMoE=0.170 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2391 layer 6: round 2 FastMoE=0.172 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2391 layer 6: round 3 FastMoE=0.171 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2401 6
[INFO] mapping updated!
[11, 1, 19, 18, 8, 30, 29, 28, 5, 24, 21, 17, 27, 22, 6, 12, 25, 14, 23, 0, 20, 3, 7, 31, 4, 9, 15, 16, 10, 26, 2, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2401 layer 6: round 0 FastMoE=0.245 SmartMoE=0.187
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2401 layer 6: round 1 FastMoE=0.177 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2401 layer 6: round 2 FastMoE=0.177 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2401 layer 6: round 3 FastMoE=0.179 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2411 6
[11, 1, 19, 18, 8, 30, 29, 28, 5, 24, 21, 17, 27, 22, 6, 12, 25, 14, 23, 0, 20, 3, 7, 31, 4, 9, 15, 16, 10, 26, 2, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2411 layer 6: round 0 FastMoE=0.259 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2411 layer 6: round 1 FastMoE=0.179 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2411 layer 6: round 2 FastMoE=0.181 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2411 layer 6: round 3 FastMoE=0.179 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2421 6
[11, 1, 19, 18, 8, 30, 29, 28, 5, 24, 21, 17, 27, 22, 6, 12, 25, 14, 23, 0, 20, 3, 7, 31, 4, 9, 15, 16, 10, 26, 2, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2421 layer 6: round 0 FastMoE=0.271 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2421 layer 6: round 1 FastMoE=0.178 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2421 layer 6: round 2 FastMoE=0.177 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2421 layer 6: round 3 FastMoE=0.177 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2431 6
[11, 1, 19, 18, 8, 30, 29, 28, 5, 24, 21, 17, 27, 22, 6, 12, 25, 14, 23, 0, 20, 3, 7, 31, 4, 9, 15, 16, 10, 26, 2, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2431 layer 6: round 0 FastMoE=0.259 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2431 layer 6: round 1 FastMoE=0.175 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2431 layer 6: round 2 FastMoE=0.176 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2431 layer 6: round 3 FastMoE=0.174 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2441 6
[INFO] mapping updated!
[11, 25, 21, 13, 8, 14, 15, 28, 26, 24, 23, 18, 5, 29, 6, 12, 1, 7, 30, 0, 20, 17, 3, 31, 4, 9, 10, 16, 19, 2, 27, 22]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2441 layer 6: round 0 FastMoE=0.261 SmartMoE=0.189
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2441 layer 6: round 1 FastMoE=0.178 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2441 layer 6: round 2 FastMoE=0.174 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2441 layer 6: round 3 FastMoE=0.177 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2451 6
[INFO] mapping updated!
[11, 25, 9, 19, 8, 18, 29, 28, 1, 24, 21, 17, 6, 22, 7, 12, 5, 14, 23, 0, 20, 3, 2, 30, 4, 31, 15, 16, 10, 27, 26, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2451 layer 6: round 0 FastMoE=0.246 SmartMoE=0.182
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2451 layer 6: round 1 FastMoE=0.174 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2451 layer 6: round 2 FastMoE=0.174 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2451 layer 6: round 3 FastMoE=0.174 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2461 6
[INFO] mapping updated!
[11, 25, 10, 7, 8, 30, 29, 28, 26, 24, 15, 18, 27, 14, 3, 12, 1, 21, 22, 0, 20, 2, 17, 23, 4, 31, 9, 16, 13, 5, 6, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2461 layer 6: round 0 FastMoE=0.264 SmartMoE=0.180
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2461 layer 6: round 1 FastMoE=0.176 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2461 layer 6: round 2 FastMoE=0.174 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2461 layer 6: round 3 FastMoE=0.173 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2471 6
[INFO] mapping updated!
[11, 1, 13, 22, 8, 14, 23, 28, 2, 24, 29, 19, 7, 18, 6, 12, 25, 17, 30, 0, 20, 27, 3, 31, 4, 9, 21, 16, 10, 5, 26, 15]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2471 layer 6: round 0 FastMoE=0.251 SmartMoE=0.192
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2471 layer 6: round 1 FastMoE=0.177 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2471 layer 6: round 2 FastMoE=0.176 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2471 layer 6: round 3 FastMoE=0.177 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2481 6
[INFO] mapping updated!
[11, 25, 10, 9, 8, 17, 30, 28, 5, 24, 14, 13, 6, 23, 22, 12, 1, 19, 15, 0, 20, 27, 18, 29, 4, 31, 7, 16, 21, 2, 26, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2481 layer 6: round 0 FastMoE=0.242 SmartMoE=0.190
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2481 layer 6: round 1 FastMoE=0.176 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2481 layer 6: round 2 FastMoE=0.178 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2481 layer 6: round 3 FastMoE=0.176 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2491 6
[11, 25, 10, 9, 8, 17, 30, 28, 5, 24, 14, 13, 6, 23, 22, 12, 1, 19, 15, 0, 20, 27, 18, 29, 4, 31, 7, 16, 21, 2, 26, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2491 layer 6: round 0 FastMoE=0.691 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2491 layer 6: round 1 FastMoE=0.177 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2491 layer 6: round 2 FastMoE=0.176 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2491 layer 6: round 3 FastMoE=0.177 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2501 6
[11, 25, 10, 9, 8, 17, 30, 28, 5, 24, 14, 13, 6, 23, 22, 12, 1, 19, 15, 0, 20, 27, 18, 29, 4, 31, 7, 16, 21, 2, 26, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2501 layer 6: round 0 FastMoE=0.581 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2501 layer 6: round 1 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2501 layer 6: round 2 FastMoE=0.174 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2501 layer 6: round 3 FastMoE=0.177 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2511 6
[INFO] mapping updated!
[11, 2, 19, 9, 8, 17, 29, 28, 26, 24, 23, 18, 5, 13, 7, 12, 25, 14, 15, 0, 20, 3, 6, 30, 4, 31, 22, 16, 10, 1, 27, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2511 layer 6: round 0 FastMoE=0.234 SmartMoE=0.182
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2511 layer 6: round 1 FastMoE=0.174 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2511 layer 6: round 2 FastMoE=0.175 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2511 layer 6: round 3 FastMoE=0.176 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2521 6
[11, 2, 19, 9, 8, 17, 29, 28, 26, 24, 23, 18, 5, 13, 7, 12, 25, 14, 15, 0, 20, 3, 6, 30, 4, 31, 22, 16, 10, 1, 27, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2521 layer 6: round 0 FastMoE=0.243 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2521 layer 6: round 1 FastMoE=0.178 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2521 layer 6: round 2 FastMoE=0.178 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2521 layer 6: round 3 FastMoE=0.176 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2531 6
[11, 2, 19, 9, 8, 17, 29, 28, 26, 24, 23, 18, 5, 13, 7, 12, 25, 14, 15, 0, 20, 3, 6, 30, 4, 31, 22, 16, 10, 1, 27, 21]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2531 layer 6: round 0 FastMoE=0.246 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2531 layer 6: round 1 FastMoE=0.179 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2531 layer 6: round 2 FastMoE=0.178 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2531 layer 6: round 3 FastMoE=0.178 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2541 6
[INFO] mapping updated!
[13, 1, 10, 9, 8, 17, 29, 28, 5, 24, 15, 18, 2, 14, 3, 12, 25, 21, 22, 0, 20, 27, 7, 23, 4, 30, 31, 16, 11, 26, 6, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2541 layer 6: round 0 FastMoE=0.248 SmartMoE=0.182
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2541 layer 6: round 1 FastMoE=0.176 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2541 layer 6: round 2 FastMoE=0.177 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2541 layer 6: round 3 FastMoE=0.176 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2551 6
[13, 1, 10, 9, 8, 17, 29, 28, 5, 24, 15, 18, 2, 14, 3, 12, 25, 21, 22, 0, 20, 27, 7, 23, 4, 30, 31, 16, 11, 26, 6, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2551 layer 6: round 0 FastMoE=0.317 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2551 layer 6: round 1 FastMoE=0.177 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2551 layer 6: round 2 FastMoE=0.176 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2551 layer 6: round 3 FastMoE=0.177 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2561 6
[13, 1, 10, 9, 8, 17, 29, 28, 5, 24, 15, 18, 2, 14, 3, 12, 25, 21, 22, 0, 20, 27, 7, 23, 4, 30, 31, 16, 11, 26, 6, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2561 layer 6: round 0 FastMoE=0.253 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2561 layer 6: round 1 FastMoE=0.177 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2561 layer 6: round 2 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2561 layer 6: round 3 FastMoE=0.177 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2571 6
[13, 1, 10, 9, 8, 17, 29, 28, 5, 24, 15, 18, 2, 14, 3, 12, 25, 21, 22, 0, 20, 27, 7, 23, 4, 30, 31, 16, 11, 26, 6, 19]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2571 layer 6: round 0 FastMoE=0.259 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2571 layer 6: round 1 FastMoE=0.178 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2571 layer 6: round 2 FastMoE=0.178 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2571 layer 6: round 3 FastMoE=0.178 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2581 6
[INFO] mapping updated!
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2581 layer 6: round 0 FastMoE=0.250 SmartMoE=0.184
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2581 layer 6: round 1 FastMoE=0.177 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2581 layer 6: round 2 FastMoE=0.177 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2581 layer 6: round 3 FastMoE=0.177 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2591 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2591 layer 6: round 0 FastMoE=0.260 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2591 layer 6: round 1 FastMoE=0.176 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2591 layer 6: round 2 FastMoE=0.176 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2591 layer 6: round 3 FastMoE=0.177 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2601 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2601 layer 6: round 0 FastMoE=0.239 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2601 layer 6: round 1 FastMoE=0.176 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2601 layer 6: round 2 FastMoE=0.173 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2601 layer 6: round 3 FastMoE=0.177 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2611 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2611 layer 6: round 0 FastMoE=0.235 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2611 layer 6: round 1 FastMoE=0.177 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2611 layer 6: round 2 FastMoE=0.180 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2611 layer 6: round 3 FastMoE=0.179 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2621 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2621 layer 6: round 0 FastMoE=0.236 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2621 layer 6: round 1 FastMoE=0.175 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2621 layer 6: round 2 FastMoE=0.175 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2621 layer 6: round 3 FastMoE=0.178 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2631 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2631 layer 6: round 0 FastMoE=0.244 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2631 layer 6: round 1 FastMoE=0.176 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2631 layer 6: round 2 FastMoE=0.176 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2631 layer 6: round 3 FastMoE=0.176 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2641 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2641 layer 6: round 0 FastMoE=0.242 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2641 layer 6: round 1 FastMoE=0.178 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2641 layer 6: round 2 FastMoE=0.175 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2641 layer 6: round 3 FastMoE=0.173 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2651 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2651 layer 6: round 0 FastMoE=0.241 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2651 layer 6: round 1 FastMoE=0.179 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2651 layer 6: round 2 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2651 layer 6: round 3 FastMoE=0.177 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2661 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2661 layer 6: round 0 FastMoE=0.246 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2661 layer 6: round 1 FastMoE=0.177 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2661 layer 6: round 2 FastMoE=0.179 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2661 layer 6: round 3 FastMoE=0.178 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2671 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2671 layer 6: round 0 FastMoE=0.248 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2671 layer 6: round 1 FastMoE=0.175 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2671 layer 6: round 2 FastMoE=0.179 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2671 layer 6: round 3 FastMoE=0.180 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2681 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2681 layer 6: round 0 FastMoE=0.239 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2681 layer 6: round 1 FastMoE=0.177 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2681 layer 6: round 2 FastMoE=0.173 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2681 layer 6: round 3 FastMoE=0.173 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2691 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2691 layer 6: round 0 FastMoE=0.246 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2691 layer 6: round 1 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2691 layer 6: round 2 FastMoE=0.174 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2691 layer 6: round 3 FastMoE=0.175 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2701 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2701 layer 6: round 0 FastMoE=0.239 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2701 layer 6: round 1 FastMoE=0.175 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2701 layer 6: round 2 FastMoE=0.176 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2701 layer 6: round 3 FastMoE=0.178 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2711 6
[11, 25, 19, 31, 8, 18, 23, 28, 1, 24, 14, 17, 27, 15, 6, 12, 26, 21, 22, 0, 20, 3, 7, 29, 4, 30, 9, 16, 10, 2, 5, 13]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2711 layer 6: round 0 FastMoE=0.240 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2711 layer 6: round 1 FastMoE=0.176 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2711 layer 6: round 2 FastMoE=0.177 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2711 layer 6: round 3 FastMoE=0.176 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2721 6
[INFO] mapping updated!
[11, 25, 9, 22, 8, 19, 31, 28, 5, 24, 14, 18, 6, 23, 3, 12, 1, 13, 15, 0, 20, 2, 7, 29, 4, 30, 21, 16, 10, 26, 27, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2721 layer 6: round 0 FastMoE=0.246 SmartMoE=0.176
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2721 layer 6: round 1 FastMoE=0.178 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2721 layer 6: round 2 FastMoE=0.181 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2721 layer 6: round 3 FastMoE=0.180 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2731 6
[INFO] mapping updated!
[11, 1, 18, 31, 8, 29, 15, 28, 26, 24, 22, 19, 27, 21, 6, 12, 25, 23, 13, 0, 20, 17, 2, 14, 4, 30, 9, 16, 10, 5, 7, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2731 layer 6: round 0 FastMoE=0.243 SmartMoE=0.191
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2731 layer 6: round 1 FastMoE=0.183 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2731 layer 6: round 2 FastMoE=0.178 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2731 layer 6: round 3 FastMoE=0.180 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2741 6
[11, 1, 18, 31, 8, 29, 15, 28, 26, 24, 22, 19, 27, 21, 6, 12, 25, 23, 13, 0, 20, 17, 2, 14, 4, 30, 9, 16, 10, 5, 7, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2741 layer 6: round 0 FastMoE=0.297 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2741 layer 6: round 1 FastMoE=0.183 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2741 layer 6: round 2 FastMoE=0.181 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2741 layer 6: round 3 FastMoE=0.182 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2751 6
[11, 1, 18, 31, 8, 29, 15, 28, 26, 24, 22, 19, 27, 21, 6, 12, 25, 23, 13, 0, 20, 17, 2, 14, 4, 30, 9, 16, 10, 5, 7, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2751 layer 6: round 0 FastMoE=0.251 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2751 layer 6: round 1 FastMoE=0.179 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2751 layer 6: round 2 FastMoE=0.181 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2751 layer 6: round 3 FastMoE=0.180 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2761 6
[11, 1, 18, 31, 8, 29, 15, 28, 26, 24, 22, 19, 27, 21, 6, 12, 25, 23, 13, 0, 20, 17, 2, 14, 4, 30, 9, 16, 10, 5, 7, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2761 layer 6: round 0 FastMoE=0.273 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2761 layer 6: round 1 FastMoE=0.186 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2761 layer 6: round 2 FastMoE=0.181 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2761 layer 6: round 3 FastMoE=0.180 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2771 6
[11, 1, 18, 31, 8, 29, 15, 28, 26, 24, 22, 19, 27, 21, 6, 12, 25, 23, 13, 0, 20, 17, 2, 14, 4, 30, 9, 16, 10, 5, 7, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2771 layer 6: round 0 FastMoE=0.245 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2771 layer 6: round 1 FastMoE=0.181 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2771 layer 6: round 2 FastMoE=0.181 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2771 layer 6: round 3 FastMoE=0.180 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2781 6
[11, 1, 18, 31, 8, 29, 15, 28, 26, 24, 22, 19, 27, 21, 6, 12, 25, 23, 13, 0, 20, 17, 2, 14, 4, 30, 9, 16, 10, 5, 7, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2781 layer 6: round 0 FastMoE=0.246 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2781 layer 6: round 1 FastMoE=0.181 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2781 layer 6: round 2 FastMoE=0.181 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2781 layer 6: round 3 FastMoE=0.181 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2791 6
[INFO] mapping updated!
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2791 layer 6: round 0 FastMoE=0.242 SmartMoE=0.191
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2791 layer 6: round 1 FastMoE=0.182 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2791 layer 6: round 2 FastMoE=0.182 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2791 layer 6: round 3 FastMoE=0.181 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2801 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2801 layer 6: round 0 FastMoE=0.267 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2801 layer 6: round 1 FastMoE=0.177 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2801 layer 6: round 2 FastMoE=0.178 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2801 layer 6: round 3 FastMoE=0.180 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2811 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2811 layer 6: round 0 FastMoE=0.243 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2811 layer 6: round 1 FastMoE=0.176 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2811 layer 6: round 2 FastMoE=0.176 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2811 layer 6: round 3 FastMoE=0.175 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2821 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2821 layer 6: round 0 FastMoE=0.235 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2821 layer 6: round 1 FastMoE=0.179 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2821 layer 6: round 2 FastMoE=0.180 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2821 layer 6: round 3 FastMoE=0.179 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2831 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2831 layer 6: round 0 FastMoE=0.236 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2831 layer 6: round 1 FastMoE=0.178 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2831 layer 6: round 2 FastMoE=0.179 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2831 layer 6: round 3 FastMoE=0.177 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2841 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2841 layer 6: round 0 FastMoE=0.233 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2841 layer 6: round 1 FastMoE=0.176 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2841 layer 6: round 2 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2841 layer 6: round 3 FastMoE=0.177 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2851 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2851 layer 6: round 0 FastMoE=0.237 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2851 layer 6: round 1 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2851 layer 6: round 2 FastMoE=0.179 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2851 layer 6: round 3 FastMoE=0.179 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2861 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2861 layer 6: round 0 FastMoE=0.239 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2861 layer 6: round 1 FastMoE=0.180 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2861 layer 6: round 2 FastMoE=0.181 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2861 layer 6: round 3 FastMoE=0.178 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2871 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2871 layer 6: round 0 FastMoE=0.228 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2871 layer 6: round 1 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2871 layer 6: round 2 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2871 layer 6: round 3 FastMoE=0.181 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2881 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2881 layer 6: round 0 FastMoE=0.234 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2881 layer 6: round 1 FastMoE=0.181 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2881 layer 6: round 2 FastMoE=0.182 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2881 layer 6: round 3 FastMoE=0.179 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2891 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2891 layer 6: round 0 FastMoE=0.232 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2891 layer 6: round 1 FastMoE=0.181 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2891 layer 6: round 2 FastMoE=0.183 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2891 layer 6: round 3 FastMoE=0.180 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2901 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2901 layer 6: round 0 FastMoE=0.241 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2901 layer 6: round 1 FastMoE=0.180 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2901 layer 6: round 2 FastMoE=0.179 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2901 layer 6: round 3 FastMoE=0.179 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2911 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2911 layer 6: round 0 FastMoE=0.234 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2911 layer 6: round 1 FastMoE=0.180 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2911 layer 6: round 2 FastMoE=0.181 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2911 layer 6: round 3 FastMoE=0.179 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2921 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2921 layer 6: round 0 FastMoE=0.232 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2921 layer 6: round 1 FastMoE=0.178 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2921 layer 6: round 2 FastMoE=0.177 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2921 layer 6: round 3 FastMoE=0.181 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2931 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2931 layer 6: round 0 FastMoE=0.260 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2931 layer 6: round 1 FastMoE=0.182 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2931 layer 6: round 2 FastMoE=0.181 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2931 layer 6: round 3 FastMoE=0.180 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2941 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2941 layer 6: round 0 FastMoE=0.248 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2941 layer 6: round 1 FastMoE=0.180 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2941 layer 6: round 2 FastMoE=0.179 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2941 layer 6: round 3 FastMoE=0.176 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2951 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2951 layer 6: round 0 FastMoE=0.246 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2951 layer 6: round 1 FastMoE=0.175 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2951 layer 6: round 2 FastMoE=0.178 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2951 layer 6: round 3 FastMoE=0.175 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2961 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2961 layer 6: round 0 FastMoE=0.242 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2961 layer 6: round 1 FastMoE=0.178 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2961 layer 6: round 2 FastMoE=0.181 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2961 layer 6: round 3 FastMoE=0.181 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2971 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2971 layer 6: round 0 FastMoE=0.252 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2971 layer 6: round 1 FastMoE=0.178 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2971 layer 6: round 2 FastMoE=0.179 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2971 layer 6: round 3 FastMoE=0.179 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2981 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2981 layer 6: round 0 FastMoE=0.250 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2981 layer 6: round 1 FastMoE=0.180 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2981 layer 6: round 2 FastMoE=0.181 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2981 layer 6: round 3 FastMoE=0.181 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2991 6
[11, 25, 19, 10, 8, 31, 30, 28, 2, 24, 22, 6, 5, 14, 27, 12, 1, 15, 21, 0, 20, 3, 18, 23, 4, 29, 9, 16, 13, 26, 17, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2991 layer 6: round 0 FastMoE=0.245 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2991 layer 6: round 1 FastMoE=0.175 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2991 layer 6: round 2 FastMoE=0.176 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2991 layer 6: round 3 FastMoE=0.177 SmartMoE=0.122
+ '[' 9 -ne 9 ']'
++ expr 8019 % 10000 + 10000
+ export MASTER_PORT=18019
+ MASTER_PORT=18019
+ export EXP_NAME=fig13_2023-05-28T17:25:28+08:00
+ EXP_NAME=fig13_2023-05-28T17:25:28+08:00
+ export NNODES=1
+ NNODES=1
+ export NODELIST=nico1
+ NODELIST=nico1
+ export EXEC=/home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py
+ EXEC=/home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py
+ export TABLE_PREFIX=/home/atc23_ae/SmartMoE-AE/moe_trace/table
+ TABLE_PREFIX=/home/atc23_ae/SmartMoE-AE/moe_trace/table
+ export MICRO_BATCH_SIZE=8
+ MICRO_BATCH_SIZE=8
+ export HIDDEN_SIZE=1536
+ HIDDEN_SIZE=1536
+ export HISTORY_LAT=0
+ HISTORY_LAT=0
+ export UPDATE_FREQ=100
+ UPDATE_FREQ=100
+ export FMOE_FASTER_GLBPLC_NETBW=8e9
+ FMOE_FASTER_GLBPLC_NETBW=8e9
+ export FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
+ FMOE_FASTER_GLBPLC_NETBW_Bcast=2e9
+ export FMOE_FASTER_GLBPLC_GPUTP=112e12
+ FMOE_FASTER_GLBPLC_GPUTP=112e12
+ export FMOE_FASTER_SCHEDULE_ENABLE=ON
+ FMOE_FASTER_SCHEDULE_ENABLE=ON
+ export FMOE_FASTER_GROUP_SIZE=16
+ FMOE_FASTER_GROUP_SIZE=16
+ export FMOE_FASTER_SHADOW_ENABLE=ON
+ FMOE_FASTER_SHADOW_ENABLE=ON
+ export FMOE_FASTER_GLBPLC_ALPHA=2
+ FMOE_FASTER_GLBPLC_ALPHA=2
+ export FMOE_FASTER_GLBPLC_DMODEL=1536
+ FMOE_FASTER_GLBPLC_DMODEL=1536
+ mkdir -p ./logs
+ mkdir -p ./logs/fig13_2023-05-28T17:25:28+08:00
++ pwd
+ LOG_DIR=/home/atc23_ae/SmartMoE-AE/scripts/benchmarks/logs/fig13_2023-05-28T17:25:28+08:00
+ tmp=/home/atc23_ae/SmartMoE-AE/moe_trace/table
++ echo /home/atc23_ae/SmartMoE-AE/moe_trace/table
++ sed 's/\//_/g'
+ result=_home_atc23_ae_SmartMoE-AE_moe_trace_table
++ date -Iseconds
+ LOG_PREFIX=1nodes_mbs8_H1536_LAT0_FREQ100_2023-05-28T17:35:31+08:00
+ LOG_NAME=1nodes_mbs8_H1536_LAT0_FREQ100_2023-05-28T17:35:31+08:00.log
+ export SCHEDULER_EXEC=srun
+ SCHEDULER_EXEC=srun
+ export GPUS_PER_NODE=8
+ GPUS_PER_NODE=8
+ '[' 1 == half ']'
+ exec_args=
+ exec_args+=' --exclusive'
+ exec_args+=' --export=ALL'
+ exec_args+=' -K'
+ exec_args+=' --ntasks-per-node=8'
+ exec_args+=' --gres=gpu:8'
+ exec_args+=' -N 1'
+ '[' nico1 '!=' None ']'
++ scontrol show hostnames nico1
++ wc -l
+ tmp=1
+ '[' 1 '!=' 1 ']'
+ exec_args+=' -w nico1'
+ exec_args+=' -p AE'
+ echo srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 1 -w nico1 -p AE
srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 1 -w nico1 -p AE
+ tee /home/atc23_ae/SmartMoE-AE/scripts/benchmarks/logs/fig13_2023-05-28T17:25:28+08:00/1nodes_mbs8_H1536_LAT0_FREQ100_2023-05-28T17:35:31+08:00.log
++ which srun
+ /usr/bin/srun --exclusive --export=ALL -K --ntasks-per-node=8 --gres=gpu:8 -N 1 -w nico1 -p AE ./wrapper_dist_smart_exchange.sh
++ scontrol show JobId=164097
++ grep BatchHost
++ scontrol show JobId=164097
++ grep BatchHost
++ scontrol show JobId=164097
++ tr = ' '
++ tr = ' '
++ grep BatchHost
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=164097
++ scontrol show JobId=164097
++ grep BatchHost
++ scontrol show JobId=164097
++ grep BatchHost
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ tr = ' '
++ awk '{print $2}'
++ awk '{print $2}'
++ scontrol show JobId=164097
++ grep BatchHost
++ scontrol show JobId=164097
++ grep BatchHost
++ tr = ' '
++ awk '{print $2}'
++ tr = ' '
++ awk '{print $2}'
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=2
+ RANK=2
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=2
+ export NODE_RANK=2
+ NODE_RANK=2
+ export CUDA_VISIBLE_DEVICES=2
+ CUDA_VISIBLE_DEVICES=2
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 100
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=6
+ RANK=6
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=6
+ export NODE_RANK=6
+ NODE_RANK=6
+ export CUDA_VISIBLE_DEVICES=6
+ CUDA_VISIBLE_DEVICES=6
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 100
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=0
+ RANK=0
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=0
+ export NODE_RANK=0
+ NODE_RANK=0
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 100
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=7
+ RANK=7
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=7
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export NODE_RANK=7
+ NODE_RANK=7
+ export CUDA_VISIBLE_DEVICES=7
+ CUDA_VISIBLE_DEVICES=7
+ export RANK=5
+ RANK=5
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=5
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 100
+ export NODE_RANK=5
+ NODE_RANK=5
+ export CUDA_VISIBLE_DEVICES=5
+ CUDA_VISIBLE_DEVICES=5
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 100
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=4
+ RANK=4
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=4
+ export NODE_RANK=4
+ NODE_RANK=4
+ export CUDA_VISIBLE_DEVICES=4
+ CUDA_VISIBLE_DEVICES=4
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 100
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=3
+ RANK=3
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=3
+ export NODE_RANK=3
+ NODE_RANK=3
+ export CUDA_VISIBLE_DEVICES=3
+ CUDA_VISIBLE_DEVICES=3
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 100
+ export MASTER_ADDR=nico1
+ MASTER_ADDR=nico1
+ export RANK=1
+ RANK=1
+ export WORLD_SIZE=8
+ WORLD_SIZE=8
+ localrank=1
+ export NODE_RANK=1
+ NODE_RANK=1
+ export CUDA_VISIBLE_DEVICES=1
+ CUDA_VISIBLE_DEVICES=1
+ exec python3 /home/atc23_ae/SmartMoE-AE/src/fastmoe/tests/test_smart_exchange.py /home/atc23_ae/SmartMoE-AE/moe_trace/table 8 1536 0 100
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1 6
[INFO] mapping updated!
[19, 30, 21, 10, 15, 25, 5, 11, 23, 8, 3, 13, 9, 26, 24, 12, 29, 0, 16, 2, 18, 1, 20, 4, 6, 7, 17, 14, 22, 31, 27, 28]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1 layer 6: round 0 FastMoE=1.269 SmartMoE=0.564
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1 layer 6: round 1 FastMoE=0.082 SmartMoE=0.077
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1 layer 6: round 2 FastMoE=0.070 SmartMoE=0.070
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1 layer 6: round 3 FastMoE=0.078 SmartMoE=0.069
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter11 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 11 layer 6: round 0 FastMoE=0.217 SmartMoE=0.177
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 11 layer 6: round 1 FastMoE=0.182 SmartMoE=0.173
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 11 layer 6: round 2 FastMoE=0.185 SmartMoE=0.174
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 11 layer 6: round 3 FastMoE=0.181 SmartMoE=0.175
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter21 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 21 layer 6: round 0 FastMoE=0.290 SmartMoE=0.179
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 21 layer 6: round 1 FastMoE=0.202 SmartMoE=0.166
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 21 layer 6: round 2 FastMoE=0.197 SmartMoE=0.176
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 21 layer 6: round 3 FastMoE=0.196 SmartMoE=0.172
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter31 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 31 layer 6: round 0 FastMoE=0.222 SmartMoE=0.190
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 31 layer 6: round 1 FastMoE=0.209 SmartMoE=0.189
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 31 layer 6: round 2 FastMoE=0.210 SmartMoE=0.193
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 31 layer 6: round 3 FastMoE=0.215 SmartMoE=0.187
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter41 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 41 layer 6: round 0 FastMoE=0.232 SmartMoE=0.180
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 41 layer 6: round 1 FastMoE=0.214 SmartMoE=0.184
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 41 layer 6: round 2 FastMoE=0.211 SmartMoE=0.176
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 41 layer 6: round 3 FastMoE=0.209 SmartMoE=0.189
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter51 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 51 layer 6: round 0 FastMoE=0.266 SmartMoE=0.166
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 51 layer 6: round 1 FastMoE=0.175 SmartMoE=0.170
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 51 layer 6: round 2 FastMoE=0.173 SmartMoE=0.166
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 51 layer 6: round 3 FastMoE=0.170 SmartMoE=0.164
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter61 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 61 layer 6: round 0 FastMoE=0.210 SmartMoE=0.182
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 61 layer 6: round 1 FastMoE=0.161 SmartMoE=0.182
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 61 layer 6: round 2 FastMoE=0.160 SmartMoE=0.182
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 61 layer 6: round 3 FastMoE=0.157 SmartMoE=0.183
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter71 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 71 layer 6: round 0 FastMoE=0.196 SmartMoE=0.167
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 71 layer 6: round 1 FastMoE=0.151 SmartMoE=0.172
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 71 layer 6: round 2 FastMoE=0.155 SmartMoE=0.164
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 71 layer 6: round 3 FastMoE=0.153 SmartMoE=0.169
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter81 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 81 layer 6: round 0 FastMoE=0.200 SmartMoE=0.198
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 81 layer 6: round 1 FastMoE=0.160 SmartMoE=0.198
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 81 layer 6: round 2 FastMoE=0.160 SmartMoE=0.196
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 81 layer 6: round 3 FastMoE=0.161 SmartMoE=0.200
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter91 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 91 layer 6: round 0 FastMoE=0.213 SmartMoE=0.191
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 91 layer 6: round 1 FastMoE=0.177 SmartMoE=0.190
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 91 layer 6: round 2 FastMoE=0.177 SmartMoE=0.192
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 91 layer 6: round 3 FastMoE=0.179 SmartMoE=0.189
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter101 6
[INFO] mapping updated!
[25, 28, 5, 23, 12, 30, 6, 8, 21, 1, 22, 15, 17, 18, 7, 0, 9, 10, 11, 13, 16, 3, 24, 2, 20, 4, 14, 31, 29, 27, 19, 26]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 101 layer 6: round 0 FastMoE=0.202 SmartMoE=0.310
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 101 layer 6: round 1 FastMoE=0.196 SmartMoE=0.141
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 101 layer 6: round 2 FastMoE=0.191 SmartMoE=0.142
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 101 layer 6: round 3 FastMoE=0.195 SmartMoE=0.142
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter111 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 111 layer 6: round 0 FastMoE=0.233 SmartMoE=0.147
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 111 layer 6: round 1 FastMoE=0.195 SmartMoE=0.146
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 111 layer 6: round 2 FastMoE=0.194 SmartMoE=0.146
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 111 layer 6: round 3 FastMoE=0.188 SmartMoE=0.145
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter121 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 121 layer 6: round 0 FastMoE=0.236 SmartMoE=0.138
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 121 layer 6: round 1 FastMoE=0.186 SmartMoE=0.136
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 121 layer 6: round 2 FastMoE=0.184 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 121 layer 6: round 3 FastMoE=0.183 SmartMoE=0.137
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter131 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 131 layer 6: round 0 FastMoE=0.246 SmartMoE=0.143
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 131 layer 6: round 1 FastMoE=0.196 SmartMoE=0.143
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 131 layer 6: round 2 FastMoE=0.197 SmartMoE=0.144
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 131 layer 6: round 3 FastMoE=0.193 SmartMoE=0.141
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter141 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 141 layer 6: round 0 FastMoE=0.208 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 141 layer 6: round 1 FastMoE=0.188 SmartMoE=0.136
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 141 layer 6: round 2 FastMoE=0.188 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 141 layer 6: round 3 FastMoE=0.190 SmartMoE=0.133
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter151 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 151 layer 6: round 0 FastMoE=0.200 SmartMoE=0.137
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 151 layer 6: round 1 FastMoE=0.182 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 151 layer 6: round 2 FastMoE=0.184 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 151 layer 6: round 3 FastMoE=0.181 SmartMoE=0.139
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter161 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 161 layer 6: round 0 FastMoE=0.188 SmartMoE=0.141
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 161 layer 6: round 1 FastMoE=0.178 SmartMoE=0.137
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 161 layer 6: round 2 FastMoE=0.179 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 161 layer 6: round 3 FastMoE=0.180 SmartMoE=0.136
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter171 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 171 layer 6: round 0 FastMoE=0.206 SmartMoE=0.138
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 171 layer 6: round 1 FastMoE=0.180 SmartMoE=0.139
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 171 layer 6: round 2 FastMoE=0.181 SmartMoE=0.139
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 171 layer 6: round 3 FastMoE=0.181 SmartMoE=0.140
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter181 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 181 layer 6: round 0 FastMoE=0.189 SmartMoE=0.139
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 181 layer 6: round 1 FastMoE=0.178 SmartMoE=0.139
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 181 layer 6: round 2 FastMoE=0.181 SmartMoE=0.138
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 181 layer 6: round 3 FastMoE=0.179 SmartMoE=0.136
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter191 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 191 layer 6: round 0 FastMoE=0.184 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 191 layer 6: round 1 FastMoE=0.175 SmartMoE=0.136
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 191 layer 6: round 2 FastMoE=0.171 SmartMoE=0.136
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 191 layer 6: round 3 FastMoE=0.177 SmartMoE=0.136
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter201 6
[25, 28, 5, 23, 12, 30, 6, 8, 21, 1, 22, 15, 17, 18, 7, 0, 9, 10, 11, 13, 16, 3, 24, 2, 20, 4, 14, 31, 29, 27, 19, 26]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 201 layer 6: round 0 FastMoE=0.195 SmartMoE=0.136
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 201 layer 6: round 1 FastMoE=0.178 SmartMoE=0.136
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 201 layer 6: round 2 FastMoE=0.180 SmartMoE=0.140
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 201 layer 6: round 3 FastMoE=0.178 SmartMoE=0.135
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter211 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 211 layer 6: round 0 FastMoE=0.193 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 211 layer 6: round 1 FastMoE=0.174 SmartMoE=0.136
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 211 layer 6: round 2 FastMoE=0.172 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 211 layer 6: round 3 FastMoE=0.177 SmartMoE=0.134
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter221 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 221 layer 6: round 0 FastMoE=0.193 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 221 layer 6: round 1 FastMoE=0.173 SmartMoE=0.137
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 221 layer 6: round 2 FastMoE=0.173 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 221 layer 6: round 3 FastMoE=0.174 SmartMoE=0.136
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter231 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 231 layer 6: round 0 FastMoE=0.196 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 231 layer 6: round 1 FastMoE=0.178 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 231 layer 6: round 2 FastMoE=0.175 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 231 layer 6: round 3 FastMoE=0.175 SmartMoE=0.134
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter241 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 241 layer 6: round 0 FastMoE=0.206 SmartMoE=0.138
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 241 layer 6: round 1 FastMoE=0.183 SmartMoE=0.143
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 241 layer 6: round 2 FastMoE=0.180 SmartMoE=0.141
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 241 layer 6: round 3 FastMoE=0.181 SmartMoE=0.142
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter251 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 251 layer 6: round 0 FastMoE=0.210 SmartMoE=0.139
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 251 layer 6: round 1 FastMoE=0.176 SmartMoE=0.139
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 251 layer 6: round 2 FastMoE=0.177 SmartMoE=0.139
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 251 layer 6: round 3 FastMoE=0.177 SmartMoE=0.136
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter261 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 261 layer 6: round 0 FastMoE=0.267 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 261 layer 6: round 1 FastMoE=0.180 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 261 layer 6: round 2 FastMoE=0.177 SmartMoE=0.137
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 261 layer 6: round 3 FastMoE=0.179 SmartMoE=0.135
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter271 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 271 layer 6: round 0 FastMoE=0.204 SmartMoE=0.138
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 271 layer 6: round 1 FastMoE=0.183 SmartMoE=0.137
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 271 layer 6: round 2 FastMoE=0.184 SmartMoE=0.137
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 271 layer 6: round 3 FastMoE=0.186 SmartMoE=0.139
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter281 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 281 layer 6: round 0 FastMoE=0.199 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 281 layer 6: round 1 FastMoE=0.178 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 281 layer 6: round 2 FastMoE=0.180 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 281 layer 6: round 3 FastMoE=0.181 SmartMoE=0.135
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter291 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 291 layer 6: round 0 FastMoE=0.183 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 291 layer 6: round 1 FastMoE=0.179 SmartMoE=0.137
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 291 layer 6: round 2 FastMoE=0.176 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 291 layer 6: round 3 FastMoE=0.176 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter301 6
[25, 28, 5, 23, 12, 30, 6, 8, 21, 1, 22, 15, 17, 18, 7, 0, 9, 10, 11, 13, 16, 3, 24, 2, 20, 4, 14, 31, 29, 27, 19, 26]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 301 layer 6: round 0 FastMoE=0.192 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 301 layer 6: round 1 FastMoE=0.178 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 301 layer 6: round 2 FastMoE=0.174 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 301 layer 6: round 3 FastMoE=0.174 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter311 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 311 layer 6: round 0 FastMoE=0.186 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 311 layer 6: round 1 FastMoE=0.177 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 311 layer 6: round 2 FastMoE=0.177 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 311 layer 6: round 3 FastMoE=0.178 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter321 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 321 layer 6: round 0 FastMoE=0.189 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 321 layer 6: round 1 FastMoE=0.173 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 321 layer 6: round 2 FastMoE=0.173 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 321 layer 6: round 3 FastMoE=0.173 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter331 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 331 layer 6: round 0 FastMoE=0.181 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 331 layer 6: round 1 FastMoE=0.174 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 331 layer 6: round 2 FastMoE=0.174 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 331 layer 6: round 3 FastMoE=0.175 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter341 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 341 layer 6: round 0 FastMoE=0.185 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 341 layer 6: round 1 FastMoE=0.169 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 341 layer 6: round 2 FastMoE=0.171 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 341 layer 6: round 3 FastMoE=0.170 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter351 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 351 layer 6: round 0 FastMoE=0.192 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 351 layer 6: round 1 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 351 layer 6: round 2 FastMoE=0.176 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 351 layer 6: round 3 FastMoE=0.174 SmartMoE=0.133
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter361 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 361 layer 6: round 0 FastMoE=0.186 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 361 layer 6: round 1 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 361 layer 6: round 2 FastMoE=0.177 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 361 layer 6: round 3 FastMoE=0.177 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter371 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 371 layer 6: round 0 FastMoE=0.178 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 371 layer 6: round 1 FastMoE=0.168 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 371 layer 6: round 2 FastMoE=0.167 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 371 layer 6: round 3 FastMoE=0.168 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter381 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 381 layer 6: round 0 FastMoE=0.184 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 381 layer 6: round 1 FastMoE=0.170 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 381 layer 6: round 2 FastMoE=0.170 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 381 layer 6: round 3 FastMoE=0.167 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter391 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 391 layer 6: round 0 FastMoE=0.174 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 391 layer 6: round 1 FastMoE=0.167 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 391 layer 6: round 2 FastMoE=0.168 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 391 layer 6: round 3 FastMoE=0.168 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter401 6
[25, 28, 5, 23, 12, 30, 6, 8, 21, 1, 22, 15, 17, 18, 7, 0, 9, 10, 11, 13, 16, 3, 24, 2, 20, 4, 14, 31, 29, 27, 19, 26]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 401 layer 6: round 0 FastMoE=0.177 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 401 layer 6: round 1 FastMoE=0.165 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 401 layer 6: round 2 FastMoE=0.167 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 401 layer 6: round 3 FastMoE=0.168 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter411 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 411 layer 6: round 0 FastMoE=0.187 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 411 layer 6: round 1 FastMoE=0.168 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 411 layer 6: round 2 FastMoE=0.167 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 411 layer 6: round 3 FastMoE=0.166 SmartMoE=0.118
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter421 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 421 layer 6: round 0 FastMoE=0.177 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 421 layer 6: round 1 FastMoE=0.170 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 421 layer 6: round 2 FastMoE=0.165 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 421 layer 6: round 3 FastMoE=0.166 SmartMoE=0.117
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter431 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 431 layer 6: round 0 FastMoE=0.173 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 431 layer 6: round 1 FastMoE=0.168 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 431 layer 6: round 2 FastMoE=0.173 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 431 layer 6: round 3 FastMoE=0.169 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter441 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 441 layer 6: round 0 FastMoE=0.207 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 441 layer 6: round 1 FastMoE=0.168 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 441 layer 6: round 2 FastMoE=0.167 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 441 layer 6: round 3 FastMoE=0.167 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter451 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 451 layer 6: round 0 FastMoE=0.181 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 451 layer 6: round 1 FastMoE=0.168 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 451 layer 6: round 2 FastMoE=0.168 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 451 layer 6: round 3 FastMoE=0.168 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter461 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 461 layer 6: round 0 FastMoE=0.179 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 461 layer 6: round 1 FastMoE=0.170 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 461 layer 6: round 2 FastMoE=0.171 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 461 layer 6: round 3 FastMoE=0.172 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter471 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 471 layer 6: round 0 FastMoE=0.193 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 471 layer 6: round 1 FastMoE=0.164 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 471 layer 6: round 2 FastMoE=0.166 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 471 layer 6: round 3 FastMoE=0.166 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter481 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 481 layer 6: round 0 FastMoE=0.175 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 481 layer 6: round 1 FastMoE=0.169 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 481 layer 6: round 2 FastMoE=0.172 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 481 layer 6: round 3 FastMoE=0.167 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter491 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 491 layer 6: round 0 FastMoE=0.179 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 491 layer 6: round 1 FastMoE=0.169 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 491 layer 6: round 2 FastMoE=0.174 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 491 layer 6: round 3 FastMoE=0.169 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter501 6
[INFO] mapping updated!
[15, 4, 31, 25, 12, 18, 19, 8, 6, 27, 9, 10, 11, 13, 26, 0, 23, 1, 2, 22, 16, 14, 3, 29, 21, 24, 17, 28, 7, 30, 20, 5]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 501 layer 6: round 0 FastMoE=0.171 SmartMoE=0.278
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 501 layer 6: round 1 FastMoE=0.173 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 501 layer 6: round 2 FastMoE=0.171 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 501 layer 6: round 3 FastMoE=0.171 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter511 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 511 layer 6: round 0 FastMoE=0.171 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 511 layer 6: round 1 FastMoE=0.165 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 511 layer 6: round 2 FastMoE=0.171 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 511 layer 6: round 3 FastMoE=0.168 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter521 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 521 layer 6: round 0 FastMoE=0.175 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 521 layer 6: round 1 FastMoE=0.170 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 521 layer 6: round 2 FastMoE=0.169 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 521 layer 6: round 3 FastMoE=0.170 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter531 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 531 layer 6: round 0 FastMoE=0.170 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 531 layer 6: round 1 FastMoE=0.168 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 531 layer 6: round 2 FastMoE=0.168 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 531 layer 6: round 3 FastMoE=0.167 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter541 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 541 layer 6: round 0 FastMoE=0.175 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 541 layer 6: round 1 FastMoE=0.167 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 541 layer 6: round 2 FastMoE=0.167 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 541 layer 6: round 3 FastMoE=0.168 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter551 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 551 layer 6: round 0 FastMoE=0.169 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 551 layer 6: round 1 FastMoE=0.171 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 551 layer 6: round 2 FastMoE=0.171 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 551 layer 6: round 3 FastMoE=0.170 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter561 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 561 layer 6: round 0 FastMoE=0.171 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 561 layer 6: round 1 FastMoE=0.164 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 561 layer 6: round 2 FastMoE=0.167 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 561 layer 6: round 3 FastMoE=0.166 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter571 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 571 layer 6: round 0 FastMoE=0.178 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 571 layer 6: round 1 FastMoE=0.171 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 571 layer 6: round 2 FastMoE=0.171 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 571 layer 6: round 3 FastMoE=0.171 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter581 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 581 layer 6: round 0 FastMoE=0.177 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 581 layer 6: round 1 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 581 layer 6: round 2 FastMoE=0.171 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 581 layer 6: round 3 FastMoE=0.172 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter591 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 591 layer 6: round 0 FastMoE=0.172 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 591 layer 6: round 1 FastMoE=0.167 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 591 layer 6: round 2 FastMoE=0.171 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 591 layer 6: round 3 FastMoE=0.169 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter601 6
[15, 4, 31, 25, 12, 18, 19, 8, 6, 27, 9, 10, 11, 13, 26, 0, 23, 1, 2, 22, 16, 14, 3, 29, 21, 24, 17, 28, 7, 30, 20, 5]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 601 layer 6: round 0 FastMoE=0.170 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 601 layer 6: round 1 FastMoE=0.169 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 601 layer 6: round 2 FastMoE=0.167 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 601 layer 6: round 3 FastMoE=0.168 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter611 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 611 layer 6: round 0 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 611 layer 6: round 1 FastMoE=0.167 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 611 layer 6: round 2 FastMoE=0.170 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 611 layer 6: round 3 FastMoE=0.168 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter621 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 621 layer 6: round 0 FastMoE=0.295 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 621 layer 6: round 1 FastMoE=0.170 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 621 layer 6: round 2 FastMoE=0.173 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 621 layer 6: round 3 FastMoE=0.172 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter631 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 631 layer 6: round 0 FastMoE=0.173 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 631 layer 6: round 1 FastMoE=0.169 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 631 layer 6: round 2 FastMoE=0.171 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 631 layer 6: round 3 FastMoE=0.167 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter641 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 641 layer 6: round 0 FastMoE=0.188 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 641 layer 6: round 1 FastMoE=0.169 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 641 layer 6: round 2 FastMoE=0.173 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 641 layer 6: round 3 FastMoE=0.169 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter651 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 651 layer 6: round 0 FastMoE=0.180 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 651 layer 6: round 1 FastMoE=0.174 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 651 layer 6: round 2 FastMoE=0.171 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 651 layer 6: round 3 FastMoE=0.173 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter661 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 661 layer 6: round 0 FastMoE=0.192 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 661 layer 6: round 1 FastMoE=0.171 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 661 layer 6: round 2 FastMoE=0.169 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 661 layer 6: round 3 FastMoE=0.168 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter671 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 671 layer 6: round 0 FastMoE=0.185 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 671 layer 6: round 1 FastMoE=0.166 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 671 layer 6: round 2 FastMoE=0.169 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 671 layer 6: round 3 FastMoE=0.169 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter681 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 681 layer 6: round 0 FastMoE=0.164 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 681 layer 6: round 1 FastMoE=0.167 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 681 layer 6: round 2 FastMoE=0.166 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 681 layer 6: round 3 FastMoE=0.164 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter691 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 691 layer 6: round 0 FastMoE=0.169 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 691 layer 6: round 1 FastMoE=0.169 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 691 layer 6: round 2 FastMoE=0.169 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 691 layer 6: round 3 FastMoE=0.172 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter701 6
[INFO] mapping updated!
[15, 21, 3, 1, 12, 2, 31, 8, 6, 24, 18, 19, 9, 10, 23, 0, 25, 11, 13, 20, 16, 7, 27, 30, 5, 22, 29, 28, 14, 26, 4, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 701 layer 6: round 0 FastMoE=0.169 SmartMoE=0.204
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 701 layer 6: round 1 FastMoE=0.169 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 701 layer 6: round 2 FastMoE=0.171 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 701 layer 6: round 3 FastMoE=0.170 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter711 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 711 layer 6: round 0 FastMoE=0.178 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 711 layer 6: round 1 FastMoE=0.168 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 711 layer 6: round 2 FastMoE=0.166 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 711 layer 6: round 3 FastMoE=0.168 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter721 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 721 layer 6: round 0 FastMoE=0.178 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 721 layer 6: round 1 FastMoE=0.168 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 721 layer 6: round 2 FastMoE=0.167 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 721 layer 6: round 3 FastMoE=0.169 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter731 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 731 layer 6: round 0 FastMoE=0.177 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 731 layer 6: round 1 FastMoE=0.167 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 731 layer 6: round 2 FastMoE=0.163 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 731 layer 6: round 3 FastMoE=0.164 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter741 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 741 layer 6: round 0 FastMoE=0.175 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 741 layer 6: round 1 FastMoE=0.166 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 741 layer 6: round 2 FastMoE=0.164 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 741 layer 6: round 3 FastMoE=0.169 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter751 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 751 layer 6: round 0 FastMoE=0.179 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 751 layer 6: round 1 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 751 layer 6: round 2 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 751 layer 6: round 3 FastMoE=0.168 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter761 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 761 layer 6: round 0 FastMoE=0.180 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 761 layer 6: round 1 FastMoE=0.165 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 761 layer 6: round 2 FastMoE=0.166 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 761 layer 6: round 3 FastMoE=0.162 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter771 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 771 layer 6: round 0 FastMoE=0.173 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 771 layer 6: round 1 FastMoE=0.161 SmartMoE=0.116
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 771 layer 6: round 2 FastMoE=0.165 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 771 layer 6: round 3 FastMoE=0.162 SmartMoE=0.116
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter781 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 781 layer 6: round 0 FastMoE=0.177 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 781 layer 6: round 1 FastMoE=0.165 SmartMoE=0.117
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 781 layer 6: round 2 FastMoE=0.166 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 781 layer 6: round 3 FastMoE=0.165 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter791 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 791 layer 6: round 0 FastMoE=0.172 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 791 layer 6: round 1 FastMoE=0.168 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 791 layer 6: round 2 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 791 layer 6: round 3 FastMoE=0.166 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter801 6
[15, 21, 3, 1, 12, 2, 31, 8, 6, 24, 18, 19, 9, 10, 23, 0, 25, 11, 13, 20, 16, 7, 27, 30, 5, 22, 29, 28, 14, 26, 4, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 801 layer 6: round 0 FastMoE=0.309 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 801 layer 6: round 1 FastMoE=0.171 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 801 layer 6: round 2 FastMoE=0.169 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 801 layer 6: round 3 FastMoE=0.165 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter811 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 811 layer 6: round 0 FastMoE=0.175 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 811 layer 6: round 1 FastMoE=0.166 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 811 layer 6: round 2 FastMoE=0.163 SmartMoE=0.118
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 811 layer 6: round 3 FastMoE=0.165 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter821 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 821 layer 6: round 0 FastMoE=0.172 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 821 layer 6: round 1 FastMoE=0.168 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 821 layer 6: round 2 FastMoE=0.169 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 821 layer 6: round 3 FastMoE=0.167 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter831 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 831 layer 6: round 0 FastMoE=0.173 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 831 layer 6: round 1 FastMoE=0.165 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 831 layer 6: round 2 FastMoE=0.168 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 831 layer 6: round 3 FastMoE=0.168 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter841 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 841 layer 6: round 0 FastMoE=0.179 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 841 layer 6: round 1 FastMoE=0.165 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 841 layer 6: round 2 FastMoE=0.165 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 841 layer 6: round 3 FastMoE=0.165 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter851 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 851 layer 6: round 0 FastMoE=0.164 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 851 layer 6: round 1 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 851 layer 6: round 2 FastMoE=0.165 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 851 layer 6: round 3 FastMoE=0.166 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter861 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 861 layer 6: round 0 FastMoE=0.160 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 861 layer 6: round 1 FastMoE=0.165 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 861 layer 6: round 2 FastMoE=0.167 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 861 layer 6: round 3 FastMoE=0.167 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter871 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 871 layer 6: round 0 FastMoE=0.167 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 871 layer 6: round 1 FastMoE=0.166 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 871 layer 6: round 2 FastMoE=0.165 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 871 layer 6: round 3 FastMoE=0.162 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter881 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 881 layer 6: round 0 FastMoE=0.178 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 881 layer 6: round 1 FastMoE=0.165 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 881 layer 6: round 2 FastMoE=0.165 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 881 layer 6: round 3 FastMoE=0.163 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter891 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 891 layer 6: round 0 FastMoE=0.171 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 891 layer 6: round 1 FastMoE=0.161 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 891 layer 6: round 2 FastMoE=0.163 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 891 layer 6: round 3 FastMoE=0.160 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter901 6
[15, 21, 3, 1, 12, 2, 31, 8, 6, 24, 18, 19, 9, 10, 23, 0, 25, 11, 13, 20, 16, 7, 27, 30, 5, 22, 29, 28, 14, 26, 4, 17]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 901 layer 6: round 0 FastMoE=0.169 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 901 layer 6: round 1 FastMoE=0.165 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 901 layer 6: round 2 FastMoE=0.165 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 901 layer 6: round 3 FastMoE=0.161 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter911 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 911 layer 6: round 0 FastMoE=0.171 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 911 layer 6: round 1 FastMoE=0.169 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 911 layer 6: round 2 FastMoE=0.167 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 911 layer 6: round 3 FastMoE=0.167 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter921 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 921 layer 6: round 0 FastMoE=0.170 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 921 layer 6: round 1 FastMoE=0.167 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 921 layer 6: round 2 FastMoE=0.166 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 921 layer 6: round 3 FastMoE=0.165 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter931 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 931 layer 6: round 0 FastMoE=0.163 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 931 layer 6: round 1 FastMoE=0.163 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 931 layer 6: round 2 FastMoE=0.161 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 931 layer 6: round 3 FastMoE=0.161 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter941 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 941 layer 6: round 0 FastMoE=0.172 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 941 layer 6: round 1 FastMoE=0.163 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 941 layer 6: round 2 FastMoE=0.166 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 941 layer 6: round 3 FastMoE=0.165 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter951 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 951 layer 6: round 0 FastMoE=0.173 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 951 layer 6: round 1 FastMoE=0.163 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 951 layer 6: round 2 FastMoE=0.165 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 951 layer 6: round 3 FastMoE=0.162 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter961 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 961 layer 6: round 0 FastMoE=0.170 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 961 layer 6: round 1 FastMoE=0.169 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 961 layer 6: round 2 FastMoE=0.167 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 961 layer 6: round 3 FastMoE=0.168 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter971 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 971 layer 6: round 0 FastMoE=0.169 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 971 layer 6: round 1 FastMoE=0.164 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 971 layer 6: round 2 FastMoE=0.161 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 971 layer 6: round 3 FastMoE=0.163 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter981 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 981 layer 6: round 0 FastMoE=0.164 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 981 layer 6: round 1 FastMoE=0.166 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 981 layer 6: round 2 FastMoE=0.167 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 981 layer 6: round 3 FastMoE=0.168 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter991 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 991 layer 6: round 0 FastMoE=0.200 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 991 layer 6: round 1 FastMoE=0.166 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 991 layer 6: round 2 FastMoE=0.162 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 991 layer 6: round 3 FastMoE=0.165 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1001 6
[INFO] mapping updated!
[15, 5, 13, 11, 12, 9, 3, 8, 25, 24, 1, 19, 18, 17, 31, 0, 6, 7, 22, 20, 16, 30, 23, 2, 4, 27, 10, 28, 14, 29, 21, 26]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1001 layer 6: round 0 FastMoE=0.170 SmartMoE=0.204
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1001 layer 6: round 1 FastMoE=0.168 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1001 layer 6: round 2 FastMoE=0.168 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1001 layer 6: round 3 FastMoE=0.170 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1011 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1011 layer 6: round 0 FastMoE=0.182 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1011 layer 6: round 1 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1011 layer 6: round 2 FastMoE=0.166 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1011 layer 6: round 3 FastMoE=0.165 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1021 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1021 layer 6: round 0 FastMoE=0.182 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1021 layer 6: round 1 FastMoE=0.162 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1021 layer 6: round 2 FastMoE=0.162 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1021 layer 6: round 3 FastMoE=0.161 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1031 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1031 layer 6: round 0 FastMoE=0.178 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1031 layer 6: round 1 FastMoE=0.164 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1031 layer 6: round 2 FastMoE=0.163 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1031 layer 6: round 3 FastMoE=0.161 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1041 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1041 layer 6: round 0 FastMoE=0.175 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1041 layer 6: round 1 FastMoE=0.168 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1041 layer 6: round 2 FastMoE=0.168 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1041 layer 6: round 3 FastMoE=0.167 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1051 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1051 layer 6: round 0 FastMoE=0.183 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1051 layer 6: round 1 FastMoE=0.167 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1051 layer 6: round 2 FastMoE=0.164 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1051 layer 6: round 3 FastMoE=0.166 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1061 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1061 layer 6: round 0 FastMoE=0.182 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1061 layer 6: round 1 FastMoE=0.163 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1061 layer 6: round 2 FastMoE=0.165 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1061 layer 6: round 3 FastMoE=0.162 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1071 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1071 layer 6: round 0 FastMoE=0.181 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1071 layer 6: round 1 FastMoE=0.161 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1071 layer 6: round 2 FastMoE=0.164 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1071 layer 6: round 3 FastMoE=0.161 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1081 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1081 layer 6: round 0 FastMoE=0.198 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1081 layer 6: round 1 FastMoE=0.163 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1081 layer 6: round 2 FastMoE=0.164 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1081 layer 6: round 3 FastMoE=0.161 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1091 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1091 layer 6: round 0 FastMoE=0.173 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1091 layer 6: round 1 FastMoE=0.166 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1091 layer 6: round 2 FastMoE=0.164 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1091 layer 6: round 3 FastMoE=0.162 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1101 6
[15, 5, 13, 11, 12, 9, 3, 8, 25, 24, 1, 19, 18, 17, 31, 0, 6, 7, 22, 20, 16, 30, 23, 2, 4, 27, 10, 28, 14, 29, 21, 26]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1101 layer 6: round 0 FastMoE=0.180 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1101 layer 6: round 1 FastMoE=0.162 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1101 layer 6: round 2 FastMoE=0.163 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1101 layer 6: round 3 FastMoE=0.163 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1111 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1111 layer 6: round 0 FastMoE=0.179 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1111 layer 6: round 1 FastMoE=0.169 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1111 layer 6: round 2 FastMoE=0.167 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1111 layer 6: round 3 FastMoE=0.160 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1121 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1121 layer 6: round 0 FastMoE=0.177 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1121 layer 6: round 1 FastMoE=0.165 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1121 layer 6: round 2 FastMoE=0.169 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1121 layer 6: round 3 FastMoE=0.164 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1131 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1131 layer 6: round 0 FastMoE=0.169 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1131 layer 6: round 1 FastMoE=0.162 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1131 layer 6: round 2 FastMoE=0.160 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1131 layer 6: round 3 FastMoE=0.164 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1141 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1141 layer 6: round 0 FastMoE=0.175 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1141 layer 6: round 1 FastMoE=0.163 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1141 layer 6: round 2 FastMoE=0.163 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1141 layer 6: round 3 FastMoE=0.164 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1151 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1151 layer 6: round 0 FastMoE=0.166 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1151 layer 6: round 1 FastMoE=0.163 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1151 layer 6: round 2 FastMoE=0.163 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1151 layer 6: round 3 FastMoE=0.164 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1161 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1161 layer 6: round 0 FastMoE=0.165 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1161 layer 6: round 1 FastMoE=0.162 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1161 layer 6: round 2 FastMoE=0.162 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1161 layer 6: round 3 FastMoE=0.164 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1171 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1171 layer 6: round 0 FastMoE=0.228 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1171 layer 6: round 1 FastMoE=0.170 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1171 layer 6: round 2 FastMoE=0.168 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1171 layer 6: round 3 FastMoE=0.168 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1181 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1181 layer 6: round 0 FastMoE=0.167 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1181 layer 6: round 1 FastMoE=0.163 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1181 layer 6: round 2 FastMoE=0.164 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1181 layer 6: round 3 FastMoE=0.161 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1191 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1191 layer 6: round 0 FastMoE=0.172 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1191 layer 6: round 1 FastMoE=0.167 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1191 layer 6: round 2 FastMoE=0.163 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1191 layer 6: round 3 FastMoE=0.164 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1201 6
[15, 5, 13, 11, 12, 9, 3, 8, 25, 24, 1, 19, 18, 17, 31, 0, 6, 7, 22, 20, 16, 30, 23, 2, 4, 27, 10, 28, 14, 29, 21, 26]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1201 layer 6: round 0 FastMoE=0.169 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1201 layer 6: round 1 FastMoE=0.164 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1201 layer 6: round 2 FastMoE=0.159 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1201 layer 6: round 3 FastMoE=0.162 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1211 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1211 layer 6: round 0 FastMoE=0.179 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1211 layer 6: round 1 FastMoE=0.162 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1211 layer 6: round 2 FastMoE=0.162 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1211 layer 6: round 3 FastMoE=0.160 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1221 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1221 layer 6: round 0 FastMoE=0.178 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1221 layer 6: round 1 FastMoE=0.168 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1221 layer 6: round 2 FastMoE=0.166 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1221 layer 6: round 3 FastMoE=0.167 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1231 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1231 layer 6: round 0 FastMoE=0.169 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1231 layer 6: round 1 FastMoE=0.161 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1231 layer 6: round 2 FastMoE=0.168 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1231 layer 6: round 3 FastMoE=0.166 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1241 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1241 layer 6: round 0 FastMoE=0.218 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1241 layer 6: round 1 FastMoE=0.165 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1241 layer 6: round 2 FastMoE=0.166 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1241 layer 6: round 3 FastMoE=0.166 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1251 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1251 layer 6: round 0 FastMoE=0.164 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1251 layer 6: round 1 FastMoE=0.168 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1251 layer 6: round 2 FastMoE=0.169 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1251 layer 6: round 3 FastMoE=0.168 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1261 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1261 layer 6: round 0 FastMoE=0.197 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1261 layer 6: round 1 FastMoE=0.164 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1261 layer 6: round 2 FastMoE=0.163 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1261 layer 6: round 3 FastMoE=0.164 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1271 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1271 layer 6: round 0 FastMoE=0.162 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1271 layer 6: round 1 FastMoE=0.169 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1271 layer 6: round 2 FastMoE=0.168 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1271 layer 6: round 3 FastMoE=0.167 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1281 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1281 layer 6: round 0 FastMoE=0.192 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1281 layer 6: round 1 FastMoE=0.166 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1281 layer 6: round 2 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1281 layer 6: round 3 FastMoE=0.165 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1291 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1291 layer 6: round 0 FastMoE=0.180 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1291 layer 6: round 1 FastMoE=0.165 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1291 layer 6: round 2 FastMoE=0.164 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1291 layer 6: round 3 FastMoE=0.166 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1301 6
[15, 5, 13, 11, 12, 9, 3, 8, 25, 24, 1, 19, 18, 17, 31, 0, 6, 7, 22, 20, 16, 30, 23, 2, 4, 27, 10, 28, 14, 29, 21, 26]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1301 layer 6: round 0 FastMoE=0.179 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1301 layer 6: round 1 FastMoE=0.167 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1301 layer 6: round 2 FastMoE=0.164 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1301 layer 6: round 3 FastMoE=0.165 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1311 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1311 layer 6: round 0 FastMoE=0.178 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1311 layer 6: round 1 FastMoE=0.163 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1311 layer 6: round 2 FastMoE=0.166 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1311 layer 6: round 3 FastMoE=0.164 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1321 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1321 layer 6: round 0 FastMoE=0.167 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1321 layer 6: round 1 FastMoE=0.165 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1321 layer 6: round 2 FastMoE=0.163 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1321 layer 6: round 3 FastMoE=0.164 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1331 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1331 layer 6: round 0 FastMoE=0.177 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1331 layer 6: round 1 FastMoE=0.168 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1331 layer 6: round 2 FastMoE=0.167 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1331 layer 6: round 3 FastMoE=0.168 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1341 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1341 layer 6: round 0 FastMoE=0.173 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1341 layer 6: round 1 FastMoE=0.163 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1341 layer 6: round 2 FastMoE=0.162 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1341 layer 6: round 3 FastMoE=0.164 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1351 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1351 layer 6: round 0 FastMoE=0.210 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1351 layer 6: round 1 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1351 layer 6: round 2 FastMoE=0.165 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1351 layer 6: round 3 FastMoE=0.168 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1361 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1361 layer 6: round 0 FastMoE=0.218 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1361 layer 6: round 1 FastMoE=0.165 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1361 layer 6: round 2 FastMoE=0.165 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1361 layer 6: round 3 FastMoE=0.166 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1371 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1371 layer 6: round 0 FastMoE=0.187 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1371 layer 6: round 1 FastMoE=0.162 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1371 layer 6: round 2 FastMoE=0.163 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1371 layer 6: round 3 FastMoE=0.161 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1381 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1381 layer 6: round 0 FastMoE=0.203 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1381 layer 6: round 1 FastMoE=0.168 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1381 layer 6: round 2 FastMoE=0.162 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1381 layer 6: round 3 FastMoE=0.165 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1391 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1391 layer 6: round 0 FastMoE=0.183 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1391 layer 6: round 1 FastMoE=0.163 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1391 layer 6: round 2 FastMoE=0.165 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1391 layer 6: round 3 FastMoE=0.164 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1401 6
[INFO] mapping updated!
[15, 25, 31, 21, 12, 1, 2, 8, 6, 24, 17, 18, 23, 19, 30, 0, 26, 7, 9, 20, 16, 29, 10, 11, 4, 22, 13, 28, 14, 27, 5, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1401 layer 6: round 0 FastMoE=0.190 SmartMoE=0.187
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1401 layer 6: round 1 FastMoE=0.166 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1401 layer 6: round 2 FastMoE=0.163 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1401 layer 6: round 3 FastMoE=0.163 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1411 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1411 layer 6: round 0 FastMoE=0.190 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1411 layer 6: round 1 FastMoE=0.164 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1411 layer 6: round 2 FastMoE=0.165 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1411 layer 6: round 3 FastMoE=0.169 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1421 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1421 layer 6: round 0 FastMoE=0.206 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1421 layer 6: round 1 FastMoE=0.166 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1421 layer 6: round 2 FastMoE=0.166 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1421 layer 6: round 3 FastMoE=0.168 SmartMoE=0.122
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1431 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1431 layer 6: round 0 FastMoE=0.162 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1431 layer 6: round 1 FastMoE=0.163 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1431 layer 6: round 2 FastMoE=0.164 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1431 layer 6: round 3 FastMoE=0.168 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1441 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1441 layer 6: round 0 FastMoE=0.165 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1441 layer 6: round 1 FastMoE=0.165 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1441 layer 6: round 2 FastMoE=0.168 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1441 layer 6: round 3 FastMoE=0.166 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1451 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1451 layer 6: round 0 FastMoE=0.176 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1451 layer 6: round 1 FastMoE=0.166 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1451 layer 6: round 2 FastMoE=0.164 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1451 layer 6: round 3 FastMoE=0.165 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1461 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1461 layer 6: round 0 FastMoE=0.158 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1461 layer 6: round 1 FastMoE=0.165 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1461 layer 6: round 2 FastMoE=0.164 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1461 layer 6: round 3 FastMoE=0.166 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1471 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1471 layer 6: round 0 FastMoE=0.163 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1471 layer 6: round 1 FastMoE=0.164 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1471 layer 6: round 2 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1471 layer 6: round 3 FastMoE=0.168 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1481 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1481 layer 6: round 0 FastMoE=0.174 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1481 layer 6: round 1 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1481 layer 6: round 2 FastMoE=0.168 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1481 layer 6: round 3 FastMoE=0.166 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1491 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1491 layer 6: round 0 FastMoE=0.165 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1491 layer 6: round 1 FastMoE=0.164 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1491 layer 6: round 2 FastMoE=0.166 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1491 layer 6: round 3 FastMoE=0.165 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1501 6
[15, 25, 31, 21, 12, 1, 2, 8, 6, 24, 17, 18, 23, 19, 30, 0, 26, 7, 9, 20, 16, 29, 10, 11, 4, 22, 13, 28, 14, 27, 5, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1501 layer 6: round 0 FastMoE=0.171 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1501 layer 6: round 1 FastMoE=0.166 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1501 layer 6: round 2 FastMoE=0.167 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1501 layer 6: round 3 FastMoE=0.168 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1511 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1511 layer 6: round 0 FastMoE=0.168 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1511 layer 6: round 1 FastMoE=0.163 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1511 layer 6: round 2 FastMoE=0.163 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1511 layer 6: round 3 FastMoE=0.164 SmartMoE=0.119
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1521 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1521 layer 6: round 0 FastMoE=0.171 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1521 layer 6: round 1 FastMoE=0.167 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1521 layer 6: round 2 FastMoE=0.167 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1521 layer 6: round 3 FastMoE=0.165 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1531 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1531 layer 6: round 0 FastMoE=0.172 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1531 layer 6: round 1 FastMoE=0.167 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1531 layer 6: round 2 FastMoE=0.164 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1531 layer 6: round 3 FastMoE=0.165 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1541 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1541 layer 6: round 0 FastMoE=0.220 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1541 layer 6: round 1 FastMoE=0.166 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1541 layer 6: round 2 FastMoE=0.166 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1541 layer 6: round 3 FastMoE=0.169 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1551 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1551 layer 6: round 0 FastMoE=0.168 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1551 layer 6: round 1 FastMoE=0.162 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1551 layer 6: round 2 FastMoE=0.165 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1551 layer 6: round 3 FastMoE=0.165 SmartMoE=0.120
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1561 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1561 layer 6: round 0 FastMoE=0.174 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1561 layer 6: round 1 FastMoE=0.171 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1561 layer 6: round 2 FastMoE=0.170 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1561 layer 6: round 3 FastMoE=0.171 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1571 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1571 layer 6: round 0 FastMoE=0.174 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1571 layer 6: round 1 FastMoE=0.165 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1571 layer 6: round 2 FastMoE=0.165 SmartMoE=0.119
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1571 layer 6: round 3 FastMoE=0.161 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1581 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1581 layer 6: round 0 FastMoE=0.170 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1581 layer 6: round 1 FastMoE=0.169 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1581 layer 6: round 2 FastMoE=0.170 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1581 layer 6: round 3 FastMoE=0.169 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1591 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1591 layer 6: round 0 FastMoE=0.173 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1591 layer 6: round 1 FastMoE=0.171 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1591 layer 6: round 2 FastMoE=0.170 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1591 layer 6: round 3 FastMoE=0.171 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1601 6
[15, 25, 31, 21, 12, 1, 2, 8, 6, 24, 17, 18, 23, 19, 30, 0, 26, 7, 9, 20, 16, 29, 10, 11, 4, 22, 13, 28, 14, 27, 5, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1601 layer 6: round 0 FastMoE=0.165 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1601 layer 6: round 1 FastMoE=0.163 SmartMoE=0.120
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1601 layer 6: round 2 FastMoE=0.166 SmartMoE=0.121
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1601 layer 6: round 3 FastMoE=0.165 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1611 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1611 layer 6: round 0 FastMoE=0.167 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1611 layer 6: round 1 FastMoE=0.171 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1611 layer 6: round 2 FastMoE=0.170 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1611 layer 6: round 3 FastMoE=0.170 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1621 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1621 layer 6: round 0 FastMoE=0.163 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1621 layer 6: round 1 FastMoE=0.167 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1621 layer 6: round 2 FastMoE=0.167 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1621 layer 6: round 3 FastMoE=0.169 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1631 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1631 layer 6: round 0 FastMoE=0.167 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1631 layer 6: round 1 FastMoE=0.172 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1631 layer 6: round 2 FastMoE=0.170 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1631 layer 6: round 3 FastMoE=0.172 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1641 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1641 layer 6: round 0 FastMoE=0.167 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1641 layer 6: round 1 FastMoE=0.170 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1641 layer 6: round 2 FastMoE=0.168 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1641 layer 6: round 3 FastMoE=0.169 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1651 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1651 layer 6: round 0 FastMoE=0.168 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1651 layer 6: round 1 FastMoE=0.172 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1651 layer 6: round 2 FastMoE=0.167 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1651 layer 6: round 3 FastMoE=0.173 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1661 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1661 layer 6: round 0 FastMoE=0.180 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1661 layer 6: round 1 FastMoE=0.169 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1661 layer 6: round 2 FastMoE=0.166 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1661 layer 6: round 3 FastMoE=0.168 SmartMoE=0.123
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1671 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1671 layer 6: round 0 FastMoE=0.175 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1671 layer 6: round 1 FastMoE=0.169 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1671 layer 6: round 2 FastMoE=0.172 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1671 layer 6: round 3 FastMoE=0.171 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1681 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1681 layer 6: round 0 FastMoE=0.166 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1681 layer 6: round 1 FastMoE=0.167 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1681 layer 6: round 2 FastMoE=0.168 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1681 layer 6: round 3 FastMoE=0.168 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1691 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1691 layer 6: round 0 FastMoE=0.168 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1691 layer 6: round 1 FastMoE=0.168 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1691 layer 6: round 2 FastMoE=0.168 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1691 layer 6: round 3 FastMoE=0.168 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1701 6
[15, 25, 31, 21, 12, 1, 2, 8, 6, 24, 17, 18, 23, 19, 30, 0, 26, 7, 9, 20, 16, 29, 10, 11, 4, 22, 13, 28, 14, 27, 5, 3]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1701 layer 6: round 0 FastMoE=0.164 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1701 layer 6: round 1 FastMoE=0.168 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1701 layer 6: round 2 FastMoE=0.167 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1701 layer 6: round 3 FastMoE=0.168 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1711 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1711 layer 6: round 0 FastMoE=0.179 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1711 layer 6: round 1 FastMoE=0.170 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1711 layer 6: round 2 FastMoE=0.171 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1711 layer 6: round 3 FastMoE=0.169 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1721 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1721 layer 6: round 0 FastMoE=0.199 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1721 layer 6: round 1 FastMoE=0.172 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1721 layer 6: round 2 FastMoE=0.172 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1721 layer 6: round 3 FastMoE=0.172 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1731 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1731 layer 6: round 0 FastMoE=0.179 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1731 layer 6: round 1 FastMoE=0.170 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1731 layer 6: round 2 FastMoE=0.168 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1731 layer 6: round 3 FastMoE=0.170 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1741 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1741 layer 6: round 0 FastMoE=0.171 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1741 layer 6: round 1 FastMoE=0.174 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1741 layer 6: round 2 FastMoE=0.173 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1741 layer 6: round 3 FastMoE=0.172 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1751 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1751 layer 6: round 0 FastMoE=0.173 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1751 layer 6: round 1 FastMoE=0.169 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1751 layer 6: round 2 FastMoE=0.167 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1751 layer 6: round 3 FastMoE=0.167 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1761 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1761 layer 6: round 0 FastMoE=0.176 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1761 layer 6: round 1 FastMoE=0.171 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1761 layer 6: round 2 FastMoE=0.171 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1761 layer 6: round 3 FastMoE=0.171 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1771 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1771 layer 6: round 0 FastMoE=0.185 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1771 layer 6: round 1 FastMoE=0.174 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1771 layer 6: round 2 FastMoE=0.173 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1771 layer 6: round 3 FastMoE=0.172 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1781 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1781 layer 6: round 0 FastMoE=0.179 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1781 layer 6: round 1 FastMoE=0.170 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1781 layer 6: round 2 FastMoE=0.167 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1781 layer 6: round 3 FastMoE=0.172 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1791 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1791 layer 6: round 0 FastMoE=0.188 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1791 layer 6: round 1 FastMoE=0.168 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1791 layer 6: round 2 FastMoE=0.173 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1791 layer 6: round 3 FastMoE=0.173 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1801 6
[INFO] mapping updated!
[15, 25, 31, 13, 12, 10, 9, 8, 26, 24, 17, 22, 7, 3, 21, 0, 5, 1, 18, 20, 16, 29, 30, 19, 4, 11, 2, 28, 14, 6, 27, 23]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1801 layer 6: round 0 FastMoE=0.179 SmartMoE=0.191
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1801 layer 6: round 1 FastMoE=0.166 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1801 layer 6: round 2 FastMoE=0.167 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1801 layer 6: round 3 FastMoE=0.166 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1811 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1811 layer 6: round 0 FastMoE=0.182 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1811 layer 6: round 1 FastMoE=0.173 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1811 layer 6: round 2 FastMoE=0.176 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1811 layer 6: round 3 FastMoE=0.171 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1821 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1821 layer 6: round 0 FastMoE=0.185 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1821 layer 6: round 1 FastMoE=0.173 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1821 layer 6: round 2 FastMoE=0.173 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1821 layer 6: round 3 FastMoE=0.172 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1831 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1831 layer 6: round 0 FastMoE=0.175 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1831 layer 6: round 1 FastMoE=0.168 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1831 layer 6: round 2 FastMoE=0.168 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1831 layer 6: round 3 FastMoE=0.169 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1841 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1841 layer 6: round 0 FastMoE=0.169 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1841 layer 6: round 1 FastMoE=0.169 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1841 layer 6: round 2 FastMoE=0.171 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1841 layer 6: round 3 FastMoE=0.171 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1851 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1851 layer 6: round 0 FastMoE=0.171 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1851 layer 6: round 1 FastMoE=0.171 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1851 layer 6: round 2 FastMoE=0.172 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1851 layer 6: round 3 FastMoE=0.170 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1861 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1861 layer 6: round 0 FastMoE=0.169 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1861 layer 6: round 1 FastMoE=0.165 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1861 layer 6: round 2 FastMoE=0.165 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1861 layer 6: round 3 FastMoE=0.165 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1871 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1871 layer 6: round 0 FastMoE=0.179 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1871 layer 6: round 1 FastMoE=0.174 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1871 layer 6: round 2 FastMoE=0.173 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1871 layer 6: round 3 FastMoE=0.173 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1881 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1881 layer 6: round 0 FastMoE=0.175 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1881 layer 6: round 1 FastMoE=0.172 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1881 layer 6: round 2 FastMoE=0.165 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1881 layer 6: round 3 FastMoE=0.166 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1891 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1891 layer 6: round 0 FastMoE=0.174 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1891 layer 6: round 1 FastMoE=0.173 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1891 layer 6: round 2 FastMoE=0.173 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1891 layer 6: round 3 FastMoE=0.171 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1901 6
[INFO] mapping updated!
[15, 5, 14, 13, 12, 10, 9, 8, 21, 24, 29, 30, 6, 19, 27, 0, 25, 7, 1, 20, 16, 22, 23, 2, 4, 3, 11, 28, 18, 26, 17, 31]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1901 layer 6: round 0 FastMoE=0.175 SmartMoE=0.197
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1901 layer 6: round 1 FastMoE=0.178 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1901 layer 6: round 2 FastMoE=0.175 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1901 layer 6: round 3 FastMoE=0.178 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1911 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1911 layer 6: round 0 FastMoE=0.184 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1911 layer 6: round 1 FastMoE=0.167 SmartMoE=0.123
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1911 layer 6: round 2 FastMoE=0.167 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1911 layer 6: round 3 FastMoE=0.166 SmartMoE=0.121
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1921 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1921 layer 6: round 0 FastMoE=0.169 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1921 layer 6: round 1 FastMoE=0.171 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1921 layer 6: round 2 FastMoE=0.171 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1921 layer 6: round 3 FastMoE=0.169 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1931 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1931 layer 6: round 0 FastMoE=0.171 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1931 layer 6: round 1 FastMoE=0.171 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1931 layer 6: round 2 FastMoE=0.169 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1931 layer 6: round 3 FastMoE=0.169 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1941 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1941 layer 6: round 0 FastMoE=0.174 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1941 layer 6: round 1 FastMoE=0.169 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1941 layer 6: round 2 FastMoE=0.170 SmartMoE=0.124
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1941 layer 6: round 3 FastMoE=0.171 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1951 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1951 layer 6: round 0 FastMoE=0.183 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1951 layer 6: round 1 FastMoE=0.174 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1951 layer 6: round 2 FastMoE=0.174 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1951 layer 6: round 3 FastMoE=0.172 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1961 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1961 layer 6: round 0 FastMoE=0.189 SmartMoE=0.122
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1961 layer 6: round 1 FastMoE=0.167 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1961 layer 6: round 2 FastMoE=0.171 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1961 layer 6: round 3 FastMoE=0.168 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1971 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1971 layer 6: round 0 FastMoE=0.206 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1971 layer 6: round 1 FastMoE=0.171 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1971 layer 6: round 2 FastMoE=0.171 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1971 layer 6: round 3 FastMoE=0.170 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1981 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1981 layer 6: round 0 FastMoE=0.182 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1981 layer 6: round 1 FastMoE=0.172 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1981 layer 6: round 2 FastMoE=0.170 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1981 layer 6: round 3 FastMoE=0.174 SmartMoE=0.125
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter1991 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1991 layer 6: round 0 FastMoE=0.180 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1991 layer 6: round 1 FastMoE=0.170 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1991 layer 6: round 2 FastMoE=0.169 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 1991 layer 6: round 3 FastMoE=0.171 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2001 6
[INFO] mapping updated!
[15, 5, 29, 11, 12, 9, 3, 8, 6, 24, 19, 30, 26, 18, 23, 0, 25, 7, 17, 20, 16, 22, 1, 2, 4, 10, 13, 28, 14, 21, 27, 31]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2001 layer 6: round 0 FastMoE=0.173 SmartMoE=0.195
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2001 layer 6: round 1 FastMoE=0.170 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2001 layer 6: round 2 FastMoE=0.173 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2001 layer 6: round 3 FastMoE=0.170 SmartMoE=0.124
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2011 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2011 layer 6: round 0 FastMoE=0.182 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2011 layer 6: round 1 FastMoE=0.175 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2011 layer 6: round 2 FastMoE=0.173 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2011 layer 6: round 3 FastMoE=0.175 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2021 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2021 layer 6: round 0 FastMoE=0.177 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2021 layer 6: round 1 FastMoE=0.174 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2021 layer 6: round 2 FastMoE=0.175 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2021 layer 6: round 3 FastMoE=0.174 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2031 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2031 layer 6: round 0 FastMoE=0.174 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2031 layer 6: round 1 FastMoE=0.170 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2031 layer 6: round 2 FastMoE=0.172 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2031 layer 6: round 3 FastMoE=0.173 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2041 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2041 layer 6: round 0 FastMoE=0.178 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2041 layer 6: round 1 FastMoE=0.173 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2041 layer 6: round 2 FastMoE=0.173 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2041 layer 6: round 3 FastMoE=0.173 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2051 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2051 layer 6: round 0 FastMoE=0.187 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2051 layer 6: round 1 FastMoE=0.172 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2051 layer 6: round 2 FastMoE=0.169 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2051 layer 6: round 3 FastMoE=0.172 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2061 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2061 layer 6: round 0 FastMoE=0.185 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2061 layer 6: round 1 FastMoE=0.172 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2061 layer 6: round 2 FastMoE=0.172 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2061 layer 6: round 3 FastMoE=0.173 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2071 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2071 layer 6: round 0 FastMoE=0.181 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2071 layer 6: round 1 FastMoE=0.172 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2071 layer 6: round 2 FastMoE=0.174 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2071 layer 6: round 3 FastMoE=0.169 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2081 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2081 layer 6: round 0 FastMoE=0.184 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2081 layer 6: round 1 FastMoE=0.173 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2081 layer 6: round 2 FastMoE=0.173 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2081 layer 6: round 3 FastMoE=0.172 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2091 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2091 layer 6: round 0 FastMoE=0.252 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2091 layer 6: round 1 FastMoE=0.174 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2091 layer 6: round 2 FastMoE=0.173 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2091 layer 6: round 3 FastMoE=0.175 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2101 6
[INFO] mapping updated!
[15, 25, 31, 11, 12, 9, 19, 8, 21, 24, 3, 2, 27, 17, 23, 0, 5, 22, 18, 20, 16, 6, 1, 10, 4, 13, 14, 28, 30, 26, 29, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2101 layer 6: round 0 FastMoE=0.182 SmartMoE=0.197
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2101 layer 6: round 1 FastMoE=0.175 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2101 layer 6: round 2 FastMoE=0.174 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2101 layer 6: round 3 FastMoE=0.174 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2111 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2111 layer 6: round 0 FastMoE=0.182 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2111 layer 6: round 1 FastMoE=0.174 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2111 layer 6: round 2 FastMoE=0.176 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2111 layer 6: round 3 FastMoE=0.172 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2121 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2121 layer 6: round 0 FastMoE=0.181 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2121 layer 6: round 1 FastMoE=0.179 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2121 layer 6: round 2 FastMoE=0.177 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2121 layer 6: round 3 FastMoE=0.173 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2131 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2131 layer 6: round 0 FastMoE=0.188 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2131 layer 6: round 1 FastMoE=0.175 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2131 layer 6: round 2 FastMoE=0.176 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2131 layer 6: round 3 FastMoE=0.177 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2141 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2141 layer 6: round 0 FastMoE=0.179 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2141 layer 6: round 1 FastMoE=0.173 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2141 layer 6: round 2 FastMoE=0.175 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2141 layer 6: round 3 FastMoE=0.176 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2151 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2151 layer 6: round 0 FastMoE=0.185 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2151 layer 6: round 1 FastMoE=0.174 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2151 layer 6: round 2 FastMoE=0.172 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2151 layer 6: round 3 FastMoE=0.174 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2161 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2161 layer 6: round 0 FastMoE=0.185 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2161 layer 6: round 1 FastMoE=0.178 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2161 layer 6: round 2 FastMoE=0.174 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2161 layer 6: round 3 FastMoE=0.175 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2171 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2171 layer 6: round 0 FastMoE=0.184 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2171 layer 6: round 1 FastMoE=0.172 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2171 layer 6: round 2 FastMoE=0.175 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2171 layer 6: round 3 FastMoE=0.175 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2181 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2181 layer 6: round 0 FastMoE=0.187 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2181 layer 6: round 1 FastMoE=0.172 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2181 layer 6: round 2 FastMoE=0.169 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2181 layer 6: round 3 FastMoE=0.173 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2191 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2191 layer 6: round 0 FastMoE=0.183 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2191 layer 6: round 1 FastMoE=0.173 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2191 layer 6: round 2 FastMoE=0.180 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2191 layer 6: round 3 FastMoE=0.176 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2201 6
[15, 25, 31, 11, 12, 9, 19, 8, 21, 24, 3, 2, 27, 17, 23, 0, 5, 22, 18, 20, 16, 6, 1, 10, 4, 13, 14, 28, 30, 26, 29, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2201 layer 6: round 0 FastMoE=0.187 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2201 layer 6: round 1 FastMoE=0.177 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2201 layer 6: round 2 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2201 layer 6: round 3 FastMoE=0.173 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2211 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2211 layer 6: round 0 FastMoE=0.186 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2211 layer 6: round 1 FastMoE=0.181 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2211 layer 6: round 2 FastMoE=0.178 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2211 layer 6: round 3 FastMoE=0.179 SmartMoE=0.133
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2221 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2221 layer 6: round 0 FastMoE=0.185 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2221 layer 6: round 1 FastMoE=0.177 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2221 layer 6: round 2 FastMoE=0.175 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2221 layer 6: round 3 FastMoE=0.175 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2231 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2231 layer 6: round 0 FastMoE=0.172 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2231 layer 6: round 1 FastMoE=0.175 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2231 layer 6: round 2 FastMoE=0.173 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2231 layer 6: round 3 FastMoE=0.173 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2241 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2241 layer 6: round 0 FastMoE=0.182 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2241 layer 6: round 1 FastMoE=0.174 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2241 layer 6: round 2 FastMoE=0.174 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2241 layer 6: round 3 FastMoE=0.178 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2251 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2251 layer 6: round 0 FastMoE=0.185 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2251 layer 6: round 1 FastMoE=0.175 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2251 layer 6: round 2 FastMoE=0.175 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2251 layer 6: round 3 FastMoE=0.175 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2261 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2261 layer 6: round 0 FastMoE=0.214 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2261 layer 6: round 1 FastMoE=0.174 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2261 layer 6: round 2 FastMoE=0.173 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2261 layer 6: round 3 FastMoE=0.174 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2271 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2271 layer 6: round 0 FastMoE=0.187 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2271 layer 6: round 1 FastMoE=0.181 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2271 layer 6: round 2 FastMoE=0.176 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2271 layer 6: round 3 FastMoE=0.178 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2281 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2281 layer 6: round 0 FastMoE=0.182 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2281 layer 6: round 1 FastMoE=0.176 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2281 layer 6: round 2 FastMoE=0.178 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2281 layer 6: round 3 FastMoE=0.176 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2291 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2291 layer 6: round 0 FastMoE=0.186 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2291 layer 6: round 1 FastMoE=0.173 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2291 layer 6: round 2 FastMoE=0.176 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2291 layer 6: round 3 FastMoE=0.175 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2301 6
[15, 25, 31, 11, 12, 9, 19, 8, 21, 24, 3, 2, 27, 17, 23, 0, 5, 22, 18, 20, 16, 6, 1, 10, 4, 13, 14, 28, 30, 26, 29, 7]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2301 layer 6: round 0 FastMoE=0.185 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2301 layer 6: round 1 FastMoE=0.170 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2301 layer 6: round 2 FastMoE=0.170 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2301 layer 6: round 3 FastMoE=0.169 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2311 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2311 layer 6: round 0 FastMoE=0.182 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2311 layer 6: round 1 FastMoE=0.172 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2311 layer 6: round 2 FastMoE=0.173 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2311 layer 6: round 3 FastMoE=0.173 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2321 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2321 layer 6: round 0 FastMoE=0.186 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2321 layer 6: round 1 FastMoE=0.175 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2321 layer 6: round 2 FastMoE=0.177 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2321 layer 6: round 3 FastMoE=0.174 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2331 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2331 layer 6: round 0 FastMoE=0.182 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2331 layer 6: round 1 FastMoE=0.175 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2331 layer 6: round 2 FastMoE=0.175 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2331 layer 6: round 3 FastMoE=0.176 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2341 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2341 layer 6: round 0 FastMoE=0.191 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2341 layer 6: round 1 FastMoE=0.175 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2341 layer 6: round 2 FastMoE=0.174 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2341 layer 6: round 3 FastMoE=0.175 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2351 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2351 layer 6: round 0 FastMoE=0.183 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2351 layer 6: round 1 FastMoE=0.175 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2351 layer 6: round 2 FastMoE=0.174 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2351 layer 6: round 3 FastMoE=0.171 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2361 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2361 layer 6: round 0 FastMoE=0.189 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2361 layer 6: round 1 FastMoE=0.174 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2361 layer 6: round 2 FastMoE=0.172 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2361 layer 6: round 3 FastMoE=0.174 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2371 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2371 layer 6: round 0 FastMoE=0.179 SmartMoE=0.125
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2371 layer 6: round 1 FastMoE=0.171 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2371 layer 6: round 2 FastMoE=0.170 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2371 layer 6: round 3 FastMoE=0.168 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2381 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2381 layer 6: round 0 FastMoE=0.184 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2381 layer 6: round 1 FastMoE=0.178 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2381 layer 6: round 2 FastMoE=0.182 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2381 layer 6: round 3 FastMoE=0.181 SmartMoE=0.134
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2391 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2391 layer 6: round 0 FastMoE=0.166 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2391 layer 6: round 1 FastMoE=0.173 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2391 layer 6: round 2 FastMoE=0.172 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2391 layer 6: round 3 FastMoE=0.171 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2401 6
[INFO] mapping updated!
[15, 21, 31, 30, 12, 10, 9, 8, 5, 24, 17, 29, 27, 18, 6, 0, 25, 2, 19, 20, 16, 23, 7, 11, 4, 13, 3, 28, 14, 26, 22, 1]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2401 layer 6: round 0 FastMoE=0.182 SmartMoE=0.197
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2401 layer 6: round 1 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2401 layer 6: round 2 FastMoE=0.175 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2401 layer 6: round 3 FastMoE=0.177 SmartMoE=0.134
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2411 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2411 layer 6: round 0 FastMoE=0.179 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2411 layer 6: round 1 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2411 layer 6: round 2 FastMoE=0.180 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2411 layer 6: round 3 FastMoE=0.176 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2421 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2421 layer 6: round 0 FastMoE=0.188 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2421 layer 6: round 1 FastMoE=0.176 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2421 layer 6: round 2 FastMoE=0.175 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2421 layer 6: round 3 FastMoE=0.176 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2431 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2431 layer 6: round 0 FastMoE=0.184 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2431 layer 6: round 1 FastMoE=0.174 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2431 layer 6: round 2 FastMoE=0.175 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2431 layer 6: round 3 FastMoE=0.174 SmartMoE=0.127
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2441 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2441 layer 6: round 0 FastMoE=0.243 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2441 layer 6: round 1 FastMoE=0.178 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2441 layer 6: round 2 FastMoE=0.178 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2441 layer 6: round 3 FastMoE=0.177 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2451 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2451 layer 6: round 0 FastMoE=0.171 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2451 layer 6: round 1 FastMoE=0.173 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2451 layer 6: round 2 FastMoE=0.173 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2451 layer 6: round 3 FastMoE=0.173 SmartMoE=0.126
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2461 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2461 layer 6: round 0 FastMoE=0.183 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2461 layer 6: round 1 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2461 layer 6: round 2 FastMoE=0.176 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2461 layer 6: round 3 FastMoE=0.176 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2471 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2471 layer 6: round 0 FastMoE=0.180 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2471 layer 6: round 1 FastMoE=0.175 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2471 layer 6: round 2 FastMoE=0.175 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2471 layer 6: round 3 FastMoE=0.174 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2481 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2481 layer 6: round 0 FastMoE=0.179 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2481 layer 6: round 1 FastMoE=0.174 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2481 layer 6: round 2 FastMoE=0.173 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2481 layer 6: round 3 FastMoE=0.172 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2491 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2491 layer 6: round 0 FastMoE=0.181 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2491 layer 6: round 1 FastMoE=0.174 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2491 layer 6: round 2 FastMoE=0.174 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2491 layer 6: round 3 FastMoE=0.175 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2501 6
[15, 21, 31, 30, 12, 10, 9, 8, 5, 24, 17, 29, 27, 18, 6, 0, 25, 2, 19, 20, 16, 23, 7, 11, 4, 13, 3, 28, 14, 26, 22, 1]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2501 layer 6: round 0 FastMoE=0.184 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2501 layer 6: round 1 FastMoE=0.177 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2501 layer 6: round 2 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2501 layer 6: round 3 FastMoE=0.176 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2511 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2511 layer 6: round 0 FastMoE=0.179 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2511 layer 6: round 1 FastMoE=0.175 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2511 layer 6: round 2 FastMoE=0.177 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2511 layer 6: round 3 FastMoE=0.175 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2521 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2521 layer 6: round 0 FastMoE=0.175 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2521 layer 6: round 1 FastMoE=0.176 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2521 layer 6: round 2 FastMoE=0.176 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2521 layer 6: round 3 FastMoE=0.175 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2531 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2531 layer 6: round 0 FastMoE=0.179 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2531 layer 6: round 1 FastMoE=0.176 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2531 layer 6: round 2 FastMoE=0.176 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2531 layer 6: round 3 FastMoE=0.175 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2541 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2541 layer 6: round 0 FastMoE=0.182 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2541 layer 6: round 1 FastMoE=0.177 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2541 layer 6: round 2 FastMoE=0.177 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2541 layer 6: round 3 FastMoE=0.175 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2551 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2551 layer 6: round 0 FastMoE=0.177 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2551 layer 6: round 1 FastMoE=0.175 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2551 layer 6: round 2 FastMoE=0.177 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2551 layer 6: round 3 FastMoE=0.177 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2561 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2561 layer 6: round 0 FastMoE=0.177 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2561 layer 6: round 1 FastMoE=0.178 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2561 layer 6: round 2 FastMoE=0.177 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2561 layer 6: round 3 FastMoE=0.178 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2571 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2571 layer 6: round 0 FastMoE=0.192 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2571 layer 6: round 1 FastMoE=0.180 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2571 layer 6: round 2 FastMoE=0.179 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2571 layer 6: round 3 FastMoE=0.178 SmartMoE=0.133
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2581 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2581 layer 6: round 0 FastMoE=0.179 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2581 layer 6: round 1 FastMoE=0.175 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2581 layer 6: round 2 FastMoE=0.177 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2581 layer 6: round 3 FastMoE=0.175 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2591 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2591 layer 6: round 0 FastMoE=0.176 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2591 layer 6: round 1 FastMoE=0.176 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2591 layer 6: round 2 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2591 layer 6: round 3 FastMoE=0.178 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2601 6
[15, 21, 31, 30, 12, 10, 9, 8, 5, 24, 17, 29, 27, 18, 6, 0, 25, 2, 19, 20, 16, 23, 7, 11, 4, 13, 3, 28, 14, 26, 22, 1]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2601 layer 6: round 0 FastMoE=0.175 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2601 layer 6: round 1 FastMoE=0.178 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2601 layer 6: round 2 FastMoE=0.175 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2601 layer 6: round 3 FastMoE=0.176 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2611 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2611 layer 6: round 0 FastMoE=0.182 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2611 layer 6: round 1 FastMoE=0.176 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2611 layer 6: round 2 FastMoE=0.178 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2611 layer 6: round 3 FastMoE=0.178 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2621 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2621 layer 6: round 0 FastMoE=0.252 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2621 layer 6: round 1 FastMoE=0.174 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2621 layer 6: round 2 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2621 layer 6: round 3 FastMoE=0.178 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2631 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2631 layer 6: round 0 FastMoE=0.181 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2631 layer 6: round 1 FastMoE=0.176 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2631 layer 6: round 2 FastMoE=0.175 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2631 layer 6: round 3 FastMoE=0.176 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2641 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2641 layer 6: round 0 FastMoE=0.174 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2641 layer 6: round 1 FastMoE=0.178 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2641 layer 6: round 2 FastMoE=0.175 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2641 layer 6: round 3 FastMoE=0.175 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2651 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2651 layer 6: round 0 FastMoE=0.184 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2651 layer 6: round 1 FastMoE=0.177 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2651 layer 6: round 2 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2651 layer 6: round 3 FastMoE=0.177 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2661 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2661 layer 6: round 0 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2661 layer 6: round 1 FastMoE=0.176 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2661 layer 6: round 2 FastMoE=0.179 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2661 layer 6: round 3 FastMoE=0.179 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2671 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2671 layer 6: round 0 FastMoE=0.181 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2671 layer 6: round 1 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2671 layer 6: round 2 FastMoE=0.178 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2671 layer 6: round 3 FastMoE=0.181 SmartMoE=0.133
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2681 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2681 layer 6: round 0 FastMoE=0.177 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2681 layer 6: round 1 FastMoE=0.176 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2681 layer 6: round 2 FastMoE=0.174 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2681 layer 6: round 3 FastMoE=0.176 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2691 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2691 layer 6: round 0 FastMoE=0.180 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2691 layer 6: round 1 FastMoE=0.177 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2691 layer 6: round 2 FastMoE=0.174 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2691 layer 6: round 3 FastMoE=0.178 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2701 6
[15, 21, 31, 30, 12, 10, 9, 8, 5, 24, 17, 29, 27, 18, 6, 0, 25, 2, 19, 20, 16, 23, 7, 11, 4, 13, 3, 28, 14, 26, 22, 1]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2701 layer 6: round 0 FastMoE=0.177 SmartMoE=0.127
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2701 layer 6: round 1 FastMoE=0.174 SmartMoE=0.126
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2701 layer 6: round 2 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2701 layer 6: round 3 FastMoE=0.177 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2711 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2711 layer 6: round 0 FastMoE=0.180 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2711 layer 6: round 1 FastMoE=0.175 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2711 layer 6: round 2 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2711 layer 6: round 3 FastMoE=0.174 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2721 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2721 layer 6: round 0 FastMoE=0.177 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2721 layer 6: round 1 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2721 layer 6: round 2 FastMoE=0.179 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2721 layer 6: round 3 FastMoE=0.180 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2731 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2731 layer 6: round 0 FastMoE=0.182 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2731 layer 6: round 1 FastMoE=0.182 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2731 layer 6: round 2 FastMoE=0.175 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2731 layer 6: round 3 FastMoE=0.178 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2741 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2741 layer 6: round 0 FastMoE=0.179 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2741 layer 6: round 1 FastMoE=0.180 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2741 layer 6: round 2 FastMoE=0.179 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2741 layer 6: round 3 FastMoE=0.178 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2751 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2751 layer 6: round 0 FastMoE=0.185 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2751 layer 6: round 1 FastMoE=0.178 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2751 layer 6: round 2 FastMoE=0.179 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2751 layer 6: round 3 FastMoE=0.178 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2761 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2761 layer 6: round 0 FastMoE=0.181 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2761 layer 6: round 1 FastMoE=0.179 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2761 layer 6: round 2 FastMoE=0.181 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2761 layer 6: round 3 FastMoE=0.177 SmartMoE=0.133
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2771 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2771 layer 6: round 0 FastMoE=0.178 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2771 layer 6: round 1 FastMoE=0.178 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2771 layer 6: round 2 FastMoE=0.181 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2771 layer 6: round 3 FastMoE=0.177 SmartMoE=0.133
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2781 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2781 layer 6: round 0 FastMoE=0.187 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2781 layer 6: round 1 FastMoE=0.178 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2781 layer 6: round 2 FastMoE=0.181 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2781 layer 6: round 3 FastMoE=0.179 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2791 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2791 layer 6: round 0 FastMoE=0.188 SmartMoE=0.136
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2791 layer 6: round 1 FastMoE=0.180 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2791 layer 6: round 2 FastMoE=0.181 SmartMoE=0.135
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2791 layer 6: round 3 FastMoE=0.180 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2801 6
[15, 21, 31, 30, 12, 10, 9, 8, 5, 24, 17, 29, 27, 18, 6, 0, 25, 2, 19, 20, 16, 23, 7, 11, 4, 13, 3, 28, 14, 26, 22, 1]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2801 layer 6: round 0 FastMoE=0.226 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2801 layer 6: round 1 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2801 layer 6: round 2 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2801 layer 6: round 3 FastMoE=0.179 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2811 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2811 layer 6: round 0 FastMoE=0.186 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2811 layer 6: round 1 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2811 layer 6: round 2 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2811 layer 6: round 3 FastMoE=0.174 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2821 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2821 layer 6: round 0 FastMoE=0.180 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2821 layer 6: round 1 FastMoE=0.178 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2821 layer 6: round 2 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2821 layer 6: round 3 FastMoE=0.179 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2831 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2831 layer 6: round 0 FastMoE=0.186 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2831 layer 6: round 1 FastMoE=0.175 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2831 layer 6: round 2 FastMoE=0.179 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2831 layer 6: round 3 FastMoE=0.176 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2841 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2841 layer 6: round 0 FastMoE=0.184 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2841 layer 6: round 1 FastMoE=0.178 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2841 layer 6: round 2 FastMoE=0.175 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2841 layer 6: round 3 FastMoE=0.176 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2851 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2851 layer 6: round 0 FastMoE=0.188 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2851 layer 6: round 1 FastMoE=0.179 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2851 layer 6: round 2 FastMoE=0.176 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2851 layer 6: round 3 FastMoE=0.176 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2861 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2861 layer 6: round 0 FastMoE=0.191 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2861 layer 6: round 1 FastMoE=0.180 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2861 layer 6: round 2 FastMoE=0.178 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2861 layer 6: round 3 FastMoE=0.176 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2871 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2871 layer 6: round 0 FastMoE=0.184 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2871 layer 6: round 1 FastMoE=0.175 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2871 layer 6: round 2 FastMoE=0.176 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2871 layer 6: round 3 FastMoE=0.177 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2881 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2881 layer 6: round 0 FastMoE=0.192 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2881 layer 6: round 1 FastMoE=0.179 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2881 layer 6: round 2 FastMoE=0.179 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2881 layer 6: round 3 FastMoE=0.177 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2891 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2891 layer 6: round 0 FastMoE=0.209 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2891 layer 6: round 1 FastMoE=0.178 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2891 layer 6: round 2 FastMoE=0.180 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2891 layer 6: round 3 FastMoE=0.177 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2901 6
[15, 21, 31, 30, 12, 10, 9, 8, 5, 24, 17, 29, 27, 18, 6, 0, 25, 2, 19, 20, 16, 23, 7, 11, 4, 13, 3, 28, 14, 26, 22, 1]
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2901 layer 6: round 0 FastMoE=0.212 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2901 layer 6: round 1 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2901 layer 6: round 2 FastMoE=0.176 SmartMoE=0.129
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2901 layer 6: round 3 FastMoE=0.176 SmartMoE=0.128
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2911 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2911 layer 6: round 0 FastMoE=0.194 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2911 layer 6: round 1 FastMoE=0.178 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2911 layer 6: round 2 FastMoE=0.179 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2911 layer 6: round 3 FastMoE=0.177 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2921 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2921 layer 6: round 0 FastMoE=0.177 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2921 layer 6: round 1 FastMoE=0.180 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2921 layer 6: round 2 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2921 layer 6: round 3 FastMoE=0.180 SmartMoE=0.131
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2931 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2931 layer 6: round 0 FastMoE=0.184 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2931 layer 6: round 1 FastMoE=0.176 SmartMoE=0.128
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2931 layer 6: round 2 FastMoE=0.179 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2931 layer 6: round 3 FastMoE=0.178 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2941 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2941 layer 6: round 0 FastMoE=0.186 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2941 layer 6: round 1 FastMoE=0.180 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2941 layer 6: round 2 FastMoE=0.177 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2941 layer 6: round 3 FastMoE=0.177 SmartMoE=0.130
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2951 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2951 layer 6: round 0 FastMoE=0.178 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2951 layer 6: round 1 FastMoE=0.177 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2951 layer 6: round 2 FastMoE=0.180 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2951 layer 6: round 3 FastMoE=0.175 SmartMoE=0.129
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2961 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2961 layer 6: round 0 FastMoE=0.185 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2961 layer 6: round 1 FastMoE=0.178 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2961 layer 6: round 2 FastMoE=0.180 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2961 layer 6: round 3 FastMoE=0.178 SmartMoE=0.132
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2971 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2971 layer 6: round 0 FastMoE=0.222 SmartMoE=0.133
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2971 layer 6: round 1 FastMoE=0.179 SmartMoE=0.137
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2971 layer 6: round 2 FastMoE=0.180 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2971 layer 6: round 3 FastMoE=0.181 SmartMoE=0.133
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2981 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2981 layer 6: round 0 FastMoE=0.186 SmartMoE=0.132
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2981 layer 6: round 1 FastMoE=0.178 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2981 layer 6: round 2 FastMoE=0.180 SmartMoE=0.134
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2981 layer 6: round 3 FastMoE=0.180 SmartMoE=0.134
test smart exchange /home/atc23_ae/SmartMoE-AE/moe_trace/table_iter2991 6
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2991 layer 6: round 0 FastMoE=0.189 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2991 layer 6: round 1 FastMoE=0.176 SmartMoE=0.130
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2991 layer 6: round 2 FastMoE=0.175 SmartMoE=0.131
test d_model 1536 seq_len 1024 batch_size 8 num_expert 4 world_size 8 iter 2991 layer 6: round 3 FastMoE=0.177 SmartMoE=0.132
/home/atc23_ae/SmartMoE-AE/plotting/from_exec/fig13
+ cd -
/home/atc23_ae/SmartMoE-AE
++ pwd
++ pwd
+ AEROOT=/home/atc23_ae/SmartMoE-AE
+ python3 ./plotting/from_exec/fig13.py /home/atc23_ae/SmartMoE-AE /home/atc23_ae/SmartMoE-AE/outputs_from_exec_2023-05-28T16:36:45+08:00
